[
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "",
    "text": "The essay is written at the level of two years of undergraduate mathematics. I will keep jargons to a minimum and use as few infinities as possible. For example, instead of particles that can be anywhere on a real-number line, I would talk about particles that can be in one of three boxes."
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#many-world-theory",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#many-world-theory",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Many-world theory",
    "text": "Many-world theory"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#pilot-wave-theory",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#pilot-wave-theory",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Pilot wave theory",
    "text": "Pilot wave theory"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#copenhagen-interpretation",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#copenhagen-interpretation",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Copenhagen interpretation",
    "text": "Copenhagen interpretation"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#relational-quantum-mechanics",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#relational-quantum-mechanics",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Relational quantum mechanics",
    "text": "Relational quantum mechanics"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#qbism",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#qbism",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "QBism",
    "text": "QBism"
  },
  {
    "objectID": "blog/posts/geometrical-mechanics/index.html",
    "href": "blog/posts/geometrical-mechanics/index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "Consider a particle in a field, in polar coordinates. We have\n\\[L = \\frac 12 m (\\dot r^2 + r^2 \\dot \\theta^2) - V(r, \\theta)\\]\nNow suppose we want to use a rotating frame at angular velocity \\(+\\Omega\\), then we can use the change of variables by \\(\\theta = \\phi + \\Omega t\\), plug into the Euler–Lagrange equations, and obtain\n\\[\\begin{cases}\nm\\ddot r = mr\\dot\\phi^2 -\\partial_r V(r, \\phi + \\Omega t)  + mr\\Omega^2 + 2m(r\\dot \\phi)\\Omega \\\\\nm(r\\ddot\\phi + 2\\dot r\\dot \\phi) = -\\frac 1r \\partial_\\theta(r, \\phi+ \\Omega t) - 2m\\dot r\\Omega\n\\end{cases}\\]\nIn the above procedure, we simply performed a change of variables, then plugged into the Euler–Lagrange equations without comment, but are we allowed to do that? Yes, but there are conditions – the change of variables must not depend on velocity.\n\n\nAt this point, it is important to be as explicit as possible, carefully distinguishing between often confused concepts:\n\nphysical state: An intuitive concept that cannot be made more precise than say \"this is what physicists study\", much like how a geometric point cannot be made more precise than say \"this is what geometers study\".\nsame: As in most modern mathematics, two things are \"the same\" when they are really just \"equivalent\" or \"not distinguished in use\". For example, there is really just one \\(\\R\\), but we can have as many 1-dimensional vector spaces as we want, and they are all equivalent to \\(\\R\\), though not literally the same as it (if they were, then we wouldn’t have as many vector spaces as we want!).\n(n-dimensional smooth) manifold \\(\\mathcal M\\): a space that is locally the same as \\(\\R^n\\). More precisely, at every point \\(x\\in \\mathcal M\\), there exists a coordinate system around \\(x\\).\ncoordinate system of a manifold \\(\\mathcal M\\): a diffeomorphism from an open subset of \\(\\mathcal M\\) to an open subset of \\(\\R^n\\).\ndiffeomorphism: a smooth, one-to-one function between two smooth spaces. (What is a smooth space? ... it’s a space smooth enough to do calculus in. Making it more precise would be too much of a detour.)\nstate space \\(\\mathcal S\\): the manifold of distinct physical states. Every point \\(x\\in \\mathcal S\\) is a particular state that the system can assume. The manifold is built such that close-by points on the manifold are close-by physical states. That is, the topology of the state space (a precisely defined mathematical concept) is an exact representation of the topology of physical states (an intuitive concept that cannot be made more precise than that).\ntangent space \\(\\mathcal T_x\\mathcal S\\) of a point \\(x\\in \\mathcal S\\): The vector space of all possible velocities at that state. It has dimension \\(n\\), and is thus isomorphic to \\(\\R^n\\) as a vector space. However, it is not literally the same as \\(\\R^n\\).\ncotangent space \\(\\mathcal T_x^\\ast\\mathcal S\\) of a point \\(x\\in \\mathcal S\\): The vector space of all possible momenta at that state. From the abstract perspective, a momentum is nothing more than a linear map of type \\(p: \\mathcal T_x\\mathcal S \\to \\R\\). That is, the only way to really \"use\" a momentum is to combine it with a velocity, mutually annihilating both of them and leaving behind nothing but a little real number representing the energy. It has dimension \\(n\\), and is thus isomorphic to \\(\\R^n\\) as a vector space. However, it is not literally the same as \\(\\R^n\\). Neither is it literally the same as \\(\\mathcal T_x\\mathcal S\\).\nconfiguration space \\(\\mathcal C = \\mathcal T\\mathcal S\\): The tangent bundle of state space. That is, at each \\(x\\in \\mathcal S\\), we \"glue\" the space of velocities \\(\\mathcal T_x \\mathcal S\\) to the point. The totality of \\(\\mathcal C\\) with all its \\(\\mathcal T_x \\mathcal S\\) is the configuration space.\nphase space \\(\\mathcal P = \\mathcal T^\\ast\\mathcal S\\): The cotangent bundle of state space. That is, at each point \\(x\\in \\mathcal S\\), we attach the space of momenta \\(\\mathcal T_x^\\ast\\mathcal S\\).\n\nDo not worry if the words do not make much sense. The example will make it clear.\nFor concreteness, consider a pendulum-cart system, shown in Figure 4. It is clear that its state space is shaped like a cylinder: one circle for the angle of the pendulum, and one line being the location of the cart.\nMore examples are shown in Table 2. Most of them are obvious, except the one about particle on a sphere.\nIt’s clear that its state space is \\(\\mathbb S^2\\). However, that is not equivalent to the torus \\(\\mathbb S^1 \\times \\mathbb S^1\\). There is simply no way to split the sphere into a direct product of two circles (as a casual comparison between a donut and a ball can verify).\nFurthermore, its configuration space \\(\\mathcal\\mathbb S^2\\) is not equivalent to \\(\\R^2 \\times \\mathbb S^2\\). To prove that, we invoke the hairy ball theorem: there is no smooth and everywhere nonzero vector field on \\(\\mathbb S^2\\). Now, if it were equivalent to \\(\\R^2 \\times \\mathbb S^2\\), then there is an obvious smooth and everywhere nonzero vector field: \\(x \\mapsto ((1, 0), x)\\).\n\n\n\nThe pendulum-cart system.\n\n\n\n\nSome physical systems and their state and configuration spaces. Note that the state spaces are not literally the same as the ones shown in the table, merely equivalent (by a diffeomorphism).\n\n\nphysical system\nstate space\nconfiguration space\n\n\n\n\nparticle in 3D space\n\\(\\R^3\\)\n\\(\\R^6\\)\n\n\npendulum\ncircle \\(\\mathbb S^1\\)\ncylinder \\(\\R^1 \\times \\mathbb S^1\\)\n\n\ndouble pendulum\ntorus \\(\\mathbb S^1 \\times \\mathbb S^1\\)\ncylinder-squared \\(\\R^2 \\times \\mathbb S^1 \\times \\mathbb S^1\\)\n\n\npendulum-cart\ncylinder \\(\\R^1 \\times \\mathbb S^1\\)\n\\(\\R^3 \\times \\mathbb S^1 \\times \\mathbb S^1\\)\n\n\nparticle on a sphere\nsphere \\(\\mathbb S^2\\)\ntangent bundle of sphere \\(\\mathcal T\\mathbb S^2\\)\n\n\n\n\n: Some physical systems and their state and configuration spaces. Note that the state spaces are not literally the same as the ones shown in the table, merely equivalent (by a diffeomorphism).\n\n\n\nWith the above formalism, we can precisely define more concepts\n\ntrajectory, or path, in a manifold \\(\\mathcal M\\): a function of type \\(\\gamma: [t_0, t] \\to \\mathcal M\\).\nLagrangian of a physical system with state manifold \\(\\mathcal S\\): a function of type\n\n\\[\\mathcal L: \\R \\times \\mathcal T\\mathcal S \\to \\R.\\]\nThat is, it is a time-varying field on configuration space.\n\nHamiltonian of a physical system with state manifold \\(\\mathcal S\\): a function of type\n\n\\[\\mathcal H: \\R \\times \\mathcal T^\\ast \\mathcal S \\to \\R.\\]\nThat is, it is a time-varying field on phase space.\n\naction of a path\n\n\\[S(\\gamma) = \\int_{t_0}^t \\mathcal L(\\tau, \\gamma(\\tau), \\dot \\gamma(\\tau))d\\tau.\\]\nNow, the convex duality between Lagrangian and Hamiltonian transfers with almost no change in notation:\n\\[\\begin{cases}\n\\mathcal L(t, q, v) = \\max_{p\\in \\mathcal T_q^\\ast \\mathcal S} (\\langle p, v\\rangle - \\mathcal H(t, q, p)) \\\\\n\\mathcal H(t, q, p) = \\max_{v\\in \\mathcal T_q \\mathcal S} (\\langle p, v\\rangle - \\mathcal L(t, q, v))\n\\end{cases}\n\\]\n\\[\n\\begin{cases}\np^\\ast(t, q, v) = \\mathop{\\mathrm{arg\\,max}}_{p\\in \\mathcal T_q^\\ast \\mathcal S} (\\langle p, v\\rangle - \\mathcal H(t, q, p)) \\\\\nv^\\ast(t, q, p) = \\mathop{\\mathrm{arg\\,max}}_{v\\in \\mathcal T_q \\mathcal S} (\\langle p, v\\rangle - \\mathcal L(t, q, v))\n\\end{cases}\n\\]\nThe economic argument almost goes through without problem, but we need to be careful with some notations. In particular, we take another look at gradients. What does it mean to write \\(\\nabla_v \\mathcal L(t, q, v)\\)? The defining property is\n\\[\\mathcal L(t, q, v + u\\tau) = \\mathcal L(t, q, v) + \\lra{\\nabla_v \\mathcal L(t, q, v), u} \\tau + O(\\tau^2)\\]\nwhich implies the following operational definition:\n\\[\\nabla_v \\mathcal L(t, q, v) := u \\mapsto \\lim_{\\tau \\to 0} \\frac 1\\tau (\\mathcal L(t, q, v + u\\tau) - \\mathcal L(t, q, v))\\]\nThis definition makes it clear that \\(\\nabla_v \\mathcal L(t, q, v)\\) is a function of type \\(\\mathcal T_q \\mathcal S \\to\\R\\), thus it is an element of \\(\\mathcal T_q^\\ast \\mathcal S\\). Similarly, \\(\\nabla_p \\mathcal H(t, q, p)\\) is an element of \\(\\mathcal T_q \\mathcal S\\). Succinctly, \\(\\nabla_q \\mathcal L, \\nabla_v \\mathcal L, \\nabla_q \\mathcal H\\) are covector fields (like momentum), and \\(\\nabla_p \\mathcal H\\) is a vector field (like velocity).\nWith these, the types of every equation come out correctly again:\n\\[\\begin{cases}\nv = \\nabla_p \\mathcal H(t, q, p^\\ast(t, q, v))\\\\\np = \\nabla_v \\mathcal L(t, q, v^\\ast(t, q, p))\n\\end{cases},\n\\frac{d}{dt}(\\nabla_v \\mathcal L) = \\nabla_q \\mathcal L,\\quad\n\\begin{cases}\n\\dot p = -\\nabla_q \\mathcal H \\\\\n\\dot q = -\\nabla_p \\mathcal H\n\\end{cases}\\]\nLet’s call these the coordinate-free equations, to be contrasted with the coordinate-based equations, to be defined below.\n\n\n\nManifolds are geometrically pristine, but you can’t calculate numerically with them unless you lay down coordinate systems over them. Concretely, consider a state space \\(\\mathcal S\\). We take an open subset \\(U\\) of it, and define a coordinate system (with a possible dependence on time):\n\\[(q_1, ..., q_N): \\R \\times U \\to \\R^N\\]\nThis coordinate system then induces a coordinate system over the configuration space:\n\\[(q_1, ..., q_N; v_1, ..., v_N): \\R \\times \\mathcal T U \\to \\R^N \\times \\R^N\\]\nNow consider a different coordinate system\n\\[(Q_1, ..., Q_N): \\R \\times U \\to \\R^N\\]\nand suppose they are related by a function \\(f: \\R \\times \\R^N \\to \\R^N\\) such that\n\\[q(t, x) = f(t, Q(t, x))\\]\nThis is then a point transformation of the coordinate system.\nThe point transformation induces a transformation of the velocities, too. To find the transformation of velocities, consider a path \\(\\gamma: \\R \\to \\mathcal S\\). Its velocity at time \\(t\\) is \\(\\dot \\gamma(t) \\in \\mathcal T_{\\gamma(t)}\\mathcal S\\), a vector that looks like it literally lives in \\(\\R^N\\), but is not. It is not a native of \\(\\R^N\\), but thanks to the \\(q\\)-coordinate system, it is represented by the \\(\\R^N\\)-vector\n\\[\\frac{d}{dt} q(t, \\gamma(t)) \\in \\R^N\\]\nNow plug in \\(q(t, x) = f(t, Q(t, x))\\), to find a relationship between the representation of \\(\\dot \\gamma(t)\\) in \\(q\\)-coordinate system and \\(Q\\)-coordinate system:\n\\[\\frac{d}{dt} q(t, \\gamma(t)) = \\frac{d}{dt} f(t, Q(t, \\gamma(t))) = \\partial_t f(t, Q(t, \\gamma(t))) + \\frac{\\partial f}{\\partial Q} \\frac{d}{dt}Q(t, \\gamma(t))\\]\nMore succinctly, we have the following transformation from \\((t, Q, V)\\) to \\((t, q, v)\\):\n\\[\\begin{cases}\nq = f(t, Q) \\\\\nv= \\partial_t f(t, Q) + \\frac{\\partial f}{\\partial Q}(t, Q) V\n\\end{cases}\\]\nA note on matrix algebra: Conventionally, vectors are written as column-matrices, that is, \\(q, v, Q, V\\) are written as \\(N\\times 1\\) matrices. Correspondingly, gradients, being covectors, are written as row-matrices, that is, \\(\\nabla_q l, \\nabla_v l, \\nabla_Q L, \\nabla_V L\\), are written as \\(1 \\times N\\) matrices. Finally, gradients of vector-valued functions, like \\(\\frac{\\partial f}{\\partial Q}\\), are \\(N\\times N\\) matrices, with each row being a gradient of one vector coordinate. This convention makes everything come out cleanly, with no need to take the transpose of anything.\nThe point transformation also induces a transformation of the Lagrangians. While the Lagrangian itself is a function \\(\\mathcal L\\) of type \\(\\R \\times \\mathcal T \\mathcal S \\to \\R\\), the Lagrangians \\(L(t, Q, V), l(t, q, v)\\) are functions of type \\(\\R \\times \\R^N \\times \\R^N \\to \\R\\). Both \\(L, l\\) are induced from \\(\\mathcal L\\) by the choice of coordinates. We have\n\\[\\mathcal(t, x, u) = L(t, Q(t, x), V(t, x, u)) = l(t, q(t, x), v(t, x, u))\\]\nand plug in \\(q(t, x) = f(t, Q(t, x)), v = \\partial_t f(t, Q) + \\frac{\\partial f}{\\partial Q}(t, Q) V\\), we have\n\\[l\\left(t, f(t, Q), \\partial_t f(t, Q)  + \\frac{\\partial f}{\\partial Q}(t, Q) V\\right) = L(t, Q, V)\\]\nBrute computation shows that\n\\[\\frac{d}{dt}(\\nabla_V L) - \\nabla_Q L = \\left(\\frac{d}{dt}(\\nabla_v l) - \\nabla_q l\\right)\\frac{\\partial f}{\\partial Q}\\]\nimplying that the coordinate-based Euler–Lagrange equation is true in \\((Q, V)\\) coordinates iff it is true in \\((q, v)\\) coordinates.\nWhat, in the final analysis, is a point transformation? It is nothing more than changing a time-varying coordinate system on the state space. Since our derivation of the coordinate-based Euler–Lagrange equations required no special property of the coordinate system, it must be preserved by point transformations. All the above verification was really nothing but \"ceremonial\".\nIn more detail: we know that the coordinate-free EL equations are true, which implies that the \\(q\\)-coordinate-based EL equations and the \\(Q\\)-coordinate-based EL equations are both true (since they are merely two coordinate-based representations on the coordinate-free equation). No \\(Q\\)-to-\\(q\\) translation is necessary!\nWhat, then, is the phrase \"point transformation\" supposed to be contrasted with? It is contrasted with more general coordinate transforms that also depend on velocities, as \\(q = f(t, Q, V)\\). From the perspective given here, the contrast is really between \"state space coordinate systems\" and \"configuration space coordinate systems\". Whereas state space coordinate system is first defined as some\n\\[(q_1, ..., q_N): \\R \\times U \\to \\R^N\\]\nand that is then extended to \\((q_1, ..., q_N; v_1, ..., v_N): \\R \\times \\mathcal T U \\to \\R^N \\times \\R^N\\), a configuration space coordinate system defines \"all at once\" a complete coordinate system\n\\[(q_1, ..., q_N; v_1, ..., v_N): \\R \\times \\mathcal T U \\to \\R^N \\times \\R^N\\]\nIt is no wonder that such overly general coordinate systems do not have nice properties, and do not satisfy the coordinate-based Euler–Lagrange equations.\n\n\n\nBefore writing this chapter, try going through (Bohn 2018, chap. 7).\nWhereas for the Lagrangian \\(\\mathcal L\\), we can only perform point-transformations \\(q = f(t, Q)\\), lest the Euler–Lagrange equation is mangled, for the Hamiltonian, we can simultaneously transform both \\(q, p\\), while preserving the Hamiltonian equations of motion. Such transformations are called canonical transformations. They are of the form:\n\\[\\begin{cases}\nQ = f_Q(t, p, q)\\\\\nP = f_P(t, p, q)\n\\end{cases}\\]\nwhere the functions \\(f_Q, f_P: \\R \\times \\R^N \\times \\R^N \\to \\R^N\\) are required to satisfy some functional equations.\nThis is usually derived by brute force without comments. However, to truly understand the meaning, we need to understand phase space from a perspective even more modern than \\(\\mathcal T^\\ast \\mathcal S\\)."
  },
  {
    "objectID": "blog/posts/geometrical-mechanics/index.html#the-geometry-of-physical-states",
    "href": "blog/posts/geometrical-mechanics/index.html#the-geometry-of-physical-states",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "Consider a particle in a field, in polar coordinates. We have\n\\[L = \\frac 12 m (\\dot r^2 + r^2 \\dot \\theta^2) - V(r, \\theta)\\]\nNow suppose we want to use a rotating frame at angular velocity \\(+\\Omega\\), then we can use the change of variables by \\(\\theta = \\phi + \\Omega t\\), plug into the Euler–Lagrange equations, and obtain\n\\[\\begin{cases}\nm\\ddot r = mr\\dot\\phi^2 -\\partial_r V(r, \\phi + \\Omega t)  + mr\\Omega^2 + 2m(r\\dot \\phi)\\Omega \\\\\nm(r\\ddot\\phi + 2\\dot r\\dot \\phi) = -\\frac 1r \\partial_\\theta(r, \\phi+ \\Omega t) - 2m\\dot r\\Omega\n\\end{cases}\\]\nIn the above procedure, we simply performed a change of variables, then plugged into the Euler–Lagrange equations without comment, but are we allowed to do that? Yes, but there are conditions – the change of variables must not depend on velocity.\n\n\nAt this point, it is important to be as explicit as possible, carefully distinguishing between often confused concepts:\n\nphysical state: An intuitive concept that cannot be made more precise than say \"this is what physicists study\", much like how a geometric point cannot be made more precise than say \"this is what geometers study\".\nsame: As in most modern mathematics, two things are \"the same\" when they are really just \"equivalent\" or \"not distinguished in use\". For example, there is really just one \\(\\R\\), but we can have as many 1-dimensional vector spaces as we want, and they are all equivalent to \\(\\R\\), though not literally the same as it (if they were, then we wouldn’t have as many vector spaces as we want!).\n(n-dimensional smooth) manifold \\(\\mathcal M\\): a space that is locally the same as \\(\\R^n\\). More precisely, at every point \\(x\\in \\mathcal M\\), there exists a coordinate system around \\(x\\).\ncoordinate system of a manifold \\(\\mathcal M\\): a diffeomorphism from an open subset of \\(\\mathcal M\\) to an open subset of \\(\\R^n\\).\ndiffeomorphism: a smooth, one-to-one function between two smooth spaces. (What is a smooth space? ... it’s a space smooth enough to do calculus in. Making it more precise would be too much of a detour.)\nstate space \\(\\mathcal S\\): the manifold of distinct physical states. Every point \\(x\\in \\mathcal S\\) is a particular state that the system can assume. The manifold is built such that close-by points on the manifold are close-by physical states. That is, the topology of the state space (a precisely defined mathematical concept) is an exact representation of the topology of physical states (an intuitive concept that cannot be made more precise than that).\ntangent space \\(\\mathcal T_x\\mathcal S\\) of a point \\(x\\in \\mathcal S\\): The vector space of all possible velocities at that state. It has dimension \\(n\\), and is thus isomorphic to \\(\\R^n\\) as a vector space. However, it is not literally the same as \\(\\R^n\\).\ncotangent space \\(\\mathcal T_x^\\ast\\mathcal S\\) of a point \\(x\\in \\mathcal S\\): The vector space of all possible momenta at that state. From the abstract perspective, a momentum is nothing more than a linear map of type \\(p: \\mathcal T_x\\mathcal S \\to \\R\\). That is, the only way to really \"use\" a momentum is to combine it with a velocity, mutually annihilating both of them and leaving behind nothing but a little real number representing the energy. It has dimension \\(n\\), and is thus isomorphic to \\(\\R^n\\) as a vector space. However, it is not literally the same as \\(\\R^n\\). Neither is it literally the same as \\(\\mathcal T_x\\mathcal S\\).\nconfiguration space \\(\\mathcal C = \\mathcal T\\mathcal S\\): The tangent bundle of state space. That is, at each \\(x\\in \\mathcal S\\), we \"glue\" the space of velocities \\(\\mathcal T_x \\mathcal S\\) to the point. The totality of \\(\\mathcal C\\) with all its \\(\\mathcal T_x \\mathcal S\\) is the configuration space.\nphase space \\(\\mathcal P = \\mathcal T^\\ast\\mathcal S\\): The cotangent bundle of state space. That is, at each point \\(x\\in \\mathcal S\\), we attach the space of momenta \\(\\mathcal T_x^\\ast\\mathcal S\\).\n\nDo not worry if the words do not make much sense. The example will make it clear.\nFor concreteness, consider a pendulum-cart system, shown in Figure 4. It is clear that its state space is shaped like a cylinder: one circle for the angle of the pendulum, and one line being the location of the cart.\nMore examples are shown in Table 2. Most of them are obvious, except the one about particle on a sphere.\nIt’s clear that its state space is \\(\\mathbb S^2\\). However, that is not equivalent to the torus \\(\\mathbb S^1 \\times \\mathbb S^1\\). There is simply no way to split the sphere into a direct product of two circles (as a casual comparison between a donut and a ball can verify).\nFurthermore, its configuration space \\(\\mathcal\\mathbb S^2\\) is not equivalent to \\(\\R^2 \\times \\mathbb S^2\\). To prove that, we invoke the hairy ball theorem: there is no smooth and everywhere nonzero vector field on \\(\\mathbb S^2\\). Now, if it were equivalent to \\(\\R^2 \\times \\mathbb S^2\\), then there is an obvious smooth and everywhere nonzero vector field: \\(x \\mapsto ((1, 0), x)\\).\n\n\n\nThe pendulum-cart system.\n\n\n\n\nSome physical systems and their state and configuration spaces. Note that the state spaces are not literally the same as the ones shown in the table, merely equivalent (by a diffeomorphism).\n\n\nphysical system\nstate space\nconfiguration space\n\n\n\n\nparticle in 3D space\n\\(\\R^3\\)\n\\(\\R^6\\)\n\n\npendulum\ncircle \\(\\mathbb S^1\\)\ncylinder \\(\\R^1 \\times \\mathbb S^1\\)\n\n\ndouble pendulum\ntorus \\(\\mathbb S^1 \\times \\mathbb S^1\\)\ncylinder-squared \\(\\R^2 \\times \\mathbb S^1 \\times \\mathbb S^1\\)\n\n\npendulum-cart\ncylinder \\(\\R^1 \\times \\mathbb S^1\\)\n\\(\\R^3 \\times \\mathbb S^1 \\times \\mathbb S^1\\)\n\n\nparticle on a sphere\nsphere \\(\\mathbb S^2\\)\ntangent bundle of sphere \\(\\mathcal T\\mathbb S^2\\)\n\n\n\n\n: Some physical systems and their state and configuration spaces. Note that the state spaces are not literally the same as the ones shown in the table, merely equivalent (by a diffeomorphism).\n\n\n\nWith the above formalism, we can precisely define more concepts\n\ntrajectory, or path, in a manifold \\(\\mathcal M\\): a function of type \\(\\gamma: [t_0, t] \\to \\mathcal M\\).\nLagrangian of a physical system with state manifold \\(\\mathcal S\\): a function of type\n\n\\[\\mathcal L: \\R \\times \\mathcal T\\mathcal S \\to \\R.\\]\nThat is, it is a time-varying field on configuration space.\n\nHamiltonian of a physical system with state manifold \\(\\mathcal S\\): a function of type\n\n\\[\\mathcal H: \\R \\times \\mathcal T^\\ast \\mathcal S \\to \\R.\\]\nThat is, it is a time-varying field on phase space.\n\naction of a path\n\n\\[S(\\gamma) = \\int_{t_0}^t \\mathcal L(\\tau, \\gamma(\\tau), \\dot \\gamma(\\tau))d\\tau.\\]\nNow, the convex duality between Lagrangian and Hamiltonian transfers with almost no change in notation:\n\\[\\begin{cases}\n\\mathcal L(t, q, v) = \\max_{p\\in \\mathcal T_q^\\ast \\mathcal S} (\\langle p, v\\rangle - \\mathcal H(t, q, p)) \\\\\n\\mathcal H(t, q, p) = \\max_{v\\in \\mathcal T_q \\mathcal S} (\\langle p, v\\rangle - \\mathcal L(t, q, v))\n\\end{cases}\n\\]\n\\[\n\\begin{cases}\np^\\ast(t, q, v) = \\mathop{\\mathrm{arg\\,max}}_{p\\in \\mathcal T_q^\\ast \\mathcal S} (\\langle p, v\\rangle - \\mathcal H(t, q, p)) \\\\\nv^\\ast(t, q, p) = \\mathop{\\mathrm{arg\\,max}}_{v\\in \\mathcal T_q \\mathcal S} (\\langle p, v\\rangle - \\mathcal L(t, q, v))\n\\end{cases}\n\\]\nThe economic argument almost goes through without problem, but we need to be careful with some notations. In particular, we take another look at gradients. What does it mean to write \\(\\nabla_v \\mathcal L(t, q, v)\\)? The defining property is\n\\[\\mathcal L(t, q, v + u\\tau) = \\mathcal L(t, q, v) + \\lra{\\nabla_v \\mathcal L(t, q, v), u} \\tau + O(\\tau^2)\\]\nwhich implies the following operational definition:\n\\[\\nabla_v \\mathcal L(t, q, v) := u \\mapsto \\lim_{\\tau \\to 0} \\frac 1\\tau (\\mathcal L(t, q, v + u\\tau) - \\mathcal L(t, q, v))\\]\nThis definition makes it clear that \\(\\nabla_v \\mathcal L(t, q, v)\\) is a function of type \\(\\mathcal T_q \\mathcal S \\to\\R\\), thus it is an element of \\(\\mathcal T_q^\\ast \\mathcal S\\). Similarly, \\(\\nabla_p \\mathcal H(t, q, p)\\) is an element of \\(\\mathcal T_q \\mathcal S\\). Succinctly, \\(\\nabla_q \\mathcal L, \\nabla_v \\mathcal L, \\nabla_q \\mathcal H\\) are covector fields (like momentum), and \\(\\nabla_p \\mathcal H\\) is a vector field (like velocity).\nWith these, the types of every equation come out correctly again:\n\\[\\begin{cases}\nv = \\nabla_p \\mathcal H(t, q, p^\\ast(t, q, v))\\\\\np = \\nabla_v \\mathcal L(t, q, v^\\ast(t, q, p))\n\\end{cases},\n\\frac{d}{dt}(\\nabla_v \\mathcal L) = \\nabla_q \\mathcal L,\\quad\n\\begin{cases}\n\\dot p = -\\nabla_q \\mathcal H \\\\\n\\dot q = -\\nabla_p \\mathcal H\n\\end{cases}\\]\nLet’s call these the coordinate-free equations, to be contrasted with the coordinate-based equations, to be defined below.\n\n\n\nManifolds are geometrically pristine, but you can’t calculate numerically with them unless you lay down coordinate systems over them. Concretely, consider a state space \\(\\mathcal S\\). We take an open subset \\(U\\) of it, and define a coordinate system (with a possible dependence on time):\n\\[(q_1, ..., q_N): \\R \\times U \\to \\R^N\\]\nThis coordinate system then induces a coordinate system over the configuration space:\n\\[(q_1, ..., q_N; v_1, ..., v_N): \\R \\times \\mathcal T U \\to \\R^N \\times \\R^N\\]\nNow consider a different coordinate system\n\\[(Q_1, ..., Q_N): \\R \\times U \\to \\R^N\\]\nand suppose they are related by a function \\(f: \\R \\times \\R^N \\to \\R^N\\) such that\n\\[q(t, x) = f(t, Q(t, x))\\]\nThis is then a point transformation of the coordinate system.\nThe point transformation induces a transformation of the velocities, too. To find the transformation of velocities, consider a path \\(\\gamma: \\R \\to \\mathcal S\\). Its velocity at time \\(t\\) is \\(\\dot \\gamma(t) \\in \\mathcal T_{\\gamma(t)}\\mathcal S\\), a vector that looks like it literally lives in \\(\\R^N\\), but is not. It is not a native of \\(\\R^N\\), but thanks to the \\(q\\)-coordinate system, it is represented by the \\(\\R^N\\)-vector\n\\[\\frac{d}{dt} q(t, \\gamma(t)) \\in \\R^N\\]\nNow plug in \\(q(t, x) = f(t, Q(t, x))\\), to find a relationship between the representation of \\(\\dot \\gamma(t)\\) in \\(q\\)-coordinate system and \\(Q\\)-coordinate system:\n\\[\\frac{d}{dt} q(t, \\gamma(t)) = \\frac{d}{dt} f(t, Q(t, \\gamma(t))) = \\partial_t f(t, Q(t, \\gamma(t))) + \\frac{\\partial f}{\\partial Q} \\frac{d}{dt}Q(t, \\gamma(t))\\]\nMore succinctly, we have the following transformation from \\((t, Q, V)\\) to \\((t, q, v)\\):\n\\[\\begin{cases}\nq = f(t, Q) \\\\\nv= \\partial_t f(t, Q) + \\frac{\\partial f}{\\partial Q}(t, Q) V\n\\end{cases}\\]\nA note on matrix algebra: Conventionally, vectors are written as column-matrices, that is, \\(q, v, Q, V\\) are written as \\(N\\times 1\\) matrices. Correspondingly, gradients, being covectors, are written as row-matrices, that is, \\(\\nabla_q l, \\nabla_v l, \\nabla_Q L, \\nabla_V L\\), are written as \\(1 \\times N\\) matrices. Finally, gradients of vector-valued functions, like \\(\\frac{\\partial f}{\\partial Q}\\), are \\(N\\times N\\) matrices, with each row being a gradient of one vector coordinate. This convention makes everything come out cleanly, with no need to take the transpose of anything.\nThe point transformation also induces a transformation of the Lagrangians. While the Lagrangian itself is a function \\(\\mathcal L\\) of type \\(\\R \\times \\mathcal T \\mathcal S \\to \\R\\), the Lagrangians \\(L(t, Q, V), l(t, q, v)\\) are functions of type \\(\\R \\times \\R^N \\times \\R^N \\to \\R\\). Both \\(L, l\\) are induced from \\(\\mathcal L\\) by the choice of coordinates. We have\n\\[\\mathcal(t, x, u) = L(t, Q(t, x), V(t, x, u)) = l(t, q(t, x), v(t, x, u))\\]\nand plug in \\(q(t, x) = f(t, Q(t, x)), v = \\partial_t f(t, Q) + \\frac{\\partial f}{\\partial Q}(t, Q) V\\), we have\n\\[l\\left(t, f(t, Q), \\partial_t f(t, Q)  + \\frac{\\partial f}{\\partial Q}(t, Q) V\\right) = L(t, Q, V)\\]\nBrute computation shows that\n\\[\\frac{d}{dt}(\\nabla_V L) - \\nabla_Q L = \\left(\\frac{d}{dt}(\\nabla_v l) - \\nabla_q l\\right)\\frac{\\partial f}{\\partial Q}\\]\nimplying that the coordinate-based Euler–Lagrange equation is true in \\((Q, V)\\) coordinates iff it is true in \\((q, v)\\) coordinates.\nWhat, in the final analysis, is a point transformation? It is nothing more than changing a time-varying coordinate system on the state space. Since our derivation of the coordinate-based Euler–Lagrange equations required no special property of the coordinate system, it must be preserved by point transformations. All the above verification was really nothing but \"ceremonial\".\nIn more detail: we know that the coordinate-free EL equations are true, which implies that the \\(q\\)-coordinate-based EL equations and the \\(Q\\)-coordinate-based EL equations are both true (since they are merely two coordinate-based representations on the coordinate-free equation). No \\(Q\\)-to-\\(q\\) translation is necessary!\nWhat, then, is the phrase \"point transformation\" supposed to be contrasted with? It is contrasted with more general coordinate transforms that also depend on velocities, as \\(q = f(t, Q, V)\\). From the perspective given here, the contrast is really between \"state space coordinate systems\" and \"configuration space coordinate systems\". Whereas state space coordinate system is first defined as some\n\\[(q_1, ..., q_N): \\R \\times U \\to \\R^N\\]\nand that is then extended to \\((q_1, ..., q_N; v_1, ..., v_N): \\R \\times \\mathcal T U \\to \\R^N \\times \\R^N\\), a configuration space coordinate system defines \"all at once\" a complete coordinate system\n\\[(q_1, ..., q_N; v_1, ..., v_N): \\R \\times \\mathcal T U \\to \\R^N \\times \\R^N\\]\nIt is no wonder that such overly general coordinate systems do not have nice properties, and do not satisfy the coordinate-based Euler–Lagrange equations.\n\n\n\nBefore writing this chapter, try going through (Bohn 2018, chap. 7).\nWhereas for the Lagrangian \\(\\mathcal L\\), we can only perform point-transformations \\(q = f(t, Q)\\), lest the Euler–Lagrange equation is mangled, for the Hamiltonian, we can simultaneously transform both \\(q, p\\), while preserving the Hamiltonian equations of motion. Such transformations are called canonical transformations. They are of the form:\n\\[\\begin{cases}\nQ = f_Q(t, p, q)\\\\\nP = f_P(t, p, q)\n\\end{cases}\\]\nwhere the functions \\(f_Q, f_P: \\R \\times \\R^N \\times \\R^N \\to \\R^N\\) are required to satisfy some functional equations.\nThis is usually derived by brute force without comments. However, to truly understand the meaning, we need to understand phase space from a perspective even more modern than \\(\\mathcal T^\\ast \\mathcal S\\)."
  },
  {
    "objectID": "blog/posts/geometrical-mechanics/index.html#the-geometry-of-phase-space",
    "href": "blog/posts/geometrical-mechanics/index.html#the-geometry-of-phase-space",
    "title": "Yuxi on the Wired",
    "section": "The geometry of phase space",
    "text": "The geometry of phase space\nGiven a state space \\(\\mathcal S\\), both its configuration space and its phase space are obtained by attaching vector spaces to every point of it. Despite this, the geometry of phase space turns out to be a far richer thing than the geometry of configuration space. This fundamentally comes down to the difference between a cotangent vector and a tangent vector.\nConsider an infinitesimal parallelogram in the phase space, around the point \\((q, p)\\). The parallelogram has (signed) sides \\((\\delta q_1, ..., \\delta q_N; \\delta p_1, ..., \\delta p_N)\\). What should be its (signed) volume? The natural answer is of course\n\\[\\prod_i \\delta q_i \\delta p_i\\]\nbut is this a meaningful answer? That is, is this a mirage in \\(\\R^N \\times \\R^N\\) created by our choice of coordinates, or is this a faithful representation of something that truly takes place in the phase space \\(\\mathcal T^\\ast\\mathcal S\\) itself?\nThis answer is critically important, since if a concept takes place in the phase space itself, then it will be coordinate-free, and every coordinate system automatically translates that one coordinate-free concept. This is how we could have predicted that the coordinate-based Euler–Lagrange equations are preserved, by going up to the coordinate-free version of it, then coming back down again.\nHaving a coordinate-free thing is like having a lingua-franca between different coordinate-based representations.\n\nPoisson bracket\nThe Poisson bracket notation is convenient:\n\\[\\{f, g\\} = \\sum_{i=1}^{N} \\left( \\frac{\\partial f}{\\partial q_{i}} \\frac{\\partial g}{\\partial p_{i}} - \\frac{\\partial f}{\\partial p_i} \\frac{\\partial g}{\\partial q_i}\\right)\\]\nFor any differentiable function \\(f(t, q, p)\\), and any path \\(p(t), q(t)\\) that conforms to a Hamiltonian \\(H(t, q, p)\\), we have by Hamiltonian’s equations of motion\n\\[\\frac{d}{dt} f(t, p(t), q(t)) = \\partial_t f(t, p(t), q(t)) + \\{f, H\\}\\]\nor more succinctly, \\(\\dot f = \\partial_t f + \\{f, H\\}\\).\n\n\nLiouville’s theorem\nProof taken from (Tolman 1980, sec. 19).\n\n\nThe interpretation of phase space geometry\nLiouville’s theorem is a delicate construction, having several moving parts. We have a phase space, a volume-measurement on the phase space, a Hamiltonian on the phase space, and a density field on the phase space which flows according to the Hamiltonian. Only when all four moving parts come together do we get Liouville’s theorem.\nWhat is a phase space, in the final analysis? A phase space \\(\\mathcal T^\\ast \\mathcal S\\) is a state space \\(\\mathcal S\\), with each state \\(x\\) attached with \\(\\mathcal T^\\ast_x \\mathcal S\\), the vector space of all possible momenta at that state. Good... but not quite! This interpretation of phase space is still bound firmly to the economic interpretation, where each momentum \\(p\\) at state \\(x\\) is a vector of prices, with which we are allowed to measure the profit flow, as \\(\\langle p, \\dot x \\rangle\\).\nWhile this perspective is how Hamiltonian mechanics got its start, the modern abstract viewpoint of Hamiltonian mechanics has sailed far away from the safe harbor of \\(\\R^{2N}\\), past \\(\\mathcal T^\\ast \\mathcal S\\), and voyaged deep into the strange seas of symplectic geometry. Since the early days of the 20th century, there is a tacit understanding among physicists where the humble origin of the phase space \\(\\mathcal T^\\ast \\mathcal S\\) is suppressed, and it is presented instead as a \\(2N\\)-dimensional smooth manifold \\(\\mathcal P\\) equipped with some \\(\\omega\\), a way to measure volumes. The seams where \\(\\mathcal T^\\ast_x \\mathcal S\\) was attached to \\(\\mathcal S\\) are now plastered over, never there, never will be mentioned again... And this abstract viewpoint actually works.\nSpeak not how the phase space was born, but what you can use it for! This is a principle useful not only in programming (encapsulation, API, abstract interfacing), but also in modern mathematics (speak not of equality, but equivalences and isomorphisms...), and perhaps in society (Ye shall know them by their fruits. Do men gather grapes of thorns, or figs of thistles?).\nWhat do we gain and what do we lose when we go from \\(\\R^{2N}\\) to \\(\\mathcal T^\\ast \\mathcal S\\) to \\((\\mathcal P, \\omega)\\)? What we gain are new interpretations, and what we lose are old interpretations. See Table 3.\n\n::: {#table:three_abstractions} \\(\\R^{2N}\\) \\(\\mathcal T^\\ast \\mathcal S\\) \\((\\mathcal P, \\omega)\\) ———————– ——————————– —————————- tuples of real number points, vectors, and covectors points, areas, and volumes multivariate calculus vector bundle geometry symplectic geometry\n\nThe three steps of abstraction. :::\n\n\nIn \\(\\R^{2N}\\), we can interpret each point \\((q, p)\\in \\R^{2N}\\) economically: \\(q_1, ..., q_N\\) are the amounts of commodities, and \\(p_1, ..., p_N\\) are their market prices. In \\(\\mathcal T^\\ast \\mathcal S\\), half of this interpretation is lost, since we are not allowed to interpret \\(x\\in \\mathcal S\\) as a tuple of commodities, unless we lay down a more or less arbitrary coordinate system over it.\nNevertheless, half of this interpretation is preserved. While we are no longer able to interpret a point \\(x\\in \\mathcal S\\) as a stock of commodities that we own, we are still able to interpret a vector \\(v \\in \\mathcal T_x \\mathcal S\\) as a flow of commodities. This then allows us to interpret \\(\\langle p, v \\rangle\\) as a flow of profits: if we are producing at speed \\(v\\), and the market price vector is \\(p\\), then our profit flow is \\(\\langle p, v \\rangle\\). In economic language, we can’t talk of the stock, but we can still talk of the flow.\nGiving up half of the economic interpretation allows us to gain in coordinate-freedom. The Hamiltonian equations of motion become coordinate-free equations on \\(\\mathcal T^\\ast \\mathcal S\\), and we are given the guarantee that it is preserved by any coordinate system on \\(\\mathcal S\\).\nWhen we get to \\((\\mathcal P, \\omega)\\), the economic interpretation is totally destroyed, because there is no more separation between commodities and prices. A point in \\(\\mathcal P\\) simply is a point \\(y\\in \\mathcal P\\), not a 2-tuple \\((x, p)\\) with \\(x\\in \\mathcal S\\) and \\(p \\in \\mathcal T^\\ast_x\\mathcal S\\). There is no way to seize the \"second half\" of \\(y\\) and interpret it as a price vector.\nFurthermore, we cannot even interpret it as a physical state with a momentum covector, either. A momentum covector still looks like an arrow. It has a direction, a length, and can be scaled linearly, and added. Out there in \\(\\mathcal P\\), every point is just a point, not \"half base point, half vector\" like for \\(\\mathcal T^\\ast \\mathcal S\\).\nGiving this much up allows us to gain in even more coordinate-freedom. We are allowed to interpret a physical system not as a base state \\(x\\in \\mathcal S\\) plus a momentum state \\(p \\in \\mathcal T^\\ast_x\\mathcal S\\) , but simply as a phase state \\(y\\in \\mathcal P\\). This in particular gives us the freedom to consider coordinate systems on \\(\\mathcal P\\) that are \"fully nonlinear\", which is what canonical transforms are all about.\nRecall how we defined point transforms in Lagrangian mechanics. We start with a coordinate system \\((q_1, ..., q_N)\\) on an open subset \\(U\\) of the state space \\(\\mathcal S\\), then induced a coordinate system \\((q_1, ..., q_N; v_1, ..., v_N)\\) on \\(\\mathcal T U\\). We also stated that, while we could have went directly for a coordinate system on \\(\\mathcal T\\), this would break the Euler–Lagrange equation.\nIt turns out that the Hamiltonian equations of motion are sturdier than the Euler–Lagrange equation: there are large families of coordinate systems \\((q_1, ..., q_N; p_1, ..., p_N)\\) that we can directly define on open subsets of \\(\\mathcal T^\\ast \\mathcal S\\), and the resulting coordinate-based Hamiltonian equations would still be \\(\\dot p = -\\nabla_q H, \\dot q = \\nabla_p H\\), even if \\((q_1, ..., q_N; p_1, ..., p_N)\\) is not induced by any coordinate system on the state space!\nTo fully exploit the freedom, of course, means that we must break down the strict segregation between a state-point and a momentum-vector. In particular, this means that we no longer require \\(\\mathcal T_x \\mathcal S\\) to be treated with the rigid dignity of a linear space, but the rough freedom of a manifold space:\n\\[p(x, ku) \\neq k p(x, u) \\text { in general, for } (x, u)\\in \\mathcal T^\\ast \\mathcal S, \\: k\\in \\R\\]\nGiven that, we can immediately see why canonical transforms are in general of the form\n\\[Q(t, x, u) = f_Q(t, q(t, x, u), p(t, x, u)),\\quad P(t, x, u) = f_P(t, q(t, x, u), p(t, x, u))\\]\nor more succinctly,\n\\[Q = f_Q(t, q, p),\\quad P = f_P(t, q, p)\\]\nThey have to nonlinearly \"mix up\" state and momentum, because that’s the only way to truly exploit all the freedoms that the sturdy Hamiltonian’s equations of motion grants us.1\n1 Mathematicians exploit every freedom that they are given... sounds evil, but it works in math.Of course, the Hamiltonian equations are not that tough. Some restraint is needed. Not everything goes. What is the restraint? The volume must be preserved! That is precisely what \\(\\omega\\) is there for: it measures areas. A coordinate system on the phase space is only given the title of \"canonical\" iff the coordinate system represents \\(\\omega\\) correctly.\nThus, we find that by exploiting exactly as much freedom as Hamiltonian mechanics gives us, while keeping track of the boundaries so that we are not giving ourselves too much freedom and shooting ourselves in the foot, we walked inexorably into treating the phase space as \\((\\mathcal P, \\omega)\\) – as an object of symplectic geometry."
  },
  {
    "objectID": "blog/posts/geometrical-mechanics/index.html#all-the-variational-principles-of-physics-that-youre-likely-to-ever-see",
    "href": "blog/posts/geometrical-mechanics/index.html#all-the-variational-principles-of-physics-that-youre-likely-to-ever-see",
    "title": "Yuxi on the Wired",
    "section": "All the variational principles of physics that you’re likely to ever see",
    "text": "All the variational principles of physics that you’re likely to ever see\nBased on (Lanczos 1970).\nOthers not covered: D’lambert’s principle. Gauss’s principle of least constraint, Hertz principle of least curvature, etc.\nLet’s be clear here.\n\na variation of a function \\(\\gamma: \\R^n \\to \\R^m\\) is a function \\(\\gamma + \\delta \\eta\\), such that \\(\\eta: \\R^n \\to \\R^m\\), and \\(\\delta\\) is an infinitesimal.\na constrained variation of a function \\(\\gamma\\) is a variation \\(\\gamma + \\delta \\eta\\), such that \\(\\eta\\) satisfies certain constraints \\(c\\).\na functional is a function that maps a function to a real number. For example, the Lagrangian action \\(S\\) is a functional, defined by\n\n\\[S(\\gamma) = \\int L(t, \\gamma(t), \\dot \\gamma(t))dt.\\]\n\na functional \\(S\\) has zero variation at \\(\\gamma\\) under constraint \\(c\\) iff for any variation \\(\\delta\\eta\\) satisfying constraint \\(c\\), we have \\(S(\\gamma + \\delta \\eta) = S(\\gamma) + o(\\delta)\\). We often write it simply as \\((\\delta S(\\gamma))_c = 0\\).\nvariational calculus is a collection of techniques for solving calculus problems involving variations.\na variational principle is a statement with the following format:\n\n::: center A trajectory \\(\\gamma\\) of the system is a physically valid trajectory iff \\((\\delta S(\\gamma))_c = 0\\). :::\nNow that we are clear on that, we can tabulate just about every variational principles of physics that you’re likely to ever see in Table [table:var-prin].\n\n\n1.0|L|L|L|L|L| name & Where is the trajectory? & specification & constraint & the functional\nHamilton’s principle & state spacetime & Lagrangian \\(L(t, q, v)\\) & fixed \\((t_0, q_0), (t_1, q_1)\\) & \\(\\int_\\gamma L(t, q, \\dot q)dt\\)\nmodified Hamilton’s principle & phase spacetime & Hamiltonian \\(H(t, q, p)\\) & fixed \\((t_0, q_0), (t_1, q_1)\\) & \\(\\int_\\gamma (\\sum_i p_i \\dot q_i - H(t, q, p))dt\\)\nMaupertuis’ principle2 & phase space & time-independent Hamiltonian \\(H(q, p)\\) & fixed \\(q_0, q_1\\), constant \\(H(q, p)\\) & \\(\\int_\\gamma \\sum_i p_i dq_i\\)\nJacobi’s form of Maupertuis’ principle & state space & Lagrangian of form \\(L(t, q, v) = \\frac 12 v^T M v - V(q)\\) & fixed \\(q_0, q_1\\), bonuded \\(V(q) \\leq 0\\) & \\(\\int_\\gamma \\sqrt{(E - V(q)) dq^T M dq}\\)\ntimed Maupertuis’ principle & state spacetime & Lagrangian of form \\(L(t, q, v) = \\frac 12 v^T M v - V(q)\\) & fixed \\(q_0, q_1\\), constant \\(\\frac 12 \\dot q^T M \\dot q + V(q)\\) & \\(\\int_\\gamma (\\dot q^T M \\dot q) dt\\)\nFermat’s principle of stationary pathlength3 & state space & Lagrangian of form \\(L(t, q, v) = \\frac 12 v^T M v\\) & fixed \\(q_0, q_1\\), constant \\(\\frac 12 \\dot q^T M \\dot q\\) & \\(\\int_\\gamma \\sqrt{dq^T M dq}\\)\nFermat’s principle of stationary time & state spacetime & Lagrangian of form \\(L(t, q, v) = \\frac 12 v^T M v\\) & fixed \\(q_0, q_1\\), constant \\(\\frac 12 \\dot q^T M \\dot q\\) & \\(t_1 - t_0\\)\n\n2 \"principle of least action\" in (Goldstein, Poole, and Safko 2008)3 Corollary: Hertz’s principle of least curvature\n\n\nHamilton’s principle and modified Hamilton’s principle\nThere are two principles that are often confused with impunity by physicists. The fact is that they are indeed equivalent (which is why they can be confused with impunity), but that is no excuse for bad mathematics.\nHamilton’s principle is a principle about trajectories of type \\(\\gamma: [t_0, t_1] \\to \\mathcal S\\). That is, \\(\\gamma(t)\\) is a state of the system. Variations can be thought of as making a state perturbation \\(\\delta \\eta(t)\\) at every time.\nIn contrast, modified Hamilton’s principle is a principle about trajectories of type \\(\\Gamma: [t_0, t_1] \\to \\mathcal P\\). That is, \\(\\Gamma(t)\\) is a phase of the system, specifying both its state and momentum. Variations can be thought of as making a state perturbation \\(\\delta \\eta_p(t)\\) and a momentum perturbation \\(\\delta \\eta_p(t)\\) at every time.\nOne may object that modified Hamilton’s principle is performing physically impossible variations: how could you perform variations on position and momentum independently of each other? Shouldn’t we have \\(\\delta p = m\\delta \\dot q = \\delta (\\nabla_{v} L(t, q, \\dot q))\\) at all times? To this objection, there are four layers of replies.\n\nThe equivalence of Hamilton’s principle and modified Hamilton’s principle, to be proved below, is a theorem in pure mathematics. It makes no demand on physical reality. It merely states that Hamilton’s principle specifies the same trajectories as modified Hamilton’s principle. Consequently, if it happens that these trajectories are physically real, then they can be specified by either principle.\nThe economic interpretation of momentum \\(p\\) is merely the market price. The equation \\(p = \\nabla_{v} L(t, q, \\dot q)\\) is true if we also assume that the producer is profit-maximizing. Now, if the trajectory \\(\\gamma\\) is optimal, then it implies that the producer is profit-maximizing. But after a perturbation of \\(\\gamma\\) to \\(\\gamma + \\delta \\eta\\), the producer is not necessarily profit-maximizing.4 Consequently, even in Hamilton’s principle, there is no requirement that \\(p = \\nabla_{v} L(t, q, \\dot q)\\). The modified Hamilton’s principle makes this interactive dance between the producer and the market explicit: we allow both the production schedule and the market price schedule to vary independently. Then, the equation \\(\\delta \\int (\\sum_i p_i \\dot q_i - H)dt = 0\\) is a statement about the trajectory of the producer-market system, and solving it would simultaneously solve both the producer and the market. In contrast, the equation \\(\\delta \\int L dt = 0\\) is a statement about the producer, and solving it by imagining a market \\(p\\) is useful, but not necessary.\nEven in classical mechanics, momentum is not real. We are fooled by our long habit of thinking about classical mechanics as if it is merely a more mathematical version of our intuition. Classical mechanics is actually unintuitive.5 In classical mechanics, there is no necessary connection between momentum and velocity – \"If \\(L = \\frac 12 v^T M v\\), then \\(p = Mv\\) on physically valid paths\" actually needs to be proved from Hamilton’s principle, not baked into the definition of momentum!\nThough in classical mechanics, both principles are equivalent, in modern physics, the modified Hamilton’s principle is primary, and the Hamilton’s principle a mere derivative. Furthermore, the phase space is primary, and the division of it into position-momentum is arbitrary. At a more fundamental level, there is no distinction between position and momentum. A \"rotation in phase space\" can transform position and momentum into each other.\n\n4 Unless the market price is perturbed in just the right way to make the producer profit-maximizing – If the producer messes up, the market can still accommodate the producer and make the producer look as if it is still profit-maximizing... Just like Potemkin villages!5 Why else did Newton come two thousand years after Aristotle? Though quantum mechanics is certainly more unintuitive.\nTheorem 1 (Hamilton’s principle and modified Hamilton’s principle are equivalent.) Given a state space \\(\\mathcal S\\), a Lagrangian \\(L(t, q, v)\\) on the configuration space, and a Hamiltonian \\(H(t, q, p)\\) on the phase space, related by convex duality, then a path \\(\\gamma: [t_0, t_1] \\to \\mathcal S\\) on state space satisfies\n\\[\\delta \\int_\\gamma L(t, q, \\dot q) dt = 0\\]\nwith fixed \\((t_0, q_0), (t_1, q_1)\\), iff its corresponding path \\(\\Gamma: [t_0, t_1]: \\to \\mathcal T^\\ast \\mathcal S\\) on phase space satisfies\n\\[\\delta \\int_\\Gamma \\sum_i p_i \\dot q_i - H(t, q, p) dt = 0\\]\nwith fixed \\((t_0, q_0), (t_1, q_1)\\) (and variable \\(p_0, p_1\\)).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\((\\Rightarrow):\\) Since \\(\\gamma\\) has zero variation for the action \\(\\int_\\gamma L(t, q, \\dot q)dt\\), and \\(H\\) is related to \\(L\\) by convex duality, by our previous results, the Hamiltonian equations of motion are satisfied along \\(\\gamma\\). That is, \\(-\\nabla_q H = \\dot p, \\nabla_p H = \\dot p\\).\nPerform variation \\(\\Gamma + \\delta \\eta\\) on phase space. The variation in the action is\n\\[\\begin{aligned} \\delta S(\\Gamma) &= \\int_{t_0}^{t_1} [\\langle \\delta p, \\dot q\\rangle + \\langle  p, \\delta \\dot q\\rangle - \\langle \\nabla_q H, \\delta q\\rangle - \\langle \\nabla_p H, \\delta p\\rangle ] dt \\\\ &= \\int_{t_0}^{t_1} [\\langle \\delta p, \\dot q\\rangle + \\langle  p, \\delta \\dot q\\rangle - \\langle -\\dot p, \\delta q\\rangle - \\langle \\dot q, \\delta p\\rangle ] dt \\\\ &= \\int_{t_0}^{t_1} [\\langle  p, \\delta \\dot q\\rangle + \\langle \\dot p, \\delta q\\rangle] dt \\\\ &= \\langle p, \\delta q\\rangle \\Big|_{t_0}^{t_1} = 0 \\end{aligned}\\]\nsince \\(\\delta q= 0\\) at the end points.\n\\((\\Leftarrow):\\) Since \\(\\delta \\int_\\Gamma \\sum_i p_i \\dot q_i - H(t, q, p) dt = 0\\) at \\(\\Gamma\\) under constraint of fixed \\((t_0, q_0), (t_1, q_1)\\), it must also have zero variation if we use the stronger constraint of fixed \\((t_0, q_0, p_0), (t_1, q_1, p_1)\\). Then the Euler–Lagrange equations state6\n\\[-\\nabla_q H = \\dot p, \\quad \\nabla_p H = \\dot p\\]\nwhich, as we proved, are precisely the conditions (no arbitrage pricing, and stationary profit flow) for \\(\\delta \\int_\\gamma L = 0\\).\n\n\n\n6 One can interpret this as treating the phase space as if it is a state space of a physical system with \\(2N\\) degrees of freedom.\n\nMaupertuis’ principle\nMaupertuis’ principle is a principle for specifying orbits in phase space. An orbit is a trajectory of the physical system, but with timing information lost. We know that the system traveled through the states on the orbit, one after another, but we don’t know how fast is the traversal.\nIn order to be very explicit about it, we will write orbits in phase space as \\(\\mu: [a, b] \\to \\mathcal P\\). Here \\(a, b\\) look like \"start and end times\" and \\(\\mu(s)\\) looks like \"location of the path at time \\(s\\)\", but \\(s\\) is not time, and \\(a, b\\) are not moments in time either. It is really just a parametrization of the curve, with no implications about how fast, or how slow, the system would actually traverse the orbit.\nThe integral \\(\\int_\\mu \\sum_i p_i \\dot q_i ds\\) is unchanged by stretching and pressing the timing of \\(\\mu\\). That is, let \\(f: [a', b'] \\to [a, b]\\) be a strictly increasing differentiable function, then \\(\\int_{\\mu\\circ f} \\sum_i p_i \\dot q_i ds = \\int_\\mu \\sum_i p_i \\dot q_i ds\\). Consequently, Maupertuis’ principle is really concerned only with the orbit, not the timing of the orbit.\nSince timing is lost, the constraint of fixed \\((t_0, q_0), (t_1, q_1)\\) cannot apply. However, merely fixing \\(q_0, q_1\\) is too little constraint. The solution is to add a new constraint: the variation must stay on the surface of constant energy \\(E\\). That is, \\(H(\\mu'(s')) = E\\) for any variation \\(\\mu'\\) and parameter \\(s'\\). This is how we arrive at Maupertuis’ principle.\n::: prop\nWhen the Hamiltonian is time-independent, Hamilton’s principle and Maupertuis’ principle are equivalent (after a retiming scaling).\nGiven phase space \\(\\mathcal P\\) and a time-independent Hamiltonian \\(H(q, p)\\) over the phase space, such that \\((\\nabla_q H, \\nabla_p H)\\) is never zero, then any trajectory \\(\\gamma: [t_0, t_1] \\to \\mathcal P\\) that satisfies Hamilton’s principle also satisfies Maupertuis’ principle.\nConversely, given any orbit \\(\\mu: [a, b] \\to \\mathcal P\\) with constant \\(H\\) that satisfies Maupertuis’ principle, there exists a \"retiming map\" \\(f: [t_0, t_1]\\to [a, b]\\) such that \\(f\\) is monotonically increasing, and \\(\\mu\\circ f\\) satisfies Hamilton’s principle. :::\n::: proof Proof. We show that Maupertuis’ principle is equivalent to Hamilton’s equations of motion after a retiming map.\nConsider orbit \\(\\mu: [a, b] \\to \\mathcal P\\) in phase space, with constant \\(H(\\mu(s)) = E_0\\). By integration-by-parts, we have\n\\[\\delta \\int \\langle p, \\dot q\\rangle ds = \\int \\langle \\delta p, \\dot q\\rangle - \\langle \\dot p, \\delta  q\\rangle ds + \\cancel{\\langle p, \\delta q\\rangle} \\Big|_{a}^{b}\\]\nwhere the variation fixes \\(q_0, q_1\\) and \\(H\\).\nNow, \\(\\delta H = \\langle \\nabla_p H, \\delta p\\rangle +  \\langle \\nabla_q H, \\delta q\\rangle = 0\\). So, if the orbit satisfies Hamilton’s equations of motion after a retiming map \\(f\\), that is,\n\\[\\begin{cases}     \\dot p = -f'(s)\\nabla_p H \\\\     \\dot q = f'(s)\\nabla_q H \\end{cases}\\]\nthen plugging it back, we get\n\\[\\delta \\int \\langle p, \\dot q\\rangle ds = \\int f'(s) \\delta H ds = 0\\]\nIt is routine to check that, given four vectors \\(a, b, c, d\\in \\R^N\\), if \\(\\forall x, y\\in \\R^N\\),\n\\[\\langle a, x\\rangle - \\langle b, y\\rangle = 0 \\implies \\langle c, x\\rangle - \\langle d, y\\rangle = 0\\]\nthen there exists \\(\\lambda &gt; 0\\) such that \\(c = \\lambda a, d = \\lambda d\\).\nThus, if the variation is zero for all \\(\\delta q, \\delta p\\) with fixed \\(H\\), then there exists some continuous and positive function \\(\\lambda: [a, b] \\to \\R\\) such that\n\\[\\begin{cases}     \\dot p = -\\lambda(s)\\nabla_p H \\\\     \\dot q = \\lambda(s)\\nabla_q H \\end{cases}\\]\nNow solve for \\(f' = \\lambda^{-1}\\) by integration7, then \\(f\\) is the desired retiming map. ◻ :::\n7 Since \\(\\lambda\\) is continuous and positive, with compact domain, its range must be bounded below by some positive constant \\(\\epsilon &gt; 0\\), thus the integration would not diverge."
  },
  {
    "objectID": "blog/posts/geometrical-mechanics/index.html#canonical-transformations",
    "href": "blog/posts/geometrical-mechanics/index.html#canonical-transformations",
    "title": "Yuxi on the Wired",
    "section": "Canonical transformations",
    "text": "Canonical transformations\n\nGenerating functions\nThe dynamics of a physical system can be fully defined by its Lagrangian function. However, the Lagrangian function is not fully defined by the dynamics. There are many possible functions that can all play the role of the Lagrangian.\nSuppose \\(L(t, q, v)\\) is a Lagrangian function, then take any twice-differentiable function \\(F(t, q)\\), and define\n\\[L' dt := L dt + dF\\]\nwhich implies\n\\[L'(t, q, v) := L(t, q, v) + \\partial_t F(t, q) + \\langle \\nabla_q F(t, q), v\\rangle\\]\nthen it is easy to directly verify that a trajectory \\(\\gamma(t)\\) satisfies the Euler–Lagrange equations for \\(L\\) iff it satisfies them for \\(L'\\). Consequently, both \\(L\\) and \\(L'\\) are different functions that can both play the role of Lagrangian for the same physical system.\nInstead of directly computing the Euler–Lagrangian equations, we can also do it directly by variational principles: For any trajectory \\(\\gamma: [t_0, t_1] \\to \\mathcal S\\) we have\n\\[\\int_{t_0}^{t_1} L'(t, \\gamma(t), \\dot \\gamma(t))dt = F(t, \\gamma(t))|_{t_0}^{t_1} + \\int_{t_0}^{t_1} L(t, \\gamma(t), \\dot \\gamma(t))dt\\]\nConsequently, \\(\\delta \\int Ldt = 0\\) iff \\(\\delta \\int L'dt = 0\\), so a trajectory has stationary action according to one Lagrangian iff according to the other.\nSuch \\(F(t, q)\\) are called a generating function for transforming a Lagrangian function. Generating functions are really just functions that are picked to play a certain role. That is, being a generating function is not an intrinsic property of a function, but extrinsic, because some human physicist has decided to use it for generating a new Lagrangian from an old one. This is why I don’t like saying \"\\(F\\) a generating function...\". Instead, I prefer to say \"Now we use \\(F\\) to generate a new Lagrangian...\" Nevertheless I am forced to use the term because it is a venerable error, a bug that became a feature.\n\n\nGenerating functions for Hamiltonians\nHamiltonians are freer than Lagrangians. Instead of one way, there are many ways to generate new Hamiltonians from old.\nTaking inspiration from Lagrangian generating functions, we write down the following equation:\n\\[\\langle p , dq \\rangle - hdt = \\langle P, dQ \\rangle - Hdt + dG\\]\nwhere \\(G: \\R \\times \\mathcal P \\to \\R\\) is any twice-differentiable function on phase spacetime.\n::: theorem If \\((q, p), (Q, P)\\) are two coordinate systems on the phase spacetime, and \\(h, H, G\\) are twice-differentiable functions on phase spacetime, and\n\\[\\langle p , dq \\rangle - hdt = \\langle P, dQ \\rangle - Hdt + dG\\]\nThen along any trajectory in phase spacetime, \\((q, p)\\) satisfies Hamiltonian equations of motion for \\(h\\), iff \\((Q, P)\\) satisfies Hamiltonian equations of motion for \\(H\\). :::\n::: proof Proof. Let \\(\\gamma: [t_0, t_1] \\to \\mathcal P\\) be any trajectory, not necessarily satisfying Hamilton’s equations of motion. Then for any variation of \\(\\gamma\\) with fixed \\(t_0, t_1\\), we have by integration-by-parts,\n\\[\\begin{aligned}     &\\delta\\int_\\gamma \\langle p , dq \\rangle - hdt = \\delta\\int_\\gamma \\langle P, dQ \\rangle - Hdt + dG\\\\     &= \\int (\\langle \\delta P, \\dot Q - \\nabla_P H\\rangle - \\langle \\dot P + \\nabla_Q H, \\delta Q\\rangle ) dt + (\\langle P, \\delta Q \\rangle + \\delta G)\\Big|_{t_0}^{t_1} \\\\     &= \\int (\\langle \\delta p, \\dot q - \\nabla_p h\\rangle - \\langle \\dot p + \\nabla_q h, \\delta q\\rangle ) dt + \\langle p, \\delta q \\rangle \\Big|_{t_0}^{t_1}  \\end{aligned}\\]\nThe boundary terms are equal, since \\(\\langle p , \\delta q \\rangle - h \\delta t = \\langle P, \\delta Q \\rangle - H\\delta t + \\delta G\\), and \\(\\delta t = 0\\) as we fixed \\(t_0, t_1\\).\nThus, for any variation of \\(\\gamma\\) with fixed \\(t_0, t_1\\),\n\\[\\int (\\langle \\delta p, \\dot q - \\nabla_p h\\rangle - \\langle \\dot p + \\nabla_q h, \\delta q\\rangle ) dt = \\int (\\langle \\delta P, \\dot Q - \\nabla_P H\\rangle - \\langle \\dot P + \\nabla_Q H, \\delta Q\\rangle ) dt\\]\nThus, if \\(\\gamma\\) satisfies Hamiltonian equations of motion for \\((q, p), h\\), then it also does so for \\((Q, P), H\\). ◻ :::\n\n\nCoordinate-based canonical transforms\nThis section might make more sense after reading the next section.\nThe equation\n\\[\\langle p , dq \\rangle - hdt = \\langle P, dQ \\rangle - Hdt + dG\\]\nlives in phase spacetime. That is, \\(dq, dt, dQ, dG\\) are all differentials in \\(\\R \\times \\mathcal P\\). This is elegant, but not good for concrete computations, which requires coordinate-based equations.\nGenerally, \\(G(t, y)\\) is a function on phase spacetime, so it could be represented in any coordinate system of phase spacetime. For example, we can represent it as \\(G(t, y) = G_{q, p}(t, q(t, y), p(t, y))\\), or \\(G(t, y) = G_{Q, P}(t, Q(t, y), P(t, y))\\), or even mixed coordinates like \\(G(t, y) = G_{q, P}(t, q(t, y), P(t, y))\\), etc.\nMost representations result in intractable coordinate-based equations, but a few are actually usable. These are traditionally classified as \"type 1\" to \"type 5\".\nType 1: \\(G(t, y) = F_1(t, q(t, y), Q(t, y))\\).\nPlugging it in, we find\n\\[\\langle p , dq \\rangle - hdt = \\langle P, dQ \\rangle - Hdt + \\partial_t F_1 dt + \\langle \\nabla_q F_1, dq \\rangle + \\langle \\nabla_Q F_1, dQ \\rangle\\]\nyielding the equations\n\\[\\begin{cases}     H(t, Q, P) = h(t, q, p) + \\partial_t F_1(t, q, Q) \\\\     p = \\nabla_q F_1(t, q, Q) \\\\     P = - \\nabla_Q F_1(t, q, Q) \\end{cases}\\]\nIn order to solve for the canonical transform, first invert \\(p = \\nabla_p F_1(t, q, Q)\\) to obtain \\(Q = f_Q(t, q, p)\\), then plug it into \\(P = - \\nabla_Q F_1(t, q, Q)\\) to obtain \\(P = f_P(t, q, p)\\). Inverting them gives us \\(q = g_q(t, Q, P), p = g_p(t, Q, P)\\).\nThen, given any Hamiltonian \\(h(t, q, p)\\), the corresponding \\(H(t, Q, P)\\) is found by \\(H(t, Q, P) = h(t, q, p) + \\partial_t F_1(t, q, Q)\\), or very explicitly,\n\\[H(t, Q, P) = h(t, g_q(t, Q, P), g_p(t, Q, P)) + \\partial_t F_1(t, g_q(t, Q, P), Q)\\]\nType 2: \\(G(t, y) = F_2(t, q(t, y), P(t, y)) - \\langle P(t, y), Q(t, y)\\rangle\\).\nWhy \\(\\langle P, Q\\rangle\\)? Directly writing down \\(G = F_2(t, q, P)\\) results in the following equation:\n\\[\\langle p , dq \\rangle - hdt = \\langle P, dQ \\rangle - Hdt + \\partial_t F_2 dt + \\langle \\nabla_q F_2, dq \\rangle + \\langle \\nabla_P F_2, dP \\rangle\\]\nHere, there is an entanglement between terms \\(dq, dQ, dP\\). Since there are only \\(2N\\) dimensions in the phase space, but there are \\(3N\\) differentials in \\(dq, dQ, dP\\), it must be possible to represent \\(N\\) of them as a linear combination of the other \\(2N\\) differentials. In particular, we can represent \\(dQ\\) as a linear combination of \\(dq, dP\\).\nInstead, we can directly cancel out \\(\\langle P, Q\\rangle\\) from the equation by writing \\(G\\) as \\(G + \\langle P, Q\\rangle - \\langle P, Q\\rangle\\), then represent \\(G + \\langle P, Q\\rangle\\) as \\(F_2(t, q, P)\\). This then gives\n\\[\\begin{cases}     H(t, Q, P) = h(t, q, p) + \\partial_t F_2(t, q, P) \\\\     Q = \\nabla_P F_2(t, q, P) \\\\     p = \\nabla_q F_2(t, q, P) \\end{cases}\\]\nType 3: \\(G = F_3(t, p, Q) + \\langle p, q\\rangle\\).\nType 4: \\(G = F_4(t, p, P) + \\langle p, q\\rangle  - \\langle P, Q\\rangle\\).\nType 5: \\(G = F(t, p_{I_1}, q_{I_2}, P_{I_3}, Q_{I_4}) + \\langle p_{I_1}, q_{I_1} \\rangle - \\langle P_{I_3}, Q_{I_3} \\rangle\\).\n\\[\\begin{cases}     H(t, Q, P) = h(t, q, p) + \\partial_t F(t, p_{I_1}, q_{I_2}, P_{I_3}, Q_{I_4}) \\\\     q_{I_1} = -\\nabla_{p_{I_1}} F(t, p_{I_1}, q_{I_2}, P_{I_3}, Q_{I_4}) \\\\     p_{I_2} = \\nabla_{q_{I_2}} F(t, p_{I_1}, q_{I_2}, P_{I_3}, Q_{I_4}) \\\\     Q_{I_3} = \\nabla_{P_{I_3}} F(t, p_{I_1}, q_{I_2}, P_{I_3}, Q_{I_4}) \\\\     P_{I_4} = -\\nabla_{Q_{I_4}} F(t, p_{I_1}, q_{I_2}, P_{I_3}, Q_{I_4}) \\end{cases}\\]\nHere, \\(I_1, I_2, I_3, I_4\\) stand for subsets of the indexing set \\(\\{1, 2, ..., N\\}\\). We also require \\(I_1 \\cup I_2 = I_3 \\cup I_4 = \\{1, 2, ..., N\\}\\)\nNote that types 1 to 4 are all special cases of type 5.\n\n\nExamples of canonical transforms\n\nPoint transforms\nIf \\(G= 0\\), then we need to solve only the equation \\(\\langle p, dq \\rangle = \\langle P, dQ \\rangle\\), which can be done in general iff \\(dQ\\) is a linear combination of \\(dq\\), thus \\(Q = f_Q(t, q)\\) for some function \\(f_Q\\). This is just the point transform, with solution\n\\[P = ([\\nabla_q f_Q]^T)^{-1}p\\]\n\n\nInterpolating a canonical transform\nGiven any canonical transform from \\((q, p)\\) to \\((Q, P)\\), for any two times \\(t_0 &lt; t_1\\), we can interpolate between \\((q, p), (Q, P)\\) over the period \\([t_0, t_1]\\). That is, we construct a canonical transform from \\((q, p)\\) to \\((\\bar q, \\bar p)\\) such that \\((\\bar q, \\bar p) = (q, p)\\) at \\(t=t_0\\), and \\((\\bar q, \\bar p) = (Q, P)\\) at \\(t = t_1\\).\nThe idea is to note that any canonical transform can be written in type 2, including the identity transform.\nThe identity transform from \\((q, p)\\) to \\((\\bar q, \\bar p)\\) can be represented in type 2 as\n\\[G_0 = \\langle q, \\bar p \\rangle - \\langle \\bar p, \\bar q \\rangle\\]\nGenerate the transform from \\((q, p)\\) to \\((Q, P)\\) by \\(G\\), and represent it in type 2 as\n\\[G_1 = F_2(t, q, P) - \\langle P, Q \\rangle\\]\nNow interpolate them by\n\\[G = \\frac{t_1-t}{t_1 - t_0} \\langle q, \\bar p \\rangle + \\frac{t - t_0}{t_1 - t_0} F_2(t, q, \\bar p)  - \\langle \\bar p, \\bar q \\rangle\\]\n\n\nTime-evolution is a canonical transform generated by the action\nRecall that, for any coordinate system \\((q, p)\\) and Hamiltonian \\(h\\), we defined the action function (\"Hamilton’s principal function\") \\(S(t_1, q_1; t_0, q_0)\\) to be the action for the path \\(\\gamma\\) from \\((t_0, q_0)\\) to \\((t_1, q_1)\\). We also proved, during derivation of the HJE,\n\\[dS = \\langle p_1, dq_1 \\rangle -h(t_1, q_1, p_1) dt_1 - \\langle p_0, dq_0 \\rangle + h(t_0, q_0, p_0) dt_0\\]\nRearrange, and using suggestive notation, we get...\n\\[\\langle p_0, dq_0 \\rangle - h_0 dt_0 = \\langle p_1, dq_1 \\rangle -h_1 dt_1 + dS\\]\nSo we find that time evolution is a canonical transformation generated by \\(S\\).\nIn more detail, fix some coordinate system \\((q, p)\\). Then for any Hamiltonian \\(\\mathcal H : \\R \\times \\mathcal P \\to \\R\\), define its time-evolution function \\(\\phi\\), such that \\(\\phi(t_0, t_1; y)\\) is the point that we end up with at time \\(t_1\\), if we start at \\(y\\) at time \\(t_0\\), and evolve according to \\(\\dot q = \\nabla_p h, \\dot p = -\\nabla_q h\\), where \\(h(t, q(t, x), p(t, x)) = \\mathcal H(t, x)\\) is the coordinate-based version of the coordinate-free \\(\\mathcal H\\).\nNow, fix some time-interval \\(s\\in \\R\\), then define the (coordinate-free) function \\(\\mathcal G\\), the action of the trajectory starting at \\((t, y)\\) and lasting for \\(s\\):\n\\[\\mathcal G(t, y) := S(t+s, q(t+s, \\phi(t, t+s; y)); t, q(t, y))\\]\nand the new coordinate system with a new Hamiltonian, obtained by \"evolving for \\(s\\) time\": \\[\\begin{aligned} Q(t, y) = q(t+s, \\phi(t, t+s; y)) \\\\ P(t, y) = p(t+s, \\phi(t, t+s; y)) \\\\ H(t, Q(t, y), P(t, y)) = \\mathcal H(t + s, \\phi(t, t+s; y)) \\end{aligned}\\]\nwhich allows a coordinate-based representation of \\(\\mathcal G\\):\n\\[G(t, q, Q) = S(t+s, Q; t, q)\\]\nWith these definitions, we have\n\\[dG = -Hdt + \\langle P, dQ \\rangle + hdt -\\langle p, dq \\rangle\\]\nthat is, \\((Q, P), H\\) is canonically transformed from \\((q, p), h\\) via the function \\(G\\).\n\n\n\nSimple harmonic oscillator\nConsider a SHO with \\(N\\) degrees of freedom. Its Hamiltonian is\n\\[H = \\frac 12 p^T M^{-1} p + \\frac 12 q^T K q\\]\nwhere \\(M\\) is the matrix representing the masses of the system, and \\(K\\) is the matrix representing the elastic constants of the system.\n\nTranslation is a canonical transform generated by momentum\n\n\nRotation is a canonical transform generated by angular momentum\n\n\n\nCanonical transforms, in general\nThere are two possible ways to define canonical transforms. The more concrete way is by using generating functions: two coordinate systems \\((q, p), (Q, P)\\) on phase spacetime are generated canonical transforms of each other iff\n\\[\\langle p, dq\\rangle - \\langle P, dQ\\rangle = dG\\]\nfor some \\(G\\) functions on phase spacetime. Remember that \\(dq, dQ\\) are differentials with constant time.\nAs for the more abstract form... long story short: every canonical transform has a generating function. This is usually called \"Carathéodory Theorem\". See (Goldstein, Poole, and Safko 2008, sec. 9.5).\nThis has a more elegant form with exterior calculus. Take exterior differentiation (again, only in phase space, not in time), we get\n\\[\\sum_i dp_i \\wedge dq_i = \\sum_i dP_i \\wedge dQ_i\\]\nNow take wedge product \\(N\\) times with itself, we get\n\\[\\bigwedge_i dp_i \\wedge dq_i = \\bigwedge_i dP_i \\wedge dQ_i\\]\nInterpretation: canonical transforms preserve phase space volumes. That is, if we have an open subset in phase space, defined coordinate-free, then we can compute its volume by writing down a canonical coordinate system \\((q, p)\\) and integrating \\(\\prod_i dp_i dq_i\\). The result is unchanged by a canonical transform to \\((Q, P)\\).\nThis gives us a new proof of Liouville’s theorem:\n\nProof. Since time-evolution is a canonical transform, time-evolution preserves volumes.\nGiven a particle flow in phase space, with density \\(\\rho(t, p, q)\\), flowing according to Hamiltonian \\(H(t, q, p)\\). Take an infinitesimal cube around \\((q, p)\\) at time \\(t\\), with volume \\(\\delta V = \\prod_i \\delta p_i \\delta q_i\\), then it contains \\(\\delta N = \\rho(t, q, p) \\delta V\\) number of particles. Then, let it flow for time \\(s\\).\nThe infinitesimal cube is transported to some other parallelogram around some point \\((q', p')\\), but its volume is unchanged, thus the density at the new location is still the same: \\(\\rho(t+s, q', p') = \\rho(t, q, p)\\). Thus \\(\\dot \\rho = 0\\). \n\n\n\nPoisson brackets are preserved by canonical transforms\nThe Poisson bracket \\(\\{f, g\\}\\) was defined in a coordinate-based way:\n\\[\\{f, g\\} = \\sum_{i=1}^{N} \\left( \\frac{\\partial f}{\\partial q_{i}} \\frac{\\partial g}{\\partial p_{i}} - \\frac{\\partial f}{\\partial p_i} \\frac{\\partial g}{\\partial q_i}\\right)\\]\nWe show that it is preserved by canonical transforms. That is, if \\((Q, P)\\) is a canonical transform of \\((p, q)\\) then\n\\[\\sum_{i=1}^{N} \\left( \\frac{\\partial f}{\\partial q_{i}} \\frac{\\partial g}{\\partial p_{i}} - \\frac{\\partial f}{\\partial p_i} \\frac{\\partial g}{\\partial q_i}\\right) = \\sum_{i=1}^{N} \\left( \\frac{\\partial f}{\\partial Q_{i}} \\frac{\\partial g}{\\partial P_{i}} - \\frac{\\partial f}{\\partial P_i} \\frac{\\partial g}{\\partial Q_i}\\right)\\]\n\nProof. (From the Landau–Lifshitz textbook) Since the Poisson bracket does not depend on time, and if \\(Q(t, q, p), P(t, q, p)\\) is a canonical transform, so is \\(Q(t, q, p), P(t, q, p)\\), so we consider only canonical transforms that are independent of time.\nIf we started with only \\(f(q, p), g(q, p)\\), then extend them to phase spacetime by\n\\[\\mathcal F(t, y) = f(q(0, y), p(0, y)),\\quad \\mathcal G(t, y) = g(q(0, y), p(0, y))\\]\nNext, impose \\(\\mathcal G\\) as a Hamiltonian, and evolve the physical system according to Hamilton’s equations of motion for \\((q, p), \\mathcal G\\). Since \\((q, p)\\) and \\((Q, P)\\) are canonical transforms of each other, we have\n\\[\\{\\mathcal F, \\mathcal G\\}_{q, p} + \\partial_t \\mathcal F = \\dot{\\mathcal F} = \\{\\mathcal F, \\mathcal G'\\}_{Q, P} + \\partial_t \\mathcal F\\]\nOkay, what is \\(\\mathcal G'\\)? It is a solution to\n\\[\\langle p, dq \\rangle - \\mathcal G dt  = \\langle P, dQ \\rangle - \\mathcal G' dt + d\\mathcal K\\]\nsince \\(\\mathcal K\\) does not depend on time, \\(d\\mathcal K\\) contains zero \\(dt\\) term, so \\(\\mathcal G = \\mathcal G'\\).\n\n\n\nInterpretation of canonical transforms\nWhat is invariant under canonical transforms is what is really real about the physical system. Other things are mirages, illusions caused by our choice of coordinates.\nThus, position and momentum are mirages. Hamiltonian equations are real. \\(p, q\\) are mirages. \\(\\int \\sum_i p_i dq_i\\) is real. \\(\\nabla_p, \\nabla_q\\) are mirages. Poisson brackets \\(\\{f, g\\}\\) are real. Phase space lengths \\(dp, dq\\) are mirages. Phase space areas \\(\\sum_i p_i dq_i\\), volumes \\(\\prod_i dp_i dq_i\\), and densities \\(\\rho\\) are real."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe geometry of physical states\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Mechanics\n\n\n\n\n\n\nmath\n\n\nphysics\n\n\nphilosophy\n\n\nprobability\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipia Philosophica Naturalium Mathematicarum\n\n\nPhilosophical Principles of Natural Mathematics\n\n\n\nmath\n\n\nphysics\n\n\nphilosophy\n\n\ncs\n\n\n\n.\n\n\n\n\n\nApr 11, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation and GDP since 10000 BC\n\n\n\n\n\n\nAI\n\n\nscaling\n\n\n\nWhen did the singularity get cancelled?\n\n\n\n\n\nJan 18, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n\n\n\n\n\n\nMathematical Interpretations of Quantum Mechanics\n\n\n\n\n\n\nphysics\n\n\n\nQuantum mechanics: what it all means, mathematically speaking.\n\n\n\n\n\nJan 10, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n\n\n\n\n\n\nBook Reviews\n\n\n\n\n\n\nbook-review\n\n\n\nBook reviews.\n\n\n\n\n\nDec 16, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n\n\n\n\n\n\nThe Decline of Mathematical Fields\n\n\n\n\n\n\nfun\n\n\nphilosophy\n\n\nhistory\n\n\nmath\n\n\n\nLosing my religion.\n\n\n\n\n\nNov 1, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does it feel like to be a mathematical object?\n\n\n\n\n\n\nfun\n\n\nphilosophy\n\n\nmath\n\n\n\nMy religion.\n\n\n\n\n\nNov 1, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n  \n  \nYuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks."
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html",
    "href": "blog/posts/ai-creativity/index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "An interesting strand of argument runs through discussions on AI: whether they can be creative or not; whether we should build them to be creative or not.\nDuring an online discussion, I explained to someone who is confused about why people are building these art machines that are “stealing art”. I explained the technological-scientific perspective on art. Their argument ran as follows:\n\nMachines cannot be creative. The apparent creativity is fake and mere copying.\nPeople build machines not because they want to make art, but because of something else. Perhaps a greed for money, a hate of artists, or some other nefarious motivation (the post is a bit vague on the precise motivation).\n\nI tried to explain the very different perspective on the other side of the cultural divide, so that they might understand, if not to accept:\n\nThere is no “magic”. Art might feel impossible to build a machine for, but we can.\nWhat I cannot create, I do not understand. (Feynman quote)\nIntrospection is unreliable. Asking artists how art was “really made” is not a reliable way to understand art.\nThus, we can build art machines, and we want to, if we are to ever understand art.\n\nThey repeated the same arguments, but more vehemently.\nIt was frustrating, though I was not surprised. This incident started my thinking: This is such a common response, that there is probably a psychological mechanism behind it. This essay describes some possible mechanisms."
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#introduction",
    "href": "blog/posts/ai-creativity/index.html#introduction",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "An interesting strand of argument runs through discussions on AI: whether they can be creative or not; whether we should build them to be creative or not.\nDuring an online discussion, I explained to someone who is confused about why people are building these art machines that are “stealing art”. I explained the technological-scientific perspective on art. Their argument ran as follows:\n\nMachines cannot be creative. The apparent creativity is fake and mere copying.\nPeople build machines not because they want to make art, but because of something else. Perhaps a greed for money, a hate of artists, or some other nefarious motivation (the post is a bit vague on the precise motivation).\n\nI tried to explain the very different perspective on the other side of the cultural divide, so that they might understand, if not to accept:\n\nThere is no “magic”. Art might feel impossible to build a machine for, but we can.\nWhat I cannot create, I do not understand. (Feynman quote)\nIntrospection is unreliable. Asking artists how art was “really made” is not a reliable way to understand art.\nThus, we can build art machines, and we want to, if we are to ever understand art.\n\nThey repeated the same arguments, but more vehemently.\nIt was frustrating, though I was not surprised. This incident started my thinking: This is such a common response, that there is probably a psychological mechanism behind it. This essay describes some possible mechanisms."
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#the-self-interest-theory",
    "href": "blog/posts/ai-creativity/index.html#the-self-interest-theory",
    "title": "Yuxi on the Wired",
    "section": "The self-interest theory",
    "text": "The self-interest theory\nThe self-interest theory is as follows: “It is hard to get someone to understand something if something they care about depends on their not understanding it.”"
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#the-non-truth-theory",
    "href": "blog/posts/ai-creativity/index.html#the-non-truth-theory",
    "title": "Yuxi on the Wired",
    "section": "The non-truth theory",
    "text": "The non-truth theory\nThe non-truth theory states that some arguments are forever mired in the same controversies, always rehashing the same arguments, because there is no truth to be found underneath the arguments.\nThere are certain social functions that are best served by saying something in language that looks like they talk about objective things. You can think of this as a hack in the programming language of humans. For example,\nThere are some social functions that"
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#why-it-seems-terrible-that-machines-might-create",
    "href": "blog/posts/ai-creativity/index.html#why-it-seems-terrible-that-machines-might-create",
    "title": "Yuxi on the Wired",
    "section": "Why it seems terrible that machines might create",
    "text": "Why it seems terrible that machines might create\nAt the “immortal dinner party” held by Benjamin Haydon on 28 December 1817, the Romantic poet John Keats agreed with Charles Lamb that Newton “had destroyed all the poetry of the rainbow, by reducing it to the prismatic colors”. Later, Keats wrote “Lamia” that included these famous lines:\nDo not all charms fly\nAt the mere touch of cold philosophy?\nThere was an awful rainbow once in heaven:\nWe know her woof, her texture; she is given\nIn the dull catalogue of common things.\nPhilosophy will clip an Angel's wings,\nConquer all mysteries by rule and line,\nEmpty the haunted air, and gnomed mine—\nUnweave a rainbow, as it erewhile made\nThe tender-person'd Lamia melt into a shade\nGPT4: Keats came up with the concept of “negative capability.” This is the ability to dwell in uncertainties, mysteries, doubts, without any compulsive reaching after fact and reason. Keats valued this ability, arguing that it was central to a poet’s creative process."
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#why-it-seems-incredible-that-machines-might-create",
    "href": "blog/posts/ai-creativity/index.html#why-it-seems-incredible-that-machines-might-create",
    "title": "Yuxi on the Wired",
    "section": "Why it seems incredible that machines might create",
    "text": "Why it seems incredible that machines might create\nHere, the arguments are easier to classify. It seems that there are several common mental models that people use when they think about machines that create. Using any of these would make it seem obvious that machines cannot be creative. So, I just need to classify the mental models!\n\nMachines as monkeys typing randomly\nIn Gulliver’s Travels (1726) by Jonathan Swift, there was a writing machine. It is a 16x16 matrix of little square blocks, with a character on each side. To use it, you turn the 32 handles randomly, then read out the few words that appeared by chance. This allowed:\n\nthe most ignorant person, at a reasonable charge, and with a little bodily labour, might write books in philosophy, poetry, politics, laws, mathematics, and theology, without the least assistance from genius or study.\n\nIt is a clear satire, possibly of Ramon Llull’ s Thinking Machine (3 concentric rotating disks that generate all possible theological arguments):\n\nThe first of these features means that all of these attributes are inherent; the second, that they are systematically interrelated in such a way as to affirm, with impeccable orthodoxy, that glory is eternal or that eternity is glorious; that power is true, glorious, good, great, eternal, powerful, wise, free, and virtuous, or benevolently great, greatly eternal, eternally powerful, powerfully wise, wisely free, freely virtuous, virtuously truthful, etc., etc.\n\n\n\nMachines as pipes for the water of creativity\n\nIt appears to me that if one wants to make progress in mathematics one should study the masters and not the pupils.\n\n— N.H. Abel (1802–1829), quoted from an unpublished source by O. Ore in Niels Henrik Abel, Mathematician Extraordinary, p. 138.\nThere is a common attitude that I can summarize as this: Like drawing water from the unsullied source at the mountain’s peak, so is the experience of returning to the writings of the masters: clear, refreshing, and devoid of later impurities.\n\nAncient Greek theory of creativity\nIn ancient Greece, the Muses were considered the source of the knowledge embodied in the poetry, lyric songs, and myths that were related orally for centuries in ancient Greek culture. Homer began his Iliad with:\n\nSing, Muse, the fatal wrath of Peleus’ son,\nWhich to the Greeks unnumb’red evils brought,\n\nNote that the Muses was doing the real singing, and Homer was a channel for their singing (back then, poetry was sang – the Iliad was written down only after a few centuries). In Plato’s dialog Ion, Socrates (perhaps a sockpuppet of Plato) argued that “it is not by art that poets compose… but by divine apportionment”:\n\nFor the poets tell us that they carry honey to us from every quarter like bees, and they fly as bees do, sipping from honey-flowing fountains in glens and gardens of the Muses. And they tell the truth. For a poet is a delicate thing, winged and sacred, and unable to create until he becomes inspired and frenzied, his mind no longer in him; as long as he keeps his hold on that, no man can compose or chant prophecy. Since, then, it is not by art that poets compose and say many beautiful things about their subjects, as you do about Homer, but by divine apportionment, they each can do well only that to which the Muse directs them-this one dithyrambs, that one odes, or encomia, or dances, or epics, or iambics-each of them worthless in respect to the others.\n\nThe same point was made repeatedly in Plato’s corpus.\n\nJust as the rhapsode says what he says about Homer not by art but by divine apportionment, without intelligence (Ion 534b-c, 536c, 542a), so in the Meno (gge-looa) politicians get their virtue by divine apportionment, without intelligence; they have no more wisdom than seers and soothsayers, who say many fine things but know nothing of what they say; politicians are divine and inspired like poets, and possessed by the god (Meno 9gb-e). The irrational effects of poetry and rhapsody are directly comparable to the irrational effect of vulgar politics, whose servant is vulgar rhetoric (cf. Gorgias 502C).\n\nBoth quotes came from The Dialogues of Plato, Volume 3: Ion, Hippias Minor, Laches, Protagoras, translated by R. Allen. (I decided not to use one of the freely available versions since they tended to mistranslate “gods” as “God”.)\nFor example, in Phaedrus 245a, Socrates claimed that “the poetry of the sane man vanishes into nothingness before that of the inspired madmen”:\n\nAnd a third kind of possession and madness comes from the Muses. This takes hold upon a gentle and pure soul, arouses it and inspires it to songs and other poetry, and thus by adorning countless deeds of the ancients educates later generations. But he who without the divine madness comes to the doors of the Muses, confident that he will be a good poet by art, meets with no success, and the poetry of the sane man vanishes into nothingness before that of the inspired madmen.\n\n\n\nLater manifestations\nIsaac Newton thought he was merely recovering what the ancients have known all along. His friend William Stukeley described Newton as “the Great Restorer of True Philosophy”.\n\n\nApplication to machine creativity\n\n\n\nMachines as flowers for the DNA of creativity\nFrom Lovelace’s “Notes by the Translator”:\n\nThe Analytical Engine has no pretensions whatever to originate any thing. It can do whatever we know how to order it to perform. (source)\n\nIn his seminal paper “Computing machinery and intelligence” (1950), Alan Turing referenced Lovelace’s observation as the sixth objection to the possibility that machines might think. He then objected:\n\nThe view that machines cannot give rise to surprises is due, I believe, to a fallacy to which philosophers and mathematicians are particularly subject. This is the assumption that as soon as a fact is presented to a mind all consequences of that fact spring into the mind simultaneously with it. It is a very useful assumption under many circumstances, but one too easily forgets that it is false. (source).\n\nTuring was led to Lovelace’s objection by debates with Douglas Hartree, who in his book “Calculating Instruments and Machines” (page 70, 1949), quoted Lovelace approvingly. He objected using the phrase “electronic brain” for devices like electronic calculating machines or automatic pilots. He clarified that these machines cannot “think for themselves” and can only execute the instructions provided to them.\nThus, machines, in this view, are akin to flowers—organisms that reproduce and grow according to a predetermined genetic code but do not originate new genetic information on their own. Creativity, like DNA, must be instilled by a designer or operator, who programs the machine with the “genetic code” of what to create.\nAs a short etymological fun fact, the word “development” is a little capsule of the “flower for the DNA” idea:\n\nFirst use 1756, from French développement (“unrolling”). Compare with envelopment (“rolling”).\n\nThe idea is that of “opening up a scroll and showing what has always been written there. In the machines’ creative process can be seen as a similar”unrolling” of predetermined instructions or codes, much like the genetic “unrolling” in a blooming flower.\nThis is most explicitly manifest in the idea of preformationism, prevalent around 17th to 18th century. It seems the same intuitive appeals of preformationism apply to Lovelace’s objection.\n(A brief personal anecdote: When I was a kid, I thought bus cards contained tiny compressed coins inside, and when you “beep” them, those tiny coins fall into the machine through tiny openings on the card. Preformationism in economics!)"
  },
  {
    "objectID": "blog/posts/decline-of-mathematics/index.html",
    "href": "blog/posts/decline-of-mathematics/index.html",
    "title": "The Decline of Mathematical Fields",
    "section": "",
    "text": "In the 1880’s and 90’s the Theory of Invariants was seen to have unified many areas of mathematics, but by 1940 mathematicians, if asked, would have said the theory was dead. … most contemporary mathematicians have difficulty in naming one practitioner of the theory.\n(Fisher 1966)\n\n\nInvariant theory has already been pronounced dead several times, and like the phoenix it has been again and again rising from its ashes. The first period in the history of the theory culminated with the discovery of the so-called “symbolic method” which in theory allowed the computation of all invariants by a quasi-mechanical process, But it was soon realized that, except in a very few simple cases, the actual computation would lead to enormous labor, disproportionate with the interest of the outcome, especially in a period when all calculations were done by hand (it might be worthwhile to push the XIXth Century computations of invariants a little further along, with the help of modern computers). Partly for that reason, the next problem in the theory was the search for “fundamental systems” of invariants, i.e., finite sets such that any invariant would be a polynomial in the fundamental invariants. It is well known that the existence of such systems was proved by Hilbert in 1890, in a brilliant paper which made him famous overnight and which may be considered as the first paper in “modern algebra,” by its conceptual approach and methods. But Hilbert’s success also spelled the doom of XIXth Century invariant theory, which was left with no big problems to solve and soon faded into oblivion.\n(Dieudonné and Carrell 1970)\n\n\nHilbert’s paper did not immediately kill the subject, but rather acted as a progressive illness, beginning with an initial shock, and slowly consuming the computational body of the theory from within, so that by the early 1920’s the subject was clearly moribund. Abstraction ruled: the disciples of Emmy Noether, a student of Gordan, led the fight against the discredited computational empire, perhaps as a reaction to Noether’s original, onerous thesis topic that involved computing the invariants for a quartic form in three variables.\nClassical invariant theory, by Peter Olver 1999\n\n\n\nConsider all degree-2 homogeneous polynomials (over complex numbers). That is, consider functions like\n\\[\nf(z) = a_1 z_1^2 + a_2 z_1z_2 + a_3 z_2^2, \\quad a_1, a_2, a_3 \\in \\C.\n\\]\nEach such polynomial \\(f\\) is equivalent to a point in \\(\\C^3\\). As usual, we always try to hit the function with linear transforms if it simplifies the function. Let \\(A\\) be a linear transform, such that\n\\[\nA(z_1, z_2) = (A_{11}z_1 + A_{12}z_2, A_{21}z_1 + A_{22}z_2)\n\\]\nIt would not do if \\(A\\) collapses everything to zero, so we require \\(A\\) to be invertible. Further, we are only interested in solving \\(f=0\\), not the value of \\(f\\) itself. Therefore, scaling \\(A\\) by a constant does not matter, so we can remove this ambiguity by requiring \\(\\det A = 1\\). That is, we only consider the group \\(SL(2)\\).\nIn fact, we are not considering the whole space \\(\\C^3\\), but only the space of lines – the projective plane \\(\\P\\C^2\\). The idea can be visualized in real space \\(\\R^3\\) by first taking a homogeneous polynomial’s solutions can be found by first solving it on the unit sphere, then zoom it in and out to get all the whole solution.\nFor example, if we have a polynomial \\(f(x, y, z) = x^3 + y^2z + 2xyz\\), then its solution \\(f=0\\) is a surface in \\(\\R^3\\). We can solve the problem by first solving the surface’s intersection with the unit sphere, then for each point on the intersection, drawing a ray from the origin to the point. You can picture it thus: Take a steel ball, draw a curve on the surface with a marker pen, then drill in at each point on the curve, resulting in a cut-out cone.\n\n\n\nRings of a tree. You can solve a polynomial by finding the intersection of the surface with the bark of the tree, then draw a line from the center to each point.\n\n\nTheorem: Any invariant of \\(f\\) is divisible by the discriminant \\(\\Delta = a_2^2 - 4a_1a_3\\).\nHilbert’s basis theorem: For any form of polynomial, the space of invariants has a finite basis."
  },
  {
    "objectID": "blog/posts/decline-of-mathematics/index.html#invariant-theory",
    "href": "blog/posts/decline-of-mathematics/index.html#invariant-theory",
    "title": "The Decline of Mathematical Fields",
    "section": "",
    "text": "In the 1880’s and 90’s the Theory of Invariants was seen to have unified many areas of mathematics, but by 1940 mathematicians, if asked, would have said the theory was dead. … most contemporary mathematicians have difficulty in naming one practitioner of the theory.\n(Fisher 1966)\n\n\nInvariant theory has already been pronounced dead several times, and like the phoenix it has been again and again rising from its ashes. The first period in the history of the theory culminated with the discovery of the so-called “symbolic method” which in theory allowed the computation of all invariants by a quasi-mechanical process, But it was soon realized that, except in a very few simple cases, the actual computation would lead to enormous labor, disproportionate with the interest of the outcome, especially in a period when all calculations were done by hand (it might be worthwhile to push the XIXth Century computations of invariants a little further along, with the help of modern computers). Partly for that reason, the next problem in the theory was the search for “fundamental systems” of invariants, i.e., finite sets such that any invariant would be a polynomial in the fundamental invariants. It is well known that the existence of such systems was proved by Hilbert in 1890, in a brilliant paper which made him famous overnight and which may be considered as the first paper in “modern algebra,” by its conceptual approach and methods. But Hilbert’s success also spelled the doom of XIXth Century invariant theory, which was left with no big problems to solve and soon faded into oblivion.\n(Dieudonné and Carrell 1970)\n\n\nHilbert’s paper did not immediately kill the subject, but rather acted as a progressive illness, beginning with an initial shock, and slowly consuming the computational body of the theory from within, so that by the early 1920’s the subject was clearly moribund. Abstraction ruled: the disciples of Emmy Noether, a student of Gordan, led the fight against the discredited computational empire, perhaps as a reaction to Noether’s original, onerous thesis topic that involved computing the invariants for a quartic form in three variables.\nClassical invariant theory, by Peter Olver 1999\n\n\n\nConsider all degree-2 homogeneous polynomials (over complex numbers). That is, consider functions like\n\\[\nf(z) = a_1 z_1^2 + a_2 z_1z_2 + a_3 z_2^2, \\quad a_1, a_2, a_3 \\in \\C.\n\\]\nEach such polynomial \\(f\\) is equivalent to a point in \\(\\C^3\\). As usual, we always try to hit the function with linear transforms if it simplifies the function. Let \\(A\\) be a linear transform, such that\n\\[\nA(z_1, z_2) = (A_{11}z_1 + A_{12}z_2, A_{21}z_1 + A_{22}z_2)\n\\]\nIt would not do if \\(A\\) collapses everything to zero, so we require \\(A\\) to be invertible. Further, we are only interested in solving \\(f=0\\), not the value of \\(f\\) itself. Therefore, scaling \\(A\\) by a constant does not matter, so we can remove this ambiguity by requiring \\(\\det A = 1\\). That is, we only consider the group \\(SL(2)\\).\nIn fact, we are not considering the whole space \\(\\C^3\\), but only the space of lines – the projective plane \\(\\P\\C^2\\). The idea can be visualized in real space \\(\\R^3\\) by first taking a homogeneous polynomial’s solutions can be found by first solving it on the unit sphere, then zoom it in and out to get all the whole solution.\nFor example, if we have a polynomial \\(f(x, y, z) = x^3 + y^2z + 2xyz\\), then its solution \\(f=0\\) is a surface in \\(\\R^3\\). We can solve the problem by first solving the surface’s intersection with the unit sphere, then for each point on the intersection, drawing a ray from the origin to the point. You can picture it thus: Take a steel ball, draw a curve on the surface with a marker pen, then drill in at each point on the curve, resulting in a cut-out cone.\n\n\n\nRings of a tree. You can solve a polynomial by finding the intersection of the surface with the bark of the tree, then draw a line from the center to each point.\n\n\nTheorem: Any invariant of \\(f\\) is divisible by the discriminant \\(\\Delta = a_2^2 - 4a_1a_3\\).\nHilbert’s basis theorem: For any form of polynomial, the space of invariants has a finite basis."
  },
  {
    "objectID": "blog/posts/mathematical-phenomenology/index.html",
    "href": "blog/posts/mathematical-phenomenology/index.html",
    "title": "What does it feel like to be a mathematical object?",
    "section": "",
    "text": "TODO: change the folder name, and title, etc.\nStructuralism"
  },
  {
    "objectID": "blog/posts/principia-mathematicarum/index.html",
    "href": "blog/posts/principia-mathematicarum/index.html",
    "title": "Principia Philosophica Naturalium Mathematicarum",
    "section": "",
    "text": "# The world as theories and interpretations\nWhat is light? Newton said it was a particle1. Huygens said it was a wave. Schrödinger said it was both. Some clever fool said it was a wavicle. And Feynman said it was whatever helps you sleep at night shuts you up and lets you calculate.\nSo is light a particle, or a wave? None of these explanations satisfied me, so I figured it out for myself."
  },
  {
    "objectID": "blog/posts/principia-mathematicarum/index.html#the-wave-particle-duality",
    "href": "blog/posts/principia-mathematicarum/index.html#the-wave-particle-duality",
    "title": "Principia Philosophica Naturalium Mathematicarum",
    "section": "The wave-particle duality",
    "text": "The wave-particle duality\nThere are more than one wave-particle duality. In fact, there is nothing particularly two-ful or wave-ful or particle-ful about physics! Why are we so confused about wave-particle duality? I blame force of habit and evolution.\n\nA particle is a curve. That is, it is a function \\(\\gamma: (a, b) \\to \\mathcal M\\), where \\(a &lt; b\\) are real numbers, and \\(\\mathcal M\\) is a manifold.\nA field is a function \\(\\phi: \\mathcal M \\to Y\\), where \\(Y\\) is any manifold you like. We say the field is \\(Y\\)-valued.\nA scalar field is a real-valued field.\nA wave is a more confusing name for a field. Why \"confusing\"? Because the word \"wave\" makes people think the field must be \"going up and down\" here, there, or everywhere, but the fact is, a field can be exactly flat everywhere, and still be a wave! So why did physicists call it a \"wave\" when they really mean a field? Well, force of habit... back in the old days, the only field they knew of is the water-wave, which can be described mathematically as \\(h: \\R^2 \\to \\R\\), where \\(h(x)\\) is the height of water at location \\(x\\).\nA particle theory over a manifold \\(\\mathcal M\\) is a physical theory that states that certain paths in \\(\\mathcal M\\) are \"physical\" while others are \"unphysical\".\nA field theory over a manifold \\(\\mathcal M\\) is a physical theory that states that certain fields over \\(\\mathcal M\\) are \"physical\" while others are \"unphysical\".\nA wave theory is a field theory.\nA wave-particle duality over a manifold \\(\\mathcal M\\) is a tuple \\((T, T', f)\\). Here, \\(T\\) is a particle theory over \\(\\mathcal M\\), and \\(T'\\) is a field theory over \\(\\mathcal M\\), and \\(f\\) is an equivalence between \\(T, T'\\).\nA wave-equation is a differential equation satisfied by a field.\nAn equation of motion (of a particle) is a differential equation satisfied by a particle.\n\nIf our physical theory has a very special \"physical space\" \\(\\mathcal M\\), then a particle is a function \\(\\gamma: (a, b) \\to \\R\\times \\mathcal M\\), where \\(a &lt; b\\) are real numbers, and \\(\\R\\) represents time. In other words, a particle is nothing more and nothing less than a trajectory in spacetime. This is what we mean by a \"particle\" by default, even though sometimes we would deal with \"timeless particles\", for which time is meaningless, and a particle must instead be a function \\(\\gamma: (a, b) \\to \\mathcal M\\).\nTimeless particles? Why yes! That’s how we study geometric optics as the study of light-rays.\n\nIn geometric optics\n\n\nIn Hamiltonian mechanics\n\n\nIn quantum mechanics"
  },
  {
    "objectID": "blog/posts/principia-mathematicarum/index.html#crash-course-in-modern-theoretical-physics",
    "href": "blog/posts/principia-mathematicarum/index.html#crash-course-in-modern-theoretical-physics",
    "title": "Principia Philosophica Naturalium Mathematicarum",
    "section": "Crash course in modern theoretical physics",
    "text": "Crash course in modern theoretical physics\nRecall that a particle is a function \\(\\gamma: (a, b) \\to \\R\\times \\mathcal M\\). But what if we don’t use an interval \\((a, b)\\), but use a square, or a cylinder, or even a cube? This leads us to the idea of strings, branes, and other such fancy frontiers of theoretical physics.\n\nString theory\nA closed string is a function \\(\\mu: (a, b) \\times \\mathbb S^1 \\to \\mathcal M\\), where \\(a &lt; b\\) are real numbers, and \\(\\mathbb S^1\\) is the circle.\nAn open string is a function \\(\\mu: (a, b) \\times (0, 1)\\to \\mathcal M\\), where \\(a &lt; b\\) are real numbers.\nA string theory is made of two parts: one, the types of strings that it decides about; two, a way to decide if a string is physical or unphysical.\n\n\nHow to make your own variational physical theory\n\nFind a manifold \\(\\mathcal A\\).\nFind another manifold \\(\\mathcal B\\).\nDefine a family of functions of type \\(\\mu: \\mathcal A \\to \\mathcal B\\). This will be the domain of your physical theory.\nWrite down an action function \\(S(\\mu)\\).\nSpecify a way to \"vary \\(\\mu\\) infinitesimally\". Write that as \\(\\delta\\).\nDerive consequences of \\[\\delta S(\\mu) = 0.\\]\n\nFollowing the recipe, we immediately get Lagrangian mechanics and Hamiltonian mechanics.\nNow, we made a small sleight of hand in the recipe. Can you spot it? It is in steps 4 and 5. Namely, we have claimed that we can \"write down an action function\" and \"vary \\(\\mu\\) infinitesimally\". However, not every \\(\\mathcal A, \\mathcal B\\) has enough structure to allow us to do that. The art of doing theoretical physics is mostly in putting in enough structure in \\(\\mathcal A, \\mathcal B\\) so that you can define \\(S(\\mu)\\) and \\(\\delta S(\\mu)\\).\n\n\nHow to clothe your manifolds\n::: epigraph The world found nothing sacred in the abstract nakedness of being human.\nHannah Arendt, The origins of totalitarianism :::\nSince a mere manifold is not structured enough for defining actions and infinitesimal variations, we will \"clothe\" the manifolds with enough structures so that they do. To make this concrete, we will consider how we could construct Lagrangian mechanics and Hamiltonian mechanics according to the recipe.\nLagrangian\nHamiltonian\nAnd if you go deep into theoretical physics, you will eventually encounter Calabi–Yau manifolds, which are \"compact Kähler manifolds with a vanishing first Chern class and a Ricci-flat metric\". All these extra structures give them enough theoretical niceness for elegant string theories.\n\n\nExercise for the reader\nApply the recipe to your favorite manifolds, and get it published in a journal of physics with an impact factor of at least 2."
  },
  {
    "objectID": "blog/posts/principia-mathematicarum/index.html#interpretations-of-classical-mechanics",
    "href": "blog/posts/principia-mathematicarum/index.html#interpretations-of-classical-mechanics",
    "title": "Principia Philosophica Naturalium Mathematicarum",
    "section": "Interpretations of classical mechanics",
    "text": "Interpretations of classical mechanics\nFigure 3 shows six main interpretations of classical mechanics. They are all equivalent in some exact mathematical sense.\n\n\n\nSix main interpretations of classical mechanics.\n\n\n\nPosthuman classical mechanics\nWhat is it like to be a bat? What is it like to be a robot? The umwelt, seeing in infrared."
  },
  {
    "objectID": "blog/posts/principia-mathematicarum/index.html#interpretations-of-quantum-mechanics",
    "href": "blog/posts/principia-mathematicarum/index.html#interpretations-of-quantum-mechanics",
    "title": "Principia Philosophica Naturalium Mathematicarum",
    "section": "Interpretations of quantum mechanics",
    "text": "Interpretations of quantum mechanics"
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html",
    "title": "Hole Argument and Inverted Qualia",
    "section": "",
    "text": "problems\neasy\nhard\nmeta\n\n\n\n\nclassical gravity\nHow to model astronomical phenomena with some distribution of mass-points in \\(\\R^3\\)?\nOut of all the equivalent models, which one is the right one?\nWhy did Newton insist on absolute space, but Leibniz on relative space?\n\n\ngeneral relativity\nHow to model astronomical phenomena with some \\((\\mathcal M, g, T)\\)?\nHole argument: Out of all the isometric models, which one is the right one?\nWhy did Einstein and Hilbert fall for the hole argument?\n\n\ncolor\nPsychophysics: How to model human perception of color?\nQualia: Why does red feel like red?\nWhy are people prone to argue about the inverted spectrum?\n\n\nlanguage\nHow to model language use?\nWhy is language use associated with a feeling of understanding?\nWhy are people prone to argue about the Chinese room?\n\n\nconsciousness\nHow to explain objective phenomena associated with consciousness, such as attention, working memory, dreaming, etc?\nWhy does paying attention, dreaming, etc, feel like something?\nWhy are people prone to argue about the hard question?\n\n\n\n\n\n\nShifted qualia and the hole argument. Figure modified from (Norton, Pooley, and Read 1999) and Wikimedia Commons\n\n\n\n\n\nInverted qualia and Leibnitz’s inverted space. Figure modified from (Norton, Pooley, and Read 1999) and Wikimedia Commons"
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#section",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#section",
    "title": "Hole Argument and Inverted Qualia",
    "section": "",
    "text": "BIB.\n\nhttps://plato.stanford.edu/ENTRIES/spacetime-holearg/\n\nThe hole argument, in Norton’s formulation\n\nGiven two distributions of metric and material fields, related by a hole transformation, they are indeterminant by both observation and theory, since\n\nThe two distributions are observationally identical.\nThe laws of the theory cannot pick between the two developments of the fields into the hole.\n\nBut by manifold substantivalism, they represent distinct physical systems.\nTherefore, manifold substantivalism has a problematic metaphysics.\n\n\n\nMacdonald, Alan. “Einstein’s hole argument.” American Journal of Physics 69.2 (2001): 223-225.\n\n\n\nRelationalism\n\nThis is Einstein’s response, and also the typical response nowadays.\nThis line of thought can be traced back to Leibniz’s theory of relative space against Newton’s theory of absolute space. In a passage that predates the inverted qualia thought experiment, Leibniz imagined the “inverted space” thought experiment: Suppose at the moment of creation, God were to switch between East and West, nothing would act different. Since God must have created the world according to the principle of sufficient reason, God must have had no such degree of freedom in the first place – ergo, space is relational, not absolute.\n\nLeibniz’s third paper, in Samuel Clarke, A Collection of Papers, Which passed between the late Learned Mr. Leibnitz, and Dr. Clarke, In the Years 1715 and 1716 (London: 1717).\n\nWhile God or the principle of sufficient reason is no longer so assured, Leibniz’s thought experiment remains potent.\n\nThe hole argument as a prototype for gauge freedom.\n\nIf two criteria are true, then it is a gauge freedom, and indeterminancy in it is no longer an issue.\n\n\nverifiability—changes in the candidate surplus structure make no difference to what can be verified in observation;\n\n\ndeterminism—the laws of the theory are unable to fix the candidate surplus structure.\n\n\n\nMetric essentialism\n\nMaudlin (1990) urges that each spacetime event carries its metrical properties essentially; that is, it would not be that very event if (after redistribution of the fields) we tried to assign different metrical properties to it.\n\nNon-duality\n\nIt is quite possible to regard smooth manifold as derivative, and Riemannian manifold as original. Indeed, we may construct a smooth manifold by starting with \\(\\R^N\\) , then quotienting out smooth deformations.\nThe point is this: mathematically speaking, there is no necessary division between substance and property, though it is a very useful intuition. Physically speaking, the division has led to errors, such as Einstein’s failed 1914 attempt at non-relativistic theory of gravity.\nThis is especially true in the case of general relativity, because it developed from 19th century differential geometry, which tended to be understood as about geometry of deformable surfaces – where there is literally an unchanging substance (a rubber sheet) with changeable property (the strain field)."
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#inverted-qualia",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#inverted-qualia",
    "title": "Hole Argument and Inverted Qualia",
    "section": "Inverted qualia",
    "text": "Inverted qualia\n\nInverted qualia\nThe inverted qualia thought experiment has been used, like the philosophical zombie, in a whole host of arguments involving consciousness and the qualia. We consider the case involving functionalism, which is currently the most fashionable among cognitive psychologists and computer scientists. Other variants are reviewed in (Byrne 2004).\nAccording to functionalism, mental states are best understood as functional states, that is, mathematical functions that map perceptual inputs to behavioral outputs. It’s the intricate web of causal relations that constitutes a mental state, rather than the specific physical makeup realizing those relations.\nIn the anti-functionalism case, we consider two individuals, “Invert” and “Nonvert”, are functionally identical. They receive the same visual input (a tomato), undergo the same internal processing, and produce the same behavioral outputs (saying “that’s a red tomato”). However, their subjective experiences – their qualia – differ drastically. Where Nonvert sees red, Invert experiences green (or another color qualia entirely). They outwardly behave in the same way, and all functional measurements, from verbal reporting, psychological experiments, to MRI scanning, all find them the same, and yet the qualia of any color the Invert sees is rotated 180 degrees compared to that of the Nonvert.\nFormally:\n\nThe following spectrum inversion scenario is possible: Invert and Nonvert are functionally alike, and are both looking at a tomato.\nThus, the mental does not supervene on functional organization.\nThus, functionalism is false.\n\n\n\nPrecursors\n(Eastwood 1986)\nAlhazen had considered the theory that the eye works like a camera obscura, and he had pronounced it impossible, as it would create an inverted image. Similarly, da Vinci developed no less than 8 different hypothetical mechanisms inside the eye to invert the image again, so that the image would land on the retina right-side-up.\n\n\n\nLeonardo da Vinci’s drawings comparing the eye to a camera obscura. From Codex Atlanticus (1490-1495). Figure from Wikimedia Commons.\n\n\n\n… certain extravagant situations are to be avoided, as they would create ‘monstrosities’, or disfigurations. The concern about hypothetical monstrous results occurs at four points in the description. (1) If the refracting surface of the vitreous were not completely regular and spherical, a monstrous visual form would appear, (2) If the refracting surface of the vitreous were the surface of a small sphere, causing the intersection of rays before even reaching the centre of curvature of the cornea and the anterior glacial surface, once again there could occur a monstrous visual form. Presumably the disfiguration anticipated here by Alhazen is simply the inverted image after intersection, but he does not say.\n\n\n\n\nIllustration from Descartes’ Treatise of Man.\n\n\n\n\nAlternative examples\nOne objection from color science states that color space is not symmetric, and does not actually allow inversion as in the thought experiment. For example, saturated yellow does not merely look different from saturated red, but also looks brighter. In this view, “simply yellow” is not simply yellow. A point in color space is not simply a point. It is already inherently structured. Yellow is the brightest of all saturated colors, while violet is the dimmest, etc. (Hilbert and Kalderon 2000) argued that every possible quality space must be asymmetrical, in the sense that the only automorphism is the identity map, of \\(x \\mapsto x\\).\nThis appears to me an objection that is too strong, as there really do exist quality spaces that are symmetric. In humans, left and right are symmetric. Indeed, there are some highly symmetric quality spaces in nature.\nLight, being electromagnetic waves, can be polarized. The space of possible polarizations is isomorphic to a ball, the Poincare ball. The mantis shrimp species Gonodactylus smithii can detect the polarization of light over the entire 3-dimensional Poincaré ball (Kleinlogel and White 2008). It performs this by building 3 kinds of ommatidia, each specialized for two kinds of polarization. One is specialized for the horizontal-vertical, one for the diagonal-antidiagonal, and one for the clockwise-anticlockwise.\n\n\n\nPolarization states on the Poincaré ball. Figure from Wikimedia Commons.\n\n\nNow, a Gonodactylus philosopher might propose the following inverted qualia problem: What if my qualia on the Poincaré ball is inverted compared to yours? That is, what if when you see a horizontally polarized light, you feel the same way as I see a vertically polarized light, and similarly across all of the ball? We can even imagine more exotic reflections, such as one that reflects across the \\((0.3, 0.3, 0.9)\\) direction, etc."
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#color-space",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#color-space",
    "title": "Hole Argument and Inverted Qualia",
    "section": "Color space",
    "text": "Color space\n\nHuman eye structure\nSpectral sensitivity and response\n\\[I_L = \\int S_L(\\lambda) R(\\lambda) d\\lambda\\]\n\\(I_L\\) is the response intensity of long-wavelength-type cone cells, in units of neural spike per second.\n\\(R(\\lambda)\\) is the spectral radiance at wavelength \\(\\lambda\\), or spectrum for short. It has units of watt per square-nanometer (of retinal area) per nanometer (of wavelength).\n\\(S_L\\) is the spectral sensitivity function of the long-type cone cells.\nWe can similarly define \\(I_M, I_S\\), for the other two cone cell types (medium and short).\nIf we ignore the rod cells, and assume no adaptation to darkness (“scotopic vision”), then human color vision is essentially a deterministic function that maps a spectral flux density to three response intensities. Mathematically, it is a function\n\\[C(P) := (I_S(P), I_M(P), I_L(P))\\]\nwith type \\((\\R^+ \\to \\R^+) \\to (\\R^+)^3\\), where \\(\\R^+ = [0, \\infty)\\) is the space of non-negative real numbers.\nThe color space is the range of the function \\(C\\). Because \\(C\\) is a linear functional, color space is a convex cone-shaped subset of \\(\\R^3\\). On the edge of the cone are the pure colors, produced by spectra that are concentrated at just one wavelength. On the tip of the cone is \\((0, 0, 0)\\), the color of pure darkness.\nBecause the cone shape is uninteresting, the color space is typically represented by chopping off the cone midway, producing a roughly horseshoe-shaped region on \\(\\R^2\\), named the gamut. Mathematically, it is the projective transform: \\[(s, m, l) := \\left(\\frac{I_S}{I_S + I_M + I_L}, \\frac{I_M}{I_S + I_M + I_L}, \\frac{I_L}{I_S + I_M + I_L} \\right)\\]\nThe curving edge of the gamut are points of pure spectral colors, from pure 700 nm line on the red end, to the pure 400 nm line on the purple end. Every point inside the gamut can be mixed by two pure spectral colors.\nFor any three spectra \\(P_1, P_2, P_3\\), we can take any convex sum, and create a mixed color. The space of all colors created by their convex sum is the convex sum of \\(C(P_1), C(P_2), C(P_3)\\), which looks like a triangular cone. It intersects the gamut at a triangle. Every color inside the triangle can be created by mixing the three spectra, but any color outside cannot."
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#the-hole-argument",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#the-hole-argument",
    "title": "Hole Argument and Inverted Qualia",
    "section": "The hole argument",
    "text": "The hole argument\nIn general relativity, the hole argument is a thought experiment that apparently shows that general covariance is impossible. Einstein in late 1913, and David Hilbert in 1915, both fell into the hole argument. Misled by the hole argument, Einstein attempted to study theories of gravity that are not generally covariant, before finally giving up and . (Norton, Pooley, and Read 1999)\n\nA brief history of spacetime\nThe history of spacetime is a history of expanding symmetries.\nIn the most ancient cosmology of China, the earth is a square, while the sky is a half-bowl covering the earth. Each direction of earth – east, west, south, north – has a mystical significance. Up is not down, and east is not west. Not only that, there is a center of earth, somewhere in The Middle Kingdom. Thus, there is no spatial symmetry. The world was born an unspecified number of years in the past out of a chaotic egg. Thus, there is no temporal symmetry. Therefore, ancient Chinese spacetime is \\(\\R^1 \\times \\R^3\\), with no (nontrivial) symmetry.\nIn Aristotle’s physics, there is a center of the universe, where everything heavy (water and earth) is moving towards, and everything light (air and fire) is fleeing from. Other than that, space is spherically symmetric – he knew that earth is round. However, though space has a center, time is translation-invariant. Therefore, Aristotle’s spacetime is \\(\\R^1 \\times \\R^3\\) with symmetry group \\(\\R^1 \\times SO(3)\\), where \\(\\R^1\\) is the time-translation symmetry group, and \\(SO(3)\\) is the spherical symmetry group.\nThe Christian spacetime, with a beginning and an end for time, has a smaller symmetry group of \\(\\{0\\} \\times SO(3)\\).\nCopernicus and Kepler replaced the sun for the earth as the center of the universe, but they still insisted on a center. The first true breakthrough was Giordano Bruno’s infinite spacetime, where both space and time are infinite and without center. Therefore, Bruno’s spacetime has symmetry group \\(\\R^1 \\times E(3)\\), where \\(E(3)\\) is the 3D Euclidean symmetry group. That is, we allow all spatial translations.\nGalileo braided together space and time, resulting in an even bigger symmetry group. Specifically, he argued that the universe does not have a special “at rest” velocity. To see why this is a breakthrough, consider what happens in Bruno’s universe. In Bruno’s universe, it matters whether you are staying still, or moving at \\(1 \\;\\mathrm{m/s}\\) relative to the universe. If you are staying still, then your trajectory is like \\(\\dots, (t_0, p), (t_1, p), (t_2, p), \\dots\\). Metaphorically speaking, you are sitting somewhere in the cinema of spacetime, and you can “peek at the number of your seat” and see that you have been sitting at the same space-point \\(p\\). However, if you are moving relative to the universe, you can see that your space-point is changing.\nGalileo rejects this. There is no “space-point”. You can take a slice of the universe at \\(t=0\\), and another slice of the universe at \\(t=1\\), but you cannot point at a point in each slice and ask, “Are these two points the same space-point?”.\nNewton’s concept of spacetime is harder to conceptualize. He understood Galileo’s point, and initially attempted to model spacetime the same way as Galileo did. However, for obscure reasons, he reintroduced absolute space and time. This had strange consequences that Leibnitz relentlessly criticized. Consider the inertial frame, relative to which the sun is standing still at this moment. Now consider another inertial frame, moving at \\(1 \\;\\mathrm{m/s}\\) in the direction of Sun-to-Mars at this moment. The laws of Newtonian mechanics are the same, and no observation or experiment could tell us whether one of them is the “absolute frame”, or neither of them is. Yet, out of the infinitely many inertial frames, Newton designated precisely one of them as the “absolute”, and all others are defined as those moving at constant velocity relative to the absolute frame. (DiSalle 2020)\nTo dramatize this seemingly arcane point, consider the following imaginary conversation between Newton and Leibniz about a grant proposal to find the absolute frame:\n\nThis is a proposal to find the absolute frame…\nHow would you find it? The only difference between absolute and non-absolute inertial frames is that one of them is designated so. Do the stars turn perfectly white when you are standing still in the absolute frame? Do the music of the spheres tune to a perfect pitch? Do you see it in your mind’s eye? And even if you do, what if every material point in the universe, by an act of God, were set off in this direction [points finger up] at one mile per day? Would anything seem amiss? You yourself admit that the very sustenance of the universe requires continuous divine forcing, that the stars would have collapsed to the same point otherwise.1\n… yet all relative frames has no existence without assuming the absolute frame, for otherwise, one would fall to a circular argument, where relative frames are relative to nought but each other, and the very meaning of inertiality becomes vacuous. One might as well The absolute might be hidden, but it is out there.\n1 \nIn the 1726 edition of the General Scholium, Newton added a new sentence: “And so that the systems of the fixed stars will not fall upon one another as a result of their gravity, he has placed them at immense distances from one another.” Once again, the implication is that gravity can be a destabilising force. An annotation in Newton’s copy of the 1713 edition after the words “send light into all the others” shows that he had considered an even more theologically powerful statement: “and the fixed stars would, through their gravity, gradually fall on each other, were they not carried back by the counsel of the supreme Being.”\n(Snobelen 2020)\n\nHe also believed that the Great Comet of 1680 would someday fall into the sun, causing a solar flare-up that would kill all life on earth. God would then repopulate earth. In general, he thought the universe as an unstable system requiring constant divine support. (Snobelen 2020)\nWith special relativity, the symmetry of spacetime becomes \\(SO(3, 1)\\), which is in a sense more “braided” than Galilean relativity. In Galilean relativity, the symmetry group of spacetime factors into a direct product between the symmetry group of space, and the symmetry group of time. In special relativity, the symmetry group of spacetime cannot be factored into a direct product. This is the deep meaning of Minkowski’s claim that “space for itself, and time for itself shall completely reduce to a mere shadow”.\nFor general relativity, any diffeomorphism on spacetime is a symmetry.2 In other words, it is a generally covariant theory. This is quite a vast generalization, and warrants further details.\n2 A function is a diffeomorphism iff itis one-to-one, smooth, and has a smooth inverse.\n\nGeneral relativity\nGeneral relativity models spacetime as a manifold \\(\\mathcal M\\), with a metric tensor field \\(g_{\\mu\\nu}\\) and an energy-momentum tensor field \\(T_{\\mu\\nu}\\). The metric tensor describes the spacetime separation between points on the manifold, and thereby the geometry of spacetime. The energy-momentum tensor describes the flow of energy and momentum in spacetime. In particular, a body with mass \\(m\\), such as a black hole, is a flow of energy \\(mc^2\\) in time, and therefore can be described within the energy-momentum tensor.\nThe metric tensor field and the energy-momentum tensor field are “braided together” by Einstein’s field equation:\n\\[\n(\\text{a polynomial equation involving components of }g) = \\kappa T\n\\]\nwhere \\(\\kappa\\) is a constant of nature, measured by experiments.\nThe spacetime manifold \\(\\mathcal M\\) can be transformed, in that we can write down a function \\(f: \\mathcal M \\to \\mathcal M\\), such that it maps one point in the manifold to another point. According to general relativity, if \\(f\\) is a diffeomorphism, then the field equation is unchanged. In this sense, all diffeomorphisms of \\(\\R^4\\) become symmetries of spacetime. Whereas in special relativity, inertial frames are distinguished from non-inertial frames, in that the coordinate lines in an inertial frame are deemed “straight”, no one gets special treatment in general relativity, and any smooth coordinate system is as good as any other. That is, Einstein’s field equation is generally covariant.\n\n\nProblem of time\nThe old couplet about general relativity goes like “Matter tells space-time how to curve, and space-time tells matter how to move.” but this is often misunderstood as saying “Matter exists, then space-time reacts to matter, and then matter reacts to space-time by changing its motion.” This fundamentally misunderstands what general relativity is. There is no time nor causality, at least as commonly understood, in special or general relativity.\nSpecial relativity is typically interpreted as an “eternalist” or “four-dimensionalist” theory. That is, all of space and time exist in the same way, and the future is as real as the past. Einstein said it as “the distinction between past, present, and future is only a stubbornly persistent illusion.”. It is typically supported by the Rietdijk–Putnam argument, as follows.\nWhereas in Newtonian spacetime, one can still imagine that the universe somehow “grows one time-slice at a time” – though this is susceptible to McTaggart’s objection – in special relativity, there does not exist such a thing as “time-slice”, because there is no absolute simultaneity. We may pick the time-slices in the inertial frame of the solar system, or in that of the Andromeda galaxy. However, just like how no observation can distinguish Newton’s absolute inertial frame from all the other relative inertial frames, no observation can distinguish between the absolute simultaneity from all the other relative simultaneities.\nThe difficulty is only amplified in general relativity. Let us imagine a universe that is swirling with stars and galaxies. Locally, the spacetime manifold is curved, but globally, it is topologically the same as \\(\\R^4\\) – no loops, no singularities, and no wormholes. Now, construct a coordinate system \\((t, x, y, z)\\). We can then select a “snapshot” of the universe by selecting the 3D submanifold at \\(t = 0\\). If we know the exact value of \\(g, T\\) on that snapshot, then we can crank the Einstein field equations to solve for \\(g, T\\) for all \\(t &gt; 0\\). Does this mean that the slice of \\(t=0\\) determines what happens afterwards?\nNot really. We could have smoothly distorted the coordinate system to \\((t', x', y', z')\\), and solve the Einstein field equations for all \\(t' &gt; 0\\). There are infinitely more degrees of freedom compared to special relativity, making the RP argument bite harder.\nYet, the issue goes even deeper. We could very well select \\(t = 10000\\) and crank the field equations to solve for \\(g, T\\) for all \\(t &lt; 10000\\). Does this mean that “the future determines the past?” Perhaps we can compromise by saying “one point in time determines both the past and the future”, but even that is not necessarily true. We can design much wilder boundary conditions. We can make two lightcones determine the rest of the universe (double-null, or Sachs), make one lightcone plus a “left side” of the universe determine the rest (null-timelike, or Winicour–Tamburino), make half of the universe’s left-side and half of the universe’s \\(t=0\\) determine the rest, etc.\n\n\n\nDifferent initial value conditions. The first is the commonly used Cauchy condition, but the others, more exotic, are also valid. Figure modified from (d’Inverno 1984)\n\n\n\n\nThe hole argument\nDuring 1912, Einstein struggled with finding a generally covariant field equation for gravity. He even considered the one he would eventually publish in 1915 and be famous for, but gave them up over certain difficulties. Then in late 1913, he tried to turn this loss into a victory by arguing that general covariance is not the right approach, because of the hole argument. (Norton, Pooley, and Read 1999)\nConsider the following two models of a small universe. The universe contains three galaxies moving away from each other. The model on the left shows that one of the galaxies passes the spacetime-point \\(E\\), while the model on the right shows that no galaxy passes the spacetime-point \\(E\\).\nIf the universe satisfies a generally covariant field equation, then we can transform the model on the left to the model on the right by a diffeomorphism, and the equation would be none the wiser. In other words, any generally covariant field equation suffers from rampant indeterminism.\n\n\n\nEinstein’s hole argument. Figure from (Norton, Pooley, and Read 1999)\n\n\nNow, the spacetime manifold is supposed to\nThe hole argument, in Norton’s formulation\n\nGiven two distributions of metric and material fields, related by a hole transformation, they are indeterminant by both observation and theory, since\n\nThe two distributions are observationally identical. The laws of the theory cannot pick between the two developments of the fields into the hole.\nBut by manifold substantivalism, they represent distinct physical systems.\n\nTherefore, manifold substantivalism has a problematic metaphysics.\n\n\n\nResponses to the hole argument\nRelationalism. This is Einstein’s response, and also the typical response nowadays.\nThis line of thought can be traced back to Leibniz’s theory of relative space against Newton’s theory of absolute space. In Leibniz’s third paper during the Leibniz–Clarke correspondence (Clarke 1717), Leibniz proposed the “inverted space” thought experiment:3 Suppose at the moment of creation, God were to switch between East and West, nothing would act different. Since God must have created the world according to the principle of sufficient reason, God must have had no such degree of freedom in the first place. Ergo, space is relational, not absolute.\n3 \n… supposing Space to be Something in it self, besides the Order of Bodies among themselves, that ’tis impossible there should be a Reason, why God, preserving the same Situations of Bodies among themselves, should have placed them in Space after one certain particular manner, and not otherwise; why every thing was not placed the quite contrary way, for instance, by changing East into West.\n(Clarke 1717)\n\nWhile God or the principle of sufficient reason is no longer so assured, Leibniz’s thought experiment remains potent.\nGauge freedom. If two criteria are true, then it is a gauge freedom, and indeterminancy in it is no longer an issue.(Tao 2008)\n\nverifiability—changes in the candidate surplus structure make no difference to what can be verified in observation;\ndeterminism—the laws of the theory are unable to fix the candidate surplus structure.\n\nMetric essentialism. Maudlin (1990) urges that each spacetime event carries its metrical properties essentially; that is, it would not be that very event if (after redistribution of the fields) we tried to assign different metrical properties to it.\nNon-duality. It is quite possible to regard smooth manifold as derivative, and Riemannian manifold as original. Indeed, we may construct a smooth manifold by starting with \\(\\R^N\\), then quotienting out smooth deformations.\nThe point is this: mathematically speaking, there is no necessary division between substance and property, though it is a very useful intuition. Physically speaking, the division has led to errors, such as Einstein’s failed 1914 attempt at non-relativistic theory of gravity.\nThis is especially true in the case of general relativity, because it developed from 19th century differential geometry, which tended to be understood as about geometry of deformable surfaces, where there is literally an unchanging substance (a rubber sheet) with changeable property (the strain field)."
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#example-calculation",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#example-calculation",
    "title": "Hole Argument and Inverted Qualia",
    "section": "example calculation",
    "text": "example calculation\nMacdonald, Alan. “Einstein’s hole argument.” American Journal of Physics 69.2 (2001): 223-225."
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#responses",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#responses",
    "title": "Hole Argument and Inverted Qualia",
    "section": "responses",
    "text": "responses\nRelationalism\nThis is Einstein’s response, and also the typical response nowadays.\nThis line of thought can be traced back to Leibniz’s theory of relative space against Newton’s theory of absolute space. In a passage that predates the inverted qualia thought experiment, Leibniz imagined the “inverted space” thought experiment: Suppose at the moment of creation, God were to switch between East and West, nothing would act different. Since God must have created the world according to the principle of sufficient reason, God must have had no such degree of freedom in the first place -\nergo, space is relational, not absolute.\nLeibniz’s third paper, in Samuel Clarke, A Collection of Papers, Which passed between the late Learned Mr. Leibnitz, and Dr. Clarke, In the Years 1715 and 1716 (London: 1717).\nWhile God or the principle of sufficient reason is no longer so assured, Leibniz’s thought experiment remains potent.\nThe hole argument as a prototype for gauge freedom.\nIf two criteria are true, then it is a gauge freedom, and indeterminancy in it is no longer an issue.\n\nverifiability—changes in the candidate surplus structure make no difference to what can be verified in observation;\ndeterminism—the laws of the theory are unable to fix the candidate surplus structure.\n\nMetric essentialism\nMaudlin (1990) urges that each spacetime event carries its metrical properties essentially; that is, it would not be that very event if (after redistribution of the fields) we tried to assign different metrical properties to it.\nNon-duality\nIt is quite possible to regard smooth manifold as derivative, and Riemannian manifold as original. Indeed, we may construct a smooth manifold by starting with \\(\\R^N\\) , then quotienting out smooth deformations.\nThe point is this: mathematically speaking, there is no necessary division between substance and property, though it is a very useful intuition. Physically speaking, the division has led to errors, such as Einstein’s failed 1914 attempt at non-relativistic theory of gravity.\nThis is especially true in the case of general relativity, because it developed from 19th century differential geometry, which tended to be understood as about geometry of deformable surfaces -\nwhere there is literally an unchanging substance (a rubber sheet) with changeable property (the strain field)."
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#human-eye-structure",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#human-eye-structure",
    "title": "Hole Argument and Inverted Qualia",
    "section": "Human eye structure",
    "text": "Human eye structure\nSpectral sensitivity and response\n\\[I_L = \\int S_L(\\lambda) R(\\lambda) d\\lambda\\]\n\\(I_L\\) is the response intensity of long-wavelength-type cone cells, in units of neural spike per second.\n\\(R(\\lambda)\\) is the spectral radiance at wavelength \\(\\lambda\\), or spectrum for short. It has units of watt per square-nanometer (of retinal area) per nanometer (of wavelength).\n\\(S_L\\) is the spectral sensitivity function of the long-type cone cells.\nWe can similarly define \\(I_M, I_S\\), for the other two cone cell types (medium and short).\nIf we ignore the rod cells, and assume no adaptation to darkness (“scotopic vision”), then human color vision is essentially a deterministic function that maps a spectral flux density to three response intensities. Mathematically, it is a function of type:\n\\[C: (\\R^+ \\to \\R^+) \\to \\R^3\\]\nwhere \\(\\R^+\\) means the space of non-negative real numbers. It is defined by\n\\[C(P) = (I_S(P), I_M(P), I_L(P))\\]\nDefine the color space as the range of the function \\(C\\). Because \\(C\\) is a linear functional, color space is a convex cone-shaped subset of \\(\\R^3\\). On the edge of the cone are the pure colors, produced by spectra that are concentrated at just one wavelength. On the tip of the cone is \\((0, 0, 0)\\), the color of pure darkness."
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#cie-color-spaces",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#cie-color-spaces",
    "title": "Hole Argument and Inverted Qualia",
    "section": "CIE color spaces",
    "text": "CIE color spaces\nBecause the cone shape is uninteresting, the color space is typically represented by chopping off the cone midway, producing a roughly horseshoe-shaped region on \\(\\R^2\\), named the gamut. Mathematically, it is the projective transform: \\[(s, m, l) := \\left(\\frac{I_S}{I_S + I_M + I_L}, \\frac{I_M}{I_S + I_M + I_L}, \\frac{I_L}{I_S + I_M + I_L} \\right)\\]\nThe curving edge of the gamut are points of pure spectral colors -\nfrom pure 700 nm line on the red end, to the pure 400 nm line on the purple end. Every point inside the gamut can be mixed by two pure spectral colors.\nFor any three spectra \\(P_1, P_2, P_3\\), we can take any convex sum, and create a mixed color. The space of all colors created by their convex sum is the convex sum of \\(C(P_1), C(P_2), C(P_3)\\), which looks like a triangular cone. It intersects the gamut at a triangle. Every color inside the triangle can be created by mixing the three spectra, but any color outside cannot.\nSubsequent color spaces are just minor adjustments to ICE 1931.\nIt defines three numbers \\(X, Y, Z\\). It was created before human cone cells were discovered. However, it turns out to be almost exactly a linear transform of \\(I_S, I_M, I_L\\), by \\[\n\\begin{bmatrix}\nX\\\\Y\\\\Z\n\\end{bmatrix}\n=\n\\left[\\begin{aligned}\n  1&.910\\,20 \\!\\!\\!&\\!\\! -1&.112\\,12 \\!\\!\\!&\\!\\! 0&.201\\,91 \\\\\n  0&.370\\,95 \\!\\!\\!&\\!\\!  0&.629\\,05 \\!\\!\\!&\\!\\! 0&         \\\\\n  0&         \\!\\!\\!&\\!\\!  0&         \\!\\!\\!&\\!\\! 1&.000\\,00\n\\end{aligned}\\right]\n\\begin{bmatrix}\nI_L\\\\I_M\\\\I_S\n\\end{bmatrix}\n\\]\nThe color gamut of CIE 1931 is defined by the projective transform\n\\[(x, y, z) := \\left(\\frac{X}{X+Y+Z},\\frac{Y}{X+Y+Z},\\frac{Z}{X+Y+Z}\\right)\\]\nusually plotted over the \\(x+y+z = 1\\) plane.\nThe RGB values required to match 1 unit of energy at each wavelength.\n700, 546.1, and 435.8 nm"
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#riemannian-metric",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#riemannian-metric",
    "title": "Hole Argument and Inverted Qualia",
    "section": "Riemannian metric",
    "text": "Riemannian metric\nThe MacAdam ellipses …\n\n\n\nMacAdam ellipses the CIE 1931 \\(xy\\)-diagram, 10× actual size. Figure from Wikimedia Commons.\n\n\n\n\n\n(da Fonseca and Samengo 2016, fig. 8b)\n\n\nFrom the \\(S_S, S_M, S_L\\) curves, we can use information theory to predict the JND in color space.\n(da Fonseca and Samengo 2016) explain ~87% of the variance of human color discrimination ability\nImagine a hiker navigating a mountain path equipped only with an altimeter and a detailed altitude map. The hiker’s ability to pinpoint their location on the map relies on sensing altitude changes. In regions where the terrain is steep (representing high sensitivity), even a small step forward (change in stimulus intensity) will register a noticeable altitude change on the altimeter (change in perceived sensation). This allows for precise localization – a small JND. However, along flatter sections of the trail (low sensitivity), the hiker might need to traverse a longer distance to observe a meaningful altitude difference, leading to a larger JND and greater uncertainty about their position on the map.\nIf the phenomenology of color is but the metric geometry of color space, then we have not only dispelled the inverted qualia problem, but also have dispelled the problem of other minds. We should be able to construct the phenomenal geometry of color from retinal measurements. By comparing my retina and your retina, we can objectively determine whether my green is the same as yours.\nIf the inverted qualia problem is dissolved by the hole argument, then the hard problem of color perception is gone as well. What remains is merely determining the metric of color space. In this way, the biophysics of the eye-brain system would mostly solve the easy problem of color perception.\nNote that, while color space is locally Riemannian, this is not so over longer distances. That is, once we are measuring the subjective distances between pairs of far-different colors, the data no longer behave like distances on a curved 3D space. (Bujack et al. 2022) reported that there is “diminishing returns” in color distances. Color difference might not even be symmetric, meaning that if we ask a subject “How far is color 1 from color 2?” and then ask the opposite direction, we might get a different answer. This reminds me of KL divergence."
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#arguments",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#arguments",
    "title": "Hole Argument and Inverted Qualia",
    "section": "arguments",
    "text": "arguments\nThe inverted qualia thought experiment has been used, like p-zombie argument, in a whole host of arguments. Let’s deal with functionalism, which seems rather urgent these days, what with the advent of AI and all.\nArgument against functionalism\nThe following spectrum inversion scenario is possible: Invert and Nonvert are functionally alike, and are both looking at a tomato.\nThus, the mental does not supervene on functional organization.\nThus, functionalism is false.\nAccording to functionalism, mental states are functional states: states defined by their causal role with respect to inputs, outputs, and other states. So, according to functionalism, necessarily, two creatures who are functionally alike are also mentally alike.\nAlhazen, Leonardo, and Late-Medieval Speculation on the Inversion of Images in the Eye (1986)\nAlhazen had considered the theory that the eye works like a camera obscura, and he had pronounced it impossible, as it would create an inverted image. Similarly, da Vinci developed no less than 8 different hypothetical mechanisms inside the eye to invert the image again, so that the image would land on the retina right-side-up.\n\n\n\nimage.png\n\n\n[File:1490-95 da vinci\ncodex atlanticus.jpg\nWikimedia Commons](https://commons.wikimedia.org/wiki/File:1490-95_da_vinci_-_codex_atlanticus.jpg) - &gt; … certain extravagant situations are to be avoided, as they would create ‘monstrosities’, or disfigurations. The concern about hypothetical monstrous results occurs at four points in the description. (1) If the refracting surface of the vitreous were not completely regular and spherical, a monstrous visual form would appear, (2) If the refracting surface of the vitreous were the surface of a small sphere, causing the intersection of rays before even reaching the centre of curvature of the cornea and the anterior glacial surface, once again there could occur a monstrous visual form. Presumably the disfiguration anticipated here by Alhazen is simply the inverted image after intersection, but he does not say."
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#problem-of-consciousness",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#problem-of-consciousness",
    "title": "Hole Argument and Inverted Qualia",
    "section": "Problem of consciousness",
    "text": "Problem of consciousness\n(Metzinger 2004, chapter TODO; Roden 2015, chap. 4)\nThe just noticeable difference (JND) in color perception possibly shows that we see metric, not colors themselves.\nIf the phenomenology of color is but the metric geometry of color space, then we have not only dispelled the inverted qualia problem, but also have dispelled the problem of other minds. We should be able to construct the phenomenal geometry of color from retinal measurements. By comparing my retina and your retina, we can objectively determine whether my green is the same as yours.\nIf the inverted qualia problem is dissolved by the hole argument, then the hard problem of color perception is gone as well. What remains is merely determining the metric of color space. In this way, the biophysics of the eye-brain system would mostly solve the easy problem of color perception.\nThe meta-problem of consciousness (Chalmers 2018) is TODO\nThe inverted polarization spectrum for mantis shrimps. Why would they be confused?\n(Kleinlogel and White 2008)\nThe mantis shrimp species Gonodactylus smithii can detect polarization of light over the entire 3-dimensional Poincare sphere. It performs this by building 3 kinds of ommatidia, each specialized for two kinds of polarization. One is specialized for the horizontal-vertical, one for the diagonal-antidiagonal, and one for the clockwise-anticlockwise.\nNow, a Gonodactylus philosopher might propose the following inverted qualia problem: What if my qualia on the Poincare sphere is inverted compared to yours? When you see a horizontally polarized light, you feel the same way as I see a vertically polarized light, etc."
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#meta-problem-of-consciousness",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#meta-problem-of-consciousness",
    "title": "Hole Argument and Inverted Qualia",
    "section": "meta-problem of consciousness",
    "text": "meta-problem of consciousness\nOn the psychological origins of dualism: Dual-process cognition and the explanatory gap (2012)\nOn the psychological origins of dualism: Dual-process cognition and the explanatory gap (2012) collapsed:: true\nWe have Type 1 and Type 2 cognitive processes for judging if something is conscious.\nType 1 processes\nPicture\n\n\n\nimage.png\n\n\nfast, domain-specific, automatic (the authors don’t argue if they are also associative)\nThree apparent features reliably produce AGENT categorization:\nhas eye-like shapes on a head-like bump;\nreacts to the environment unpredictably;\nmoves on its own, not a slave to mere inertia.\nConfirmed by judgment-speed experiments - &gt; … presented subjects with a sequence of Object/Attribution pairs (e.g., ant/feels pain), and the subjects were asked to respond as quickly as possible (Yes or No) whether the object had the attribute… Participants responded significantly more slowly when they denied conscious states to objects that do have the superficial AGENCY cues, namely, insects. This result is neatly explained by our hypothesis that insects automatically activate the low road to consciousness attribution; in order to deny that insects have conscious states, subjects had to “override” the low-road output, which explains why reaction times are slower in such cases.\nType 2 processes\nrational deliberation, theory application, or conscious reasoning\nAny brain-based physical theory of consciousness can at most convince Type 2 process, not the Type 1 process.\nThe brain doesn’t have eyes\nThe brain seems to do nothing by itself, stewing alone in a dark cave;\nThe brain doesn’t display any motion, let alone non-inertial motion. - &gt; Since the two systems generate the same answer in typical cases, there is typically no resistance to the idea that other people are conscious. However, when we consider the mass of grey matter that composes the human brain (and on which the majority of physicalist reductions of consciousness will focus), the result is altogether different.\n- &gt; Jenny might believe that consciousness is identical to populations of neurons firing in synchrony at a rate between 40Hz and 60Hz; on this basis she could infer (using the high road) that specific brain regions that are firing synchronously are conscious experiences. (Crick & Koch, 1990). If Jenny knew that Jimmy’s brain had regions that were firing synchronously between 40-60Hz, she could infer (using the high road) that Jimmy’s brain states are conscious experiences. But since this description of Jimmy’s brain does not advert to any of the featural cues that trigger AGENCY categorization, Jenny’s low road is not activated, and thus remains silent on whether the synchronously firing neurons are conscious\nAlternative physicalist theory of consciousness designed to satisfy Type 1 process won’t satisfy Type 2 process.\nThe eyes are clearly unnecessary, as even eyeless people (Anophthalmia) can be conscious.\nAnd lock-in syndrome people don’t interact and don’t display noninertial motions.\nEvolutionary origin of the dual process\nOnly very recently is it possible to see neurons firing. So for millions of years, humans could have only inferred consciousness through external features, such as eye motion, non-inertial motion, etc.\nThus arose Type 1 process for detecting consciousness, which is incompatible with modern neuroscience.\nThe Type 2 process is a general process for understanding abstract theories, not for consciousness specifically.\n(Non-)Analogies\nThe authors thought that there is no Type 1 intuition for general relativity, so there’s no explanatory gap there. But I beg to differ.\nGeneral Relativity is acceptable to Type 2 processes, but not to Type 1 processes, which has an intuitive understanding of the world as having Newtonian spacetime. Consequently, there is a persistent “explanatory gap”, as a nagging feeling “but how do we know which one is the real spacetime manifold? The theory is incomplete because it doesn’t tell us that.”.\nThis is probably what made the hole argument so perplexing even to Einstein. The hole argument appeals to the Type 1 intuition that there’s a unique spacetime structure.\nAs another example, Bergson famously debated Einstein over the nature of time.\nIntentionality explanatory gap.\nSome philosophers did propose an explanatory gap.\nAlthough most people seem to have no difficulty granting intentionality to computers and other things they regard as unconscious.\nThis would be explainable if the Type 1 process for intentionality-attribution happens to fit well with modern physics. -"
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#the-meta-problem",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#the-meta-problem",
    "title": "Hole Argument and Inverted Qualia",
    "section": "The meta-problem",
    "text": "The meta-problem\nWe have Type 1 and Type 2 cognitive processes for judging if something is conscious.\nType 1 processes\n(Fiala, Arico, and Nichols 2012)\nfast, domain-specific, automatic (the authors don’t argue if they are also associative)\nThree apparent features reliably produce AGENT categorization:\nhas eye-like shapes on a head-like bump;\nreacts to the environment unpredictably;\nmoves on its own, not a slave to mere inertia.\nConfirmed by judgment-speed experiments\n\n… presented subjects with a sequence of Object/Attribution pairs (e.g., ant/feels pain), and the subjects were asked to respond as quickly as possible (Yes or No) whether the object had the attribute… Participants responded significantly more slowly when they denied conscious states to objects that do have the superficial AGENCY cues, namely, insects. This result is neatly explained by our hypothesis that insects automatically activate the low road to consciousness attribution; in order to deny that insects have conscious states, subjects had to “override” the low-road output, which explains why reaction times are slower in such cases.\n\nType 2 processes\nrational deliberation, theory application, or conscious reasoning\nAny brain-based physical theory of consciousness can at most convince Type 2 process, not the Type 1 process.\nThe brain doesn’t have eyes\nThe brain seems to do nothing by itself, stewing alone in a dark cave;\nThe brain doesn’t display any motion, let alone non-inertial motion.\n\nSince the two systems generate the same answer in typical cases, there is typically no resistance to the idea that other people are conscious. However, when we consider the mass of grey matter that composes the human brain (and on which the majority of physicalist reductions of consciousness will focus), the result is altogether different.\n\n\nJenny might believe that consciousness is identical to populations of neurons firing in synchrony at a rate between 40Hz and 60Hz; on this basis she could infer (using the high road) that specific brain regions that are firing synchronously are conscious experiences. (Crick & Koch, 1990). If Jenny knew that Jimmy’s brain had regions that were firing synchronously between 40-60Hz, she could infer (using the high road) that Jimmy’s brain states are conscious experiences. But since this description of Jimmy’s brain does not advert to any of the featural cues that trigger AGENCY categorization, Jenny’s low road is not activated, and thus remains silent on whether the synchronously firing neurons are conscious\n\nAlternative physicalist theory of consciousness designed to satisfy Type 1 process won’t satisfy Type 2 process.\nThe eyes are clearly unnecessary, as even eyeless people (Anophthalmia) can be conscious.\nAnd lock-in syndrome people don’t interact and don’t display noninertial motions.\nEvolutionary origin of the dual process\nOnly very recently is it possible to see neurons firing. So for millions of years, humans could have only inferred consciousness through external features, such as eye motion, non-inertial motion, etc.\nThus arose Type 1 process for detecting consciousness, which is incompatible with modern neuroscience.\nThe Type 2 process is a general process for understanding abstract theories, not for consciousness specifically.\n(Non-)Analogies\nThe authors thought that there is no Type 1 intuition for general relativity, so there’s no explanatory gap there. But I beg to differ.\nGeneral Relativity is acceptable to Type 2 processes, but not to Type 1 processes, which has an intuitive understanding of the world as having Newtonian spacetime. Consequently, there is a persistent “explanatory gap”, as a nagging feeling “but how do we know which one is the real spacetime manifold? The theory is incomplete because it doesn’t tell us that.”.\nThis is probably what made the hole argument so perplexing even to Einstein. The hole argument appeals to the Type 1 intuition that there’s a unique spacetime structure.\nAs another example, Bergson famously debated Einstein over the nature of time.\nIntentionality explanatory gap.\nSome philosophers did propose an explanatory gap.\nAlthough most people seem to have no difficulty granting intentionality to computers and other things they regard as unconscious.\nThis would be explainable if the Type 1 process for intentionality-attribution happens to fit well with modern physics."
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#the-geometry-of-color",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#the-geometry-of-color",
    "title": "Hole Argument and Inverted Qualia",
    "section": "The geometry of color",
    "text": "The geometry of color\n\nSmooth geometry\nThe human eye, abstractly speaking, is a light detector with 4 kinds of sensors: the rod cell, active in the dark, and 3 kinds of cone cells, active when it’s bright. Each cell carries its own kind of light-sensitive proteins (“opsins”), which are molecular switches. If a photon hits an opsin in the “passive” shape, then the opsin may absorb the photon and flip into its “active” shape. An active opsin would then set off a molecular chain-reaction in the cell, that may result in an electric signal down the optic nerve.\nMathematically, suppose we shine a light on a patch of long-wavelength-type cone cells, we can represent the electric response as:\n\\[I_L = \\int S_L(\\lambda) R(\\lambda) d\\lambda\\]\nwhere\n\n\\(I_L\\) is the response intensity of long-wavelength-type cone cells, in units of neural spike per second. Though each cell’s operation is quantum-mechanically random, when averaged over many cone cells, the response is deterministic.\n\\(R(\\lambda)\\) is the spectral radiance at wavelength \\(\\lambda\\), or spectrum for short. It has units of watt per square-nanometer (of retinal area) per nanometer (of wavelength).\n\\(S_L\\) is the spectral sensitivity function of the long-type cone cells.\n\nWe can similarly define \\(I_M, I_S\\), for the other two cone cell types (medium and short). Each \\(S_L, S_M, S_S\\) is approximately bell-shaped.\n\n\n\nSchematic diagram of human cone cell sensitivity. Each curve is “normalized”, meaning that it is multiplied by a positive real number, so that its maximal value is exactly 1.\n\n\nIf we ignore the rod cells, and assume no adaptation to darkness (“scotopic vision”), then human color vision is just a deterministic function that maps a spectrum to three real numbers:\n\\[C(P) := (I_S(P), I_M(P), I_L(P))\\]\nwith type \\((\\R^+ \\to \\R^+) \\to (\\R^+)^3\\), where \\(\\R^+ = [0, \\infty)\\) is the space of non-negative real numbers. Define this as the \\((I_S, I_M, I_L)\\) as the LMS color space.4 Furthermore, the biochemical limit on neural firing is 1000 Hz (“Neuron Firing Rates in Humans” 2015), thus the LMS color space is bounded within a cube.\n4 This seems as close to “sense data” (Hatfield 2021) as it gets in science.Any smooth deformation of the LMS color space gives us another color space. In theory, it doesn’t matter which one we use, because the underlying color space is still the same. In practice, some color spaces are easier to use than others.\n\n\n\nDifferent depictions of the same color space. By a smooth map, we can deform the LMS color space into any shape we want, such as a cone, a cube, a cylinder, a double cone, etc. Figure from Wikimedia Commons\n\n\nIn people with only two types of cone cells, the color vision function \\(C\\) loses a dimension. For example, if that person has deuteranopia, without medium-wavelength cone cells, then they would see all colors in LMS space with the same \\((S, L)\\)-coordinates as the same.\n\n\nProjective geometry\nBecause \\(C\\) is a linear functional, and any two colors can be mixed to give a third color, LMS color space is a convex cone. On the tip of the cone is \\((0, 0, 0)\\), the color of pure darkness. It is an old experimental fact that the geometry of colors is invariant under scaling. So, if you have two lights with spectra \\(P, P'\\), such that their colors look the same/different/very different, then we make them brighter or dimmer, to \\(cP, cP'\\) where \\(c &gt; 0\\), then their colors will still look the same/different/very different.\nThus, we can factor the space of colors into two components: an apparent lightness, and an apparent chromaticity. So, if we take two dim red lights, and shine both of them on the same pane of frosted glass, the frosted glass would look lighter, but have the same chromaticity. The space of chromaticities is the space of lines passing the origin, which allows us to use projective geometry.\nThe space of all colors looks like a cone, and since each line in the cone can be represented as a point on the line, the space of all chromaticities looks like the intersection of the cone with a plane – each line is represented by its intersection with the plane. What does the space of all chromaticities look like?\nBecause any spectrum \\(I\\) is the convex sum of pure spectra\n\\[\\{I_{\\lambda}: \\lambda \\in (400 \\;\\mathrm{nm}, 700\\;\\mathrm{nm})\\},\\]\nthe space of all colors is the convex sum of all pure spectral colors\n\\[\\{C(I_{\\lambda}): \\lambda \\in (400 \\;\\mathrm{nm}, 700\\;\\mathrm{nm})\\}.\\]\nConsider a wall covered with a “pure spectral paint”, in the sense that it reflects exactly light at wavelength \\(500 \\;\\mathrm{nm}\\), and nothing else. Then, under any illumination, the color of the wall has the same chromaticity. Pure spectral colors are special colors, in the following diagram, on the edge of the cone are lines of pure spectral color, each produced by a spectrum that is concentrated at just one wavelength.\n\n\n\nThe pure spectral colors in LMS color space. The rainbow curve represents the spectrum of visible light, from violet to red. Each point on this curve corresponds to a specific wavelength of light and its unique combination of stimulations to the three types of cone cells. For each point on the spectral curve, we can draw a straight line to the origin. Each point on the line has the same color, but appears increasingly bright.\n\n\nBecause the cone shape is uninteresting, the color space is typically represented by chopping off the cone midway, producing a roughly horseshoe-shaped region on \\(\\R^2\\), named the gamut. Mathematically, it is the projective transform: \\[(s, m, l) := \\left(\\frac{I_S}{I_S + I_M + I_L}, \\frac{I_M}{I_S + I_M + I_L}, \\frac{I_L}{I_S + I_M + I_L} \\right)\\]\nThe curving edge of the chromaticity space are points of pure spectral colors, from pure \\(700 \\;\\mathrm{nm}\\) line on the red end, to the pure \\(400 \\;\\mathrm{nm}\\) line on the purple end. Every point inside the gamut can be mixed by two pure spectral colors. However, this is not the entirety of chromaticity space. The ray at the shortest-wavelength end (pure spectral purple) and the ray at the longest-wavelength end (pure spectral red) do not touch each other. Instead, they shoot out like two ends of a horseshoe. Chromaticity space, then, has a second, straight edge, obtained by mixing the shortest and the longest wavelength. This is the purple boundary.\n\n\n\nSchrödinger’s diagram of chromaticity space. Spektralkurve: spectral curve. Schnitt mit einer Ebene: intersection with an arbitrarily inclined plane. R: red. G: green. I: indigo. V: violet. O: origin. (Schrödinger 1920, fig. 3)\n\n\nWe can construct the chromaticity space of someone with deuteranopia by starting with the purple line, then draw one line for each point on the purple line that is perpendicular to the \\((S, L)\\)-plane in LMS color space. The deuteranopic observer sees each line of chromaticities as a single chromaticity.\nTheoretically, we can imagine creating the world’s best computer display by putting in a full-spectral display unit into each pixel. It will then be able to cover the entire gamut space. It will not only display true-life colors for humans, but also for dogs, bees, and mantis shrimps. Unfortunately, we don’t have that luxury, and computer displays are built for human-use only, with just three spectra.\nNow, if we have a pixel containing three little LED units, capable of emitting light of spectra \\(P_1, P_2, P_3\\), then we can take any convex sum, and create a mixed color. The space of all colors created by their convex sum is the convex sum of \\(C(P_1), C(P_2), C(P_3)\\), which looks like a triangular cone. Thus, the chromaticity that this pixel can display is a triangle. Every color inside the triangle can be created by mixing the three spectra, but any color outside cannot.\n\n\n\nThe triangle of displayable chromaticities for cathode-ray televisions (left) and LCDs (right). Figure from Wikimedia Commons\n\n\nWhen you print on a page, the page does not emit color, and can only acquire color by selectively reflecting light. When under a standard white light, the more ink you lavish on a page, the more saturated the color can be, but the darker it would be, because more light is absorbed. Conversely, if all the light is reflected, then it would look white. Because of this trade-off, the gamut of printable colors is even smaller.\n\n\n\nThe gamut of printable colors when placed under standard white illumination. (MacAdam 1944, plate 1)\n\n\nEvolution has created a multi-spectral display in some octopuses and chameleons. The best octopus camouflagers have 2 kinds of color organs in their skins: the chromatophores, the leucophores, and the iridocytes. Chromatophores contain pigment cells, which expand and contract by radial muscles like wheel spokes around an axis. The leucophores are roughly ideal “matte” reflectors, meaning they reflect incoming light uniformly, with little loss.\nThe iridocytes are the most exotic, and approximates our “world’s best television screen”. Specifically, they are dielectric mirrors, which reflects light at a specific wavelength. They are alternating layers of guanine crystals and cytoplasms. To change color, it simply adjusts the water content of the cytoplasm, which makes them expand or contract, changing the distance between guanine layers. More pictures are found in (Cloney and Brocco 1983).\n\n\n\nHow the iridocytes work in cephalopods. Figure from (Cossins 2013)\n\n\n\n\n\nA, B. cephalopod before and after camouflage. C. structure of cephalopod skin. D. before and after chromatophore expansion. F. before and after iridocyte turning iridescent. G. the dielectric mirrors inside an iridocyte. (Chatterjee 2022, fig. 4)\n\n\nMeanwhile, chameleons have iridocytes that operate by a different mechanism: photonic crystals (Teyssier et al. 2015).\n\n\nLinear geometry\nGrassmann, famous for originating linear algebra, studied color theory and applied linear algebra to it. Essentially, he discovered that the human color vision function \\(C\\), defined previously, is a linear function. He discovered this by color-mixing experiments, in the style of 19th century psychophysics. Considering it was 50 years before the neuron doctrine became accepted, and 100 years before cone cells were observed, he did very well.\nFor any three spectra \\(P_1, P_2, P_3\\), we can define their colors as \\(C_i := C(I_i)\\). Since \\(C\\) is a linear function, as long as \\(\\{C_1, C_2, C_3\\}\\) are linearly independent, we can represent any color as \\(C(x_1 P_1 + x_2 P_2 + x_3 P_3)\\) for some \\((x_1, x_2, x_3) \\in \\R^3\\).\nFor example, we can go to a scientific standard shop and buy a set of standard lamps, which when plugged into a standard plug, viewed in a standard room, at a standard distance and a standard angle, by a standard observer,5 will create a standard red, a standard green, and a standard blue. Then, using opaque to cover up parts of the lamp, and combining the lights, we can create any color \\(C(x_1 P_1 + x_2 P_2 + x_3 P_3)\\), for any \\((x_1, x_2, x_3) \\in [0, 1]^3\\). By buying more lamps, we can create all colors with \\((x_1, x_2, x_3) \\in (\\R^+)^3\\).\n5 Because humans are resistant to standardization, the standard observer is obtained by taking data from real observers in good health that are physiologically similar, and their average. The methodology resembles l’homme moyen (“the average man”) of Adolphe Quetelet, a fanatic for anthropometry. Also, the standard observer is not required to drink standard cups of tea.Here, we notice a difficulty: we can’t take a negative amount of lamp. Fortunately, we can bypass the difficulty by adding a fourth lamp, a “standard white” lamp emitting a spectrum \\(P_0\\). Then, for any other spectrum \\(I\\), there exists \\((x_1, x_2, x_3, x_4) \\in (\\R^+)^4\\), such that\n\\[\nC(I) + C(x_0 P_0) = C(x_1 P_1 + x_2 P_2 + x_3 P_3)\n\\]\nwhich allows us to place the color of \\(I\\) at the unique point. Of course, the choice of \\((x_1, x_2, x_3, x_4)\\) is not unique. However, since color space is linear, the sum \\(C(x_1 P_1 + x_2 P_2 + x_3 P_3 - x_0 P_0)\\) is unique. Once \\(C(P_0)\\) is itself constructed as a linear sum of \\(C(\\sum_{i=1}^3 x_{i, 0}P_i)\\), we would have located \\(C(I)\\) in color space, at\n\\[\nC(I) = \\sum_{i=1}^3 (x_i - x_0x_{i, 0}) C(P_i)\n\\]\nThis is essentially the state of the art of colorimetry in 1931, when CIE 1931 was constructed by color-mixing experiments. An observer is seated in a standard room, and sees two light sources. On the left, a to-be-measured light \\(I\\) is mixed with a standard white light \\(P_0\\), and on the right, are three standard blue, green, red lights \\(P_1, P_2, P_3\\). The observer turns the 4 knobs until two sides look indistinguishable. This was repeated for many observers, over many days, for many light sources. The result is a table with three columns, and many rows. Each row is an industrially important light source, and the three columns are the standard red, standard green, standard blue. It schematically looks like this (I made up the data):\n\n\n\ncolor\nstandard red\nstandard green\nstandard blue\n\n\n\n\nstandard red\n1.000\n0.000\n0.000\n\n\nstandard green\n0.000\n1.000\n0.000\n\n\nstandard blue\n0.000\n0.000\n1.000\n\n\nstandard white\n0.334\n0.334\n0.332\n\n\n…\n…\n…\n…\n\n\n\n\n\n\n\n\n\nTechnically\n\n\n\nTechnically, the CIE 1931 color of a spectrum \\(I\\) is a point in \\(\\R^3\\) defined by\n\\[\nC_{\\text{CIE 1931}}(I) := \\left(\\int I(\\lambda) \\bar r(\\lambda) d\\lambda , \\int I(\\lambda) \\bar g(\\lambda) d\\lambda , \\int I(\\lambda) \\bar b(\\lambda) d\\lambda \\right)\n\\]\nwhere \\(\\bar r, \\bar g, \\bar b\\) are “standard observer color matching functions”. They are not any real observer’s sensitivities, because they have negative values. Instead, they are roughly a linear transform of the real sensitivities \\(S_S, S_M, S_L\\), meaning CIE 1931 color space is roughly a linear transform of LMS color space.\nWhy did they go for a roughly linear transform? I know it’s confusing (it confused me), but it’s simply a temporary hack. Back then, they had no way to measure the neural spikes, so they had to infer the real sensitivities by indirect psychophysics data. And the negative values are for some kind of numerical stability considerations. Point being, it’s really not fundamental to science, but rather a 1930s technical hack.\n\n\n\n\nOpponent process\nHave you ever wondered why things seem bluer just after sunset, or under a high full moon? This is where opponent process theory and Purkinje effect comes in.\nWhile the retina might be operating with the LMS color space, it is not what gets sent to the brain. Specifically, before leaving the retina, the spikes from the 3 cone cells and the rod cell (we are finally accounting for them now!) are linearly transformed by 3 paired-kinds of neurons within the retina, before sending down the optic nerve. Greatly simplified, the linear transform is:\n\\[\n\\begin{cases}\n  I_{\\text{Red-Green}} &= I_L - I_M \\\\\n  I_{\\text{Yellow-Blue}} &= I_L + I_M - 2 I_S \\\\\n  I_{\\text{Brightness}} &= 2I_L + I_M + 0.05 I_S + I_R\n\\end{cases}\n\\]\n\n\n\n(Hunt and Pointer 2011, fig. 1.4)\n\n\nIn words, the Red-Green-pair of neurons take the long-wavelength (reddish) cone cells, and subtract away the medium-wavelength (greenish) cone cells. If the result is positive, then the positive half of the pair sends down a signal at the rate of \\(I_L - I_M\\), otherwise, the negative half of the pair sends down a signal at the rate of \\(-(I_L - I_M)\\). This linear transform, while mathematically equivalent (as long as the rod cells don’t appear) to LMS space, allows the optic nerves to carry more information in Homo Sapiens’ natural habitat (Buchsbaum, Gottschalk, and Barlow 1997).\nWhen the light level is around \\(0.5 \\;\\mathrm{lux}\\), which corresponds to twilight, or a full high moon, both the rod cells and the cone cells are active (Dominy and Melin 2020).\nSo, let us look at the linear transform\n\\[\n\\begin{cases}\n  I_{\\text{Red-Green}} &= I_L - I_M \\\\\n  I_{\\text{Yellow-Blue}} &= I_L + I_M - 2 I_S \\\\\n  I_{\\text{Brightness}} &= 2I_L + I_M + 0.05 I_S + I_R\n\\end{cases}\n\\]\nLet’s pretend we are the brain, interpreting the signals sent down the optic nerves. Suppose the retina secretly increases \\(I_R\\) by a small amount of \\(\\Delta I\\), but we don’t know that. How would we interpret it? We would interpret it as a color in LMS space with color\n\\[(I_S + \\Delta I', I_M + \\Delta I', I_L + \\Delta I')\\]\nwhere \\(2.05\\Delta I' = \\Delta I\\). That is, it looks as if each type of cone cell has increased firing rate by the same amount. Looking at the sensitivity curve, this effect can be created by shifting the spectrum to the shorter wavelength, then increase its power slightly. Thus, things look bluer.\n\n\n\nPurkinje effect illustrated with a flower. As the lighting condition dims, the entire scene shifts more to the bluish shade. At low enough lighting, all cone cells deactivate, and the entire scene becomes monochromatic. Figure modified from Wikimedia Commons\n\n\n\n\nRiemannian geometry\n\nMagnitude-notions are only possible where there is an antecedent general notion which admits of different specialisations… the only simple notions whose specialisations form a multiply extended manifoldness are the positions of perceived objects and colours. More frequent occasions for the creation and development of these notions occur first in the higher mathematic.\nRiemann’s Habilitation dissertation, 1854 (Riemann 2016)\n\nNow that we have a space of colors, how do we measure distances in it? Some colors are close, while some colors are far apart. How do we quantify it? This question occupied the minds of some famous scientists, including Riemann, Grassmann, Helmholtz, and Schrödinger (Pavlidis 2021).\nIn 1920, Schrödinger (more famous for his other equation) hypothesized that color space has a Riemannian geometry, and the subjective difference between two colors is the geodesic distance between the two points in color space (Schrödinger 1920). This is the foundation of modern colorimetry (Niall 2017). Over the years, there had been a mess of increasingly detailed theoretical models for the Riemannian metric of color space, of interest only to specialists – see (Wyszecki and Stiles 1982, chap. 8.4) for a review. Here, we bypass most of the theory by experimental data.\nGiven two spectra \\(I, I'\\), if their colors \\(C(I), C(I')\\) are close enough, an observer would judge them as equal. This is the concept of “just noticeable difference” (JND), a foundational concept of psychophysics.6 In general, the JND method goes like this:\n6 One can get a good feel for the JND by playing the Color Game. In case this fails in the future, try the archived link.\nFix one stimulus \\(S\\), and vary the other stimulus \\(S'\\). The prior probability that \\(S = S'\\) is \\(1/2\\).\nPresent both \\(S, S'\\) to the observer.\nThe observer judges whether they are the same or different.\nRepeat many times.\nIf, when truly \\(S' = S\\), the observer judges that they are the same with probability \\(p_0\\), then the JND point is the point where the observer judges that \\(S' = S\\) with probability \\(p_0/2\\).\n\nIn the original experiments, MacAdam fixed one spectrum \\(I\\), and varied the other spectrum \\(I'\\) on a curve that passes \\(I\\). He repeated the JND measurement along many curves across many spectra, and found that around each spectrum, the JND points make up a rough ellipsoid.\n\n\n\nThe JND of a single observer around a single color, when approached from 14 different directions. The JND points fall roughly on an ellipse. (Wyszecki and Stiles 1982, fig. 1(5.4.1))\n\n\nIf the JND measurement is binary classification in color space, then what is real-valued regression in color space? Answer: color matching experiment.\nSpecifically, suppose we fix \\(I\\), and let the observer turn a knob that varies \\(I'\\) along a curve passing \\(I\\), then we would find that \\(I'\\) is normally distributed centered upon \\(I\\). Perform the experiment with 3 knobs, and we would obtain an ellipsoidal cluster. The ellipsoids of \\(1\\sigma\\) are the MacAdam ellipsoids. As ellipsoids are very hard to draw, we typically only see 2D slices of them – the MacAdam ellipses.\n\n\n\nThe color matching experiment data of a single observer around a single color. The points are projected from 3D space to three orthogonal views. The ellipsoid is the ellipsoid of \\(3\\sigma\\). (Wyszecki and Stiles 1982, fig. 1(5.4.3))\n\n\n\n\n\n\n\n\nConjecture: perceived lightness and hues are totally geodesic foliations\n\n\n\nGiven two colors \\(C_0, C_1\\), we can construct the geodesic curve between them as the shortest sequence of colors \\(C_a, C_b, C_c, \\dots\\), such that \\(C_0, C_a\\) are JND, and \\(C_a, C_b\\) are also JND, etc. It sounds reasonable in my head that, if \\(C_0, C_1\\) have the same perceived lightness, then the geodesic connecting them should all have the same perceived lightness, because it seems like we would be wasting some precious JND on “jumping up in lightness, only to jump down again”. Similarly, if \\(C_0, C_1\\) have the same perceived hue, then I guess the geodesic through them would stay along the same perceived hue.\nIf this is true, then we can construct two families of foliations in color space, one for equal-lightness surfaces, and one for equal-hue surfaces. Each surface is a totally geodesic foliation (Johnson and Whitt 1980), meaning that each geodesic within a foliation is also a geodesic in the total color space.\nHowever, this definitely isn’t true for perceived saturation, as the shortest path between slightly saturated red and slightly saturated green (opposite of red) goes through perfect gray, so who knows whether this conjecture is true or not?\n\n\nThough JND and color matching are two different methods, they are both using people as statistical detectors, and it stands to reason that they should measure the same thing. Indeed, the ellipsoids of JND are roughly the \\(3\\sigma\\) MacAdam ellipsoids (Wyszecki and Stiles 1982, sec. 5.4).\n\n\n\nMacAdam ellipses plotted on the CIE 1931 \\(xy\\)-diagram, 10× actual size. Figure from Wikimedia Commons.\n\n\nGiven the Tissot’s indicatrix, it is natural to try to draw a distortion-less map of earth, where all Tissot ellipses are equally-sized circles. This is impossible, and Gauss knew exactly why: earth has positive gaussian curvature, but a flat sheet of paper has zero gaussian curvature.\n\n\n\nTissot’s indicatrix on Behrmann projection. Figure from Wikimedia Commons\n\n\nGiven the MacAdam ellipses, it is natural to try to draw a distortion-less map of color space. This is impossible, for the same reason: color space has nonzero curvature. It was already known to MacAdam in the 1940s that his experimental data shows color space has significant curvature.\n\n\n\nA paper model of a 2D subspace of the color space – the space of colors with unit subjective brightness. The metric on the paper model faithfully matches the metric implied by MacAdam ellipse. We can see the curvature. (MacAdam 1944, fig. 5)\n\n\n\n\n\n\n\n\nConjecture: color space is not conformally flat\n\n\n\nIs it possible to at least stretch the MacAdam ellipses into spheres, even though they aren’t of the same radius? That is, is color space conformally flat? For example, in Mercator’s projection, the Tissot ellipses are indeed circular, though they become larger near the poles, so earth is conformally flat.\nHowever, by Liouville’s theorem, conformal flatness is very stringent at 3 dimensions and above, so my conjecture is that color space is not conformally flat. Proof sketch: download the metric tensor from CIE, and check its Cotton tensor is (statistically) nonzero.\n\n\n\n\n\nA nonlinear map of CIE 1931 \\(xy\\)-graph designed to make MacAdam ellipses look roughly circular. (Wyszecki and Stiles 1982, fig. 4(5.4.1))\n\n\nCIELAB color space is a smooth mapping from CIE 1931 color space to \\(\\R^3\\), such that the MacAdam ellipses are stretched spherical enough for practical purposes.\n\n\n\nAll visible colors, plotted in CIELAB color space. Figure from Wikimedia Commons\n\n\n\n\nInformation geometry\nImagine a hiker navigating a mountain path equipped only with an altimeter and a detailed altitude map. The hiker’s ability to pinpoint their location on the map relies on sensing altitude changes. In regions where the terrain is steep (representing high sensitivity), even a small step forward (change in stimulus intensity) will register a noticeable altitude change on the altimeter (change in perceived sensation). This allows for precise localization – a small JND. However, along flatter sections of the trail (low sensitivity), the hiker might need to traverse a longer distance to observe a meaningful altitude difference, leading to a larger JND and greater uncertainty about their position on the map.\nSimilarly, as we move around in color space, we may distinguish colors by the photoreceptor responses, which can be inferred from the sensitivity curves \\(S_S, S_M, S_L\\). That is, we can reduce Riemannian metric to information geometry. Working this out in detail, (da Fonseca and Samengo 2016) showed that the Riemannian metric in color space is roughly the same (“explains 87% variance”) as the Fisher information metric.\n\n\n\nThe ellipses measured by MacAdam (green) vs ellipses predicted by information theory (red). (da Fonseca and Samengo 2016, fig. 8b)\n\n\nIn the same vein, people have argued for centuries about why certain colors are perceived as “pure” or “primary” (white, black, red, blue, green, etc), while others are “mixed” or “derived” from the primary colors. (MacEvoy 2015) argues that the primary colors are “landmarks” in the geometry of color space, much like how on a map, the peaks and troughs are local maxima of gaussian curvature, and the mountain passes are the local minima, or how on a spacetime, a black hole singularity is the point where the Kretschmann scalar is infinite. Intuitively, we can see this on the CIELAB color solid. The top-most point is white, the bottom-most color is violet, and you can just see yellow at another point behind the back, etc.\n\n\n\n\n\n\nBeyond Riemannian geometry\n\n\n\nWhile color space is locally Riemannian, this is not so over longer distances. That is, once we are measuring the subjective distances between pairs of far-different colors, the data no longer behave like distances on a curved 3D space. (Bujack et al. 2022) reported that there is “diminishing returns” in color distances.\nIndeed, this non-Riemannian geometry has been known for a while. CIE in 1994 proposed a color difference, \\(\\Delta E\\), that is not symmetric. That is, if we ask a subject “How far is color 1 from color 2?” and then ask the opposite direction, we usually get a different numerical answer. This reminds me of information-geometric divergence, which is also not symmetric. I cannot find anyone who has studied this in detail, but it ought to interest the information geometers."
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#consciousness",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#consciousness",
    "title": "Hole Argument and Inverted Qualia",
    "section": "Consciousness",
    "text": "Consciousness\n\nEasy, hard, meta\nDavid Chalmers proposed three problems of consciousness.\nThe easy problem is essentially the problem of consciousness information processing as studied by scientists: how memories work; how the brain recognizes objects; how to create human-level intelligence; etc. By “easy”, Chalmers was not dismissing them as “intellectually easy”, as he expects it would take the best minds a century or more to solve these. Rather, by “easy”, he meant that “within the power of science as currently understood”. We would need no new metaphysics, or faith, or\nThe hard problem is the easy problem, but with something extra that seems impossible to even fit into a scientific system. What that something extra is, philosophers are unable to say, but they typically give it the name of “qualia”, “experience”, “phenomenal awareness”, etc.\n\nWhat makes the hard problem hard and almost unique is that it goes beyond problems about the performance of functions. To see this, note that even when we have explained the performance of all the cognitive and behavioral functions in the vicinity of experience—perceptual discrimination, categorization, internal access, verbal report–there may still remain a further unanswered question: Why is the performance of these functions accompanied by experience?\n(D. J. Chalmers 1995)\n\n\nWhat exactly are the problem data that need explaining? They can be construed as verbal reports (my saying ‘Consciousness is hard to explain’), as judgments (my forming the judgment that consciousness is hard to explain), or as dispositions to make these reports and judgments. Verbal reports are perhaps the most objective data here, but they are also a relatively superficial expression of an underlying state that is really what we want to explain. So I will generally focus on dispositions to make verbal reports and judgments as what we want to explain.\n\nWhile decades of science have made good progress on the easy problem, centuries of philosophical disputations have not made progress on the hard problem. To bypass the impasse, Chalmers proposed the meta problem: Why is the hard problem a problem? (D. Chalmers 2018)\nLet’s consider an analogy. The easy problem of biology would be: How does biological machines work? The hard problem: Why is the performance of these functions accompanied by life? The meta problem: What kind of cognition do people have, such that they can see a machine performing all the motions of life, and yet still call it “lifeless”?\nIn John Searle’s Chinese room story, a man who knows nothing of Chinese, by executing an algorithm with pen and paper, could converse in Chinese writing. Many, including Searle, thought that the Chinese room does not really understand Chinese. This gives us another analogy.\nThe easy problem of Chinese: What algorithms can converse in Chinese text? The hard problem of Chinese: Why is the performance of Chinese speaking in a Chinese-speaker accompanied by understanding? The meta problem: What kind of cognition do people have, such that they say the Chinese room “lacks understanding”?\n\n\nDark phenomena\n\n[Folk Psychology] suffers explanatory failures on an epic scale, that it has been stagnant for at least 25 centuries, and that its categories appear (so far) to be incommensurable with or orthogonal to the categories of the background physical science whose long-term claim to explain human behavior seems undeniable. Any theory that meets this description must be allowed a serious candidate for outright elimination.\n(Churchland 1981)\n\n\nNeurophenomenology is possible; phenomenology is impossible.\n(Metzinger 2004, 83)\n\nEverything I see, I know that I see. Everything that I hear, I know that I hear. Everything that I think, I know that I think. What could be clearer? Descartes based his entire philosophy on these kinds of self-evident truths, and these are still the starting points of many modern philosophies of the mind and consciousness.\nHowever, such self-evident truths can be questioned. In blindsight, I see things that I don’t know that I see. In Anton’s syndrome, I don’t see things, yet I think that I see.6 In Cotard’s delusion, I live yet I think that I am dead.\n6 There was a philosopher who had taken Anton’s Syndrome very seriously, but in the opposite direction, in the spirit of one man’s modus ponens is another man’s modus tollens:\n\nI still vividly remember one heated debate at an interdisciplinary conference in Germany a number of years ago, at which a philosopher insisted, in the presence of eminent neuropsychologists, that Anton’s syndrome does not exist because a priori it cannot exist.\n(Metzinger 2004, 235)\n\nAs a mathematician, I often know things without knowing how I know. When doing mental arithmetics, usually I do it both ways. One algorithm, operating consciously, goes from the highest digit down; the other algorithm, operating unconsciously, goes from the lowest digit up. As I consciously grind out digits from one end, digits simply “emerge” out of the other end. Like two teams digging a tunnel, they finally meet in the middle; the digits ripple-carry; the mouth vocalizes the final answer.\nDuring deep contemplations of high-dimensional geometric objects, my self-awareness is turned down to a whimper, dimly illuminated by the sparks and piezoluminescence of vast gears and pulleys turning in the dark mill of the brain,7 where the light of consciousness can never penetrate. A few times, I came back to consciousness on the carpet, not knowing how I got there, but with a clear feeling that an answer is close. Then I find the answer – or not. The non-conscious parts of the brain make plenty of mistakes too.\n7 I really wanted to write “dark Satanic mills of the mind”, but that would be too much purple prose.Consider a pair of pure lights, at \\(550 \\;\\mathrm{nm}\\) and \\(554 \\;\\mathrm{nm}\\). For an observer with good vision, they are separated by a JND, so if the observer sees two patches of light shining on two plates of frosted glass placed close to each other, then the observer can just barely see that they are not the same color. However, as soon as the two lights are turned off, the difference disappears. The observer cannot recall one as “green-550” and the other as “green-554”. Both would be recalled as “kind of green”. The observer cannot tell if a single patch of light is closer to green-550 or green-554. The observer cannot tune a laser by sight so that its color matches green-550 rather than green-554.\nThere are several ways to interpret this result.\nDaniel Dennett’s approach would be to eliminate inaccessible phenomena – there is neither green-550 phenomenon nor green-554 phenomenon, but only the “one patch looks greener than another” phenomenon, which is available for conscious information processing.\nThomas Metzinger and David Roden’s approach is “dark phenomenology” (Roden 2015, chap. 4). A dark phenomenon rises from dust, does its job, then falls back to dust. It cannot be interrogated, redirected, paused, vocalized, remembered, threatened, or inspected. In this way, green-550 and green-554 are dark phenomena. They are real phenomena and have real mental functions, but they cannot be captured or interrogated. A dark phenomenon, such as green-550, is an information object that only flows along hardwired circuits. The conscious part of the brain might echo a command “Store this phenomenon in long-term memory!” or “Reroute this phenomenon for verbal report!” but such commands are futile. The green-550 and green-554 phenomena are sent to some visual comparison module then discarded. The visual comparison module might output a bright phenomenon “They are different.”, but this bright phenomenon is merely an impoverished derivative of the dark phenomena that came before.\nThe just noticeable difference (JND) in color perception possibly shows that we see metric, not colors themselves.\nIf the phenomenology of color is but the metric geometry of color space, then we have not only dispelled the inverted qualia problem, but also have dispelled the problem of other minds. We should be able to construct the phenomenal geometry of color from retinal measurements. By comparing my retina and your retina, we can objectively determine whether my green is the same as yours.\nIf the inverted qualia problem is dissolved by the hole argument, then the hard problem of color perception is gone as well. What remains is merely determining the metric of color space. In this way, the biophysics of the eye-brain system would mostly solve the easy problem of color perception.\n\n\nDual-process theory of the meta-problem\nIn cognitive psychology, there are many dual-process theories for explaining many cognitive processes. A theory is a dual-process theory if it follows the dual-process template. That is, if it models a cognitive process with an algorithm that has two parts, termed System 1 and System 2. System 1 is characterized by automatic, fast, and intuitive processing, while System 2 is deliberative, slower, and more analytical.\n(Fiala, Arico, and Nichols 2012) proposed a dual-process theory for the meta-problem of consciousness. According to them, people recognize something as an agent or not by a dual process. This is evolutionarily important for ancestral humans, because detecting whether that shaking in the grass is caused by an animal or not could be a life-and-death decision.\nSystem 1 for detecting agency uses the following heuristics: eye-like shapes on a head-like bump, unpredictable environmental reactions, and self-initiated movement beyond mere inertia. System 2 for detecting agency involves rational deliberation, theory application, and conscious reasoning. These processes are engaged when evaluating complex concepts, such as brain-based theories of consciousness.\nNow, the meta-problem of consciousness occurs when one attributes agency to the brain. The brain, lacking visible features like eyes, appearing inert within the skull, and not exhibiting self-propelled motion, is not an agent according to System 1. The persistent conflict between System 1 and System 2 is verbalized into the hard problem of consciousness: System 2 admits that the brain is enough for agency, while System 1 insists that it is still lacking something, be it “consciousness”, “qualia”, or “experience”.\nSimilarly, this explains how both Einstein and Poincare stumbled over the hard problem of General Relativity. General Relativity is acceptable to System 2 processes, but not to System 1 processes, which insists that spacetime is \\(\\R^4\\), and that General Relativity may describe a metric field over it, but not what spacetime is. Consequently, there is a persistent “explanatory gap”, as a nagging feeling of the hard problem. “Even when we have explained the observable results from the astronomical to the microscopic, there may still remain a further unanswered question: Why did the galaxy pass over point A, not point A’?”\nThey start with \\(\\R^N\\), then quotient out smooth deformations.\nMathematically speaking, there is no necessary division between substance and property, though it is a very useful intuition. Physically speaking, the division has led to errors, such as Einstein’s failed 1914 attempt at non-relativistic theory of gravity. This is especially true in the case of general relativity, because it developed from 19th century differential geometry, which tended to be understood as about geometry of deformable surfaces – where there is literally a substance with changeable property (the strain field). Indeed, the famous image of “earth sitting on a rubber sheet” is a stubbornly persistent illusion created by System 1.\n\n\n\nThe stubbornly persistent illusion. Figure from xkcd: Teaching Physics"
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#abstract",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#abstract",
    "title": "Hole Argument and Inverted Qualia",
    "section": "",
    "text": "problems\neasy\nhard\nmeta\n\n\n\n\nclassical gravity\nHow to model astronomical phenomena with some distribution of mass-points in \\(\\R^3\\)?\nOut of all the equivalent models, which one is the right one?\nWhy did Newton insist on absolute space, but Leibniz on relative space?\n\n\ngeneral relativity\nHow to model astronomical phenomena with some \\((\\mathcal M, g, T)\\)?\nHole argument: Out of all the isometric models, which one is the right one?\nWhy did Einstein and Hilbert fall for the hole argument?\n\n\ncolor\nPsychophysics: How to model human perception of color?\nQualia: Why does red feel like red?\nWhy are people prone to argue about the inverted spectrum?\n\n\nlanguage\nHow to model language use?\nWhy is language use associated with a feeling of understanding?\nWhy are people prone to argue about the Chinese room?\n\n\nconsciousness\nHow to explain objective phenomena associated with consciousness, such as attention, working memory, dreaming, etc?\nWhy does paying attention, dreaming, etc, feel like something?\nWhy are people prone to argue about the hard question?\n\n\n\n\n\n\nShifted qualia and the hole argument. Figure modified from (Norton, Pooley, and Read 1999) and Wikimedia Commons\n\n\n\n\n\nInverted qualia and Leibnitz’s inverted space. Figure modified from (Norton, Pooley, and Read 1999) and Wikimedia Commons"
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#the-geometry-of-spacetime",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#the-geometry-of-spacetime",
    "title": "Hole Argument and Inverted Qualia",
    "section": "The geometry of spacetime",
    "text": "The geometry of spacetime\nIn general relativity, the hole argument is a thought experiment that apparently shows that general covariance is impossible. Einstein in late 1913, and David Hilbert in 1915, both fell into the hole argument. Misled by the hole argument, Einstein attempted to study theories of gravity that are not generally covariant, before finally giving up and . (Norton, Pooley, and Read 1999)\n\nA brief history of spacetime\nThe history of spacetime is a history of expanding symmetries.\nIn the most ancient cosmology of China, the earth is a square, while the sky is a half-bowl covering the earth. Each direction of earth – east, west, south, north – has a mystical significance. Up is not down, and east is not west. Not only that, there is a center of earth, somewhere in The Middle Kingdom. Thus, there is no spatial symmetry. The world was born an unspecified number of years in the past out of a chaotic egg. Thus, there is no temporal symmetry. Therefore, ancient Chinese spacetime is \\(\\R^1 \\times \\R^3\\), with no (nontrivial) symmetry.\nIn Aristotle’s physics, there is a center of the universe, where everything heavy (water and earth) is moving towards, and everything light (air and fire) is fleeing from. Other than that, space is spherically symmetric – he knew that earth is round. However, though space has a center, time is translation-invariant. Therefore, Aristotle’s spacetime is \\(\\R^1 \\times \\R^3\\) with symmetry group \\(\\R^1 \\times SO(3)\\), where \\(\\R^1\\) is the time-translation symmetry group, and \\(SO(3)\\) is the spherical symmetry group.\nThe Christian spacetime, with a beginning and an end for time, has a smaller symmetry group of \\(\\{0\\} \\times SO(3)\\).\nCopernicus and Kepler replaced the sun for the earth as the center of the universe, but they still insisted on a center. The first true breakthrough was Giordano Bruno’s infinite spacetime, where both space and time are infinite and without center. Therefore, Bruno’s spacetime has symmetry group \\(\\R^1 \\times E(3)\\), where \\(E(3)\\) is the 3D Euclidean symmetry group. That is, we allow all spatial translations.\nGalileo braided together space and time, resulting in an even bigger symmetry group. Specifically, he argued that the universe does not have a special “at rest” velocity. To see why this is a breakthrough, consider what happens in Bruno’s universe. In Bruno’s universe, it matters whether you are staying still, or moving at \\(1 \\;\\mathrm{m/s}\\) relative to the universe. If you are staying still, then your trajectory is like \\(\\dots, (t_0, p), (t_1, p), (t_2, p), \\dots\\). Metaphorically speaking, you are sitting somewhere in the cinema of spacetime, and you can “peek at the number of your seat” and see that you have been sitting at the same space-point \\(p\\). However, if you are moving relative to the universe, you can see that your space-point is changing.\nGalileo rejects this. There is no “space-point”. You can take a slice of the universe at \\(t=0\\), and another slice of the universe at \\(t=1\\), but you cannot point at a point in each slice and ask, “Are these two points the same space-point?”.\nNewton’s concept of spacetime is harder to conceptualize. He understood Galileo’s point, and initially attempted to model spacetime the same way as Galileo did. However, for obscure reasons, he reintroduced absolute space and time. This had strange consequences that Leibnitz relentlessly criticized. Consider the inertial frame, relative to which the sun is standing still at this moment. Now consider another inertial frame, moving at \\(1 \\;\\mathrm{m/s}\\) in the direction of Sun-to-Mars at this moment. The laws of Newtonian mechanics are the same, and no observation or experiment could tell us whether one of them is the “absolute frame”, or neither of them is. Yet, out of the infinitely many inertial frames, Newton designated precisely one of them as the “absolute”, and all others are defined as those moving at constant velocity relative to the absolute frame. (DiSalle 2020)\nTo dramatize this seemingly arcane point, consider the following imaginary conversation between Newton and Leibniz about a grant proposal to find the absolute frame:\n\nThis is a proposal to find the absolute frame…\nHow would you find it? The only difference between absolute and non-absolute inertial frames is that one of them is designated so. Do the stars turn perfectly white when you are standing still in the absolute frame? Do the music of the spheres tune to a perfect pitch? Do you see it in your mind’s eye? And even if you do, what if every material point in the universe, by an act of God, were set off in this direction [points finger up] at one mile per day? Would anything seem amiss? You yourself admit that the very sustenance of the universe requires continuous divine forcing, that the stars would have collapsed to the same point otherwise.1\n… yet all relative frames has no existence without assuming the absolute frame, for otherwise, one would fall to a circular argument, where relative frames are relative to nought but each other, and the very meaning of inertiality becomes vacuous. One might as well The absolute might be hidden, but it is out there.\n1 \nIn the 1726 edition of the General Scholium, Newton added a new sentence: “And so that the systems of the fixed stars will not fall upon one another as a result of their gravity, he has placed them at immense distances from one another.” Once again, the implication is that gravity can be a destabilising force. An annotation in Newton’s copy of the 1713 edition after the words “send light into all the others” shows that he had considered an even more theologically powerful statement: “and the fixed stars would, through their gravity, gradually fall on each other, were they not carried back by the counsel of the supreme Being.”\n(Snobelen 2020)\n\nHe also believed that the Great Comet of 1680 would someday fall into the sun, causing a solar flare-up that would kill all life on earth. God would then repopulate earth. In general, he thought the universe as an unstable system requiring constant divine support. (Snobelen 2020)\nWith special relativity, the symmetry of spacetime becomes \\(SO(3, 1)\\), which is in a sense more “braided” than Galilean relativity. In Galilean relativity, the symmetry group of spacetime factors into a direct product between the symmetry group of space, and the symmetry group of time. In special relativity, the symmetry group of spacetime cannot be factored into a direct product. This is the deep meaning of Minkowski’s claim that “space for itself, and time for itself shall completely reduce to a mere shadow”.\nFor general relativity, any diffeomorphism on spacetime is a symmetry.2 In other words, it is a generally covariant theory. This is quite a vast generalization, and warrants further details.\n2 A function is a diffeomorphism iff itis one-to-one, smooth, and has a smooth inverse.\n\nGeneral relativity\nGeneral relativity models spacetime as a manifold \\(\\mathcal M\\), with a metric tensor field \\(g_{\\mu\\nu}\\) and an energy-momentum tensor field \\(T_{\\mu\\nu}\\). The metric tensor describes the spacetime separation between points on the manifold, and thereby the geometry of spacetime. The energy-momentum tensor describes the flow of energy and momentum in spacetime. In particular, a body with mass \\(m\\), such as a black hole, is a flow of energy \\(mc^2\\) in time, and therefore can be described within the energy-momentum tensor.\nThe metric tensor field and the energy-momentum tensor field are “braided together” by Einstein’s field equation:\n\\[\n(\\text{a polynomial equation involving components of }g) = \\kappa T\n\\]\nwhere \\(\\kappa\\) is a constant of nature, measured by experiments.\nThe spacetime manifold \\(\\mathcal M\\) can be transformed, in that we can write down a function \\(f: \\mathcal M \\to \\mathcal M\\), such that it maps one point in the manifold to another point. According to general relativity, if \\(f\\) is a diffeomorphism, then the field equation is unchanged. In this sense, all diffeomorphisms of \\(\\R^4\\) become symmetries of spacetime. Whereas in special relativity, inertial frames are distinguished from non-inertial frames, in that the coordinate lines in an inertial frame are deemed “straight”, no one gets special treatment in general relativity, and any smooth coordinate system is as good as any other. That is, Einstein’s field equation is generally covariant.\n\n\nThe hole argument\nDuring 1912, Einstein struggled with finding a generally covariant field equation for gravity. He even considered the one he would eventually publish in 1915 and be famous for, but gave them up over certain difficulties. Then in late 1913, he tried to turn this loss into a victory by arguing that general covariance is not the right approach, because of the hole argument. (Norton, Pooley, and Read 1999)\nConsider the following two models of a small universe. The universe contains three galaxies moving away from each other. The model on the left shows that one of the galaxies passes the spacetime-point \\(E\\), while the model on the right shows that no galaxy passes the spacetime-point \\(E\\).\nIf the universe satisfies a generally covariant field equation, then we can transform the model on the left to the model on the right by a diffeomorphism, and the equation would be none the wiser. In other words, any generally covariant field equation suffers from rampant indeterminism.\n\n\n\nEinstein’s hole argument. Figure from (Norton, Pooley, and Read 1999)\n\n\nWe cannot fault Einstein for “falling into the hole”, because Hilbert fell into the hole as well around the same time, though he fell in via the route of Cauchy boundary value problem. In Hilbert’s formulation, any generally covariant field equation suffers from indeterminism, in the sense that no amount of initial value on the field is enough to determine the future of the field.\nThat is, if we start off from a slice of simultaneity (“Cauchy surface”), and solve the equations forwards in time, we would find that we are lacking conditions. Concretely, consider the motion of water. If the universe is Newtonian, full of Newtonian water, then cosmology is just hydrodynamics. If we know the precise velocity field at a single moment in time, then we can solve the equations forward and find all there is to know about the cosmos. However, Hilbert found that any generally covariant theory fails this: No amount of knowledge of the field at a single slice of simultaneity is sufficient to determine any future or past of the field. Unlike a clockwork Newtonian universe ticking forward by ironclad laws of motion, the Einsteinian universe would “go off the rails” immediately. (Stachel 2014)\n\n\nProblem of time\nThe old couplet about general relativity goes like “Matter tells space-time how to curve, and space-time tells matter how to move.” but this is often misunderstood as saying “Matter exists, then space-time reacts to matter, and then matter reacts to space-time by changing its motion.” This fundamentally misunderstands what general relativity is. There is no time nor causality, at least as commonly understood, in special or general relativity.\nSpecial relativity is typically interpreted as an “eternalist” or “four-dimensionalist” theory. That is, all of space and time exist in the same way, and the future is as real as the past. Einstein said it as “the distinction between past, present, and future is only a stubbornly persistent illusion.”. It is typically supported by the Rietdijk–Putnam argument, as follows.\nWhereas in Newtonian spacetime, one can still imagine that the universe somehow “grows one time-slice at a time” – though this is susceptible to McTaggart’s objection – in special relativity, there does not exist such a thing as “time-slice”, because there is no absolute simultaneity. We may pick the time-slices in the inertial frame of the solar system, or in that of the Andromeda galaxy. However, just like how no observation can distinguish Newton’s absolute inertial frame from all the other relative inertial frames, no observation can distinguish between the absolute simultaneity from all the other relative simultaneities.\nThe difficulty is only amplified in general relativity. Let us imagine a universe that is swirling with stars and galaxies. Locally, the spacetime manifold is curved, but globally, it is topologically the same as \\(\\R^4\\) – no loops, no singularities, and no wormholes. Now, construct a coordinate system \\((t, x, y, z)\\). We can then select a “snapshot” of the universe by selecting the “slice of simultaneity” (that is, a “Cauchy surface”) at \\(t = 0\\). If we know the exact value of \\(g, T\\) on that snapshot, then we can crank the Einstein field equations to solve (up to general covariance) for \\(g, T\\) for all \\(t &gt; 0\\). Does this mean that the slice of \\(t=0\\) determines what happens afterwards?\nNot really. We could have smoothly distorted the coordinate system to \\((t', x', y', z')\\), and solve the Einstein field equations for all \\(t' &gt; 0\\) starting from \\(t' = 0\\). There are infinitely more degrees of freedom compared to special relativity, making the RP argument bite harder.\nYet, the issue goes even deeper. We could very well select \\(t = 10000\\) and crank the field equations to solve for \\(g, T\\) for all \\(t &lt; 10000\\). Does this mean that “the future determines the past?” Perhaps we can compromise by saying “one point in time determines both the past and the future”, but even that is not necessarily true. We can design much wilder boundary conditions. We can make two lightcones determine the rest of the universe (double-null, or Sachs), make one lightcone plus a “left side” of the universe determine the rest (null-timelike, or Winicour–Tamburino), make half of the universe’s left-side and half of the universe’s \\(t=0\\) determine the rest, etc.\n\n\n\nDifferent initial value conditions. The first is the commonly used Cauchy condition, but the others, more exotic, are also valid. Figure modified from (d’Inverno 1984)\n\n\n\n\nGauge freedom\nThe standard solution, one that is inscribed in every textbook, is the gauge freedom point of view. Even the most practical textbook on general relativity must handle the issue somehow, for the same problem that tripped up Hilbert: the Cauchy problem is not well-posed.\nConcretely, consider the first serious problem in every textbook, where there is a single mass point in the universe. Pick a spherical coordinate system in which the mass point is in the center, and assume that the metric field is constant over time, and spherically symmetric, as\n\\[g_{\\mu\\nu}dx^\\mu dx^\\nu = f_{\\text{radial}}dr^2 + f_{\\text{orthogonal}}(r^2d\\theta^2 + r^2 \\sin^2\\theta d\\phi^2)\\]\nThis then reduces to an ordinary differential equation for \\(f_{\\text{radial}}, f_{\\text{orthogonal}}\\). Note how we have made an assumption for the form of the equation for the metric field. This is by no means a benign assumption, as it is this choice that banishes the spectre of the hole argument.\nSuppose we had chosen the metric field to be not constant over time, but merely spherically symmetric, then we would find that we have a whole family of solutions. Take the previous solution of \\(g_{\\mu\\nu}\\), and construct a spherical shell around the mass point. Now, we can apply the hole argument on the spherical shell to “ring spacetime like a bell”, obtaining another spherically symmetric solution. 3\n3 \nAs all the Heavens were a Bell,\nAnd Being, but an Ear,\nAnd I, and Silence, some strange Race,\nWrecked, solitary, here –\n\n\n\n\nRinging spacetime like a bell, creating a whole family of spherically symmetric solutions. Figures modified from (Norton, Pooley, and Read 1999)\n\n\nThe gauge freedom interpretation states that any solution to the field equations are equivalent. They are practically equivalent, in the sense that no observable differences exist between them. They are ontologically equivalent, in the sense that no solution is “absolute” compared to others that are “derivative”. The first claim is an experimental fact, while the second claim is a metaphysical interpretation. The metaphysical interpretation does not necessarily follow from the experimental fact, yet it is hard to admit the experimental fact and reject the metaphysical interpretation.\nThe parallel with the Newtonian absolute vs relative inertial frames is obvious. (Rynasiewicz 2012) noted if we apply the hole argument to a universe with no mass, we obtain a hole argument for special relativity that should imply that the one-way speed of light is also a gauge freedom. So for example, if we apply a shearing transform \\(x \\mapsto x + 0.1 t\\), we would increase the one-way speed of light in the \\(+x\\) direction, and decrease it in the \\(-x\\) direction, without changing any observable predictions."
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#the-geometry-of-feeling",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#the-geometry-of-feeling",
    "title": "Hole Argument and Inverted Qualia",
    "section": "The geometry of feeling",
    "text": "The geometry of feeling\n\nEasy, hard, meta\nDavid Chalmers proposed three problems of consciousness.\nThe easy problem is the scientific problem of human perception, cognition, and motor control: how memories work, how vision recognizes objects, etc. By “easy”, Chalmers was not dismissing them as “intellectually easy”, but that they are within the paradigm of science as currently understood. They are about as easy as colonizing Mars or curing cancer.\nThe hard problem is the easy problem, but with something extra that seems impossible to even fit into a scientific system. What that something extra is, philosophers are unable to say, but they typically give it the name of “qualia”, “experience”, “phenomenal awareness”, etc.\n\nWhat makes the hard problem hard and almost unique is that it goes beyond problems about the performance of functions. To see this, note that even when we have explained the performance of all the cognitive and behavioral functions in the vicinity of experience—perceptual discrimination, categorization, internal access, verbal report–there may still remain a further unanswered question: Why is the performance of these functions accompanied by experience?\n(D. J. Chalmers 1995)\n\nWhile decades of science have made good progress on the easy problem, centuries of philosophical disputations have not made progress on the hard problem. To bypass the impasse, Chalmers proposed the meta problem: Why is the hard problem a problem?\n\nWhat exactly are the problem data that need explaining? They can be construed as verbal reports (my saying ‘Consciousness is hard to explain’), as judgments (my forming the judgment that consciousness is hard to explain), or as dispositions to make these reports and judgments. Verbal reports are perhaps the most objective data here, but they are also a relatively superficial expression of an underlying state that is really what we want to explain. So I will generally focus on dispositions to make verbal reports and judgments as what we want to explain.\n(D. Chalmers 2018)\n\nThe easy problem of consciousness, being part of the domain of science, excited little philosophical attention, while the hard and the meta problems excited a vast discourse. It is not within my power to review the literature, though it might be within my power to solve them.\nLet’s consider an analogy. The easy problem of biology would be: How does biological machines work? The hard problem: Why is the performance of these functions accompanied by life? The meta problem: What kind of cognition do people have, such that they can see a machine performing all the motions of life, and yet still call it “lifeless”? This analogy was considered by Dennett and rejected by Chalmers as a disanalogy. (Garrett 2006)\nIn John Searle’s Chinese room story, a man who knows nothing of Chinese, by executing an algorithm with pen and paper, could converse in Chinese writing. Many, including Searle, thought that the Chinese room does not really understand Chinese. This gives us another analogy.\nThe easy problem of Chinese: What algorithms can converse in Chinese text? The hard problem of Chinese: Why is the performance of Chinese speaking in a Chinese-speaker accompanied by understanding? The meta problem: What kind of cognition do people have, such that they say the Chinese room “lacks understanding”?\nIt is my aim to dissolve the hard problem of consciousness by showing that it has no explanandum. That is, the non-reducible form of consciousness does not exist, so the hard problem is meaningless.\n\n\nInverted qualia\n\n\n\nA normal-colored photo and an inverted version of it. Figure from (Byrne 2004, fig. 4)\n\n\nThe inverted qualia thought experiment has been used, like the p-zombie, in a whole host of arguments involving consciousness and the qualia. We consider the case involving functionalism, which is currently the most fashionable among cognitive psychologists and computer scientists. Other variants are reviewed in (Byrne 2004).\nAccording to functionalism, mental states are best understood as functional states, that is, mathematical functions that map perceptual inputs to behavioral outputs. It’s the intricate web of causal relations that constitutes a mental state, rather than the specific physical makeup realizing those relations.\nIn the anti-functionalism case, we consider two individuals, “Invert” and “Nonvert”, are functionally identical. They receive the same visual input (a tomato), undergo the same internal processing, and produce the same behavioral outputs (saying “that’s a red tomato”). However, their subjective experiences – their qualia – differ drastically. Where Nonvert sees red, Invert experiences green (or another color qualia entirely). They outwardly behave in the same way, and all functional measurements, from verbal reporting, psychological experiments, to MRI scanning, all find them the same, and yet the qualia of any color the Invert sees is rotated 180 degrees compared to that of the Nonvert.\nFormally:\n\nThe following spectrum inversion scenario is possible: Invert and Nonvert are functionally alike, and are both looking at a tomato.\nThus, the mental does not supervene on functional organization.\nThus, functionalism is false.\n\nSimilarly the shifted qualia thought experiment does not completely invert color space, but simply shift it, so that dark red looks slightly bluer, etc.\n\n\nOther inversions\nIn his Book of Optics, Alhazen rejected the theory that the eye works like a camera obscura, as it would create an inverted image. He also rejected any significant deviation from perfect regularity of the eye, as it would create “monstrous” images like seeing everything in a funhouse mirror. Similarly, da Vinci knew that the pupil acts as a camera obscura, but developed no less than 8 different hypothetical mechanisms inside the eye to re-invert the image again, so the image lands right-side-up on the retina. (Eastwood 1986)\n\n\n\nLeonardo da Vinci’s drawings comparing the eye to a camera obscura. From Codex Atlanticus (1490-1495). Note the double-inversion to make sure the image lands right-side-up on the retina. Figure from Wikimedia Commons.\n\n\nIn his famed Treatise of Man, Descartes solved the problem according to functionalism: It is not an issue if the image on the retina is “upside-down”. It only matters if the retina is wired to the brain in a regular way, such that the neural control of motion by sight works. Informally speaking, it doesn’t matter if the wires are wired upside-down, as long as the wires are not crossed.\n\n\n\nIllustration from Descartes’ Treatise of Man, showing how the eye can guide the arm despite an “upside down” image. Suppose the eye sees the hand is pointing at the middle of the arrow, and then the brain needs to have the finger pointing at the tail of the arrow, then the control circuit goes from the retinal-C point, to the optical-nerve-C point, to the brain-C point, and finally to the arm abductor muscle.\n\n\nIn Leibniz’s third paper of the Leibniz–Clarke correspondence, Leibniz proposed the “inverted space” thought experiment:7 Suppose at the moment of creation, God were to switch East and West, then nothing would act different. Since God must have created the world according to the principle of sufficient reason, God could not have chosen arbitrarily. Ergo, such a choice never existed in the first place, meaning that space is relational, not absolute.\n7 \n… supposing Space to be Something in it self, besides the Order of Bodies among themselves, that ’tis impossible there should be a Reason, why God, preserving the same Situations of Bodies among themselves, should have placed them in Space after one certain particular manner, and not otherwise; why every thing was not placed the quite contrary way, for instance, by changing East into West.\n(Clarke 1717)\n\n\n\nThe broken symmetry argument\nA sophisticated objection to inverted qualia, based on color science, states that since color space has no nontrivial symmetries, the thought experiment is impossible. For example, saturated yellow does not merely look different from saturated red, but also looks brighter. In this view, “simply yellow” is not simply yellow. A point in color space is not simply a point. It is already inherently structured. Yellow is the brightest of all saturated colors, while violet is the dimmest, etc. (Hilbert and Kalderon 2000) argued that every possible quality space must be asymmetrical, in the sense that the only automorphism is the identity map, of \\(x \\mapsto x\\).\nThis appears to me an objection that is too strong, as there really do exist quality spaces that are symmetric. In humans, left and right are symmetric. And if there exists someone with a color space that is mirror-symmetric across some plane, then the following kind of experiment might be possible:\n\n“They are different.” [points at this patch and that patch]\nThe experimenter takes both patches away, then brings back one patch, and asks “Is this the left patch or the right patch?”\n“I don’t know.”\n\nThis is similar to the case of dark phenomena of JND color pairs (to be detailed below), but it is different in that, unlike JND color pairs, there would be color patches that are as different as left hand from right, as different as red and green, but they simply cannot see color-in-itself, only color-between-them. It might not be strange for them, since they were born this way, but it would be strange for normal people. However, the strangeness is conceivable, perhaps even unremarkable. When I was a kid, left and right was like that for a long time. I suspect that some people never acquire a distinction between left and right, until they acquire some asymmetric scar, such as a burn-mark on one hand.\nIf we look beyond the human umwelt, then this may not be just a thought experiment, as there are some highly symmetric quality spaces in non-human animals.\nLight, being electromagnetic waves, can be polarized. The space of possible polarizations is isomorphic to a ball, the Poincare ball. The mantis shrimp species Gonodactylus smithii can detect the polarization of light over the entire 3-dimensional Poincaré ball (Kleinlogel and White 2008). It performs this by building 3 kinds of ommatidia, each specialized for two kinds of polarization. One is specialized for the horizontal-vertical, one for the diagonal-antidiagonal, and one for the clockwise-anticlockwise.\n\n\n\nPolarization states on the Poincaré ball. Figure from Wikimedia Commons.\n\n\nNow, a Gonodactylus philosopher might propose the following inverted qualia problem: What if my qualia on the Poincaré ball is inverted compared to yours? That is, what if when you see a horizontally polarized light, you feel the same way as I see a vertically polarized light, and similarly across all of the ball? We can even imagine more exotic reflections, such as one that reflects across the \\((0.3, 0.3, 0.9)\\) direction, etc.\n\n\nGauge freedom in qualia-space\nAt this point, we have a formal analogy between the thought experiments concerning qualia and those concerning spacetime. Explicitly:\n\n\n\ncolor\nspacetime\n\n\n\n\nshifted qualia\nhole argument\n\n\ninverted qualia\nLeibniz inversion\n\n\ncolor space\nspacetime\n\n\nqualia of a color\na point in spacetime\n\n\ngeometry of color space\ngeometry of spacetime\n\n\nqualia realism\nmanifold substantialism\n\n\ncolor space is but its geometry\ngauge freedom\n\n\n\nIn manifold substantialism, points on the spacetime manifold exist in themselves, and one can ask what a point on the spacetime manifold is, independent of the metric or the energy-momentum field, and what happens if the fields are stretched differently over the same manifold. In qualia realism, points in the space of color exist in themselves, and one can ask what happens if the same photons are mapped to different points in color space.\nJND in color perception possibly shows that we see metric, not colors themselves.\nPoints in spacetime do not exist in themselves, but reduces to the interplay of metric and energy-momentum tensor fields up to gauge freedom. Similarly, qualias do not exist in themselves, but reduces to the interplay between brain, the rest of the body, and the outside world. Arguments in support of gauge freedom in general relativity can be directly translated to arguments in support of functionalism in consciousness.\nFurthermore, if the phenomenology of color is but the metric geometry of color space, then we have not only dispelled the inverted qualia problem, but also have dispelled the problem of other minds. We would be able to scientifically answer Nagel’s question of what it is like to be a bat. The hard problem of being a bat dissolves, leaving the easy problem. It simply requires us to construct the geometry of a bat’s color space from its neurophysiology. We may not even need to perform psychophysical experiments, such as asking a bat to make JND distinctions (though psychophysics is possible with animal subjects, see for example (Jacobs 1982)). We may construct it by information-geometric methods, like how MacAdam ellipses can be constructed by the light sensitivity curves of retinal cells.\n\n\nDark phenomena\n\nNeurophenomenology is possible; phenomenology is impossible.\n(Metzinger 2004, 83)\n\nEverything I see, I know that I see. Everything that I hear, I know that I hear. Everything that I think, I know that I think. What could be clearer? Descartes based his entire philosophy on these kinds of self-evident truths, and these are still the starting points of many modern philosophies of the mind and consciousness.\nThe concept of qualia attempts to formalize this self-evident truths. There is no generally agreed-on definition of qualia, but according to the original proposer, David Lewis, a qualia must contain several properties (Metzinger 2004, 68). Of those, we consider the first one:\n\nSubjective identity criteria are available, by which we can introspectively recognize their transtemporal identity.\n\nWhile the concept of qualia might seem self-evident, such self-evident truths can be questioned. In blindsight, I see things that I don’t know that I see. In Anton’s syndrome, I don’t see things, yet I think that I see.8 In Cotard’s delusion, I live yet I think that I am dead.\n8 There was a philosopher who had taken Anton’s Syndrome very seriously, but in the opposite direction, in the spirit of one man’s modus ponens is another man’s modus tollens:\n\nI still vividly remember one heated debate at an interdisciplinary conference in Germany a number of years ago, at which a philosopher insisted, in the presence of eminent neuropsychologists, that Anton’s syndrome does not exist because a priori it cannot exist.\n(Metzinger 2004, 235)\n\nAs a mathematician, I often know things without knowing how I know. When doing mental arithmetics, usually I do it both ways. One algorithm, operating consciously, goes from the highest digit down; the other algorithm, operating unconsciously, goes from the lowest digit up. As I consciously grind out digits from one end, digits simply “emerge” out of the other end. Like two teams digging a tunnel, they finally meet in the middle; the digits ripple-carry; the mouth vocalizes the final answer.\nDuring deep contemplations of high-dimensional geometric objects, my self-awareness is turned down to a whimper, dimly illuminated by the sparks and piezoluminescence of vast gears and pulleys turning in the dark mill of the brain,9 where the light of consciousness can never penetrate. A few times, I came back to consciousness on the carpet, not knowing how I got there, but with a clear feeling that an answer is close. Then I find the answer – or not. The non-conscious parts of the brain make plenty of mistakes too.\n9 I really wanted to write “dark Satanic mills of the mind”, but that would be too much purple prose.Consider a pair of pure lights, at \\(550 \\;\\mathrm{nm}\\) and \\(554 \\;\\mathrm{nm}\\). For an observer with good vision, they are separated by a JND, so if the observer sees two patches of light shining on two plates of frosted glass placed close to each other, then the observer can just barely see that they are not the same color. However, as soon as the two lights are turned off, the difference disappears. The observer cannot recall one as “green-550” and the other as “green-554”. Both would be recalled as “kind of green”. The observer cannot tell if a single patch of light is closer to green-550 or green-554. The observer cannot tune a laser by sight so that its color matches green-550 rather than green-554. According to experiments in the 1950s, though people can distinguish around 150 pure spectral colors in the sense of JND, they can identify only around 15 pure spectral colors in the absolute sense. (Halsey and Chapanis 1951)\nAccording to Thomas Metzinger (Metzinger 2004, chap. 2.4), such “dark phenomena” (Roden 2015, chap. 4) constitute an empirical disproof of qualia as defined by Lewis, because subjective identity criteria are not available, since we cannot introspectively recognize the transtemporal identity of green-550.\nA dark phenomenon rises from dust, does its job, then falls back to dust. It cannot be interrogated, redirected, paused, vocalized, remembered, threatened, or inspected. In this way, green-550 and green-554 are dark phenomena. They are real phenomena and have real mental functions, but they cannot be captured or interrogated. A dark phenomenon, such as green-550, is an information object that only flows along hardwired circuits. The conscious part of the brain might echo a command “Store this phenomenon in long-term memory!” or “Reroute this phenomenon for verbal report!” but such commands are futile. The green-550 and green-554 phenomena are sent to some visual comparison module then discarded. The visual comparison module might output a bright phenomenon “They are different.”, but this bright phenomenon is merely an impoverished derivative of the dark phenomena that came before.10\n10 If a thought enters the mind but nobody talks about it, does it make a sound?What dark phenomena do resemble, however, are the gauge-freedom conception of spacetime. The metric in color space is just like the metric in spacetime. Just like how each color qualia has no individual transtemporal existence, each point in the spacetime manifold has no individual trans-manifold existence. Just like how the spacetime manifold is defined only up to gauge freedom, the color space is defined only up to diffeomorphism. Just like how the hole argument is dispelled by the gauge freedom viewpoint, the inverted or shifted qualia argument is dispelled if there is no ontological difference between diffeomorphically equivalent color spaces.\nContinuing the same argument, absolute color identification is like identifying large-scale structures of the universe. The point of perfect gray is the point furthest from the edge of chromaticity space. The grayscale axis is the line of perfect grays. The ring of saturated colors is the region close to the edge of chromaticity space. The island of absolute yellow is the part in the ring of saturated colors that is furthest away from black, etc."
  },
  {
    "objectID": "blog/posts/hole-argument-inverted-qualia/index.html#the-future-of-an-illusion",
    "href": "blog/posts/hole-argument-inverted-qualia/index.html#the-future-of-an-illusion",
    "title": "Hole Argument and Inverted Qualia",
    "section": "The Future of an Illusion",
    "text": "The Future of an Illusion\n\n[Folk Psychology] suffers explanatory failures on an epic scale, that it has been stagnant for at least 25 centuries, and that its categories appear (so far) to be incommensurable with or orthogonal to the categories of the background physical science whose long-term claim to explain human behavior seems undeniable. Any theory that meets this description must be allowed a serious candidate for outright elimination.\n(Churchland 1981)\n\n\nif commonsense intentional psychology really were to collapse, that would be, beyond comparison, the greatest intellectual catastrophe in the history of our species; if we’re that wrong about the mind, then that’s the wrongest we’ve ever been about anything. The collapse of the supernatural, for example, didn’t compare; theism never came close to being as intimately involved in our thought and practice – especially our practice – as belief/desire explanation is.\n(Fodor 1987, vii)\n\nThe above, I hope, constitutes a good argument for dissolving the hard problem of consciousness as a pseudoproblem, much as the hole argument of general relativity is dissolved as a pseudoproblem. However, even if the hard problem resists philosophical dissolution by the gauge freedom argument, it might still be dissolved practically, since the meta-problem has a purely material basis, and thus cannot resist materialist invasions in practice, metaphysical defences be damned.\n\nDual-process theory\nWhile some argue that the meta-problem is just the hard problem in disguise, and therefore is not solvable by purely materialist methods, the opinion seems to be a minority. In the paper that proposed it (D. Chalmers 2018), Chalmers discussed no less than 12 possible solutions to the meta-problem that are within the materialist paradigm of modern science. Here I describe one that gives a flavor of how things might go.\nIn cognitive psychology, there are many dual-process theories for explaining many cognitive processes. A theory is a dual-process theory if it follows the dual-process template. That is, if it models a cognitive process with an algorithm that has two parts, termed System 1 and System 2. System 1 is characterized by automatic, fast, and intuitive processing, while System 2 is deliberative, slower, and more analytical.\n(Fiala, Arico, and Nichols 2012) proposed a dual-process theory for the meta-problem of consciousness. According to them, people recognize something as an agent or not by a dual process. This is evolutionarily important for ancestral humans, because detecting whether that shaking in the grass is caused by an animal or not could be a life-and-death decision.\nSystem 1 for detecting agency uses the following heuristics: eye-like shapes on a head-like bump, unpredictable environmental reactions, and self-initiated movement beyond mere inertia. System 2 for detecting agency involves rational deliberation, theory application, and conscious reasoning. These processes are engaged when evaluating complex concepts, such as brain-based theories of consciousness.\nNow, the meta-problem of consciousness occurs when one attributes agency to the brain. The brain, lacking visible features like eyes, appearing inert within the skull, and not exhibiting self-propelled motion, is not an agent according to System 1. The persistent conflict between System 1 and System 2 is verbalized into the hard problem of consciousness: System 2 admits that the brain is enough for agency, while System 1 insists that it is still lacking something, be it “consciousness”, “qualia”, or “experience”.\n\n\nMathematical illusion\nAnother answer to the meta-problem of general relativity is an accident of mathematics. Traditionally, general relativity is written as a theory about a triple of mathematical objects: \\((\\mathcal M, g, T)\\). Looking at it, it seems obvious that there are three things: an underlying manifold \\(\\mathcal M\\), and two tensor fields \\(g, T\\) stretched over it. This then brings us the problem of interpreting each of them separately. In particular, what is the ontological status of the spacetime-in-itself \\(\\mathcal M\\) is, independently of \\(g, T\\)? However, this is an artifact of mathematical symbolism.\nDue to the historical development of differential geometry in the 19th century, its overarching paradigm in the Kuhnian sense – the prototypical examples – include the continuum mechanics of deformable surfaces, and practical geodesy. In continuum mechanics, there is literally an unchanging substance (a rubber sheet) with changeable property (the strain field). In practical geodesy, there is literally an unchanging substance (a map-paper) with changeable property (the geodesic lines).\nThe abstracted paradigm is then a substance-property duality, where the substance is a “raw” structure, upon which we may decorate with properties. However, it is quite possible to regard \\(\\mathcal M\\) as a derivative to the original \\((\\mathcal M, g, T)\\). Concretely, we would begin with \\(\\R^4\\) with \\(g, T\\) “baked in”, then quotient out these structures. This is analogous to how we may construct the affine space \\(\\mathbb A^3\\) by starting with \\(\\R^3\\), then quotient out the group of translations.\nIn short, there is no necessary division between substance and property, only conventional. Taking the conventional division as necessary leads to the meta-problem of general relativity.\n\n\n\nThe stubbornly persistent illusion. Figure from xkcd: Teaching Physics\n\n\n\n\nPhilosophy with a deadline\n\nLevel-1 or world space is an anthropomorphically scaled, predominantly vision-configured, massively multi-slotted reality system that is obsolescing very rapidly.\nGarbage time is running out.\nCan what is playing you make it to level-2?\n— Nick Land, Meltdown\n\nA 2009 survey of professional philosophers (largely analytical) over 30 major problems of philosophy showed only two problems commanded a rough consensus (over 70% agreement): the existence of the external world, and the nonexistence of God. Chalmers noted that despite some progress in the sophistication of arguments, there has been little convergence towards any consensus over time, other than the question of God. (D. J. Chalmers 2015)\n\n… none of these methods have led to recent convergence on answers to the big questions of philosophy. In the wake of each of these methods, philosophical disagreement is as rife as it ever was. Even within a tradition, there are few cases where the big questions are regarded as settled. Instead, these methods have led us again to more sophisticated versions of old disagreements.\n(D. J. Chalmers 2015)\n\nWhile the existence of the external world had always commanded a near-consensus, the question of God is interesting, as it has a dramatic and unidirectional change in opinions, from almost total consensus on the existence of God, to 73% consensus on His nonexistence. This is an example of historical change in the meta-problem of theology.\nI consider the meta-problem of consciousness as an example of “philosophy with a deadline” (Bostrom 2014). Traditionally, philosophy is a rather calmly affair, a great conversation down the ages with no deadline. It was simply assumed that, barring some kind of human extinction event, there has been and will always be time to think things through. However, if the meta problem is indeed amenable to scientific explanation, then it is exposed to technological intervention, because if the meta problem is naturalizable in theory, then it will be naturalized in practice.\nWhat might cause a historical change in the meta-problem of consciousness? Consider analyzing the meta-problem from the ecological point of view. A problem-ecology creates a distribution of answers to the hard problem, and a change in problem-ecology changes the distribution. Specifically, technical or biological changes to the human world might shift the population of future people from 95% consensus that the hard problem has an explanandum, to 80% consensus that it has no explanandum.\n(Chiang 2005) sketches out just such a scenario concerning free will. The retrocausal device is a button that lights up exactly 1 second before it is pressed. A human that interacts with such a device undergoes permanent and irreversible change of its neural circuitry. Before interaction, a human is prone to say and act as if believing in free will, but afterwards, the human is prone to say and act as if not believing in free will. The proliferation of the retrocausal device has created a dramatic shift in the problem-ecology of free will, from almost total consensus on free will, to almost total consensus on the opposite, more dramatic than the shift to atheism during the past few centuries.\nThe “Gordian scenario” (cutting the Gordian knot of consciousness) seems patently conceivable. Indeed, it has been conceived previously, although in the opposite direction. In an influential paper, Empiricism and the Philosophy of Mind, Wilfrid Sellars sketches out The Myth of Jones scenario (Sellars 1956). In the myth, humans were originally without a theory of mind. They lived, worked, and spoke, but not of mind, consciousness, or qualia. Instead, they spoke only of skin, sky, and everything there is to see and touch. These ancient humans were continuously frustrated by the black box of other people, as the same person in the same situation might do a totally different thing, and a person, unprovoked, might suddenly stand up and run off ot its own. Eventually, a genius named Jones made up a new theory – the theory of mind. According to the theory of mind, people not only speak with their mouths, but might also speak inaudibly “inside their heads”. People not only have skin and hair, but might also have invisible but real personalities. And so on. The effect is that, in the problem-ecology of primitive human society, theory of mind appeared as an adaptive solution.\nAnd the process might be reversed.\nIn the p-zombie version of the conceivability argument, by merely entertaining the possibility of p-zombies, one may be convinced that consciousness is non-physical. Similarly, by merely entertaining the possibility of the Gordian scenario, one may be convinced that qualia is debunked. This debunking argument may or may not be philosophically valid (D. Chalmers 2020). However, its practical consequences do not hinge on its philosophical validity. Even if it has no philosophical import, it is still a future where consciousness is rendered ineffectual. It is entirely possible that God has existed and still exists, but if He does, He has certainly become rather hands-off recently.\nFor the sake of argument, let us assume that souls are real.11 That is, consciousness is real and not purely reducible to the material. As the Cotard delusion shows, how a soul responds to the meta-problem of consciousness depends on what kind of brain it has. We can imagine one way to arrive at the Gordian scenario, where by technological intervention, high-functioning Cotard delusionists become healthier, more productive, and generally more rational (rationalism is about consistent winning) than baseline humans, and thus the human population evolves to mostly Cotard delusionists. What might this “Disneyland without Kids” be like?\n11 It is just convenient to use the word “soul”, as so much talk of non-reductive consciousness resembles the soul in its mysterious resistance to reduction to mere materialism.It is as if the material world has again awoken from a bad dream of consciousness, into the second eternal night, while the fate of the soul world becomes even more immaterial. Perhaps they will continue to experience the qualia of saying “I don’t exist” forever (a Cartesian nightmare), or perhaps they will bud off into a world of pure qualias (a Berkeleyan dream). No matter what, it seems that no matter how much consciousness tries to claim its (partial) priority or autonomy over the material, the Gordian scenario is an existential threat. Even immortals can’t subsist on pure aether."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html",
    "href": "blog/posts/statistical-mechanics/index.html",
    "title": "Statistical Mechanics",
    "section": "",
    "text": "Liouville’s theorem: Hamiltonian dynamics preserves density in phase space."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#introduction",
    "href": "blog/posts/statistical-mechanics/index.html#introduction",
    "title": "Statistical Mechanics",
    "section": "",
    "text": "Liouville’s theorem: Hamiltonian dynamics preserves density in phase space."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#max-ent-statistical-mechanics",
    "href": "blog/posts/statistical-mechanics/index.html#max-ent-statistical-mechanics",
    "title": "Statistical Mechanics",
    "section": "Max-ent statistical mechanics",
    "text": "Max-ent statistical mechanics\nThe idea of Gibbsian statistical mechanics is to study the evolution of an entire probability distribution over all possible states.\nWe have a state space \\(X\\), which by default is the phase space.\nWe study probability distributions (ensembles) over the state space.\nThe entropy of a probability distribution \\(\\rho\\) is \\[S[\\rho] := -\\int dx\\; \\rho(x) \\ln \\rho(x)\\]\n\nProposition 1 (compound entropy) \\(S_{X,Y} = S_Y + \\braket{S_{X|y}}_y\\)\n\n\nProof. Consider a compound system in ensemble \\(\\rho(x, y)\\). Its entropy is \\[S[\\rho] = -\\int dxdy \\; \\rho(x, y) \\ln \\rho(x, y)\\]\nWe can take the calculation in two steps: \\[S[\\rho] = -\\int dxdy \\; \\rho(x|y)\\rho(y) (\\ln \\rho(x|y) + \\ln  \\rho(y)) = S[\\rho_Y] + \\mathbb{E}_y[S[\\rho_{X|y}]]\\]\nHere, \\[S[\\rho_Y] = -\\int dy \\; \\rho(y) \\ln \\rho(y)\\]\nis the entropy of system \\(Y\\),\n\\[S[\\rho_{X | y}] = -\\int dx \\; \\rho(x|y) \\ln \\rho(x|y)\\]\nis the entropy of system \\(X\\) conditional on the state of system \\(Y\\) being equal to \\(y\\), and\n\\[\\mathbb{E}_y\\]\nmeans taking expectation over the state of system \\(Y\\).\nMore succinctly, \\(S_{X,Y} = S_Y + \\braket{S_{X|y}}_y\\).\n\nASSM. (maximum entropy distribution under constraint)\nIn most systems, everything measurable can be found by studying it as if it has the maximum entropy distribution under constraint.\n\nDifferential entropy depends on coordinates choice\nConsider the uniform distribution on \\([0, 1]\\). It is the maxent distribution on \\([0, 1]\\)… relative to the Lebesgue measure.\nSuppose we now stretch the \\([0, 1]\\) interval nonlinearly, by \\(f(x) = x^2\\), then the maxent distribution relative to that would no longer be the uniform distribution on \\([0, 1]\\). Instead, it would be the uniform distribution after stretching.\nThe problem is essentially this: if we change the coordinates, we change the base measure, and the differential entropy changes.\nTo fix this, we can use the KL-divergence, which is invariant under a change of base measure, as in \\[-D_{KL}(\\rho \\| \\mu) := - \\int dx\\; \\rho(x) \\ln\\frac{\\rho(x)}{\\mu(x)}\\]\nIn typical situations, we don’t need to worry ourselves with KL-divergence, as we just pick the uniform distribution \\(\\mu\\). When the state space is infinite in volume, the uniform distribution is not a probability measure, but it will work. Bayesians say that it is an improper prior.\nIn this interpretation, the principle of “maximum entropy distribution under constraint” becomes the principle of “minimal KL-divergence under constraint”, which is Bayesian inference, with exactly the same formulas.\nIn almost all cases, we use the uniform prior over phase space. This is how Gibbs did it, and he didn’t really justify it other than saying that it just works, and suggesting it has something to do with Liouville’s theorem. Now with a century of hindsight, we know that it works because of quantum mechanics: we should use the uniform prior over phase space, because phase space volumes have a natural unit of measurement: \\(h\\), Planck’s constant. Planck’s constant is a universal constant, so we should weight all of the phase space equally, giving a uniform prior.\nDEF. extensivity\nIn classical thermodynamics, extensivity means that entropy of the compound system can be calculated in a two-step process: calculate the entropy of each subsystem, then add them up. The important fact is that a subsystem still has enough independence to have its own entropy.\nThis is not always obvious. If we have two galaxies of stars, we can think of each as a “cosmic gas” where each particle is a star. Now, if we put them near each other, then the gravity between the two galaxies would mean it is no longer meaningful to speak of “the entropy of galaxy 1”, but only “the entropy of galaxy-compound 1-2”.\nIn statistical mechanics, extensivity usually means the Hamiltonian of each subsystem is unaffected by the state of the other subsystems, and the total Hamiltonian is the sum of all the Hamiltonians. However, other definitions exist.\nSETUP. the microcanonical ensemble\nIf the only constraint is the energy \\(E\\), then the maximal entropy distribution is the uniform distribution on the shell of constant energy \\(H = E\\). This is the microcanonical ensemble:\n\\[\\rho_E(x) \\propto 1_{H(x) = E}\\]\nIf we have a small system connected to a large system, then we typically don’t care about the large system, and only want to study the ensemble of the small system. In this case, we would first find the microcanonical ensemble for the total system, then integrate out of the large system, resulting in an ensemble over just the small system, like\n\\[\\rho_{small}(x) = \\int \\rho_{total}(x, y) dy\\]\nwhere \\(x\\) ranges over the state of the small system, and \\(y\\) that of the large system.\nIf we further assume that the compound system is extensive, then we can derive all the other ensembles – canonical, grand canonical, etc – depending on how the two systems are connected.\nIn the following theorem, we assume that the total system is extensive, and is already in the maximal entropy distribution (microcanonical ensemble). We derive the distribution of the small system by marginalizing out the large system.\n\nProposition 2 (Entropy is preserved in Hamiltonian systems) If a system is a Hamiltonian system with any Hamiltonian (which can change with time), then for any probability distribution \\(\\rho\\) over its phase space, its entropy is conserved over time.\nThe microcanonical ensemble is the unique maximizer of entropy under the constraint of constant energy.\n\n\nProof. The first result is by Liouville’s theorem.\nLet \\(\\rho\\) be the microcanonical distribution on the energy shell of \\(H = E\\). Given any other distribution \\(\\rho'\\), we have \\[S[\\rho'] = S[\\rho] - D_{KL}(\\rho' \\| \\rho)\\]\nwhich is maximized at only \\(\\rho' = \\rho\\).\n\n\nCorollary 1 Constrained maxent distributions are preserved in Hamiltonian systems\nThat is, given any set of constraints, if the Hamiltonian preserves these constraints over time, then any maximal entropy distribution satisfying those constraints is preserved by time-evolution under the Hamiltonian. In particular, if the Hamiltonian is constant over time, then any microcanonical ensemble is preserved by time-evolution.\n\n\nTheorem 1 (Canonical ensembles) If the two systems are in energy-contact, then the small system has the canonical ensemble\n\\[\n\\rho(x) \\propto e^{-\\beta H(x)}\n\\]\nwhere \\(\\beta\\) is the marginal entropy of energy of the large system:\n\\[\\beta := \\partial_E S[\\rho_{bath, E}]\\]\n\nSimilarly, if the two systems are in energy-and-particle-contact, then the small system has the grand canonical ensemble\n\\[\n\\rho(x) \\propto e^{-(\\beta H(x) + (-\\beta \\mu) N(x))}\n\\]\nwhere \\(-\\beta\\mu\\) is the marginal entropy of particle of the large system:\n\\[-\\beta\\mu := (\\partial_N S[\\rho_{bath, E, N}])_{E}\\]\nMore generally, if the two systems are in \\(q_1, \\dots, q_m\\) contact, then \\[\\rho(x) \\propto e^{-\\sum_i p_i q_i(x)}\\]\nwhere \\(p_i = (\\partial_{q_i} S[\\rho_{bath, q}])_{q}\\) is the marginal entropy for conserved quantity \\(q_i\\).\n\nProof. We prove the case for the energy-contact. The other cases are similar.\nSince the total distribution of the whole system is the maximal entropy distribution, we are faced with a constrained maximization problem:\n\\[\\max_\\rho S[\\rho]\\]\nThe entropy of the compound system satisfies\n\\[S = S_{system} + \\braket{S_{bath|system}}_{system}\\]\nSince the bath is so much larger than the system, we can take just the first Taylor expansion:\n\\[S_{bath|system} = S_{bath}(E) - \\beta E_{system}\\]\nwhere \\(E\\) is the total energy, and \\(E_{system}\\) is the energy of the system.\nThis gives us the constraint maximization problem of\n\\[\\max (S_{system} - \\beta \\braket{E_{system}})\\]\nThis is the statistical mechanics analog of the “maximize Helmholtz free entropy” problem.\nIn detail, we have a problem of variational calculus: \\[\n\\begin{cases}\n  \\min\\int dx\\; &\\rho(x)(\\ln\\rho(x) + \\beta E_{system}(x)) \\\\\n      \\int dx\\; &\\rho (x) = 1\n\\end{cases}\n\\]\nwhich by using the Lagrange multiplier, gives us the solution.\n\n\nProof. Alternatively, we don’t need the Lagrange multiplier. Define the Boltzmann distribution as\n\\[\\rho_B(x) = Z(\\beta)^{-1} e^{-\\beta E_{system}(x)}\\]\nwhere \\(Z(\\beta)\\) is the normalization constant (partition function), defined by\n\\[Z(\\beta) = \\int dx\\; e^{-\\beta E_{system}(x)}\\]\nIt remains to prove that \\(\\rho = \\rho_B\\).\nBy routine calculation, the optimization problem is equivalent to\n\\[\n\\min \\left(\\int dx\\; \\rho(x) \\ln\\frac{\\rho(x)}{\\rho_B(x)} - \\ln Z(\\beta)\\right)\n\\]\nand since \\(Z(\\beta)\\) is independent of \\(\\rho\\), we just need to solve\n\\[\n\\min \\int dx\\; \\rho(x) \\ln\\frac{\\rho(x)}{\\rho_B(x)} = \\min D_{KL}(\\rho \\| \\rho_B)\n\\] which is just the KL-divergence, and is minimized exactly at \\(\\rho = \\rho_B\\).\n\n\n\n\n\n\n\nextensivity\n\n\n\nExtensivitiy in statistical mechanics yields extensivity in thermodynamics. Specifically, writing \\(S_{bath}(E)\\), instead of \\(S_{bath}(E, E_{system})\\), requires the assumption of extensivity. Precisely because the bath and the system do not affect each other, we are allowed to calculate the entropy of the bath without knowing anything about the energy of the system.\n\\(S_{bath}\\) is the logarithm of the surface area of the energy shell \\(H_{bath} = E_{bath}\\). By extensivity, \\(H(x_{bath}, x_{system}) = H_{bath}(x_{bath}) + H_{system}(x_{system})\\), so the energy shells of the bath depends on only \\(E_{bath}\\), not \\(E_{system}\\).\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe proof showed something extra: If the small system is in distribution \\(\\rho\\) that does not equal to the equilibrium distribution \\(\\rho_B\\), then the total system’s entropy is\n\\[S = S_{max} - D_{KL}(\\rho \\| \\rho_B)\\]\nwhich reminds me of Sanov theorem and large deviation theory…\n\n\nEXR. Try deriving some other ensembles.\nFor example, what if we have a system in volume-contact, but not thermal-contact? This might happen when the system is a flexible bag of gas held in an atmosphere, but the bag is made of thermally insulation. Notice that in this case, energy exchange still occurs, so you should solve\nAs another example, what if we have a system in particle-contact, but not energy-contact? I don’t know when this might happen, but it could happen!\n\n\n\n\n\n\nTip\n\n\n\nIn statistics, such ensemble are called exponential families, so we can abstractly describe this as follows:\nIf a small system is in contact with a large system, and the total system is in the microcanonical ensemble, then the marginal distribution of the small system is a distribution from an exponential family."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#using-the-partition-function",
    "href": "blog/posts/statistical-mechanics/index.html#using-the-partition-function",
    "title": "Statistical Mechanics",
    "section": "Using the partition function",
    "text": "Using the partition function\nDEF. Let \\(Z = \\int dx\\; e^{-\\beta E(x)}\\) be the partition function, and let \\(f^* := \\ln Z\\).\n\\(Z\\) is a function of \\(\\beta\\), and other possible constraints that might directly change the energy levels of the system. We can more explicitly write it as \\[Z(\\beta; c) = \\int dx \\; e^{-\\beta E_c(x)}\\]\nwhere \\(c\\) stands for the other constraints on the system, such as the size of the container, the number of particles, etc.\n\nProposition 1 (The partition function generates all moments of energy) Let a system be in canonical ensemble with inverse temperature \\(\\beta\\), and let \\(K(t) := \\ln \\braket{e^{tE}}\\) be the cumulant generating function of its energy, then \\[K(t) = \\ln Z(\\beta-t) - \\ln Z(\\beta)\\]\nIn particular, the \\(n\\)-th cumulant of energy is\n\\[\\kappa_n(E) = K^{(n)}(t) |_{t=0} = (-\\partial_\\beta)^n (\\ln Z)\\]\nFor example, the first two cumulants are the mean and variance: \\[\\braket{E} = (-\\partial_\\beta) (\\ln Z), \\quad \\mathrm{Var}(E) = \\partial_\\beta^2 (\\ln Z)\\]\n\nIn typical systems made of \\(N\\) particles, where \\(N\\) is large, we have \\(\\ln Z \\propto N\\), thus showing that \\(\\sqrt{\\mathrm{Var}(E)}/\\braket{E} \\propto N^{-1/2}\\), meaning that the distribution of energy converges to the average value as \\(N \\to \\infty\\).\nA similar proposition applies for the grand canonical ensemble, etc."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#free-entropies",
    "href": "blog/posts/statistical-mechanics/index.html#free-entropies",
    "title": "Statistical Mechanics",
    "section": "Free entropies",
    "text": "Free entropies\nIn the above, the only constraint we have imposed is constant energy, yielding the microcanonical ensemble. We can allow more constraints, yielding the fully general Gibbsian statistical mechanics.\nA common trick in statistical mechanics is to characterize the same equilibrium in many different perspectives. For example, the canonical ensemble has three characterizations at least.\nWe have a state space \\(X\\), and probability distributions \\(\\rho\\) over \\(X\\).\nMacroscopic observables \\(A, B, C, \\dots\\) are functions of type \\(X \\to \\mathbb{R}\\).\nA constraint can be an equality or inequality on observables. For example, if we have a tube of jelly in a box of volume \\(10\\), then the constraint is \\(0 \\leq V(x) \\leq 10\\).\nA constraint can also be an equality or inequality on the distribution. For example, we can specify that the average energy is exactly 1, by \\[\\int dx\\; \\rho(x) E(x) = 1\\]\nThe equilibrium distribution under constraint is the maximal entropy distribution satisfying the constraints.\nDEF. (free entropies)\nJust like in thermodynamics, we can take convex duals of the entropy function, to obtain various free entropies.\nHelmholtz free entropy: \\[f[\\rho] := S[\\rho] - \\beta \\braket{E} = \\int dx \\; \\rho(x) (-\\ln \\rho(x) - \\beta E(x))\\]\nGibbs free entropy: \\[g[\\rho] = S[\\rho] - \\beta \\braket{E} - \\beta P \\braket{V}\\]\nAnd similarly for others.\n\nProposition 2 (The chain rule for free entropies) \\(f_X = S_Y + \\braket{f_{X|y}}_y\\), and similarly \\(g_X = S_Y + \\braket{g_{X|y}}_y\\), and similarly for all other convex duals of the entropy.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n  f_X &= S_X - \\beta \\braket{E}_x  \\\\\n  &= S_Y + \\braket{S_{X|y}}_y - \\beta \\braket{\\braket{E}_{x \\sim X|y}}_y \\\\\n  &= S_Y + \\braket{f_{X|y}}_y\n\\end{aligned}\n\\]\n\n\n\n\nProposition 3 (4 characterizations of the canonical ensemble)  \n\n(total entropy under fixed energy constraint) The canonical ensemble maximizes total entropy when the system is in contact with an energy bath that satisfies \\(\\partial_E S_{bath} = \\beta\\), and the total energy is fixed.\n(entropy under mean energy constraint) A system maximizes its entropy under constraint \\(\\braket{E} = E_0\\) when it assumes the canonical ensemble with \\(\\beta\\) that is the unique solution to \\(\\int dx \\; e^{-\\beta E(x)} = E_0\\).\n(thermodynamic limit): Take \\(N\\) copies of a system, and connect them by energy-contacts. Inject the system with total energy \\(NE_0\\), and let the system reach its microcanonical ensemble. Then at the thermodynamic limit of \\(N\\to \\infty\\), the distribution of a single system is the canonical distribution with \\(\\beta\\) that is the unique solution to \\(\\int dx \\; e^{-\\beta E(x)} = E_0\\).\n(free entropy) A system maximizes its Helmholtz free entropy when it assumes the canonical ensemble. At that point, the maximal Helmholtz free entropy is \\(f^* = \\ln Z\\), where \\(Z = \\int dx \\; e^{-\\beta E(x)}\\) is the partition function.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\nWe already proved this.\nUse the Lagrange multiplier.\nIsolate one system, and treat the rest as an energy-bath.\n\\(f[\\rho] = \\ln Z - D_{KL}(\\rho \\| \\rho_B)\\)."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#conditional-entropies",
    "href": "blog/posts/statistical-mechanics/index.html#conditional-entropies",
    "title": "Statistical Mechanics",
    "section": "Conditional entropies",
    "text": "Conditional entropies\n\nTheorem 5 (Conditional entropy) Given any random variable \\(X\\), and an “observable” variable \\(Y\\) that is determined by \\(X\\), if \\(X\\) is the maximal entropy distribution under constraint with entropy \\(S_X^*\\), then the observable \\(Y\\) follows a Boltzmann distribution, as \\[\\rho_Y^*(y) = e^{S_{X|y}^* - S_X^*}\\]\nwhere \\(S_{X|y}^*\\) is the maximal entropy for \\(X\\) conditional on the same constraints, plus the constraint that \\(Y = y\\).\nFurthermore, \\[e^{S_X^*} = \\int dy\\; e^{S_{X|y}^*}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy assumption, \\(X\\) is the unique solution to the constrained optimization problem\n\\[\n\\begin{cases}\n    \\max S_X \\\\\n    \\text{constraints on $x$}\n\\end{cases}\n\\]\nBecause \\(S_X = S_Y + \\braket{S_{X|y}}_{y \\sim Y}\\), we realize that solving that one constrained optimization problem is really solving an entire family of constrained optimization problems. In particular, it also solves the problem\n\\[\n\\begin{cases}\n    \\max S_Y + \\braket{S_{X|y}}_{y\\sim Y} \\\\\n    \\text{constraints on $x$}\n\\end{cases}\n\\]\nNow, we can solve the original problem in a two-step process: For each possible observable \\(y\\sim Y\\), we solve an extra-constrained problem:\n\\[\n\\begin{cases}\n    \\max S_{X|y} \\\\\n    \\text{original constraints on $x$} \\\\\n    \\text{$x$ must be chosen such that the observable $Y = y$}\n\\end{cases}\n\\]\nThen, each such problem gives us a maximal conditional3 entropy \\(S_{X|y}^*\\), and we can solve for \\(Y\\) by\n\\[\\max(S_Y + \\braket{S_{X|y}^*}_{y \\sim Y})\\]\nAgain, the solution is immediate once we see it is just the KL-divergence:\n\\[S_Y + \\braket{S_{X|y}^*}_{y \\sim Y} = - \\int dy \\; \\rho_Y(y) \\ln\\frac{\\rho_Y(y)}{e^{S_{X|y}^*}} = \\ln Z - D_{KL}(\\rho_Y \\| \\rho_Y^*)\\]\nwhere\n\\[Z = \\int dy\\; e^{S_{X|y}^*}, \\quad \\rho_Y^*(y) = \\frac{e^{S_{X|y}^*}}{Z}\\]\nAt the optimal point, the entropy for \\(X\\) is maximized at \\(S_X^* = \\ln Z - 0\\), so \\(Z = e^{S_X^*}\\).\n\n\n\n3 If you’re a pure mathematician, you can formalize this using measure disintegration.EXP. the canonical ensemble again\nConsider a small system with energy states \\(E_1, E_2, \\dots\\) and a large bath system, in energy contact. We can set \\(X\\) to be the combined state of the whole system, and \\(Y\\) to be the state of the small system.\nOnce we observe \\(y\\), we have fully determined the small system, so the small system has zero entropy, and so all the entropy comes from the bath system: \\[S_{X|y}^* = S_{bath} = S_{bath}(E_{total}) - \\beta E_y\\]\nConsequently, the distribution of the small system is \\[\\rho_Y(y) \\propto e^{-\\beta E_y}\\]\nwhich is the Boltzmann distribution, as expected.\nSimilar calculation gives us the grand canonical ensemble, etc.\nThe above theorem can be generalized to conditional free entropies.\n\nTheorem 6 (conditional free entropy) Given any random variable \\(X\\), and an “observable” variable \\(Y\\) that is determined by \\(X\\), if \\(X\\) is the distribution that maximizes Helmholtz free entropy under constraint with Helmholtz free entropy \\(f_X^*\\), then the observable \\(Y\\) is distributed as \\[\\rho_Y^*(y) = e^{f_{X|y}^* - f_X^*}\\]\nwhere \\(f_{X|y}^*\\) is the maximal Helmholtz free entropy for \\(X\\) conditional on the same constraints, plus the constraint that \\(Y = y\\).\nFurthermore, \\[e^{f_X^*} = \\int dy\\; e^{f_{X|y}^*}\\]\nSimilarly for Gibbs free entropy, and all other free entropies.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst note that \\(f_X = S_Y + \\braket{f_{X|y}}_y\\). Then we proceed to argue in the same way."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#fluctuation-relations",
    "href": "blog/posts/statistical-mechanics/index.html#fluctuation-relations",
    "title": "Statistical Mechanics",
    "section": "Fluctuation relations",
    "text": "Fluctuation relations\n\nFluctuation of observables\nSuppose we have a tank of oxygen gas, and it is in the equilibrium distribution (Maxwell-Boltzmann). Now, if we sample its pressure \\(P\\), then every time we sample it, we sample a particular microstate \\(x\\) from its equilibrium distribution, and each corresponds to a different pressure \\(P(x)\\). We know that these particular pressures should be tightly bunched around its average value – the thermodynamic pressure \\(\\braket{P}\\)… but how bunched-up is it?\nMore generally, suppose we have a system in the equilibrium state (maximal entropy under constraint), how much fluctuation does it have?\nEXP. systems in energy-contact, the zeroth law\nTake several systems, and let them exchange energy, but nothing else. For concreteness, we can imagine taking several copper tanks of gas, and let them touch each other. The tanks hold their shape, not expanding or contracting.\nThe system has total entropy \\[S = \\sum_i S_i(E_i, A_i)\\]\nwhere \\(A_i\\) stand for the other state variables we don’t care about, because they are held constant\nThere is a single constraint of constant total energy: \\[E = \\sum_i E_i\\]\nIn the thermodynamical limit, the compound system reaches the maximal entropy state \\(E_1^*, \\dots, E_n^*\\), which solves the following constrained maximization \\[\n\\begin{cases}\n    \\max \\sum_i S_i(E_i, A_i)\\\\\n    E = \\sum_i E_i\n\\end{cases}\n\\]\nBy calculus, at the optimal point, all systems satisfy\n\\[\n(\\partial_{E_i} S_i)_{A_i} = \\beta\n\\]\nfor some number \\(\\beta\\). This is the zeroth law of thermodynamics.\nHowever, we are in statistical mechanics, so the compound system actually does not stay exactly at the optimal point. Instead, the energy levels fluctuate.\nLet us write the fluctuation vector as\n\\[Y = (\\Delta E_1, \\dots, \\Delta E_n)\\]\nwhich satisfies the constraint \\(\\sum_i \\Delta E_i = 0\\).\nLet the fluctuation vector be the observable. As proved previously, the fluctuation satisfies\n\\[\\rho_Y(y) \\propto e^{S^*_{X|y}}\\]\nwhere \\(S^*_{X|y}\\) is the entropy of the compound system, given \\(Y = y\\). For small fluctuations, this is just:\n\\[S^*_{X|y} = \\sum_i S_i(E_i^*) + (\\partial_{E_i} S_i)_{A_i} \\Delta E_i + \\frac 12 (\\partial_{E_i}^2 S_i)_{A_i} (\\Delta E_i)^2 + \\cdots\\]\nSince \\(\\sum_i \\Delta E_i = 0\\), this just gives\n\\[\\rho_Y(y) \\propto e^{\\sum_i \\frac 12 (\\partial_{E_i}^2 S_i)_{A_i} (\\Delta E_i)^2}\\]\nNow, \\(\\partial_E S = \\beta\\), and \\(\\partial_E^2 S = -\\frac{1}{T^2 C}\\) in typical thermodynamic notation, where \\(C\\) is \\(\\partial_T E\\), the heat capacity (holding all other variables \\(A\\) constant).\nIn particular, for gases, it is\n\\[\\rho_Y(y) \\propto e^{-\\sum_i \\frac{1}{2T^2 C_{V, i}} (\\Delta E_i)^2}\\]\nwhere \\(C_{V, i}\\) is the constant-volume heat capacity of the \\(i\\) -th gas.\nIn particular, if we have monoatomic ideal gas, with \\(C_V = \\frac 32 N\\), then the size of a typical fluctuation is on the order of\n\\[\\sim\\sqrt N T \\sim N^{-1/2} E^*\\]\nThat is, a typical fluctuation energy is only \\(N^{-1/2}\\) that of the mean energy.\n\n\nDensity fluctuation in the canonical ensemble\nTODO sethna section 6.7"
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#maximum-caliber-statistical-mechanics",
    "href": "blog/posts/statistical-mechanics/index.html#maximum-caliber-statistical-mechanics",
    "title": "Statistical Mechanics",
    "section": "Maximum caliber statistical mechanics",
    "text": "Maximum caliber statistical mechanics\n(Ghosh et al. 2006; Ghosh et al. 2020)\nIn general, maximal caliber statistical mechanics is like maximal entropy statistical mechanics. You write down a formula for path-space entropy, and a constraint on allowed paths. Maximize the entropy under constraint, and you would obtain a Boltzmann distribution in path space.\n\nMarkov chains\n\\(N\\) timesteps in total.\n\\(s_t\\) is the state of timestep \\(t\\).\nThere are \\(m\\) states in total.\n\nTheorem 10 If we fix the singleton probability \\(p_i\\) of each state, then the maximum entropy distribution is the Markov chain with uniform initial probability and \\(p_{i\\to j} = p_i p_j\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy Lagrange multipliers.\nIf we fix the singleton probability of each state, then the problem is a constrained maximization problem\n\\[\n\\begin{cases}\n    \\max S \\\\\n    \\frac 1N \\sum_t 1[s_t = k] = p_k, \\quad \\forall k = 1, \\dots, m\n\\end{cases}\n\\]\nThis is the same problem as \\(N\\) balls in a certain constrained microcanonical ensemble. When \\(N\\) is large enough, the discrete “macrostate” \\(\\frac 1N \\sum_t 1[s_t = k]\\) becomes continuous, and we can use the Lagrange multiplier to obtain \\[\\rho(s_1, \\dots, s_N) = \\frac 1Z e^{-\\sum_{k=1}^m \\lambda_k (\\frac 1N \\sum_{t=1}^N 1[s_t =k] - p_k)}\\]\nThis factors over time \\(t\\), giving us \\[\\rho(s_1, \\dots, s_N) = \\prod_{t=1}^N \\rho(s_t)\\]\nwith\n\\[\\rho(s_t=k) \\propto e^{-\\sum_{k=1}^m\\frac{\\lambda_k}{N}(1[s_t =k] - p_k)} \\propto e^{-\\frac{\\lambda_k}{N}}\\]\nThe multiplier \\(\\lambda_k\\) can be found by the typical method of solving \\(p_k = -\\partial_{\\lambda_k}\\ln Z\\), or we can take the shortcut and notice that \\(\\rho(s_t=k) = p_k\\), and be done with it.\n\n\n\n\nTheorem 11 If we fix the transition probability \\(p_{i \\to j}\\) of each state-pair, then the maximum entropy distribution is the Markov chain with uniform initial probability. If we fix the initial probability, then it’s still the Markov chain with the same transition probabilities.\nAnd more generally, if we fix \\(n\\)-th order transition probability \\(p_{i_1, \\dots, i_n \\to j}\\), then we obtain an \\(n\\)-th order Markov chain model.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSimilarly as above, the path-space distribution is \\[\\rho(s_1, \\dots, s_N) \\propto \\prod_{t=1}^{N-1} e^{-\\sum_{k, k' \\in 1:m} \\frac{\\lambda_{k, k'}}{N} 1[s_t = k, s_{t+1} = k']} \\propto \\prod_{t=1}^{N-1} p_{s_t \\to s_{t+1}}\\]\nBecause the distribution does not specify \\(s_1\\), it is uniformly distributed on \\(s_1\\). Otherwise, we can constrain \\(s_1\\) with yet another set of Lagrange multipliers and obtain \\(\\rho(s_1, \\dots, s_N) \\propto \\rho(s_1) \\times_{t=1}^{N-1} \\rho(s_t, s_{t+1})\\). Similarly for higher orders.\n\n\n\n\n\ndiffusion\nThe path-space entropy\n\\[\nS = - \\int \\rho(x) \\ln \\rho(x) D[x]\n\\]\nwhere \\(D[x]\\) means we integrate over path space, and \\(x: [0, T] \\to \\mathbb{R}^n\\) is a path.\nWe discretize the path into \\(x: \\{0, 1, 2, \\dots, N\\} \\to \\mathbb{R}^n\\), then because we can decompose\n\\[\n\\rho(x) = \\rho(x_0) \\rho(x_1 | x_0) \\cdots \\rho(x_N | x_{0:N-1})\n\\]\nthe path-space entropy decomposes sequentially:\n\\[\nS = S[x_0] + E[S[x_1 | x_0]] + E[S[x_2 | x_{0:1}]] + \\dots + E[S[x_N | x_{0:N-1}]]\n\\]\nTo prevent the entropy from diverging, we need to impose some constraints. If we constrain the second moment of each step, as\n\\[(E[x_t|x_{0:t-1}], E[x_t^2|x_{0:t-1}]) = (0, \\sigma^2), \\quad \\forall t \\in 0:N\\]\nby reasoning backwards from \\(t = N\\) to \\(t=0\\), we find that the maximal entropy distribution is a white noise:\n\\[\n\\rho(x) = \\prod_{t\\in 0:N} \\rho(x_t), \\quad \\rho(x_t) \\propto e^{-\\frac{\\|x_t\\|^2}{2\\sigma^2}}\n\\]\nIf you have studied dynamic programming and cybernetics, this should look very similar to the argument by which you derived the LQR.\nTo keep the path from exploding into white noise, we instead impose the constraints on the step sizes\n\\[(E[x_t - x_{t-1}|x_{0:t-1}], E[\\|x_t - x_{t-1}\\|^2|x_{0:t-1}]) = (0, \\sigma^2), \\quad \\forall t \\in 1:N\\]\nand \\(x_0 = 0\\).\nNow, as in dynamical programming, we can reason backwards from \\(t = N\\) to \\(t=0\\), and we find that the maximal entropy distribution is the Brownian motion\n\\[\\rho(x) \\propto e^{-\\frac{\\sum_{t\\in 1:N} \\| x_t-x_{t-1}\\|^2}{2\\sigma^2}}\\]\nIf we constrain the first and second moments of each step, and allow them to be affected by the previous step, as in\n\\[\n\\begin{cases}\n    E[x_t - x_{t-1}|x_{0:t-1}] &= \\mu(t, x_{t-1}) \\\\\n    E[(x_t - x_{t-1}) (x_t - x_{t-1})^T|x_{0:t-1}] &= \\Sigma(t, x_{t-1})\n\\end{cases}, \\quad \\forall t \\in 1:N\n\\]\nthen, reasoning backwards as before, we would obtain the Fokker–Planck equation.\nOther results, such as the Green-Kubo relation, the Onsager reciprocal relations, etc, can be similarly derived by imposing the right constraints in path space. (Hazoglou et al. 2015)\n\n\nFluctuation-dissipation relations\nImagine if you have a weird liquid. You put a piece of pollen into it, and watch it undergo Brownian motion. Now I flip a switch and suddenly all friction disappears from the liquid. What would happen? The pollen is still being hit by random pieces of molecules, so its velocity is still getting pushed this and that way. However, without friction, the velocity at time \\(t\\) is now obtained by integrating all past impulses, with no dissipation whatsoever. Consequently, its velocity undergoes a random walk, and its position undergoes \\(\\int_0^t W_s ds\\). This is very different from what we actually observe, which is that its position undergoes a random walk, not a time-integral of it.\nIn order to reproduce the actual observed behavior, each fluctuation of its velocity must be quickly dissipated by friction, in a perfect balance such that \\(\\frac 12 m \\braket{v^2} = k_BT\\) exactly. This perfect conspiracy is the fluctuation-dissipation relation (FDR), or rather, the family of FDRs, because there have been so many of those.\nEach FDR is a mathematical equation of form\n\\[\n\\text{something about fluctuation} = \\text{something about dissipation}\n\\]\nThe prototypical FDR is the Einstein relation, to be derived below:\n\\[\n\\underbrace{(\\beta D)^{-1}}_{\\text{fluctuation}} = \\underbrace{\\gamma}_{\\text{dissipation}}\n\\]\nwhere the left side can be interpreted as the strength of molecular noise, and the right side as the strength of molecular damping.\n\n\nEquality before the law\nWhy should there be a mathematical relation between fluctuation and dissipation? I think the deep reason is “equality before the second law”.\nIf we pause and think about it, then isn’t it strange that, despite the molecular storm happening within a placid cup of water, it does not actually fall out of equilibrium? That, despite every molecule of water rushing this way and that, the cup of water as a whole continues to stay without a ripple? What is this second law, the invisible hand pushing it towards statistical equilibrium and keeping it stable? And that is not a question I’m going to answer here. Perhaps calling it an “invisible hand” is already a bad metaphor. Nevertheless, the second law requires the stability of equilibrium distributions (if equilibria exist).\nNow, a system in a statistical equilibrium would, once in a while, deviate significantly from the maximally likely state, just like how we might occasionally flip 10 heads in a row. Nevertheless, if the second law is to be upheld, then significant deviations must be pushed back. In other words, fluctuation is dissipated.\nAssuming that in equilibrium, the system is undergoing a random walk, then by the theory of random walks, internally generated fluctuation must be dissipated according to a mathematical law. The law does not need any further input from physics. Indeed, it has no dependence on any physical observations, and belongs to the pure mathematical theory of random walks.\nFor example, a little pollen in a dish of water might occasionally have a large momentum, but it is very rare. Because it is rare, if we do observe it, we can predict that the next time we look at a pollen, it would have a much lower momentum.\nNow, when physicists use the word “dissipation”, they mean the restoring effect of a system under external dissipation, such as pulling on a pollen by an electrostatic field. However, physical laws are equal, so internal dissipation is external dissipation.\nThus, we see that each FDR manifests as an “equality before the law”:\n\\[\n\\begin{aligned}\n&\\text{fluctuation} \\\\\n\\underbrace{=}_{\\text{random walk theory}} &\\text{dissipation (of internal fluctuations)} \\\\\n\\underbrace{=}_{\\text{equality before the law}} &\\text{dissipation (of external fluctuations)}\n\\end{aligned}\n\\]\n\n\nOne-dimensional FDR\nConsider a simple model of Brownian motion. We have a particle in a sticky fluid, moving on a line. The particle starts at \\(x = 0, t = 0\\), and at each time-step of \\(\\Delta t\\), it moves by \\(\\Delta x\\) to the left or the right.\nThe fluid is at temperature \\(\\beta^{-1}\\), and we pull on the particle at constant force \\(F\\). We expect that \\(F = \\gamma \\braket{v}\\), where \\(v\\) is the ensemble-average velocity of the particle, and \\(\\gamma\\) is the viscosity constant.\nNow, we let the particle move for a time \\(t = N\\Delta t\\), where \\(N\\) is a large number. The particle would have arrived at some point \\(x\\), which is a random variable. The particle’s time-averaged velocity is \\(v = x/t\\).\nThe number of possible paths that connect \\((0, 0)\\) with \\((t, x)\\) is \\(\\binom{N}{\\frac N2 - \\frac{x}{2\\Delta x}}\\), therefore, the path-space entropy is\n\\[S_{path} = \\ln \\binom{N}{\\frac N2 - \\frac{x}{2\\Delta x}} \\approx N \\left(\\ln 2 - \\left(\\frac{x}{N\\Delta x}\\right)^2\\right)\\]\nwhere the approximation is either by Stirling’s approximation, or the binary entropy function.\nBecause the external force performs work \\(Fx\\), which is dissipated into the sticky liquid at temperature \\(\\beta\\), we also have\n\\[S_{work} = \\beta F x\\]\nBecause \\(N\\) is large, \\(\\braket{x}\\) should be highly concentrated around the point of maximal entropy. That is, we should have\n\\[\n\\braket{x} \\approx \\mathop{\\mathrm{argmax}}_x (S_{path} + S_{work})\n\\]\nNotice how this is the exact same problem as the case where we have a rubber band.\nThe equation on the right is quadratic in \\(\\braket{x}\\), and achieves maximum at \\(2\\braket{x} = \\beta FN(\\Delta x)^2\\), which simplifies to the Einstein relation\n\\[\\beta D \\gamma = 1\\]\nwhere \\(D = \\frac{\\Delta x^2}{2\\Delta t}\\) is the diffusion coefficient.\n\n\n\n\n\n\nTip\n\n\n\nWe can calculate not just the mean \\(\\braket{x}\\), but also its variance \\(\\braket{x^2}\\) if we expand \\(S_{path} + S_{work}\\) to second order around its maximum, then apply Theorem 8."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#crooks-fluctuation-theorem",
    "href": "blog/posts/statistical-mechanics/index.html#crooks-fluctuation-theorem",
    "title": "Statistical Mechanics",
    "section": "Crooks fluctuation theorem",
    "text": "Crooks fluctuation theorem\n\nIn a closed system (microcanonical)\nSETUP. The system is a classical-mechanical system, with time-reversible dynamics, and follows Liouville’s theorem.\nWe have a thermodynamic system held under variable constraints \\(x\\).\nThe system starts in microcanonical ensemble of energy \\(E_1\\). Then we change the constraints \\(x(t)\\), quickly or slowly, over a time interval \\(t\\in [0, \\tau]\\). Let the microstate trajectory of the system be \\(y(t)\\), arriving at the energy shell of \\(E_2\\).\nDuring the forward process, if the system undergoes microstate trajectory \\(y(t)\\), then we have to expend work \\(W[x(t), y(t)] = E_2 - E_1\\).\nLet \\(S_1^*\\) be the maximal entropy of the system when held under the constraints of \\(x(0)\\), and when the system has energy \\(E_1\\). Similarly for \\(S_2^*\\).\n\n\n\n\n\n\nWarning\n\n\n\n\\(S_1 = S_1^*\\), since the system starts in thermal equilibrium. However, by Liouville’s theorem, entropy is conserved! So we actually have \\(S_2 = S_1 \\neq S_2^*\\), because the system does not end in thermal equilibrium.\n\n\n\\(x', y'\\) are \\(x, y\\) time-reversed.\nFor example, if we have a piston of gas made of only a few gas molecules, then the constraint is the volume \\(V\\), and we want to study the probability of expending work \\(W\\) if we give the piston head a push. The push can be slow or fast – arbitrarily far from equilibrium. Crooks theorem applies no matter how we push the piston head.\n\n\n\n(Sethna 2021, fig. 4.10)\n\n\n\n\n\n(Sethna 2021, fig. 4.11)\n\n\nSETUP. probability density over path-space.\nLet \\(\\delta E_1, \\delta E_2\\) be infinitesimals, and let \\(E_1, E_2\\) be real numbers.\nGiven a small bundle of microtrajectories \\(y\\), we can measure its path-space volume as \\(D[y]\\). Suppose they start on the energy shell \\([E_1, E_1 + \\delta E_1]\\), then they would end up somewhere. If we’re lucky, they would end up on the energy shell \\([E_2 + \\delta E_2]\\).\nSuppose the system starts in the microcanonical ensemble on the energy shell \\([E_1, E_1 + \\delta E_1]\\), and we perform the constraint-variation \\(x\\), then there is a certain probability \\(\\delta P\\) that we would sample a trajectory from the small bundle. That small probability is \\[\\rho(y | x) D[y]\\]\nwhere \\(\\rho(y | x)\\) is a probability density over path-space. In particular, \\(\\rho(y | x) = 0\\) identically, unless \\(y(0)\\) is on the energy shell \\([E_1, E_1 + \\delta E_1]\\).\nRunning the argument backwards, we can define \\(\\rho'(y' | x')\\), another probability density over paths. This one satisfies \\(\\rho'(y'| x') = 0\\) unless \\(y'(0)\\) is on the energy shell \\([E_2, E_2 + \\delta E_2]\\).\n\nTheorem 12 (Crooks fluctuation theorem (microcanonical)) For any trajectory \\(y\\) such that it starts on the \\([E_1, E_1 + \\delta E_1]\\) energy shell, and ends on the \\([E_2, E_2 + \\delta E_2]\\) energy shell,\n\\[\\frac{\\rho(y | x)}{\\rho'(y' | x')} = e^{\\Delta S}\\]\nwhere \\(\\Delta S= \\ln\\Omega_2 - \\ln\\Omega_1\\), \\(\\Omega_1\\) is the phase space volume of the first energy shell, and \\(\\Omega_2\\) the second.\n\n\n\n\n\n\n\nNote\n\n\n\nIf \\(y\\) does not start on the first energy shell, or does not end on the second energy shell, then either the nominator or the denominator is zero, and so the equation fails to hold.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIn the forward process, the probability of going along that trajectory is \\[\\rho(x|y) D[x] = \\frac{\\delta V}{\\Omega_1}\\]\nwhere \\(\\delta V\\) is the phase-space volume of the shaded set.\nIn the backward process, the probability of reversing that trajectory is \\[\\rho'(x'|y') D[x']= \\frac{\\delta V'}{\\Omega_2}\\]\n\\(\\delta V' = \\delta V\\) by Liouville’s theorem, and \\(D[x] = D[x']\\) because \\(x'\\) is just \\(x\\) time-reversed.\n\n\n\n\n\nIn an energy bath (canonical)\nNow, suppose we take the same piston of gas, and put it in energy-contact with an energy bath, then at thermal equilibrium, the piston of gas would have the Boltzmann distribution \\(\\propto e^{-\\beta E}\\). We can then give the piston head a push, which would cause it to undergo\n\nTheorem 13 (Crooks fluctuation theorem) \\[\\frac{\\rho(y | x)}{\\rho'(y' | x')} =  e^{S[x, y]} = e^{\\beta (W[x, y] - \\Delta F^*)}\\]\nwhere \\(S[x, y]\\) is the entropy produced during the forward process, after the system has equilibrated, and \\(\\Delta F^* = F^*_2 - F^*_2\\) is the increase in equilibrium Helmholtz free energy of the system.\n\n\n\n\n\n\n\nWarning\n\n\n\nIn both forward and backward cases, we start with a thermal equilibrium, and end with a thermal disequilibrium.\nFor example, suppose we have a small tank of a few gas molecules in thermal equilibrium with a large energy bath.\nNow, we quickly push the piston head in according to the function \\(x(t)\\). The trajectory of the system would go through is \\(y(t)\\), which is determined by both \\(x(t)\\) and the initial state of both the system and the energy bath.\nNow, we wait a long time, until the tank is in thermal equilibrium again. Then we pull the piston head out with time-reversed trajectory. Because the forward trajectory was quick, the backward trajectory was also quick.\n\n\n\n\n\n\n\n\nequilibrium Helmholtz\n\n\n\nWe wrote \\(F^*\\) instead of \\(F\\), to emphasize that we are dealing with equilibrium Helmholtz free energy, defined by \\(F^* = \\min_\\rho (\\braket{E} - TS[\\rho])\\), and not the generic version \\(\\braket{E} - TS[\\rho]\\).\nThis is vitally important, because at time \\(\\tau\\), when the constraints have just reached their new values, the system is not in equilibrium. We would have to hold the constraints constant for a while for the system to return to equilibrium with the energy bath. Despite this, Crooks fluctuation theorem uses \\(\\Delta F^*\\), which is computed at equilibrium.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nApply the microcanonical version of Crooks theorem to the entire compound system that includes both the bath and the system, then integrate over all possible microstate trajectories of the bath \\(y_{bath}\\).\n\\[\\begin{aligned}\n  S[x, y] &= \\Delta S_{bath} + \\Delta S_{system} \\\\\n  &= \\beta \\Delta E_{bath} + \\Delta S_{system} \\\\\n  &= \\beta(W[x, y] - \\braket{\\Delta E_{system}}_2) + \\Delta S_{system} \\\\\n  &= \\beta (W[x, y] - \\Delta F^*)\n\\end{aligned}\\]\nwhere \\(\\braket{\\cdot}_2\\) means the canonical ensemble average under constraint \\(x(\\tau)\\).\nNotice that the work expended/entropy produced depends only on the system’s microtrajectory \\(y(t)\\), and not on the bath’s microtrajectory \\(y_{bath}(t)\\). That is,\n\\[S[x, y, y_{bath}] = S[x, y]\\]\nThis will be used again in the next step when we integrate over \\(D[y_{bath}]\\).\n\\[\\begin{aligned}\n  \\rho(y|x) &= \\int_{y, y_{bath}, x \\text{ is valid}}D[y_{bath}]\\; \\rho(y, y_{bath} | x)  \\\\\n  &=  \\underbrace{\\int_{y', y'_{bath}, x' \\text{ is valid}}D[y'_{bath}]}_{\\text{reversible dynamics}}\\; \\underbrace{e^{S[x, y, y_{bath}]}\\rho'(y', y'_{bath} | x')}_{\\text{microcanonical Crooks}}  \\\\\n  &=  \\int D[y'_{bath}] \\; e^{\\red{S[x, y]}}\\rho'(y', y'_{bath} | x') \\\\\n  &=  e^{S[x,y]} \\int D[y'_{bath}] \\; \\rho'(y', y'_{bath} | x') \\\\\n  &=  e^{S[x,y]} \\rho'(y'|x')\n\\end{aligned}\\]\n\n\n\n\nCorollary 3 \\[\\frac{\\rho(W | x)}{\\rho'(-W | x')} =  e^{\\beta (W - \\Delta F^*)}\\]\nwhere \\(\\rho(W|x)\\) is the probability density of expending work \\(W\\) in the forward process.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIntegrate over all forward microtrajectories \\(y\\) satisfying \\(W[x, y] \\in [W, W+\\delta W]\\). By reversibility, \\(W[x', y'] = [-W - \\delta W, -W]\\) for such microtrajectories.\n\n\n\n\n\nOther ensembles\nLooking at the proof for Crooks theorem for the canonical ensemble, we immediately obtain many other possible Crooks theorems, one per free entropy.\nEXP. Crooks theorem for Gibbs free energy \\(G\\)\nSuppose we have a piston of magnetic gas in energy-and-volume contact with a bath. Now suppose the gas is in equilibrium with the bath, and we vary the external magnetic field over a trajectory \\(x\\). Over the microstate trajectory \\(x\\), the external world would expend both some energy \\(W[x, y]\\) and some volume \\(V[x, y]\\).\n\\[\\frac{\\rho(y | x)}{\\rho'(y' | x')} =  e^{S[x, y]} = e^{\\beta (W[x, y] + PV[x, y] - \\Delta G^*)}\\]\nEXP. Crooks theorem for Landau free energy \\(\\Omega\\)\nSuppose we have a chemical reaction chamber of fixed volume, and in energy-and-particle contact with a bath with chemical potentials \\(\\mu_i\\).\n\\[\\frac{\\rho(y | x)}{\\rho'(y' | x')} =  e^{S[x, y]} = e^{\\beta (W[x, y] -  \\sum_i \\mu_i N_i[x, y] - \\Delta \\Omega^*)}\\]"
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#jarzynski-equality",
    "href": "blog/posts/statistical-mechanics/index.html#jarzynski-equality",
    "title": "Statistical Mechanics",
    "section": "Jarzynski equality",
    "text": "Jarzynski equality\nLet \\(W\\) be the total work we expended by changing the constraints during the interval \\([0, \\tau]\\). Since the work expended depends on the details of the heat bath and the starting state of the system at \\(t=0\\), this is a random variable.\n\nTheorem 14 (Jarzynski equality) \\[\\braket{e^{-\\beta W}} = e^{-\\beta \\Delta F^*}\\]\nwhere the expectation is taken over many repeats of the same experiment (ensemble average).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIntegrate Crooks over all forward trajectories \\(D[x]\\).\n\\[\\rho(y | x) e^{-\\beta W[x, y] } = \\rho'(y' | x') e^{-\\beta\\Delta F}\\]\nnow integrate over \\(\\int D[y]\\), using the fact that \\(D[y] = D[y']\\).\n\n\n\n\nCorollary 4 (violation of second law is exponentially unlikely) \\[Pr((W - \\Delta F^*) \\leq - \\delta W) \\leq e^{-\\beta \\delta W}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nApply Markov’s inequality.\n\n\n\nEXP. high probability of work extraction\nClassically, if we have a single system in thermal equilibrium with a single energy-bath, and we perform a cyclic operation on it, then we can’t extract work, lest we violate the second law.\nStatistically, \\(\\braket{e^{-\\beta W}} = 1\\), and so it is entirely possible for us to extract work with high probability, as long as there is a small probability to lose a large enough amount of work.\n(Maillet et al. 2019) constructed a quantum mechanical device with a single-electron transistor. The electron can expend work. They managed to extract work from the device with over 75% probability.\n\n\n\nFigure from (Maillet et al. 2019, fig. 3.c)\n\n\n\nWorked example: bouncing ball\nThis example is from (Sethna 2021, exercise 4.8), which itself derives from (Lua and Grosberg 2005). See also (Híjar and de Zárate 2010) for another solved example, of a chest expander with mass points stuck in the middle of the springs. You might need to read my tutorial on field-theoretic calculations before attempting that example.\nWe have a one-dimensional system, of a single ball bounding between two walls of a piston. The only control we have is that we can move one of the piston heads. At the start, the piston has length \\(L\\), and the system is in thermal equilibrium at inverse temperature \\(\\beta\\). We plunge the piston head at velocity \\(v\\) for time \\(\\Delta L / v\\), then immediately reverse it, taking another \\(\\Delta L / v\\). We explicitly calculate that \\(\\braket{e^{-\\beta W}} = 1\\).\n\n\n\n(Sethna 2021, fig. 4.12)\n\n\nThe phase space of the ball has 2 dimensions, \\((p, x)\\). The Boltzmann distribution is\n\\[\\rho(p, x) = \\rho(p) \\rho(x) = \\frac{1}{\\sqrt{2\\pi m/\\beta}}e^{-\\frac{\\beta}{2m}p^2} \\times \\frac{1}{L}\\]\nWe assume that \\(L\\) is large enough, such that the ball hits the piston head at most once. There are three possibilities:\n\nIf the piston head hits the ball during the in-stroke, then the ball’s velocity increases by \\(2v\\), and its kinetic energy increases by \\[W = \\Delta KE = 2v(mv - p)\\]\nIf the piston head hits the ball during the out-stroke, then the ball’s velocity decreases by \\(2v\\), and its kinetic energy increases by\n\\[W = 2v(mv+p)\\]\nOtherwise, the piston head avoids the ball, and we have \\(W = 0\\).\n\nIf at \\(t=0\\), the ball is in the phase space region labelled “in region”, then it will be hit in the in-stroke. If at \\(t=\\Delta L/v\\), the ball is in the phase space region labelled “out region”, then it will be hit in the out-stroke. Otherwise, it will not be hit.\n\n\n\nJarzynski_bouncing_ball_1.jpg\n\n\n\n\n\nJarzynski_bouncing_ball_2.jpg\n\n\nTherefore,\n\\[\n\\begin{aligned}\n    \\braket{e^{-\\beta W}} &= \\int_{in} \\rho dpdx \\; e^{-\\beta 2v(mv-p)} + \\int_{out} \\rho dpdx \\; e^{-\\beta 2v(mv+p)} + \\int_{other} \\rho dpdx \\; 1 \\\\\n    &= e^{-2\\beta mv^2} \\left(\\int_{in} \\rho dpdx \\; e^{2\\beta vp} + \\int_{out} \\rho dpdx \\; e^{-2\\beta vp}\\right) +  \\int_{other} \\rho dpdx \\; 1\n\\end{aligned}\n\\]\nSince \\(\\rho(p, x) = \\rho(-p, x - \\Delta L)\\), the first two integrals can be combined by flipping the “out region”, then moving it by \\(\\Delta L\\), to “out’ region”.\n\n\n\n\n\n\nNote\n\n\n\nBecause \\(L\\) is large, this is mostly correct, as the regions where this is incorrect has \\(\\rho\\) so small that it is negligible, as seen in the figure.\n\n\nNow we continue:\n\\[\n\\begin{aligned}\n\\braket{e^{-\\beta W}} &\\approx e^{-2\\beta mv^2} \\int_{in, out'} \\rho dpdx \\; e^{2\\beta vp} +  \\int_{other} \\rho dpdx \\; 1 \\\\\n&= \\frac{1}{L\\sqrt{2\\pi m/\\beta}} \\left(\\int_{in, out'} e^{-\\frac{\\beta}{2m}(p - 2mv)^2} dpdx + \\int_{other} e^{-\\frac{\\beta}{2m}p^2} dpdx\\right)\n\\end{aligned}\n\\]\nBecause the “in-out’ region” is symmetric across the \\(p = mv\\) line, we can reflect the first integral across the \\(p=mv\\) line and obtain \\[\n= \\frac{1}{L\\sqrt{2\\pi m/\\beta}} \\left(\\int_{in, out'} e^{-\\frac{\\beta}{2m}p^2} dpdx + \\int_{other} e^{-\\frac{\\beta}{2m}p^2} dpdx\\right) = 1\n\\]\n\n\nFluctuation-dissipation relations\n\nCorollary 5 (Fluctuation-dissipation relations) Since \\(e^t\\) is convex, we have \\(\\Delta F^* \\leq \\braket{W}\\), meaning that the average work expended is more than the increase in Helmholtz free energy. This has better be true, else we would be violating the second law of thermodynamics on average.\nSince \\(\\Delta F^* = -\\frac{1}{\\beta} \\ln\\braket{e^{-\\beta W}}\\), we find that to second order, \\[\\underbrace{\\braket{W} - \\Delta F^*}_{\\text{work dissipation}} = \\frac 12 \\beta \\underbrace{\\sigma_W^2}_{\\text{work fluctuation}}\\]\nIt is more familiarly written as \\[\\mu = D\\beta\\]\nwhere \\(D = \\frac 12 \\sigma_W^2\\) is the fluctuation coefficient, and \\(\\mu = (\\braket{W} - \\Delta F^*)\\) is the dissipation coefficient.\n\nFor periodic forcing, the CFT has a simpler form.\nConsider a time-reversible dynamical system immersed in an energy bath with inverse temperature \\(\\beta\\) , driven by periodically varying constraints. For example, a pendulum in a sticky fluid subjected to a periodic driving torque, or a water-cooled electric circuit driven by a periodic voltage.\nSuch a system will settle into a “dynamical equilibrium” ensemble, much like a canonical ensemble. If it is driven by the same process time-reversed, then, it will settle into another dynamical equilibrium ensemble.\n\nTheorem 15 (Gallavotti–Cohen fluctuation theorem) \\[\\frac{\\rho(Q)}{\\rho'(-Q)} = e^{\\beta Q}\\]\nwhere \\(\\rho(Q)\\) is the probability density that a forward cycle, randomly sampled from the dynamical equilibrium ensemble, emits energy \\(Q\\) into the energy bath. Similarly, \\(\\rho'\\) is for the backward cycle.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConstruct a process that starts at equilibrium, then mount up the periodic driving, runs it for \\(NT\\) time where \\(N\\) is a large integer, then remove the driving. At the \\(N\\to\\infty\\) limit, the CFT reduces to the result.\n\n\n\n\n\nArrow of time\n(Jarzynski 2011)\nSuppose a scientist has recorded a microscopic movie of pulling on an RNA, flipped a coin to decide whether to reverse the movie, then given you the movie. Your task is to guess whether the movie is reversed.\nBy Bayes theorem, \\[Pr(\\text{forward}|x, y) = \\frac{1}{1 + e^{-S[x, y]}}\\]\nwhere \\(S[x, y] = \\beta(W[x, y] - \\Delta F^*)\\).\nIn words, the higher the entropy production, the more likely it is that the movie is playing forward. This is one way to say that entropy production provides an arrow of time.\n(Parrondo, Horowitz, and Sagawa 2015)\n\\[\\braket{e^{-\\beta W - I}} = e^{-\\beta \\Delta F^*}\\]"
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#fluctuation-dissipation-relations-1",
    "href": "blog/posts/statistical-mechanics/index.html#fluctuation-dissipation-relations-1",
    "title": "Statistical Mechanics",
    "section": "Fluctuation-dissipation relations",
    "text": "Fluctuation-dissipation relations\nSince \\(e^t\\) is convex, we have \\(\\Delta F^* \\leq \\braket{W}\\), meaning that the average work expended is more than the increase in Helmholtz free energy. This has better be true, else we would be violating the second law of thermodynamics on average.\nSince \\(\\Delta F^* = -\\frac{1}{\\beta} \\ln\\braket{e^{-\\beta W}}\\), we find that to second order, \\[\\underbrace{\\braket{W} - \\Delta F^*}_{\\text{work dissipation}} = \\frac 12 \\beta \\underbrace{\\sigma_W^2}_{\\text{work fluctuation}}\\]\nIt is more familiarly written as \\[\\mu = D\\beta\\]\nwhere \\(D = \\frac 12 \\sigma_W^2\\) is the fluctuation coefficient, and \\(\\mu = (\\braket{W} - \\Delta F^*)\\) is the dissipation coefficient.\n\nTheorem 9 (Gallavotti–Cohen fluctuation theorem) A system in an energy bath, and driven by periodically varying its constraints (such as a pendulum in a sticky fluid subjected to a driving force), will settle into a “dynamical equilibrium” ensemble, much like a canonical ensemble.\nIf it is driven by the same process time-reversed, then, it will settle into another dynamical equilibrium ensemble.\n\\[\\frac{\\rho(S)}{\\rho'(-S)} = e^{S}\\]\nwhere \\(\\rho(S)\\) is the probability density for a forward cycle producing entropy \\(S\\), and \\(\\rho'(S)\\) is for the backward cycle.\nMore generally, since at the dynamical equilibrium, the system’s microstates are changing as a Markov chain, we have \\(\\rho(i \\to j)\\), which are basically \\[\\frac{\\rho(i\\to j)}{\\rho'(j \\to i)} = e^{S[i \\to j]}\\]\nwhere \\(i, j\\) are microstates.\n\n\nProof. (sketch)\nConstruct a process that starts at equilibrium, then mount up the periodic driving, runs it for \\(NT\\) time where \\(N\\) is a large integer, then remove the driving. At the \\(N\\to\\infty\\) limit, the CFT reduces to the theorem."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#maxwells-demon",
    "href": "blog/posts/statistical-mechanics/index.html#maxwells-demon",
    "title": "Statistical Mechanics",
    "section": "Maxwell’s demon",
    "text": "Maxwell’s demon\n(Parrondo, Horowitz, and Sagawa 2015)\n\\[\\braket{e^{-\\beta W - I}} = e^{-\\beta \\Delta F^*}\\]\narrow of time\n(Jarzynski 2011)\nSuppose a scientist has recorded a microscopic movie of pulling on an RNA, flipped a coin to decide whether to reverse the movie, then given you the movie. Your task is to guess whether the movie is reversed.\nBy Bayes theorem, \\[Pr(\\text{forward}|x, y) = \\frac{1}{1 + e^{-S[x, y]}}\\]\nwhere \\(S[x, y] = \\beta(W[x, y] - \\Delta F^*)\\).\nIn words, the higher the entropy production, the more likely it is that the movie is playing forward. This is one way to say that entropy production provides an arrow of time."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#arrhenius-law",
    "href": "blog/posts/statistical-mechanics/index.html#arrhenius-law",
    "title": "Statistical Mechanics",
    "section": "Arrhenius’ law",
    "text": "Arrhenius’ law\nOften in chemistry and biology, we have a particle that is stuck in a shallow potential well. If it ever gets over an energetic barrier, then it can drop to a much deeper minimum. If the particle is in thermal equilibrium with a bath, then it will eventually gain enough energy by random chance to get over the barrier. Of course, conversely, a particle in the deeper minimum would occasionally gain enough energy to go back to the shallow well.\nThe point is that “getting over a potential barrier” is inherently a non-equilibrium phenomenon, and therefore does not logically belong to an essay on equilibrium statistical mechanics. Still, if we are willing to approximate, then we can derive it easily.\nWe model a particle in a shallow potential well as a simple harmonic oscillator. If it ever gains more than \\(\\Delta E\\) of energy, then it would escape the well. Thus, we can take a look at the particle. If it has yet to escape, then we wait for time \\(\\tau\\) (relaxation time) until the particle has reached thermal equilibrium again with the bath, and take another look. The time-until-escape is \\(N \\tau\\), where \\(N\\) is the number of times we look at the system.\n\n\n\n\n\n\nRelaxation time\n\n\n\nWhen we look at the oscillator, its state is totally fixed at some \\((q, p)\\). If we immediately look again, we would not find it in a Boltzmann distribution over phase space, but still at \\((q, p)\\). How long must we wait? That is beyond the scope. Suffice to say that we should wait a while, not too short, called relaxation time \\(\\tau\\), after which the system is close enough to equilibrium. But, here’s the key – we cannot wait for too long, because in the long run, the particle would have escaped the potential well. Formalizing all these things requires a proper stochastic calculus, which I might write about later.\n\n\nLet the oscillator have \\(n\\) dimensions, then its energy function is \\(H = \\sum_{i=1}^n \\frac{p_i^2}{2m_i} + \\frac{k_i q_i^2}{2}\\), where \\(q_i, p_i\\) are the generalized position and momentum, and \\(m_i, k_i\\) are the effective masses and elastic constants. The Boltzmann distribution is \\(\\rho(q, p) = Z^{-1} e^{-\\beta H}\\), and the probability that it has enough energy to overcome the barrier is\n\\[\nP = \\frac{\\int_{H \\geq \\Delta E} \\rho(q, p)dqdp}{\\int_{H \\geq 0} \\rho(q, p)dqdp}\n\\]\nNotice that the proportionality constant \\(Z\\) is removed. After a change of variables by \\(x_i = \\frac{p_i}{\\sqrt{2m_i}}, y_i = \\sqrt{\\frac{k_i}{2}} q_i\\), we get\n\\[\nP = \\frac{\\int_{\\sum_i x_i^2 + y_i^2 \\geq \\Delta E} e^{-\\beta \\sum_i (x_i^2 + y_i^2)} dxdy}{\\int_{\\sum_i x_i^2 + y_i^2 \\geq 0} e^{-\\beta \\sum_i (x_i^2 + y_i^2)} dxdy}\n\\]\nIntegrating in spherical coordinates, and simplifying, we get \\(P = e^{-\\beta \\Delta E}\\). Thus, the expected time until escape is\n\\[\nT = \\braket{N}\\tau = \\frac{1}{P}\\tau = \\tau e^{\\beta \\Delta E}\n\\]\nthe Arrhenius equation."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#arrhenius-equation",
    "href": "blog/posts/statistical-mechanics/index.html#arrhenius-equation",
    "title": "Statistical Mechanics",
    "section": "Arrhenius equation",
    "text": "Arrhenius equation\nOften in chemistry and biology, we have a particle that is stuck in a shallow potential well. If it ever gets over an energetic barrier, then it can drop to a much deeper minimum. If the particle is in thermal equilibrium with a bath, then it will eventually gain enough energy by random chance to get over the barrier. Of course, conversely, a particle in the deeper minimum would occasionally gain enough energy to go back to the shallow well.\nThe point is that “getting over a potential barrier” is inherently a non-equilibrium phenomenon, and therefore does not logically belong to an essay on equilibrium statistical mechanics. Still, if we are willing to approximate, then we can derive it easily.\nWe model a particle in a shallow potential well as a simple harmonic oscillator. If it ever gains more than \\(\\Delta E\\) of energy, then it would escape the well. Thus, we can take a look at the particle. If it has yet to escape, then we wait for time \\(\\tau\\) (relaxation time) until the particle has reached thermal equilibrium again with the bath, and take another look. The time-until-escape is \\(N \\tau\\), where \\(N\\) is the number of times we look at the system.\n\n\n\nGetting over a potential well.\n\n\n\n\n\n\n\n\nrelaxation time\n\n\n\nWhen we look at the oscillator, its state is totally fixed at some \\((q, p)\\). If we immediately look again, we would not find it in a Boltzmann distribution over phase space, but still at \\((q, p)\\). How long must we wait before the oscillator state has reasonably “relaxed” from a pointy distribution to the serene bell-shape of the Boltzmann distribution?\nThat is beyond the scope. Suffice to say that we should wait a while, not too short, after which the system is close enough to equilibrium. But we cannot wait for too long either, because in the long run, the particle would have escaped the potential well. So there is a time-scale, neither too long nor too short, which we call the relaxation time \\(\\tau\\).\nFormalizing all these things requires stochastic calculus, which I might write about later.\n\n\n\n\n\n\nGoldilocks and the three chefs\n\n\n\n\n\nGoldilocks, with a rumbling stomach, stumbles upon the house of the three chefs. Each chef is holding a pan in one hand and a bottle of Brownian batter in the other. “Excuse me, might I have some pancakes?”\n“Of course!” exclaims the first chef, whose pan holds a small, quivering mound. “It’s just been poured, still brimming with energy!”\nGoldilocks frowns. “It hasn’t even reached the edges! This batter needs more time to settle.”\nThe second chef, whose pan appears curiously empty, sighs. “And what of mine? I’ve given it all the time in the world, allowed it to explore every corner of its potential.” He gestures at the pan, now bereft of batter, a few crumbs clinging to the edges.\n“But… there’s nothing left!” exclaims Goldilocks, aghast. “Given too much time, the batter has vanished completely!”\nThe third chef, whose batter has flowed smoothly to coat the pan, smiles warmly. “Perhaps this will be more to your liking. Given much time, but not too much, it’s achieved perfect consistency.”\n“Ah, this is perfect!” exclaims Goldilocks, taking a bite of the fluffy pancake. “It’s had enough time to spread evenly, but not so long that it’s dried out.”\n“Indeed,” said the second chef, “timing is everything. Too brief, and the batter remains confined to its starting point, unable to fulfill its pan-sized destiny. But too long, it would have escaped the edge of the pan to reach its true destiny – on the ground. Not too short, not too long, just right… that is meta-stability.”\n— Guest entry written by Gemini-1.5-Pro.\n\n\n\n\n\nLet the oscillator have \\(n\\) dimensions, then its energy function is \\(H = \\sum_{i=1}^n \\frac{p_i^2}{2m_i} + \\frac{k_i q_i^2}{2}\\), where \\(q_i, p_i\\) are the generalized position and momentum, and \\(m_i, k_i\\) are the effective masses and elastic constants. The Boltzmann distribution is \\(\\rho(q, p) = Z^{-1} e^{-\\beta H}\\), and the probability that it has enough energy to overcome the barrier is\n\\[\nP = \\frac{\\int_{H \\geq \\Delta E} \\rho(q, p)dqdp}{\\int_{H \\geq 0} \\rho(q, p)dqdp}\n\\]\nNotice that the proportionality constant \\(Z\\) is removed. After a change of variables by \\(x_i = \\frac{p_i}{\\sqrt{2m_i}}, y_i = \\sqrt{\\frac{k_i}{2}} q_i\\), we get\n\\[\nP = \\frac{\\int_{\\sum_i x_i^2 + y_i^2 \\geq \\Delta E} e^{-\\beta \\sum_i (x_i^2 + y_i^2)} dxdy}{\\int_{\\sum_i x_i^2 + y_i^2 \\geq 0} e^{-\\beta \\sum_i (x_i^2 + y_i^2)} dxdy}\n\\]\nIntegrating in spherical coordinates, and simplifying, we get \\(P = e^{-\\beta \\Delta E}\\). Thus, the expected time until escape is\n\\[\nT = \\braket{N}\\tau = \\frac{1}{P}\\tau = \\tau e^{\\beta \\Delta E}\n\\]\nthe Arrhenius equation. The same calculation, for a system held in contact with an energy-and-volume bath, gives us \\(T = \\tau e^{\\beta \\Delta G}\\), where \\(\\Delta G\\) is the Gibbs free energy barrier.\nIn practice, the Arrhenius equation is used in the form of \\(\\ln f = - a T^{-1} + Const\\), where \\(f = 1/T\\) is the “reaction rate” or “characteristic frequency”, and \\(a = \\frac{\\Delta E}{k_B}\\) is the slope of the \\(T^{-1} - \\ln f\\) plot (“the Arrhenius plot”).\n\nApplications\nThe argument given above for the Arrhenius equation is quite generic. It only assumes there is a system that is stuck in some kind of potential well, and is held at a constant temperature somehow. There is no requirement for the system to be an actual particle in an actual well. The “particle” can very well be the 100-dimensional configuration of a protein during folding, or even the simultaneous position of \\(10^{23}\\) helium atoms in a helium gas. Indeed, the Arrhenius equation pops up everywhere as the time until a system escapes an energy barrier.\nIn a glass of water held at constant temperature \\(300 \\;\\mathrm{K}\\), each water molecule might occasionally reach enough energy to escape into open air. This is evaporation. By this argument, the rate of evaporation follows the Arrhenius law, and indeed it does. I suspect that Arrhenius law holds for the spontaneous crystallization of supercooled liquid and spontaneous conversion of disappearing polymorphs, but I cannot find good data on this.\nIn the simplest model for biochemical process, we just have one chemical reaction following another, until it is complete. If there is a single biochemical step that is much slower than the other steps, then the waiting time for that step dominates, and the total reaction should depend on the temperature by an Arrhenius law. This might explain the observed Arrhenius-law-like dependence on temperature in biological phenomena like tree cricket chirping, alpha brain wave frequency, etc.\n\n\n\n(1) Tree cricket chirping frequency. (2) Firefly flashing frequency. (3) Terrapin heartbeat frequency. (4) Human silent counting rate (Hoagland 1933). These figures are reproduced in (Laidler 1972). (5) evaporation rate of octane (Brennan, Shapiro, and Watton 1974). (6) Alpha frequency of brains of normal, syphilitic paretic, and very paretic humans (Hoagland 1936).\n\n\nWhile writing this, I suddenly recognized (Hoagland 1933) from when I read Feynman’s book all those years ago!\n\nWhen I was in graduate school at Princeton [1939–1942] a kind of dumb psychology paper came out that stirred up a lot of discussion. The author had decided that the thing controlling the “time sense” in the brain is a chemical reaction involving iron… his wife had a chronic fever which went up and down a lot. Somehow he got the idea to test her sense of time. He had her count seconds to herself (without looking at a clock), and checked how long it took her to count up to 60. He had her counting – the poor woman – all during the day: when her fever went up, he found she counted quicker; when her fever went down, she counted slower… he tried to find a chemical reaction whose rates varied with temperature in the same amounts as his wife’s counting did. He found that iron reactions fit the pattern best… it all seemed like a lot of baloney to me – there were so many things that could go wrong in his long chain of reasoning.\n(Feynman 1989, 55)\n\nAnd yes, that is the one!\n\nMy wife, having fallen ill with influenza, was used in the first of several experiments. Without, in any way, hinting to her the nature of the experiment, she was asked to count 60 seconds to herself at what she believed to be a rate of 1 per second. Simultaneously the actual duration of the count was observed with a stop-watch.\n(Hoagland 1933)"
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#overview",
    "href": "blog/posts/statistical-mechanics/index.html#overview",
    "title": "Statistical Mechanics",
    "section": "Overview",
    "text": "Overview\n\nPhilosophical comments\nIt is fair to say that, although it originated in the 19th century like all other classical fields of physics, statistical mechanics is unsettled.\nTrajectory-centric statistical mechanics. In this view, we start with the equations of motion for a physical system, then study statistical properties of individual trajectories, or collections of them. For example, if we have a pendulum hanging in air, being hit by air molecules all the time, we would study the total trajectory \\((\\theta, x_1, y_1, z_1, x_2, y_2, z_2, \\dots)\\), where \\(\\theta\\) is the angle of the pendulum swing, and \\((x_i, y_i, z_i)\\) is the location of the \\(i\\)-th air molecule. Then we may ask that, over a long enough period, how frequent would the pendulum visit a certain angle range of \\([\\theta_0, \\theta_0 + \\delta\\theta]\\):\n\\[\nPr(\\theta \\in [\\theta_0, \\theta_0 + \\delta\\theta]) = \\lim_{T \\to \\infty} \\frac{1}{2T} \\int_{-T}^{+T} 1[\\theta \\in [\\theta_0, \\theta_0 + \\delta\\theta]] dt\n\\]\nIn the trajectory-centric view, there are the following issues:\n\nProblem of ergodicity: When does time-average equal ensemble-average? A system is called “ergodic” iff for almost all starting conditions, the time-average of the trajectory is the ensemble-average over all trajectories.\nProblem of entropy: How is entropy defined on a single trajectory?\nH-theorem: In what sense, and under what conditions, does entropy increase?\nProblem of equilibrium: What does it mean to say that a trajectory is in equilibrium?\nApproach to equilibrium: In what sense, and under what conditions, does the trajectory converge to an equilibrium?\nReversibility problem (Umkehreinwand): If individual trajectories are reversible, why does entropy increase instead of decrease?\n\nWhile these philosophical problems are quite diverting, we will avoid them as much as possible, because we will be working with the ensemble-centric equilibrium statistical mechanics. This is the statistical mechanics that every working physicist uses, and this is what we will present. If you are interested in the philosophical issues, read the Stanford Encyclopedia entry on the Philosophy of Statistical Mechanics.\n\n\nPrinciples of statistical mechanics\n\nA physical system is a classical system with a state space, evolving according to some equation of motion.\nAn ensemble of that system is a probability distribution over its state space.\nThe idea of (ensemble-centric) statistical mechanics is to study the evolution of an entire probability distribution over all possible states.\nThe entropy of a probability distribution \\(\\rho\\) is\n\n\\[S[\\rho] := -\\int dx\\; \\rho(x) \\ln \\rho(x)\\]\n\nUnder any constraint, there exists a unique ensemble, named the equilibrium ensemble, which maximizes entropy under constraint.\n\nMost of the times, the state space is a phase space, and the equation of motion is described by a Hamiltonian function. However, the machinery of statistical mechanics, as given above, is purely mathematical. It can be used to study any problem in probability whatsoever, even those with no physical meaning.\nBelieve it or not, the above constitutes the entirety of equilibrium statistical mechanics. So far, it is a purely mathematical theory, with no falsifiability (Popperians shouting in the background). To make it falsifiable, we need to add one more assumption, necessarily fuzzy:1\n1 \nAs far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.\n— Albert Einstein, Address to Prussian Academy of Sciences (1921)\n\n\nThe equilibrium ensemble is physically meaningful and describes the observable behavior of physical systems.\n\nIn other words, when a physical system is at equilibrium, then everything observable can be found by studying it as if it has the maximum entropy distribution under constraint.\nOf course, just what that “is physically meaningful” means, is another source of endless philosophical arguments. I would trust that you will know what is physically meaningful, and leave it at that, while those who have a taste for philosophy can grapple with the Duhem–Quine thesis.\n\n\nDifferential entropy depends on coordinates choice\nThere is a well-known secret among information theorists: differential entropy is ill-defined.\nConsider the uniform distribution on \\([0, 1]\\). It is the maxent distribution on \\([0, 1]\\) – relative to the Lebesgue measure. However, why should we pick the Lebesgue measure, and what happens if we don’t?\nSuppose we now stretch the \\([0, 1]\\) interval nonlinearly, by \\(f(x) = x^2\\), then the maxent distribution relative to that would no longer be the uniform distribution on \\([0, 1]\\). Instead, it would be the uniform distribution after stretching.\nThe problem is this: Differential entropy is not coordinate-free. If we change the coordinates, we change the base measure, and the differential entropy changes as well.\nTo fix this, we need to use the KL-divergence, which is invariant under a change of base measure, as in \\[-D_{KL}(\\rho \\| \\mu) := - \\int dx\\; \\rho(x) \\ln\\frac{\\rho(x)}{\\mu(x)}\\]\nIn typical situations, we don’t need to worry ourselves with KL-divergence, as we just pick the uniform distribution \\(\\mu\\). When the state space is infinite in volume, the uniform distribution is not a probability measure, but it will work. Bayesians say that it is an improper prior.\nIn this interpretation, the principle of “maximum entropy distribution under constraint” becomes the principle of “minimal KL-divergence under constraint”, which is Bayesian inference, with exactly the same formulas.\nIn almost all cases, we use the uniform prior over phase space. This is how Gibbs did it, and he didn’t really justify it other than saying that it just works, and suggesting it has something to do with Liouville’s theorem. Now with a century of hindsight, we know that it works because of quantum mechanics: We should use the uniform prior over phase space, because phase space volume has a natural unit of measurement: \\(h^N\\), where \\(h\\) is Planck’s constant, and \\(2N\\) is the dimension of phase space. As Planck’s constant is a universal constant, independent of where we are in phase space, we should weight all of the phase space equally, resulting in a uniform prior."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#the-partition-function",
    "href": "blog/posts/statistical-mechanics/index.html#the-partition-function",
    "title": "Statistical Mechanics",
    "section": "The partition function",
    "text": "The partition function\nWhen the system is in a canonical ensemble, we can define a convenient variable \\(Z = \\int dx\\; e^{-\\beta E(x)}\\) as the partition function, and let \\(f^* := \\ln Z\\).\n\\(Z\\) is a function of \\(\\beta\\), and other possible constraints that might directly change the energy levels of the system. We can more explicitly write it as\n\\[Z(\\beta; c) = \\int dx \\; e^{-\\beta E_c(x)}\\]\nwhere \\(c\\) stands for the other constraints on the system, such as the size of the container, the number of particles, etc.\n\nProposition 1 (The partition function generates all moments of energy) Let a system be in canonical ensemble with inverse temperature \\(\\beta\\), and let \\(K(t) := \\ln \\braket{e^{tE}}\\) be the cumulant generating function of its energy, then \\[K(t) = \\ln Z(\\beta-t) - \\ln Z(\\beta)\\]\nIn particular, the \\(n\\)-th cumulant of energy is\n\\[\\kappa_n(E) = K^{(n)}(t) |_{t=0} = (-\\partial_\\beta)^n (\\ln Z)\\]\n\nFor example, the first two cumulants are the mean and variance:\n\\[\\braket{E} = (-\\partial_\\beta) (\\ln Z), \\quad \\mathrm{Var}(E) = \\partial_\\beta^2 (\\ln Z)\\]\nTypical systems are made of \\(N\\) particles, where \\(N\\) is large, and that these particles are only weakly interacting. In this case, we have \\(\\ln Z \\propto N\\) for large \\(N\\), thus showing that the distribution of energy is roughly \\(\\mathcal(\\braket{E}, \\sigma^2_E/N)\\) for some \\(\\sigma_E\\). In particular, its fluctuation is on the order of \\(N^{-1/2}\\).\nA similar proposition applies for the grand canonical ensemble, etc."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#thermodynamic-limit",
    "href": "blog/posts/statistical-mechanics/index.html#thermodynamic-limit",
    "title": "Statistical Mechanics",
    "section": "Thermodynamic limit",
    "text": "Thermodynamic limit\n\nFluctuation of observables\nSuppose we have a tank of oxygen gas, and it is in the equilibrium distribution (Maxwell-Boltzmann). Now, if we sample its pressure \\(P\\), then every time we sample it, we sample a particular microstate \\(x\\) from its equilibrium distribution, and each corresponds to a different pressure \\(P(x)\\). We know that these particular pressures should be tightly bunched around its average value – the thermodynamic pressure \\(\\braket{P}\\)… but how bunched-up is it?\nMore generally, suppose we have a system in the equilibrium state (maximal entropy under constraint), how much fluctuation does it have?\nEXP. systems in energy-contact, the zeroth law\nTake several systems, and let them exchange energy, but nothing else. For concreteness, we can imagine taking several copper tanks of gas, and let them touch each other. The tanks hold their shape, not expanding or contracting.\nThe system has total entropy \\[S = \\sum_i S_i(E_i, A_i)\\]\nwhere \\(A_i\\) stand for the other state variables we don’t care about, because they are held constant\nThere is a single constraint of constant total energy: \\[E = \\sum_i E_i\\]\nIn the thermodynamical limit, the compound system reaches the maximal entropy state \\(E_1^*, \\dots, E_n^*\\), which solves the following constrained maximization \\[\n\\begin{cases}\n    \\max \\sum_i S_i(E_i, A_i)\\\\\n    E = \\sum_i E_i\n\\end{cases}\n\\]\nBy calculus, at the optimal point, all systems satisfy\n\\[\n(\\partial_{E_i} S_i)_{A_i} = \\beta\n\\]\nfor some number \\(\\beta\\). This is the zeroth law of thermodynamics.\nHowever, we are in statistical mechanics, so the compound system actually does not stay exactly at the optimal point. Instead, the energy levels fluctuate.\nLet us write the fluctuation vector as\n\\[Y = (\\Delta E_1, \\dots, \\Delta E_n)\\]\nwhich satisfies the constraint \\(\\sum_i \\Delta E_i = 0\\).\nLet the fluctuation vector be the observable. As proved previously, the fluctuation satisfies\n\\[\\rho_Y(y) \\propto e^{S^*_{X|y}}\\]\nwhere \\(S^*_{X|y}\\) is the entropy of the compound system, given \\(Y = y\\). For small fluctuations, this is just:\n\\[S^*_{X|y} = \\sum_i S_i(E_i^*) + (\\partial_{E_i} S_i)_{A_i} \\Delta E_i + \\frac 12 (\\partial_{E_i}^2 S_i)_{A_i} (\\Delta E_i)^2 + \\cdots\\]\nSince \\(\\sum_i \\Delta E_i = 0\\), this just gives\n\\[\\rho_Y(y) \\propto e^{\\sum_i \\frac 12 (\\partial_{E_i}^2 S_i)_{A_i} (\\Delta E_i)^2}\\]\nNow, \\(\\partial_E S = \\beta\\), and \\(\\partial_E^2 S = -\\frac{1}{T^2 C}\\) in typical thermodynamic notation, where \\(C\\) is \\(\\partial_T E\\), the heat capacity (holding all other variables \\(A\\) constant).\nIn particular, for gases, it is\n\\[\\rho_Y(y) \\propto e^{-\\sum_i \\frac{1}{2T^2 C_{V, i}} (\\Delta E_i)^2}\\]\nwhere \\(C_{V, i}\\) is the constant-volume heat capacity of the \\(i\\) -th gas.\nIn particular, if we have monoatomic ideal gas, with \\(C_V = \\frac 32 N\\), then the size of a typical fluctuation is on the order of\n\\[\\sim\\sqrt N T \\sim N^{-1/2} E^*\\]\nThat is, a typical fluctuation energy is only \\(N^{-1/2}\\) that of the mean energy.\n\n\nDensity fluctuation in the canonical ensemble\nTODO sethna section 6.7"
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#mathematical-developments",
    "href": "blog/posts/statistical-mechanics/index.html#mathematical-developments",
    "title": "Statistical Mechanics",
    "section": "Mathematical developments",
    "text": "Mathematical developments\n\nFundamental theorems\n\nTheorem 1 (Liouville’s theorem) For any phase space and any Hamiltonian over it (which can change with time), phase-space volume is conserved under motion.\nFor any probability distribution \\(\\rho_0\\), if after time \\(t\\), it evolves to \\(\\rho_t\\), and a point \\(x(0)\\) evolves to \\(x(t)\\), then \\(\\rho_0(x(0)) = \\rho_t(x(t))\\).\n\nThe proof is found in any textbook, and also Wikipedia. Since it is already simple enough, and I can’t really improve upon it, I won’t.\n\nCorollary 1 (conservation of entropy) For a Hamiltonian system, with any Hamiltonian (which can change with time), for any probability distribution \\(\\rho\\) over its phase space, its entropy is conserved over time.\n\nIn particular, we have the following corollary:\n\nCorollary 2 Given any set of constraints, if the Hamiltonian preserves these constraints over time, then any constrained-maximal entropy distribution remains constrained-maximal under time-evolution.\n\nIn most cases, the constraint is of a particular form: the expectation is known. In that case, we have the following theorem:\n\nTheorem 2 (maximal entropy under linear constraints) For the following constrained optimization problem\n\\[\n\\begin{cases}\n\\max_\\rho S[\\rho] \\\\\n\\int A_1(x) \\rho(x) &= \\bar A_1 \\\\\n\\cdots &= \\cdots \\\\\n\\int A_n(x) \\rho(x) &= \\bar A_n \\\\\n\\end{cases}\n\\]\nConsider the following ansatz\n\\[\n\\rho(x) = \\frac{1}{Z(a_1, \\dots, a_n)} e^{-\\sum_i a_i A_i(x)}\n\\]\nwhere \\(Z(a_1, \\dots, a_n) = \\int e^{-\\sum_i a_i A_i(x)} dx\\), and \\(a_1, \\dots, a_n\\) are chosen such that the constraints \\(\\int A_i(x) \\rho(x) = \\bar A_i\\) are satisfied.\nIf the ansatz exists, then it is the unique solution.\n\nThe ansatz solution is what you get by Lagrangian multipliers. For a refresher, see the Analytical Mechanics#Lagrange’s devil at Disneyland. The theorem shows that the solution is unique – provided that it exists. Does it exist? Yes, in physics. If it doesn’t exist, then we are clearly not modelling a physically real phenomenon.\n\n\n\n\n\n\nTip\n\n\n\nIn physics, these are “Boltzmann distributions” or “Gibbs distributions”. In statistics, these are exponential families. Because they are everywhere, they have many names.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nDefine a distribution \\(\\rho\\) as given in the statement of the theorem. That is,\n\\[\n\\rho(x) = \\frac{1}{Z(a_1, \\dots, a_n)} e^{-\\sum_i a_i A_i(x)}\n\\]\netc.\nNow, it remains to prove that for any other \\(\\rho'\\) that satisfies the constraints, we have \\(S[\\rho] \\geq S[\\rho']\\).\nBy routine calculation, for any probability distribution \\(\\rho'\\),\n\\[\nD_{KL}(\\rho' \\| \\rho) = -S[\\rho'] + \\sum_i a_i \\braket{A_i}_{\\rho'} + \\ln Z(a_1, \\dots, a_n)\n\\]\nIf \\(\\rho'\\) satisfies the given constraints, then \\(D_{KL}(\\rho' \\| \\rho) = -S[\\rho'] + Const\\) where the constant does not depend on \\(\\rho'\\), as long as it satisfies the constraints. Therefore, \\(S[\\rho']\\) is maximized when \\(D_{KL}(\\rho' \\| \\rho)\\) is minimized, which is exactly \\(\\rho\\).\n\n\n\nThe following proposition is often used when we want to maximize entropy in a two-step process:\n\nTheorem 3 (compound entropy) If \\(\\rho_{X,Y}\\) is a probability distribution over two variables \\((X, Y)\\), then\n\\[S[\\rho_{X,Y}] = S[\\rho_Y] + \\braket{S[\\rho_{X|y}]}_y\\]\nor more succinctly,\n\\[S_{X,Y} = S_Y + \\braket{S_{X|y}}_y\\]\n\n\n\n\n\n\n\nNotations\n\n\n\n\\(\\rho_Y\\) is the probability distribution over \\(Y\\), after we integrate/marginalize \\(X\\) away:\n\\[\n\\rho_Y(y) := \\int \\rho_{X,Y}(x,y)dx\n\\]\n\\(\\rho_{X|y}\\) is the conditional probability distribution over \\(X\\), conditional on \\(Y=y\\):\n\\[\n\\rho_{X|y}(x) := \\frac{\\rho_{X,Y}(x,y)}{\\int \\rho_{X,Y}(x,y) dx}\n\\]\n\\(\\braket{\\cdot}_y\\) is the expectation over \\(\\rho_Y\\):\n\\[\n\\braket{S_{X|y}}_y := \\int S_{X|y} \\rho_Y(y)dy\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider a compound system in ensemble \\(\\rho(x, y)\\). Its entropy is\n\\[S[\\rho] = -\\int dxdy \\; \\rho(x, y) \\ln \\rho(x, y)\\]\nWe can take the calculation in two steps:\n\\[S[\\rho] = -\\int dxdy \\; \\rho(x|y)\\rho(y) (\\ln \\rho(x|y) + \\ln  \\rho(y)) = S[\\rho_Y] + \\mathbb{E}_y[S[\\rho_{X|y}]]\\]\n\n\n\nIntuitively, what does \\(S_{X,Y} = S_Y + \\braket{S_{X|y}}_y\\) mean? It means that the entropy in \\((X, Y)\\) can be decomposed into two parts: the part due to \\(Y\\), and the part remaining after we know \\(Y\\), but not yet knowing \\(X\\). In the language of information theory, the total information in \\((X, Y)\\) is equal to the information in \\(Y\\), plus the information of \\(X\\) conditional over \\(Y\\):\n\\[\nI(X, Y) = I(Y) + I(X|Y)\n\\]\n\n\nMicrocanonical ensembles\nIf the only constraint is the constant-energy constraint \\(H(x) = E\\), then the maximal entropy distribution is the uniform distribution on the shell of constant energy \\(H = E\\). It is uniform, because once we enforce \\(H(x) = E\\), there are no other constraints, and so by Theorem 2, the distribution is uniform.\nThus, we obtain the microcanonical ensemble:\n\\[\\rho_E(x) \\propto 1_{H(x) = E}\\]\nIt is sometimes necessary to deal with the “thickness” of the energy shell. In that case, \\(\\rho_E(x) \\propto \\delta(H(x) - E)\\), where \\(\\delta\\) is the Dirac delta function.\nBy Theorem 2, the microcanonical ensemble is the unique maximizer of entropy under the constraint of constant energy. In particular, if the Hamiltonian does not change over time, then any microcanonical ensemble is preserved over time. In words, if we uniformly “dust” the energy shell of \\(H(x) = E\\) with a cloud of system states, and let all of them evolve over time, then though the dust particles move about, the cloud remains exactly the same.\nMore generally, we can impose more (in)equality constraints, and still obtain a microcanonical ensemble. For example, consider a ball flying around in an empty room with no gravity. The Hamiltonian is \\(H(q, p) = \\frac{p^2}{2m}\\), and its microcanonical ensemble is \\(\\rho(q, p) \\propto \\delta(p = \\sqrt{2mE})1[p \\in \\text{the room}]\\). That is, its velocity is on the energy shell, while its position is uniform over the entire room.\nIf we want to specify the number of particles for each chemical species, then that can be incorporated into the microcanonical ensemble as well. For example, if we want the number of species \\(i\\) be exactly \\(N_{i0}\\), then we multiply \\(\\rho\\) by \\(1[N_i = N_{i0}]\\).\n\n\nCanonical ensembles\nIf we have a small system connected to a large system, then we typically don’t care about the large system, and only want to study the ensemble of the small system. In this case, we would first find the microcanonical ensemble for the total system, then integrate out of the large system, resulting in an ensemble over just the small system, as in\n\\[\\rho_{\\text{small}}(x) = \\int \\rho_{\\text{total}}(x, y) dy\\]\nwhere \\(x\\) ranges over the states of the small system, and \\(y\\) of the large system.\nAssuming that the energy of the compound system is extensive, we obtain the canonical ensemble. Assuming that the energy and volume are both extensive, we obtain the grand canonical ensemble, etc. The following table would be very useful\n\n\n\nextensive constraint\nensemble\nfree entropy\n\n\n\n\nnone\nmicrocanonical\nentropy\n\n\nenergy\ncanonical\nHelmholtz free entropy\n\n\nenergy, volume\n?\nGibbs free entropy\n\n\nenergy, particle count\ngrand canonical\nLandau free entropy\n\n\nenergy, volume, particle count\n?\n?\n\n\n\nThere are some question marks in the above table, because there are no consensus names for those question marks. What is more surprising is that there is no name for the ensemble of constrained energy and volume. I would have expected something like the “Gibbs ensemble”, but history isn’t nice to us like that. Well, then I will name it first, as the big canonical ensemble. And while we’re at it, let’s fill the last row as well:\n\n\n\nextensive constraint\nensemble\nfree entropy\n\n\n\n\nnone\nmicrocanonical\nentropy\n\n\nenergy\ncanonical\nHelmholtz free entropy\n\n\nenergy, volume\nbig canonical\nGibbs free entropy\n\n\nenergy, particle count\ngrand canonical\nLandau free entropy\n\n\nenergy, volume, particle count\ngross canonical\nEVN free energy\n\n\n\n\n\n\n\n\n\nExtensivity\n\n\n\nIn classical thermodynamics, extensivity means that entropy of the compound system can be calculated in a two-step process: calculate the entropy of each subsystem, then add them up. The important fact is that a subsystem still has enough independence to have its own entropy.\nThis is not always obvious. If we have two galaxies of stars, we can think of each as a “cosmic gas” where each particle is a star. Now, if we put them near each other, then the gravity between the two galaxies would mean it is no longer meaningful to speak of “the entropy of galaxy 1”, but only “the entropy of galaxy-compound 1-2”.\nIn statistical mechanics, extensivity means a certain property of each subsystem is unaffected by the state of the other subsystems, and the total is the sum of them. So for example, if \\(A\\) is an extensive property, then it means\n\\[\nA(x_1, \\dots, x_n) = A_1(x_1) + \\dots + A_n(x_n)\n\\]\nLike most textbooks, we assume extensivity by default, although as we noted in Classical Thermodynamics and Economics, both classical thermodynamics and statistical mechanics do not require extensivity. We assume extensivity because it is mathematically convenient, and good enough for most applications.\n\n\nIn the following theorem, we assume that the total system is extensive, and is already in the maximal entropy distribution (microcanonical ensemble)\n\nTheorem 4 If the two systems are in energy-contact, and energy is conserved, and energy is extensive, and the compound system is in a microcanonical ensemble, the small system is in a canonical ensemble\n\\[\n\\rho(x) \\propto e^{-\\beta H(x)}\n\\]\nwhere \\(\\beta\\) is the marginal entropy of energy of the large system:\n\\[\\beta := \\partial_E S[\\rho_{bath, E}]\\]\nSimilarly, if the two systems are in energy-and-particle-contact, then the small system has the grand canonical ensemble\n\\[\n\\rho(x) \\propto e^{-(\\beta H(x) + (-\\beta \\mu) N(x))}\n\\]\nwhere \\(-\\beta\\mu\\) is the marginal entropy of particle of the large system:\n\\[-\\beta\\mu := (\\partial_N S[\\rho_{bath, E, N}])_{E}\\]\nMost generally, if the two systems are in \\(q_1, \\dots, q_m\\) contact, and \\(q_1, \\dots, q_m\\) are conserved and extensive quantity, then\n\\[\\rho(x) \\propto e^{-\\sum_i p_i q_i(x)}\\]\nwhere \\(p_i = (\\partial_{q_i} S[\\rho_{bath, q}])_{q}\\) is the marginal entropy of \\(q_i\\) of the large system.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the case for the canonical ensemble. The other cases are similar.\nSince the total distribution of the whole system is the maximal entropy distribution, we are faced with a constrained maximization problem:\n\\[\\max_\\rho S[\\rho]\\]\nBy Theorem 3,\n\\[S = S_{system}(E_{system}) + \\braket{S_{bath|system}(E_{total} - E_{system})}_{system}\\]\nSince the bath is so much larger than the system, we can take just the first term in its Taylor expansion:\n\\[S_{bath|system}(E_{total} - E_{system}) = S_{bath}(E_{total}) - \\beta E_{system}\\]\nwhere \\(E_{total}\\) is the total energy for the compound system, \\(\\beta = \\partial_E S_{bath}|_{E = E_{total}}\\) is the marginal entropy per energy, and \\(E_{system}\\) is the energy of the system.\nThis gives us the linearly constrained maximization problem of\n\\[\\max_{\\rho_{system}} (S_{system} - \\beta \\braket{E_{system}}_{\\rho_{system}})\\]\nand we apply Lagrange multipliers to finish the proof.\n\n\n\n\n\n\n\n\n\nextensivity\n\n\n\nExtensivitiy in statistical mechanics yields extensivity in thermodynamics. Specifically, writing \\(S_{bath}(E)\\), instead of \\(S_{bath}(E, E_{system})\\), requires the assumption of extensivity. Precisely because the bath and the system do not affect each other, we are allowed to calculate the entropy of the bath without knowing anything about the energy of the system.\n\\(S_{bath}\\) is the logarithm of the surface area of the energy shell \\(H_{bath} = E_{bath}\\). By extensivity, \\(H(x_{bath}, x_{system}) = H_{bath}(x_{bath}) + H_{system}(x_{system})\\), so the energy shells of the bath depends on only \\(E_{bath}\\), not \\(E_{system}\\).\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe proof showed something extra: If the small system is in distribution \\(\\rho\\) that does not equal to the equilibrium distribution \\(\\rho_B\\), then the total system’s entropy is\n\\[S = S_{max} - D_{KL}(\\rho \\| \\rho_B)\\]\nwhich reminds me of Sanov theorem and large deviation theory…\n\n\n\n\n\n\n\n\nEnthalpic ensemble\n\n\n\nWhat if we have a system in volume-contact, but not thermal-contact? This might happen when the system is a flexible bag of gas held in an atmosphere, but the bag is thermally insulating. Notice that in this case, the small system still exchanges energy with the large system via \\(d\\braket{E} = -Pd\\braket{V}\\). We don’t have \\(E = -PdV\\), because the small system might get unlucky. During a moment of weakness, all its particles has abandoned their frontier posts, and the bath has taken advantage of this by encroaching on its land. The system loses volume by \\(\\delta V\\), without earning a compensating \\(\\delta E = P \\delta V\\). In short, the thermodynamic equality \\(E = -PdV\\) is inexact in statistical mechanics, and only holds true on the ensemble average.\nIn this case, because pressure is a constant, we have \\(d(E + PV) = 0\\), and so we have the enthalpic ensemble \\(\\rho \\propto e^{-\\beta H}\\), where \\(H := E + PV\\) is the enthalpy,2.\nSpecifically, if you work through the same argument, you would end up with the following constrained maximization problem:\n\\[\n\\begin{cases}\n\\max_{\\rho_{system}} (S_{system} - \\beta \\braket{E_{system}}_{\\rho_{system}} - \\beta P \\braket{V}) \\\\\n\\braket{E_{system}} + P\\braket{V_{system}} = Const\n\\end{cases}\n\\]\nyielding the enthalpic ensemble (or the isoenthalpic-isobaric ensemble).\n\n\n2 Sorry, I know this is not Hamiltonian, but we are running out of letters.\n\nFree entropies\nJust like in thermodynamics, it is useful to consider free entropies, which are the convex duals of the entropy:\n\nHelmholtz free entropy: \\(f[\\rho] := S[\\rho] - \\beta \\braket{E} = \\int dx \\; \\rho(x) (-\\ln \\rho(x) - \\beta E(x))\\);\nGibbs free entropy: \\(g[\\rho] = S[\\rho] - \\beta \\braket{E} - \\beta P \\braket{V}\\);\nLandau free entropy: \\(\\omega[\\rho] = S[\\rho] - \\beta \\braket{E} + \\beta \\mu \\braket{N}\\);\n\netc. Of those, we would mostly use the Helmholtz free energy, so I will write it down again:\n\\[\nf[\\rho] := S[\\rho] - \\beta \\braket{E} = \\int dx \\; \\rho(x) (-\\ln \\rho(x) - \\beta E(x))\n\\]\n\nTheorem 5 (chain rule for free entropies) \\(f_X = S_Y + \\braket{f_{X|y}}_y\\), and similarly \\(g_X = S_Y + \\braket{g_{X|y}}_y\\), and similarly for all other free entropies.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n  f_X &= S_X - \\beta \\braket{E}_x  \\\\\n  &= S_Y + \\braket{S_{X|y}}_y - \\beta \\braket{\\braket{E}_{x \\sim X|y}}_y \\\\\n  &= S_Y + \\braket{f_{X|y}}_y\n\\end{aligned}\n\\]\n\n\n\nA common trick in statistical mechanics is to characterize the same equilibrium in many different perspectives. For example, the canonical ensemble has at least 4 characterizations at least. “Muscle memory” in statistical mechanics would allow you to nimbly applying the most suitable one for any occasion.\n\nTheorem 6 (4 characterizations of the canonical ensemble)  \n\n(total entropy under fixed energy constraint) The canonical ensemble maximizes total entropy when the system is in contact with an energy bath that satisfies \\(\\partial_E S_{bath} = \\beta\\), and the total energy is fixed.\n(entropy under mean energy constraint) A system maximizes its entropy under constraint \\(\\braket{E} = E_0\\) when it assumes the canonical ensemble with \\(\\beta\\) that is the unique solution to \\(\\int dx \\; e^{-\\beta E(x)} = E_0\\).\n(thermodynamic limit): Take \\(N\\) copies of a system, and connect them by energy-contacts. Inject the system with total energy \\(NE_0\\), and let the system reach its microcanonical ensemble. Then at the thermodynamic limit of \\(N\\to \\infty\\), the distribution of a single system is the canonical distribution with \\(\\beta\\) that is the unique solution to \\(\\int dx \\; e^{-\\beta E(x)} = E_0\\).\n(free entropy) A system maximizes its Helmholtz free entropy when it assumes the canonical ensemble. At the optimal distribution \\(\\rho^*\\), the maximal Helmholtz free entropy is \\(f[\\rho^*] = \\ln Z\\), where \\(Z = \\int dx \\; e^{-\\beta E(x)}\\) is the partition function.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\nWe already proved this.\nUse the Lagrange multiplier.\nIsolate one system, and treat the rest as an energy-bath.\n\\(f[\\rho] = \\ln Z - D_{KL}(\\rho \\| \\rho_B)\\).\n\n\n\n\n\n\nThe partition function\nWhen the system is in a canonical ensemble, we can define a convenient variable \\(Z = \\int dx\\; e^{-\\beta E(x)}\\) called the partition function. As proven in Theorem 6, the partition function is equal to \\(e^f\\), where \\(f\\) is the Helmholtz free entropy of the canonical ensemble.\n\nTheorem 7 (the partition function is the cumulant generating function of energy) Let a system be in canonical ensemble with inverse temperature \\(\\beta\\), and let \\(K(t) := \\ln \\braket{e^{tE}}\\) be the cumulant generating function of its energy, then \\[K(t) = \\ln Z(\\beta-t) - \\ln Z(\\beta)\\]\nIn particular, the \\(n\\)-th cumulant of energy is\n\\[\\kappa_n(E) = K^{(n)}(t) |_{t=0} = (-\\partial_\\beta)^n (\\ln Z)\\]\nA similar proposition applies for the other ensembles and their free entropies.\n\nThe proof is by direct computation.\nFor example, the first two cumulants are the mean and variance:\n\\[\\braket{E} = (-\\partial_\\beta) (\\ln Z), \\quad \\mathrm{Var}(E) = \\partial_\\beta^2 (\\ln Z)\\]\nTypical systems are made of \\(N\\) particles, where \\(N\\) is large, and that these particles are only weakly interacting. In this case, the total Helmholtz free entropy per particle converges at the thermodynamic limit of \\(N \\to \\infty\\):\n\\[\n\\lim_N \\frac 1N \\ln Z \\to \\bar f_\\beta\n\\]\nThus, for large but finite \\(N\\), we have\n\\[\\braket{E} \\approx -N \\partial_\\beta \\bar f_\\beta, \\quad \\mathrm{Var}(E) = N\\partial_\\beta^2 \\bar f_\\beta\\]\nIn particular, the relative fluctuation scales like \\(\\frac{\\sqrt{\\mathrm{Var}(E)}}{\\braket{E}} \\sim N^{-1/2}\\).\n\n\nConditional entropies\nGiven any two random variable \\(X, Y\\), and an “observable” variable \\(Y\\) that is determined by \\(X\\) by some function \\(h\\), such that \\(Y = h(X)\\). If we know \\(X\\), we would know \\(Y\\), but it is not so conversely, as multiple \\(X\\) may correspond to the same \\(Y\\). Typically, we use \\(Y\\) as a “summary statistic” for the more detailed, but more complicated \\(X\\). For example, we might have multiple particles in a box, such that \\(X\\) is their individual locations, while \\(Y\\) is their center of mass.\n\nTheorem 8 (conditional entropy) Given any random variable \\(X\\), and an “observable” variable \\(Y\\) that is determined by \\(X\\), and some constraints \\(c\\) on \\(X\\), if \\(X\\) is the distribution that maximizes entropy under constraints \\(c\\), with entropy \\(S_X^*\\), then the observable \\(Y\\) is distributed as\n\\[\\rho_Y^*(y) = e^{S_{X|y}^* - S_X^*}, \\quad e^{S_X^*} = \\int dy\\; e^{S_{X|y}^*}\\]\nwhere \\(S_{X|y}^*\\) is the maximal entropy for \\(X\\) conditional on the same constraints, plus the extra constraint that \\(Y = y\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy assumption, \\(X\\) is the unique solution to the constrained optimization problem\n\\[\n\\begin{cases}\n    \\max S_X \\\\\n    \\text{constraints on $x$}\n\\end{cases}\n\\]\nBy Theorem 3, the problem is equivalent to:\n\\[\n\\begin{cases}\n    \\max S_Y + \\braket{S_{X|y}}_{y\\sim Y} \\\\\n    \\text{constraints on $x$}\n\\end{cases}\n\\]\nNow, we can solve the original problem in a two-step process: For each possible observable \\(y\\sim Y\\), we solve an extra-constrained problem:\n\\[\n\\begin{cases}\n    \\max S_{X|y} \\\\\n    \\text{original constraints on $x$} \\\\\n    \\text{$x$ must be chosen such that the observable $Y = y$}\n\\end{cases}\n\\]\nThen, each such problem gives us a maximal conditional3 entropy \\(S_{X|y}^*\\), and we can follow it up by solving for \\(Y\\) with\n\\[\\max\\left(S_Y + \\braket{S_{X|y}^*}_{y \\sim Y}\\right)\\]\nAgain, the solution is immediate once we see it is just the KL-divergence:\n\\[S_Y + \\braket{S_{X|y}^*}_{y \\sim Y} = - \\int dy \\; \\rho_Y(y) \\ln\\frac{\\rho_Y(y)}{e^{S_{X|y}^*}} = \\ln Z - D_{KL}(\\rho_Y \\| \\rho_Y^*)\\]\nwhere\n\\[Z = \\int dy\\; e^{S_{X|y}^*}, \\quad \\rho_Y^*(y) = \\frac{e^{S_{X|y}^*}}{Z}\\]\nAt the optimal point, the entropy for \\(X\\) is maximized at \\(S_X^* = \\ln Z - 0\\), so \\(Z = e^{S_X^*}\\).\n\n\n\n3 If you’re a pure mathematician, you can formalize this using measure disintegration.\n\n\n\n\n\nderiving the canonical ensemble yet again\n\n\n\nConsider a small system with energy states \\(E_1, E_2, \\dots\\) and a large bath system, in energy contact. We can set \\(X\\) to be the combined state of the whole system, and \\(Y\\) to be the state of the small system. Once we observe \\(y\\), we have fully determined the small system, so the small system has zero entropy, and so all the entropy comes from the bath system: \\[S_{X|y}^* = S_{bath} = S_{bath}(E_{total}) - \\beta E_y\\]\nConsequently, the distribution of the small system is \\(\\rho_Y(y) \\propto e^{-\\beta E_y}\\), as we expect.\nA similar calculation gives us the grand canonical ensemble, etc.\n\n\n\nTheorem 9 (conditional free entropy) Given any random variable \\(X\\), and an “observable” variable \\(Y\\) that is determined by \\(X\\), and some constraints \\(c\\) on \\(X\\), if \\(X\\) is the distribution that maximizes Helmholtz free entropy under constraints \\(c\\), with Helmholtz free entropy \\(f_X^*\\), then the observable \\(Y\\) is distributed as \\[\\rho_Y^*(y) = e^{f_{X|y}^* - f_X^*}, \\quad e^{f_X^*} = \\int dy\\; e^{f_{X|y}^*}\\]\nwhere \\(f_{X|y}^*\\) is the maximal Helmholtz free entropy for \\(X\\) conditional on the same constraints, plus the constraint that \\(Y = y\\).\nSimilarly for Gibbs free entropy, and all other free entropies.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst note that \\(f_X = S_Y + \\braket{f_{X|y}}_y\\), then argue in the same way."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#maximum-caliber",
    "href": "blog/posts/statistical-mechanics/index.html#maximum-caliber",
    "title": "Statistical Mechanics",
    "section": "Maximum caliber",
    "text": "Maximum caliber\nIn typical developments, statistical mechanics is a “static” theory: It deals with the ensemble of states, but not with how the states change over time. Maximal caliber statistical mechanics handles trajectories in the most obvious way: Collect all paths into a “path space”, define a measure over path space, then study constrained entropy maximization over path space. Jaynes, who proposed the idea, called entropy in path space “caliber”, so the name “maximum caliber” stuck, even though it is really just another instance of maximum entropy.\n\nMarkov chains\n\\(N\\) timesteps in total.\n\\(s_t\\) is the state of timestep \\(t\\).\nThere are \\(m\\) states in total.\n\nTheorem 10 If we fix the singleton probability \\(p_i\\) of each state, then the maximum entropy distribution is the Markov chain with uniform initial probability and \\(p_{i\\to j} = p_i p_j\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy Lagrange multipliers.\nIf we fix the singleton probability of each state, then the problem is a constrained maximization problem\n\\[\n\\begin{cases}\n    \\max S \\\\\n    \\frac 1N \\sum_t 1[s_t = k] = p_k, \\quad \\forall k = 1, \\dots, m\n\\end{cases}\n\\]\nThis is the same problem as \\(N\\) balls in a certain constrained microcanonical ensemble. When \\(N\\) is large enough, the discrete “macrostate” \\(\\frac 1N \\sum_t 1[s_t = k]\\) becomes continuous, and we can use the Lagrange multiplier to obtain \\[\\rho(s_1, \\dots, s_N) = \\frac 1Z e^{-\\sum_{k=1}^m \\lambda_k (\\frac 1N \\sum_{t=1}^N 1[s_t =k] - p_k)}\\]\nThis factors over time \\(t\\), giving us \\[\\rho(s_1, \\dots, s_N) = \\prod_{t=1}^N \\rho(s_t)\\]\nwith\n\\[\\rho(s_t=k) \\propto e^{-\\sum_{k=1}^m\\frac{\\lambda_k}{N}(1[s_t =k] - p_k)} \\propto e^{-\\frac{\\lambda_k}{N}}\\]\nThe multiplier \\(\\lambda_k\\) can be found by the typical method of solving \\(p_k = -\\partial_{\\lambda_k}\\ln Z\\), or we can take the shortcut and notice that \\(\\rho(s_t=k) = p_k\\), and be done with it.\n\n\n\n\nTheorem 11 If we fix the transition probability \\(p_{i \\to j}\\) of each state-pair, then the maximum entropy distribution is the Markov chain with uniform initial probability. If we fix the initial probability, then it’s still the Markov chain with the same transition probabilities.\nAnd more generally, if we fix \\(n\\)-th order transition probability \\(p_{i_1, \\dots, i_n \\to j}\\), then we obtain an \\(n\\)-th order Markov chain model.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSimilarly as above, the path-space distribution is \\[\\rho(s_1, \\dots, s_N) \\propto \\prod_{t=1}^{N-1} e^{-\\sum_{k, k' \\in 1:m} \\frac{\\lambda_{k, k'}}{N} 1[s_t = k, s_{t+1} = k']} \\propto \\prod_{t=1}^{N-1} p_{s_t \\to s_{t+1}}\\]\nBecause the distribution does not specify \\(s_1\\), it is uniformly distributed on \\(s_1\\). Otherwise, we can constrain \\(s_1\\) with yet another set of Lagrange multipliers and obtain \\(\\rho(s_1, \\dots, s_N) \\propto \\rho(s_1) \\times_{t=1}^{N-1} \\rho(s_t, s_{t+1})\\). Similarly for higher orders.\n\n\n\n\n\ndiffusion\nThe path-space entropy\n\\[\nS = - \\int \\rho(x) \\ln \\rho(x) D[x]\n\\]\nwhere \\(D[x]\\) means we integrate over path space, and \\(x: [0, T] \\to \\mathbb{R}^n\\) is a path.\nWe discretize the path into \\(x: \\{0, 1, 2, \\dots, N\\} \\to \\mathbb{R}^n\\), then because we can decompose\n\\[\n\\rho(x) = \\rho(x_0) \\rho(x_1 | x_0) \\cdots \\rho(x_N | x_{0:N-1})\n\\]\nthe path-space entropy decomposes sequentially:\n\\[\nS = S[x_0] + E[S[x_1 | x_0]] + E[S[x_2 | x_{0:1}]] + \\dots + E[S[x_N | x_{0:N-1}]]\n\\]\nTo prevent the entropy from diverging, we need to impose some constraints. If we constrain the second moment of each step, as\n\\[(E[x_t|x_{0:t-1}], E[x_t^2|x_{0:t-1}]) = (0, \\sigma^2), \\quad \\forall t \\in 0:N\\]\nby reasoning backwards from \\(t = N\\) to \\(t=0\\), we find that the maximal entropy distribution is a white noise:\n\\[\n\\rho(x) = \\prod_{t\\in 0:N} \\rho(x_t), \\quad \\rho(x_t) \\propto e^{-\\frac{\\|x_t\\|^2}{2\\sigma^2}}\n\\]\nIf you have studied dynamic programming and cybernetics, this should look very similar to the argument by which you derived the LQR.\nTo keep the path from exploding into white noise, we instead impose the constraints on the step sizes\n\\[(E[x_t - x_{t-1}|x_{0:t-1}], E[\\|x_t - x_{t-1}\\|^2|x_{0:t-1}]) = (0, \\sigma^2), \\quad \\forall t \\in 1:N\\]\nand \\(x_0 = 0\\).\nNow, as in dynamical programming, we can reason backwards from \\(t = N\\) to \\(t=0\\), and we find that the maximal entropy distribution is the Brownian motion\n\\[\\rho(x) \\propto e^{-\\frac{\\sum_{t\\in 1:N} \\| x_t-x_{t-1}\\|^2}{2\\sigma^2}}\\]\nIf we constrain the first and second moments of each step, and allow them to be affected by the previous step, as in\n\\[\n\\begin{cases}\n    E[x_t - x_{t-1}|x_{0:t-1}] &= \\mu(t, x_{t-1}) \\\\\n    E[(x_t - x_{t-1}) (x_t - x_{t-1})^T|x_{0:t-1}] &= \\Sigma(t, x_{t-1})\n\\end{cases}, \\quad \\forall t \\in 1:N\n\\]\nthen, reasoning backwards as before, we would obtain the Fokker–Planck equation.\nOther results, such as the Green-Kubo relation, the Onsager reciprocal relations, etc, can be similarly derived by imposing the right constraints in path space. (Hazoglou et al. 2015)\n\n\nFluctuation-dissipation relations\nImagine if you have a weird liquid. You put a piece of pollen into it, and watch it undergo Brownian motion. Now I flip a switch and suddenly all friction disappears from the liquid. What would happen? The pollen is still being hit by random pieces of molecules, so its velocity is still getting pushed this and that way. However, without friction, the velocity at time \\(t\\) is now obtained by integrating all past impulses, with no dissipation whatsoever. Consequently, its velocity undergoes a random walk, and its position undergoes \\(\\int_0^t W_s ds\\). This is very different from what we actually observe, which is that its position undergoes a random walk, not a time-integral of it.\nIn order to reproduce the actual observed behavior, each fluctuation of its velocity must be quickly dissipated by friction, in a perfect balance such that \\(\\frac 12 m \\braket{v^2} = k_BT\\) exactly. This perfect conspiracy is the fluctuation-dissipation relation (FDR), or rather, the family of FDRs, because there have been so many of those.\nEach FDR is a mathematical equation of form\n\\[\n\\text{something about fluctuation} = \\text{something about dissipation}\n\\]\nThe prototypical FDR is the Einstein relation, to be derived below:\n\\[\n\\underbrace{(\\beta D)^{-1}}_{\\text{fluctuation}} = \\underbrace{\\gamma}_{\\text{dissipation}}\n\\]\nwhere the left side can be interpreted as the strength of molecular noise, and the right side as the strength of molecular damping.\n\n\nEquality before the law\nWhy should there be a mathematical relation between fluctuation and dissipation? I think the deep reason is “equality before the second law”.\nIf we pause and think about it, then isn’t it strange that, despite the molecular storm happening within a placid cup of water, it does not actually fall out of equilibrium? That, despite every molecule of water rushing this way and that, the cup of water as a whole continues to stay without a ripple? What is this second law, the invisible hand pushing it towards statistical equilibrium and keeping it stable? And that is not a question I’m going to answer here. Perhaps calling it an “invisible hand” is already a bad metaphor. Nevertheless, the second law requires the stability of equilibrium distributions (if equilibria exist).\nNow, a system in a statistical equilibrium would, once in a while, deviate significantly from the maximally likely state, just like how we might occasionally flip 10 heads in a row. Nevertheless, if the second law is to be upheld, then significant deviations must be pushed back. In other words, fluctuation is dissipated.\nAssuming that in equilibrium, the system is undergoing a random walk, then by the theory of random walks, internally generated fluctuation must be dissipated according to a mathematical law. The law does not need any further input from physics. Indeed, it has no dependence on any physical observations, and belongs to the pure mathematical theory of random walks.\nFor example, a little pollen in a dish of water might occasionally have a large momentum, but it is very rare. Because it is rare, if we do observe it, we can predict that the next time we look at a pollen, it would have a much lower momentum.\nNow, when physicists use the word “dissipation”, they mean the restoring effect of a system under external dissipation, such as pulling on a pollen by an electrostatic field. However, physical laws are equal, so internal dissipation is external dissipation.\nThus, we see that each FDR manifests as an “equality before the law”:\n\\[\n\\begin{aligned}\n&\\text{fluctuation} \\\\\n\\underbrace{=}_{\\text{random walk theory}} &\\text{dissipation (of internal fluctuations)} \\\\\n\\underbrace{=}_{\\text{equality before the law}} &\\text{dissipation (of external fluctuations)}\n\\end{aligned}\n\\]\n\n\nOne-dimensional FDR\nConsider a simple model of Brownian motion. We have a particle in a sticky fluid, moving on a line. The particle starts at \\(x = 0, t = 0\\), and at each time-step of \\(\\Delta t\\), it moves by \\(\\Delta x\\) to the left or the right.\nThe fluid is at temperature \\(\\beta^{-1}\\), and we pull on the particle at constant force \\(F\\). We expect that \\(F = \\gamma \\braket{v}\\), where \\(v\\) is the ensemble-average velocity of the particle, and \\(\\gamma\\) is the viscosity constant.\nNow, we let the particle move for a time \\(t = N\\Delta t\\), where \\(N\\) is a large number. The particle would have arrived at some point \\(x\\), which is a random variable. The particle’s time-averaged velocity is \\(v = x/t\\).\nThe number of possible paths that connect \\((0, 0)\\) with \\((t, x)\\) is \\(\\binom{N}{\\frac N2 - \\frac{x}{2\\Delta x}}\\), therefore, the path-space entropy is\n\\[S_{path} = \\ln \\binom{N}{\\frac N2 - \\frac{x}{2\\Delta x}} \\approx N \\left(\\ln 2 - \\left(\\frac{x}{N\\Delta x}\\right)^2\\right)\\]\nwhere the approximation is either by Stirling’s approximation, or the binary entropy function.\nBecause the external force performs work \\(Fx\\), which is dissipated into the sticky liquid at temperature \\(\\beta\\), we also have\n\\[S_{work} = \\beta F x\\]\nBecause \\(N\\) is large, \\(\\braket{x}\\) should be highly concentrated around the point of maximal entropy. That is, we should have\n\\[\n\\braket{x} \\approx \\mathop{\\mathrm{argmax}}_x (S_{path} + S_{work})\n\\]\nNotice how this is the exact same problem as the case where we have a rubber band.\nThe equation on the right is quadratic in \\(\\braket{x}\\), and achieves maximum at \\(2\\braket{x} = \\beta FN(\\Delta x)^2\\), which simplifies to the Einstein relation\n\\[\\beta D \\gamma = 1\\]\nwhere \\(D = \\frac{\\Delta x^2}{2\\Delta t}\\) is the diffusion coefficient.\n\n\n\n\n\n\nTip\n\n\n\nWe can calculate not just the mean \\(\\braket{x}\\), but also its variance \\(\\braket{x^2}\\) if we expand \\(S_{path} + S_{work}\\) to second order around its maximum, then apply Theorem 8."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#a-bunch-of-examples",
    "href": "blog/posts/statistical-mechanics/index.html#a-bunch-of-examples",
    "title": "Statistical Mechanics",
    "section": "A bunch of examples",
    "text": "A bunch of examples\n\nIdeal gas\nConsider a tank of ideal gas consisting of \\(N\\) point-masses, flying around in a free space with volume \\(V\\). The tank of gas has inverse temperature \\(\\beta\\), so its phase-space distribution is\n\\[\n\\rho(q_{1:N}, p_{1:N}) = \\prod_{i\\in 1:N} \\rho(q_i, p_i), \\quad \\rho(q, p) = \\underbrace{\\frac{1}{V}}_{\\text{free space}} \\times \\underbrace{\\frac{e^{-\\beta \\frac{\\|p_i\\|^2}{2m}}}{(2\\pi m/\\beta)^{3/2}}}_{\\text{Boltzmann velocity distribution}}\n\\]\nThe total energy of the gas has no positional term, so it is all due to momentum. Because the momenta coordinates \\(p_{1,x}, p_{1,y}, \\dots, p_{N,y}, p_{N,z}\\) do not interact, their kinetic energies simply sum, giving\n\\[\nU = 3N \\times \\int_{\\mathbb{R}}dp\\; \\frac{p^2}{2m} \\frac{e^{-\\frac{p^2}{2m/\\beta}}}{\\sqrt{2\\pi m/\\beta}} = \\frac{3N}{2\\beta}\n\\]\nThis is the same as Boltzmann’s derivation so far. However, although entropy is exactly defined when there are only finitely or countably many possible states, as \\(\\sum_{j \\in \\mathbb{N}} -p_j \\ln p_j\\), this is not so when state space is uncountably large, like \\(\\mathbb{R}^{6N}\\). When Boltzmann encountered the issue, he solved it by discretizing the phase space into arbitrary but small cubes. The effect is that he could rederive the ideal gas laws, but the entropy has an additive constant that depends on the exact choice of the cube size. This was not a problem for Boltzmann, who was trying to found classical thermodynamics upon statistical mechanics, and in classical thermodynamics, entropy does have an indeterminant additive constant.\nLater, Planck in his derivation of the blackbody radiation law, used the same trick. Ironically, Planck did not believe in atoms nor quantized light, but he did make the correct assumption that there is a natural unit of measurement for phase space area, which he called \\(h\\), and which we know as Planck’s constant. (Duncan and Janssen 2019, chap. 2).\nFollowing Planck, we discretize the phase space into little cubes of size \\(h^{3N}\\), and continue:\n\\[\n\\begin{aligned}\n    S &= -\\sum_{i \\in\\text{Little cubes}} p_i \\ln p_i \\\\\n    &\\approx -\\sum_{i \\in\\text{Little cubes}} (\\rho(i) h^{3N}) \\ln (\\rho(i) h^{3N}) \\\\\n    &\\approx -\\int_{\\mathbb{R}^{6N}} dp_{1:N}dq_{1:N} \\; \\rho(p_{1:N}, q_{1:N}) \\ln (\\rho(p_{1:N}, q_{1:N}) h^{3N}) \\\\\n    &= -\\int_{\\mathbb{R}^{6N}} dp_{1:N}dq_{1:N} \\; \\rho(p_{1:N}, q_{1:N}) \\ln \\rho(p_{1:N}, q_{1:N}) - 3N \\ln h \\\\\n    &= -\\underbrace{N\\int_{\\mathbb{R}^{6}} dpdq \\; \\rho(p, q) \\ln \\rho(p, q)}_{\\text{non-interacting particles}} - 3N \\ln h\n\\end{aligned}\n\\]\nNow, the entropy of a single atom \\(\\int_{\\mathbb{R}^{6}} dpdq \\; \\rho(p, q) \\ln \\rho(p, q)\\) factors again into one position space and three momentum spaces:\n\\[\n\\begin{aligned}\n    -\\int_{\\mathbb{R}^{6}} dpdq \\; \\rho(p, q) \\ln \\rho(p, q) &= -\\int_{\\mathbb{R}^3} dq \\rho(q) \\ln \\rho(q) - \\sum_{i = x, y, z} \\int_{\\mathbb{R}} dp_i \\ln \\rho(p_i) \\\\\n    &= \\ln V + 3 \\times \\underbrace{(\\text{entropy of }\\mathcal N(0, m/\\beta))}_{\\text{check Wikipedia}} \\\\\n    &= \\ln V + \\frac 32 \\ln(2\\pi m/\\beta) + \\frac 32 \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nDoes this remind you of our previous discussion about how differential entropy is ill-defined? Finally that discussion is paying off! The choice of a natural unit of measurement in phase space is equivalent to fixing a natural base measure on phase space, such that differential entropy becomes well-defined.\n\n\nThe above is not yet correct, because permuting the atoms does not matter. That is, we have grossly inflated the state space. For example, if \\(N = 2\\), then we have counted the state \\((q_1, p_1, q_2, p_2)\\), then \\((q_2, p_2, q_1, p_1)\\), as if they are different, but they must be counted as the same. We must remove this redundancy by “quotienting out” the permutation group over the particles. The effect is dividing the phase space by \\(\\ln N!\\):\n\\[\n\\begin{aligned}\n    \\frac SN &= \\ln V + \\frac 32 \\ln(2\\pi m/\\beta) + \\frac 32 - 3 \\ln h - \\underbrace{\\frac 1N \\ln N!}_{\\text{Stirling's approximation}} \\\\\n    &= \\ln\\left[\\frac{V}{N} \\left(\\frac{2\\pi m}{\\beta h^2}\\right)^{\\frac 32}\\right] + \\frac 52\n\\end{aligned}\n\\]\ngiving us the Sackur–Tetrode formula:\n\\[\nS(U, V, N) = \\ln\\left[\\frac{V}{N} \\left(\\frac{4\\pi m U}{3N h^2}\\right)^{\\frac 32}\\right] + \\frac 52\n\\]\nAll other thermodynamic quantities can then be derived from this. For example, the pressure is\n\\[P = \\beta^{-1}(\\partial_V S)_{U, N} = \\frac{1}{\\beta V}\\]\nmore conventionally written as \\(PV = \\beta^{-1} = Nk_BT\\), the ideal gas equation, where we have re-inserted the Boltzmann constant in respect for tradition.\n\n\n\n\n\n\nHow Tetrode measured \\(h\\)\n\n\n\nIn early 1900s, Walther Nernst proposed the third law of thermodynamics. The history is rather messy, but suffice to say that the version we are going to care about says, “At the absolute zero of temperature the entropy of every chemically homogeneous solid or liquid body has a zero value.”. In support, he studied experimentally the thermodynamic properties of many materials at temperatures approaching absolute zero. He had a hydrogen liquefier and could reach around \\(20 \\;\\mathrm{K}\\).\nWorking on the assumption that \\(S = 0\\) in any chemical at \\(T = 0\\), he could measure the entropy of any substance by slowing heating up a substance (or cooling down), measuring its heat capacity at all temperatures, then take an integral:\n\\[\nk_B S = \\int \\frac{CdT}{T}\n\\]\nThe low-temperature data for mercury was the most available (mercury was also the substance with which Onnes discovered superconductivity). However, mercury is mostly in a liquid form at low temperatures. Fortunately, the latent heat of vaporization \\(\\Delta L\\) can be measured, and then we can get\n\\[\nS_{\\text{vapor}} = S_{\\text{liquid}} + \\frac{\\Delta L}{k_BT}\n\\]\nBack then, \\(k_B = \\frac{\\text{Gas constant}}{\\text{Avogadro constant}}\\), and the \\(S_{\\text{liquid}}, \\Delta L\\) of mercury were all measured, so combining these, Tetrode calculated a value of \\(h\\) that is within \\(30\\%\\) of modern measurement. (Grimus 2013)\n\n\n\n\nThe blackbody radiation law"
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#kinetic-gas-theory",
    "href": "blog/posts/statistical-mechanics/index.html#kinetic-gas-theory",
    "title": "Statistical Mechanics",
    "section": "Kinetic gas theory",
    "text": "Kinetic gas theory\nKinetic gas theory is the paradigm for pre-1930 statistical mechanics. Boltzmann devoted his best years to kinetic gas theory. The connection between kinetic gas theory and statistical mechanics was so strong that it was often confused as one. Modern statistical mechanics has grown to be so much more than this, so we will only settle for deriving the van der Waals equation. This strikes a balance between triviality (the ideal gas equation could be derived in literally two lines) and complication (Boltzmann’s monumental Lectures on Gas Theory has 500 pages (Boltzmann 2011)).\nTo review, the van der Waals gas equation is\n\\[P = \\frac{N/\\beta}{V- bN} - \\frac{cN^2}{V^2}\\]\nwhere \\(b, c\\) are real numbers that depend on the precise properties of the gas molecules. The term \\(V - bN\\) accounts for the fact that each gas molecule excludes some volume, so that, as \\(N\\) grows, it corrects for the ideal gas pressure \\(P_{ideal}\\) by \\(\\sim P_{ideal}\\frac{bN}{V}\\). The term \\(\\frac{cN^2}{V^2}\\) accounts for overall interaction energy between gas molecules. Suppose the interaction is overall attractive, then we would have \\(c &gt; 0\\), and otherwise \\(c &lt; 0\\).\n\nIdeal gas\nConsider a tank of ideal gas consisting of \\(N\\) point-masses, flying around in a free space with volume \\(V\\). The tank of gas has inverse temperature \\(\\beta\\), so its phase-space distribution is\n\\[\n\\rho(q_{1:N}, p_{1:N}) = \\prod_{i\\in 1:N} \\rho(q_i, p_i), \\quad \\rho(q, p) = \\underbrace{\\frac{1}{V}}_{\\text{free space}} \\times \\underbrace{\\frac{e^{-\\beta \\frac{\\|p_i\\|^2}{2m}}}{(2\\pi m/\\beta)^{3/2}}}_{\\text{Boltzmann momentum distribution}}\n\\]\nThe total energy of the gas has no positional term, so it is all due to momentum. Because the momenta coordinates \\(p_{1,x}, p_{1,y}, \\dots, p_{N,y}, p_{N,z}\\) do not interact, their kinetic energies simply sum, giving\n\\[\nU = 3N \\times \\int_{\\mathbb{R}}dp\\; \\frac{p^2}{2m} \\frac{e^{-\\frac{p^2}{2m/\\beta}}}{\\sqrt{2\\pi m/\\beta}} = \\frac{3N}{2\\beta}\n\\]\nThis is the same as Boltzmann’s derivation so far. However, although entropy is exactly defined when there are only finitely or countably many possible states, as \\(\\sum_{j \\in \\mathbb{N}} -p_j \\ln p_j\\), this is not so when state space is uncountably large, like \\(\\mathbb{R}^{6N}\\). When Boltzmann encountered the issue, he solved it by discretizing the phase space into arbitrary but small cubes. The effect is that he could rederive the ideal gas laws, but the entropy has an additive constant that depends on the exact choice of the cube size. This was not a problem for Boltzmann, who was trying to found classical thermodynamics upon statistical mechanics, and in classical thermodynamics, entropy does have an indeterminant additive constant.\nLater, Planck in his derivation of the blackbody radiation law, used the same trick. Ironically, Planck did not believe in atoms nor quantized light, but he did make the correct assumption that there is a natural unit of measurement for phase space area, which he called \\(h\\), and which we know as Planck’s constant. (Duncan and Janssen 2019, chap. 2).\nFollowing Planck, we discretize the phase space into little cubes of size \\(h^{3N}\\), and continue:\n\\[\n\\begin{aligned}\n    S &= -\\sum_{i \\in\\text{Little cubes}} p_i \\ln p_i \\\\\n    &\\approx -\\sum_{i \\in\\text{Little cubes}} (\\rho(i) h^{3N}) \\ln (\\rho(i) h^{3N}) \\\\\n    &\\approx -\\int_{\\mathbb{R}^{6N}} dp_{1:N}dq_{1:N} \\; \\rho(p_{1:N}, q_{1:N}) \\ln (\\rho(p_{1:N}, q_{1:N}) h^{3N}) \\\\\n    &= -\\int_{\\mathbb{R}^{6N}} dp_{1:N}dq_{1:N} \\; \\rho(p_{1:N}, q_{1:N}) \\ln \\rho(p_{1:N}, q_{1:N}) - 3N \\ln h \\\\\n    &= -\\underbrace{N\\int_{\\mathbb{R}^{6}} dpdq \\; \\rho(p, q) \\ln \\rho(p, q)}_{\\text{non-interacting particles}} - 3N \\ln h\n\\end{aligned}\n\\]\nNow, the entropy of a single atom \\(\\int_{\\mathbb{R}^{6}} dpdq \\; \\rho(p, q) \\ln \\rho(p, q)\\) factors again into one position space and three momentum spaces:\n\\[\n\\begin{aligned}\n    -\\int_{\\mathbb{R}^{6}} dpdq \\; \\rho(p, q) \\ln \\rho(p, q) &= -\\int_{\\mathbb{R}^3} dq \\rho(q) \\ln \\rho(q) - \\sum_{i = x, y, z} \\int_{\\mathbb{R}} dp_i \\ln \\rho(p_i) \\\\\n    &= \\ln V + 3 \\times \\underbrace{(\\text{entropy of }\\mathcal N(0, m/\\beta))}_{\\text{check Wikipedia}} \\\\\n    &= \\ln V + \\frac 32 \\ln(2\\pi m/\\beta) + \\frac 32 \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nDoes this remind you of our previous discussion about how differential entropy is ill-defined? Finally that discussion is paying off! The choice of a natural unit of measurement in phase space is equivalent to fixing a natural base measure on phase space, such that differential entropy becomes well-defined.\n\n\nThe above is not yet correct, because permuting the atoms does not matter. That is, we have grossly inflated the state space. For example, if \\(N = 2\\), then we have counted the state \\((q_1, p_1, q_2, p_2)\\), then \\((q_2, p_2, q_1, p_1)\\), as if they are different, but they must be counted as the same. We must remove this redundancy by “quotienting out” the permutation group over the particles. The effect is dividing the phase space by \\(\\ln N!\\):\n\\[\n\\begin{aligned}\n    \\frac SN &= \\ln V + \\frac 32 \\ln(2\\pi m/\\beta) + \\frac 32 - 3 \\ln h - \\underbrace{\\frac 1N \\ln N!}_{\\text{Stirling's approximation}} \\\\\n    &= \\ln\\left[\\frac{V}{N} \\left(\\frac{2\\pi m}{\\beta h^2}\\right)^{\\frac 32}\\right] + \\frac 52\n\\end{aligned}\n\\]\ngiving us the Sackur–Tetrode formula:\n\\[\nS(U, V, N) = \\ln\\left[\\frac{V}{N} \\left(\\frac{4\\pi m U}{3N h^2}\\right)^{\\frac 32}\\right] + \\frac 52\n\\]\nAll other thermodynamic quantities can then be derived from this. For example, the pressure is\n\\[P = \\beta^{-1}(\\partial_V S)_{U, N} = \\frac{1}{\\beta V}\\]\nmore conventionally written as \\(PV = \\beta^{-1} = Nk_BT\\), the ideal gas equation, where we have re-inserted the Boltzmann constant in respect for tradition.\n\n\n\n\n\n\nHow Tetrode measured \\(h\\)\n\n\n\nIn early 1900s, Walther Nernst proposed the third law of thermodynamics. The history is rather messy, but suffice to say that the version we are going to care about says, “At the absolute zero of temperature the entropy of every chemically homogeneous solid or liquid body has a zero value.”. In support, he studied experimentally the thermodynamic properties of many materials at temperatures approaching absolute zero. He had a hydrogen liquefier and could reach around \\(20 \\;\\mathrm{K}\\).\nWorking on the assumption that \\(S = 0\\) in any chemical at \\(T = 0\\), he could measure the entropy of any substance by slowing heating up a substance (or cooling down), measuring its heat capacity at all temperatures, then take an integral:\n\\[\nk_B S = \\int \\frac{CdT}{T}\n\\]\nThe low-temperature data for mercury was the most available (mercury was also the substance with which Onnes discovered superconductivity). However, mercury is mostly in a liquid form at low temperatures. Fortunately, the latent heat of vaporization \\(\\Delta L\\) can be measured, and then we can get\n\\[\nS_{\\text{vapor}} = S_{\\text{liquid}} + \\frac{\\Delta L}{k_BT}\n\\]\nBack then, \\(k_B = \\frac{\\text{Gas constant}}{\\text{Avogadro constant}}\\), and the \\(S_{\\text{liquid}}, \\Delta L\\) of mercury were all measured, so combining these, Tetrode calculated a value of \\(h\\) that is within \\(30\\%\\) of modern measurement. (Grimus 2013)\n\n\n\n\nIdeal gas (again)\nWe rederive the thermodynamic properties of ideal monoatomic gas via Helmholtz free entropy.\n\\[Z = \\int e^{-\\beta E} = \\underbrace{\\frac{1}{N!}}_{\\text{identical particles}} \\underbrace{V^N}_{\\text{position}} \\underbrace{(2\\pi m/\\beta )^{\\frac 32 N}}_{\\text{momentum}}\\]\nBy the same formula from classical thermodynamics,\n\\[\nd\\ln Z = -\\braket{E}d\\beta + \\beta\\braket{P} dV \\implies\n\\begin{cases}\n    \\braket{E}   &= \\frac 32 \\frac{N}{\\beta} \\\\\n    \\braket{P}V  &= \\frac{N}{\\beta}\n\\end{cases}\n\\]\nNotice how the \\(\\ln N!\\) part simply does not matter in this case.\n\n\nHard ball gas (dilute gas limit)\nIn order to refine the approach, we need to account for two effects.\n\nEach particle takes up finite volume, which forces the total volume of positional space to be smaller than \\(V^N\\).\nParticle pairs have interactions, which changes the Boltzmann distribution.\n\nThe first effect can be modelled by assuming each atom is a hard ball of radius \\(r\\). The particles still have no interaction except that their positions cannot come closer than \\(2r\\).\nBecause there is no potential energy, the Boltzmann distribution on momentum space is the same, and so the Helmholtz free entropy \\(\\ln Z\\) still splits into the sum of positional entropy and momentous entropy. The momentum part is still \\(\\frac 32 N \\ln\\frac{2\\pi}{\\beta m}\\), as the hard balls do not interfere with each other’s momentum, but the position part is smaller, because the balls mutually exclude each other.\nLet \\(a = 8V_{ball} = \\frac{32}{3}\\pi r^3\\) be a constant for the gas.\nTo measure the volume of the diminished position space, we can add one hard ball at a time. The first hard ball can take one of \\(V\\) possible positions, as before. The next ball’s center cannot be within \\(2r\\) of the center of the first ball, so its position can only take one of \\((V - a)\\) positions, where \\(a = 8V_{ball} = \\frac{32}{3}\\pi r^3\\) is a constant that depends on the shape of the hard balls. We continue this argument, obtaining the total volume in position space:\n\\[V(V- a) \\cdots (V - (N-1)a) \\approx V^N e^{0 -\\frac{a}{V}-2\\frac{a}{V} -\\dots -(N-1)\\frac{a}{V}} \\approx V^N\\left(1- \\frac{N^2 a}{2V} \\right)\\]\nThis gives us\n\\[\\braket{E} = \\frac{3N}{2\\beta}, \\quad \\braket{P}V \\approx \\frac N\\beta \\left(1 + \\frac{a N}{2V}\\right) \\approx \\frac{N/\\beta}{V - \\frac a2 N}\\]\nThe second equation is the van der Waals equation when the term \\(c = 0\\), meaning there is neither attraction nor repulsion between particles.\n\n\n\n\n\n\nvirial expansion\n\n\n\nIn the above derivation, we are assuming that only pairwise exclusion matters. That is, we ignore the possibility that three or more balls may simultaneously intersecting each other. We can make a more accurate counting argument via the inclusion-exclusion principle, which would lead us to a virial expansion for gas.\nSpecifically, if the balls \\(A, B\\) are intersecting, which has probability \\(a/V\\), and \\(B, C\\) are also intersecting, also with probability \\(a/V\\), then \\(A, C\\) are quite likely to be also intersecting, with probability much higher than \\(a/V\\). Therefore, if we have excluded the cases where \\(A, B\\) are intersecting by subtracting with \\(a/V\\), and the cases where \\(B, C\\) are intersecting by subtracting another \\(a/V\\), then we should be subtracting with something less than \\(a/V\\). The cluster expansion principle makes this precise. Unfortunately, it requires some difficult combinatorics. The interested reader should study (Andersen 1977).\n\n\n\n\nSoft ball gas (high temperature and dilute gas limit)\nIn the above derivation, we got one part of van der Waals equation right – the part where particles take up space. However, we have not yet accounted for the force between particles. We expect that if the particles attract each other, then \\(P\\) should be smaller, and if the particles repel each other, then \\(P\\) should be larger.\nLet’s assume the gas is made of balls that has a hard core and a soft aura. That is, they repulse or attract each other at a distance, and when a pair comes too close. We also assume the force law depends only on the distances between particles.\nThat is, we can write such a system as having a gas potential energy \\(V(q_1, \\dots, q_N) = \\sum_{i &lt; j} V(\\|q_i - q_j\\|)\\). To enforce the hard core, we should have \\(V(r) = \\infty\\) when \\(r \\in [0, r_0]\\).\n\n\n\nExample potential energy field. This is the Lennard-Jones potential, with a hard (not perfectly hard, but hard enough!) exclusive core, a soft repelling middle, and an attraction when far away. Figure from Wikimedia Commons\n\n\nNow, the partition function becomes\n\\[\nZ = \\int e^{-\\beta\\sum_i \\frac{p_i^2}{2m} - \\beta\\sum_{i &lt; j}V(\\|q_i - q_j\\|)} dqdp\n\\]\nThe momentum part is still the same \\((2\\pi/\\beta m)^{\\frac 32 N}\\), but the position part is more difficult now. Still, we hope it will be close to \\(V^N\\).\nThat is, we need to calculate:\n\\[Z = \\underbrace{V^N (2\\pi/\\beta m)^{\\frac 32 N} \\frac{1}{V^N}}_{\\text{ideal gas}} \\int_{V^N} e^{ - \\beta\\sum_{i &lt; j}V(\\|q_i - q_j\\|)} dq\\]\nThe integral \\(\\int_{V^N} e^{ - \\beta\\sum_{i &lt; j}V(\\|q_i - q_j\\|)} dq\\) can be evaluated piece-by-piece: \\[\n\\int_{V^N} e^{ - \\beta\\sum_{i &lt; j}V(\\|q_i - q_j\\|)} dq = \\int dq_1 \\left(\\int dq_2 \\; e^{-\\beta V(\\| q_1 - q_2 \\|)} \\left(\\int dq_3 \\; e^{-\\beta (V(\\| q_1 - q_3 \\|) + V(\\| q_2 - q_3 \\|))} \\cdots\\right)\\right)\n\\]\nBecause the chamber is so much larger than the molecular force-field, it is basically infinite. So for almost all of \\(q_1\\) (except when it is right at the walls of the chamber), \\(\\int dq_2 \\; e^{-\\beta V(\\| q_1 - q_2 \\|)} \\approx V - \\delta\\), where \\(\\delta\\) is some residual volume:\n\\[\\delta := \\int_{V} dq_2 \\; (1 - e^{-\\beta V(\\|q_2 \\|)})\\]\nFurthermore, because we are dealing with a dilute gas, the higher-order interactions don’t matter (see previous remark about the virial expansion). Therefore, the integral \\[\\int_{V} dq_3 \\; e^{-\\beta (V(\\| q_1 - q_3 \\|) + V(\\| q_2 - q_3 \\|))} \\approx \\int_{V_1 \\cup V_2 \\cup V_3} dq_3 \\; e^{-\\beta (V(\\| q_1 - q_3 \\|) + V(\\| q_2 - q_3 \\|))} \\]\nwhere \\(V_1\\) is the “turf” of particle \\(1\\), and \\(V_2\\) is the turf of particle \\(2\\), and \\(V_3\\) is the rest of the volume. Because the gas is dilute, we have basically \\(V_1\\) disjoint from \\(V_2\\), giving us\n\\[\\approx \\sum_{j = 1, 2, 3}\\int_{V_j} dq_3\\; e^{-\\beta V(\\| q_j - q_3 \\|)} \\approx V - 2\\delta\\]\nTogether, we have \\[\\int_{V^N} e^{ - \\beta\\sum_{i &lt; j}V(\\|q_i - q_j\\|)} dq  \\approx V(V-\\delta) \\cdots(V - (N-1)\\delta)\\]\nGiving us \\[\\ln Z \\approx \\ln Z_{\\text{ideal}} - \\frac{N^2 \\delta}{2V}\\]\nIt remains to calculate the residual volume. It has two parts, one due to the hard core and one due to the soft halo: \\[\\delta = \\int_{\\|q_2 \\| \\leq r_0} dq_2 \\; (1 - e^{-\\infty}) + \\int_{\\|q_2 \\| &gt; r_0} dq_2 \\; (1 - e^{-\\beta V(\\|q_2\\|)})\\]\nThe first part is just \\(a\\), as calculated previously. The second part depends on the exact shape of the potential well. However, when temperature is high, \\(\\beta\\) would be very small, so the second part is approximately \\(\\int dq_2 (\\beta V)\\), which is a constant times \\(\\beta\\).\nThus, we have \\[\\ln Z \\approx \\ln Z_{\\text{ideal}} - \\frac{N^2}{V}(a + b \\beta)\\]\nfor some constants \\(a, b\\). This gives us the van der Waals equation: \\[\\braket{P} V = \\frac{N}{\\beta} + \\frac{N^2}{\\beta V}a + \\frac{N^2}{V} b\\]"
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#other-classical-examples",
    "href": "blog/posts/statistical-mechanics/index.html#other-classical-examples",
    "title": "Statistical Mechanics",
    "section": "Other classical examples",
    "text": "Other classical examples\n\nCountably many states\nIn a surprising number of applications, we have a single system in an energy bath. The system has finitely many, or countably infinitely many, distinguishable states, each with a definite energy: \\(E_0 \\leq E_1 \\leq E_2 \\leq \\cdots\\). In particular, this covers most of the basic examples from quantum mechanics. In such a system, the probability of being in state \\(i\\) is \\(p_i = \\frac 1Z e^{-\\beta E_i}\\) where\n\\[\nZ = \\sum_i e^{-\\beta E_i}\n\\]\nBecause I don’t like sections that are literally two paragraphs long, I will reformulate this as multinomial regression in mathematical statistics.\nIn the problem of classification, we observe some vector \\(\\vec X\\), and we need to classify it into one of finitely many states \\(\\{1, 2, \\dots\\}\\). With multinomial regression, we construct one vector \\(\\vec b_i\\) for each possible state \\(i\\), and then declare that the probability of being in state \\(k\\) is\n\\[\nPr(i | X) = \\frac{e^{-\\vec X \\cdot \\vec b_i}}{Z(X)}, \\quad Z(\\vec X) = \\sum_j e^{-\\vec b_j \\cdot \\vec X}\n\\]\nTo make the parallel clearer:\n\\[\n\\begin{aligned}\n\\text{log probability} & & \\text{observable } & &\\text{ feature} & & \\text{normalization constant} \\\\\n    \\ln p(i | \\beta) &=& -\\beta & \\; & E_i & & - \\ln Z \\\\\n    \\ln Pr(i | \\vec X) &=&    -\\vec X     & \\cdot & \\vec b_i & & - \\ln Z\n\\end{aligned}\n\\]\nWe can make the analogy exact by adding multiple observables. Specifically, if we solve the following constrained optimization problem\n\\[\n\\begin{cases}\n\\max S \\\\\n\\braket{\\vec b} = \\vec b_0\n\\end{cases}\n\\]\nthen the solution is a multinomial classifier, with \\(\\vec X\\) playing the role of \\(\\beta\\).\nInterpreting the physics as statistics, we can think of \\(\\beta\\) as an “observable”. It is as if we are asking the physical system “What state are you in?” but we can only ask a very crude question “What is your energy on average?” Knowing that, we can make a reasonable guess by using the maximal entropy compatible with that answer.\nInterpreting the statistics as physics, we can think of the observable \\(\\vec X\\) as “entropic forces”, trying to push the system towards the distribution of maximal entropy. At the equilibrium of zero entropic force, we have a multinomial classifier. This is the prototypical idea of energy-based statistical modelling.\n\n\nBlackbody radiation\nPlanck’s derivation of the blackbody radiation is the first great success of quantum statistical mechanics. We give a brief presentation here that tracks Planck’s original argument.\nConsider a hollow cubic box with side lengths \\(L\\). The box has perfectly reflecting walls. At thermal equilibrium, the box is full of standing electromagnetic waves. Each standing EM wave has form \\(\\vec E(x, y, z, t) = \\vec E_0 \\sin(\\omega t)\\sin(\\frac{n_x \\pi x}{L})\\sin(\\frac{n_y \\pi y}{L})\\sin(\\frac{n_z \\pi z}{L})\\), for some positive integers \\(n_x, n_y, n_z\\). Each wave has wavevectors \\(\\vec k = (n_x, n_y, n_z) \\frac{\\pi}{L}\\). If we draw a region of volume \\(\\delta K\\) in the space of wavevectors, then the region would contain about \\(\\delta K \\frac{L^3}{\\pi^3}\\) valid wavevectors. Thus, we say that the wavevector space is \\([0, +\\infty)^3\\), and has density of states \\(\\frac{L^3}{\\pi^3}\\). We can picture it as \\([0, +\\infty)^3\\) with a rectangular grid of points being the valid wavevectors, such that the numerical density of such grid points is \\(\\frac{L^3}{\\pi^3}\\).\n\n\n\nThe rectangular grid of valid wavevectors in the space of possible wavevectors.\n\n\nAt this point, we depart from Planck’s derivation. Instead of considering standing waves in a perfectly reflecting chamber, we consider planar waves in a chamber with periodic boundaries. That is, we imagine that we have opened 6 portals, so that its top wall is “ported” to the bottom, etc. In this case, the planar waves have valid wavevectors \\(\\vec k = (n_x, n_y, n_z) \\frac{2\\pi}{L}\\).\n\n\n\n\n\n\nTip\n\n\n\nWait, the numerical density of grid points is now just \\(\\frac{L^3}{8\\pi^3}\\), which is \\(1/8\\) of what we found previously?\nYes, indeed, but it will work out correctly, because whereas the density of states has dropped to just \\(1/8\\) of previously, the state space has increased \\(8\\times\\), from \\([0, +\\infty)^3\\) to \\(\\mathbb{R}^3\\).\n\n\nNow, we need to allow two states at each valid wavevector, to account for polarization.\nAt this point, we have decomposed the state space into a composition of oscillators. Because there is no interaction between these oscillators,4 it remains to calculate the partition function of each oscillator.\n4 That is, two photons do not interact, except when the energy levels are so high that you would need a quantum field theorist to know what is going on.\nLight resembles ghosts and spirits in that they are massless, untouchable, moving very fast, bright, and vaguely associated with good feelings. During the 19th century, the best scientific theory for light, that of ether theory, became the foundation of many spiritualist world systems. (Asprem 2011) The connection of electromagnetism with animal magnetism did not help. Whereas modern spiritualists talk of electromagnetic fields and quantum vibrations, a century ago they talked of subatomic structures and ether vibrations.Planck considered an ensemble of \\(N\\) oscillators, all at the same wavevector and polarization. If they have average energy \\(\\braket{E}\\), the question is to find the total entropy for the whole system, which, when divided by \\(N\\), should yield the entropy of a single oscillator. Here he used the celebrated quantum hypothesis: The energy levels are divided into integer levels of \\(nh\\nu\\), where \\(n = 0, 1, 2, \\dots\\). By the stars and bars argument, there are \\(\\binom{N + M-1}{M}\\) ways do distribute these energy-quanta between these oscillators, where \\(M = \\frac{N\\braket{E}}{h\\nu}\\).\n\\[S = \\frac 1N \\ln \\binom{N+M-1}{M} \\underbrace{\\approx}_{\\text{Stirling}} (1 + a) \\ln (1+a) - a \\ln a, \\quad a = \\frac{\\braket{E}}{h\\nu}\\]\nGiven the entropy function, he then matched \\(\\braket{E}\\) to temperature \\(\\beta\\) by the equality \\(\\beta = \\partial_{\\braket{E}} S\\), giving\n\\[\n\\braket{E} = \\left(\\frac{h\\nu}{e^{\\beta h\\nu}-1}\\right)\n\\]\nThe rest of the derivation is fairly obvious. I will just point out one more interesting fact.\nAccording to Kirchhoff’s law of thermal radiation, a chunk of matter is exactly as absorptive as it is emissive. A black body absorbs all light, and conversely it emits light at the maximal level. A white body absorbs no light, and conversely it does not emit light. This can be understood as a consequence of the second law: If a body emits more light than it absorbs, then it would spontaneously get colder when placed inside a black body radiation chamber.\nHowever, much more can be said than this. Not only is it exactly as absorptive as it is emissive, it is as absorptive as it is emissive at any angle, at any wavelength, and any polarization. So for example, if a piece of leaf is not absorptive when viewed from an angle, at the green light wavelength, of clockwise polarization, then it is not emissive under the same angle, wavelength, polarization.\nWhy is that? The standard argument (Reif 1998, chap. 9.15) uses a time-reversal argument, but I like to think of it as yet more instances of protecting the second law. If you look inside a black body radiation chamber, you would see a maximal entropy state. Light rushes in all directions equally, at all polarizations equally, and the energy is distributed optimally across the spectrum to maximize entropy (because \\(\\beta\\) is constant across the whole spectrum). If we have a material that takes in blue light and outputs green light, then it would spontaneously decrease entropy. Similarly, if it can absorb vertically polarized light to emit diagonally polarized light, it would also spontaneously decrease entropy, etc."
  },
  {
    "objectID": "blog/posts/statistical-mechanics/index.html#even-more-examples",
    "href": "blog/posts/statistical-mechanics/index.html#even-more-examples",
    "title": "Statistical Mechanics",
    "section": "Even more examples",
    "text": "Even more examples\n\nrubber bands\nSETUP.\nWe have a long chain molecule with \\(N\\) joints. Each joint can go forward or backward, with equal energy. Each link between two joints has length \\(d\\).\nThe total length of the system is \\(L\\).\n\n\n\nRubber band.\n\n\nThe entropy of the system, conditional on \\(L\\), is\n\\[S = \\ln \\binom{N}{\\frac{N + L/d}{2}}\\]\nThe thermodynamic equation for the rubber band is \\[0 = TdS + FdL\\]\nbecause the internal energy of the rubber band is constant, no matter how the joint turns.\nTherefore, the elastic force is \\[F = -T \\partial_L S \\approx -T \\frac{S(L+2d) - S(L)}{2d} \\approx \\frac{T}{2d }\\ln\\frac{Nd+L}{Nd - L}\\]\nWhen \\(Nd \\gg L\\), that is, we have not stretched it close to the breaking point, the elastic force is \\[F \\approx \\frac{TL}{Nd^2} = k L\\]\nwhere \\(k = \\frac{T}{Nd^2}\\) is the elastic constant, proportional to temperature.\nWhy does the rubber band stiffen when temperature rise? We can interpret it as follows. When we place the rubber band in a chamber of hot air, the air particles would often collide with the links in the rubber band, flipping it. When there are more links going to the right than the left, then the air particles would tend to flip the links to the left, decreasing \\(L\\), and conversely. The net force is zero only when there are an equal number of links going either way, which is when \\(L = 0\\).\nDER. ensemble of lengths\nBecause the rubber band has \\(dE = TdS + FdL\\), the corresponding free entropy is \\[S + \\beta F\\braket{L}\\]\nand under the canonical distribution, that free entropy is maximized, meaning that under canonical distribution,\n\\[\\rho(x) = \\frac{1}{Z}e^{\\beta FL(x)}\\]\nwhere \\(x\\) is a microstate of the rubber band (i.e. the precise position of each link), and \\(L(x)\\) is the corresponding length (macrostate).\nSince for each \\(L\\) there are \\(\\binom{N}{\\frac{N + L/d}{2}}\\) possible microstates, we have \\[\\rho(L) \\propto  \\binom{N}{\\frac{N + L/d}{2}} e^{\\beta F L}\\]\nWhen \\(Nd \\gg L\\), the binomial coefficient is approximately \\(\\exp{[N H_2(\\frac 12 + \\frac{L}{2dN})]}\\), where \\(H_2\\) is the binary entropy function, therefore \\[\\ln \\rho(L) \\approx \\beta F L + NH_2\\left(\\frac 12 + \\frac{L}{2dN}\\right) + Const \\approx \\beta FL - \\frac{L^2}{2Nd^2} + Const\\]\nIn particular, we see that \\(\\ln \\rho(L)\\) is maximized at \\(\\braket{L} = \\beta F Nd^2\\), exactly what we have found previously. But this time we have also the variance: \\[\\sigma_L^2 = \\frac{(Nd)^2}{N}\\]\nwhich again is an example of the general result that \\(\\sigma \\propto N^{-1/2}\\).\n\n\nCombinatorics\n\nMultinomials\nLet \\(p_1, \\dots, p_k\\) be a discrete probability distribution, and let \\(n_1, \\dots, n_k\\) be integers that depend on \\(N\\), such that\n\\[\\lim_{N \\to \\infty}n_i/N = p_i\\]\nthen we have\n\\[\\lim_{N \\to \\infty}\\frac 1N \\ln \\binom{N}{n_1, \\dots, n_k} = -\\sum_i p_i \\ln p_i\\]\nImagine we have a system made of a single ball that can go into one out of \\(k\\) labelled boxes. Now, let us copy paste the system \\(N\\) times, so we have \\(N\\) balls and \\(kN\\) boxes. Then we can interpret \\[\\frac 1N \\ln \\binom{N}{n_1, \\dots, n_k}\\]\nas the maximal average entropy per ball under the following constraints: There are exactly \\(n_1\\) balls in boxes labelled \\(1\\), etc.\nThinking thermodynamically, we can pick one ball as the system, and put it in contact with a bath made of \\(N\\) balls. Once we calculate the marginal entropies of the bath, we can infer the Boltzmann distribution for the system, and thus the marginal entropy of the system.\nNow, here is the non-rigorous part: We hope that as \\(N \\to \\infty\\), all correlations (pairwise, triple-wise, etc) between balls decay fast enough, such that the interaction-entropy between the balls drop to zero, leaving \\[\\text{average entropy per ball} = \\text{marginal entropy of a ball}\\]\nJustifying this rigorously is generally very difficult, and in fact, the assumption is false at phase transitions, where correlations do not decay fast enough.\nIf this is so, then we need to only show that the marginal distribution of a single ball converges to \\((p_1, \\dots, p_k)\\).\nSuppose we move the ball from box \\(i\\) to box \\(j\\), then it would force the bath to change all its balls, changing its entropy by \\[\n\\ln\\binom{N}{n_1', \\dots, n_k'} -\\ln \\binom{N}{n_1, \\dots, n_k}\n\\]\nwhere \\(n_i' = n_i + 1, n_j' = n_j - 1\\), and otherwise unchanged.\nBy definition of multinomials, this is equal to\n\\[\\ln n_j - \\ln(n_i+1)\\]\nwhich converges to \\((\\ln p_j - \\ln p_i)\\) at large enough \\(N\\).\nLet \\(X\\) stand for the total state, including the bath and the singled-out system, and let \\(Y\\) stand for the state of the singled-out system. By the conditional entropy theorem, when the entire system is at the maximal entropy distribution, the distribution of the singled-out system is\n\\[\\rho^*_Y(y) \\propto e^{S_{X|y}^*}\\]\nFrom the previous calculation, we have \\[S_{X|j}^* - S_{X|j}^* = \\ln p_j - \\ln p_i\\]\nyielding \\(S_{X|i}^* = S_0 + \\ln p_i\\) for some constant \\(S_0\\), and so \\(\\rho^*_Y(y) \\propto p_i\\), finishing the proof.\n\n\nSanov theorem\nConsider a population of particles, all in energy-contact with an energy bath of \\(\\beta = 1\\). Each particle has \\(n\\) states, with state \\(i\\) having energy \\(-\\ln p_i\\). Thus, at the Boltzmann distribution, each particle is precisely sampled from the categorical distribution \\((p_1, \\dots, p_n)\\).\nLet \\(X\\) stand for the state of the entire population of particles. That is, \\(X\\) describes the precise state of each particle. Let \\(Y\\) stand for the vector of \\(N_1, \\dots, N_n\\). That is, it counts the number of particles in each state.\nThe free entropy of a single particle is \\[\\bar f_X^* = S - \\beta \\braket{E} = \\sum_i (-p_i \\ln p_i) - \\sum_i p_i E_i = 0\\]\nBecause the particles have no interaction energy, the total free entropy of \\(X\\) is \\(f_X^* = N \\bar f_X^* = 0\\).\nBy the conditional free entropy theorem, \\[\\rho^*_Y(N_1, \\dots, N_n) = e^{f_{X|N_1, \\dots, N_n}^*}\\]\nwhere \\(f_{X|N_1, \\dots, N_n}^*\\) is the maximal free entropy of the entire system conditional on \\(Y = (N_1, \\dots, N_n)\\).\nNow, by definition of \\(f_{X|N_1, \\dots, N_n}^*\\), it is the maximal free entropy under constraint: \\[\n\\begin{cases}\n\\max_\\rho \\sum_{(s_1, \\dots, s_N) \\in n^N} \\rho(s) (-\\ln \\rho(s) - \\beta E(s))  \\\\\nY = (N_1, \\dots, N_n)\n\\end{cases}\n\\]\nwhere \\(s_i\\) is the state of particle \\(i\\).\nFor any \\(\\rho\\) that satisfies the constraint of \\(Y = (N_1, \\dots, N_n)\\), the term is a constant\n\\[E(s) = \\sum_{i=1}^n N_i E_i\\]\nTherefore, the problem reduces to maximizing just the entropy under constraint of \\(Y = (N_1, \\dots, N_n)\\), which is a “microcanonical distribution”, the uniform distribution over all possible \\(\\binom{N}{N_1, \\dots, N_n}\\) ways to satisfy the \\(Y = (N_1, \\dots, N_n)\\) constraint.\nTherefore, \\[f_{X|N_1, \\dots, N_n}^* = \\ln \\binom{N}{N_1, \\dots, N_n} + \\sum_i N_i \\ln p_i \\approx -ND_{KL}(q \\| p)\\]\nwhere \\(q = (N_1/N, \\dots, N_n/N)\\) is the empirical distribution.\nTherefore, \\[\n\\frac 1N \\ln \\rho^*_Y(N_1, \\dots, N_n) \\to D_{KL}(q \\| p)\n\\]\n\n\nSurface area of high-dimensional spheres\nLet the surface area of a sphere of radius \\(\\sqrt N\\) in dimension \\(N\\) be \\(\\Omega_N\\), then\n\\[\\ln\\Omega_N = \\frac N2 \\ln (2\\pi e)-\\frac 12 \\ln (\\pi e) + O(N^{-1})\\]\nLet \\(x_1, \\dots, x_N\\) be sampled IID from \\(\\mathcal N(0, 1)\\), and let \\(r_N^2 = \\sum_i x_i^2\\). By routine calculation, \\(\\mathbb{E}[r_N^2] = N\\), and \\(\\mathbb{E}[r_N^4] = 2N + N^2\\). Therefore, \\(r_N^2\\) is approximately distributed as \\(\\mathcal N(N, 2N)\\), and so \\(r_N\\) is approximately distributed as \\(\\mathcal N(\\sqrt N, 1/2)\\).\nNow consider two distributions on \\(\\mathbb{R}^N\\). The first is a microcanonical ensemble: \\(x_{1:N}\\) is distributed uniformly on the thin energy shell of \\(r_N^2 \\in [N, N + \\delta N]\\). The second is a canonical ensemble: each \\(x_i\\) is distributed independently according to \\(\\mathcal N(0, 1)\\).\nWe can think of them as particles in a potential well of form \\(V(x) = \\frac 12 x^2\\). The first ensemble is the microcanonical ensemble where the total energy is fixed, and the second is the canonical ensemble at temperature \\(\\beta = 1\\).\nWe can calculate the entropy of the canonical ensemble in two ways. We can calculate it by adding up the entropy of each particle, which are the same since there is no interaction energy between particles. We can also calculate it indirectly, by first sampling a random radius, then a random point from the microcanonical ensemble, then multiplying them together.\nBecause the canonical ensemble is spherically symmetric, the radius and the direction of the vector \\(x_{1:N}\\) are independent. Therefore,\n\\[S_{\\text{canonical}} = S_{\\text{microcanonical}} + S_{\\text{radius}}\\]\nor\n\\[\\ln \\Omega_N = S_{\\text{canonical}} - S_{\\text{radius}}\\]\nBecause the entropy of \\(\\mathcal N(0, \\sigma^2)\\) is \\(\\frac 12 \\ln(2\\pi e \\sigma^2)\\), we plug them in and obtain the result.\n\n\n\n\n\n\nTip\n\n\n\nThis is an example of a general pattern: the microcanonical ensemble and the canonical ensemble become indistinguishable when the number of particles goes infinite.\n\n\n\n\n\nHow elastic is the skin of red blood cell?\nIn 2000, Discher set out to measure the elasticity of the red blood cell’s skin. To a good approximation, it is just a 2D spring, following Hooke’s law. He attached a tiny particle to the surface of a red blood cell, and measured its thermal motion.\nAs usual, in the microscopic world, inertia might as well not exist, so the oscillator’s energy is entirely elastic. Let it be of form \\(\\frac 12 k(x^2 + y^2)\\). By equipartition of energy, we would have\n\\[\\frac 12 k\\braket{x^2} = \\frac 12 \\beta^{-1}\\]\nBut just to be sure, I checked the original numbers: It is a particle of diameter \\(40 \\;\\mathrm{nm}\\), and moving at about \\(100 \\;\\mathrm{nm/s}\\), so its kinetic energy is \\(\\sim 10^{-33}\\;\\mathrm{J}\\).\nThe data shows that \\(\\braket{x^2} = (35 \\;\\mathrm{nm})^2\\) at temperature \\(T = 310 \\;\\mathrm{K}\\), giving us an effective elastic constant of \\(k \\sim 0.004 \\;\\mathrm{pN/nm}\\).\nA red blood cell has diameter \\(10^4 \\;\\mathrm{nm}\\), so dragging a patch of its skin all across the surface would take only about \\(40 \\;\\mathrm{pN}\\). This is about the force output of 10 kinesins working in concert. Thus we can say that the skin of red blood cell is very slack.\n\n\n\n(Discher 2000, fig. 6)\n\n\n\n\nUnzipping RNA hairpins\n(Sethna 2021, exercise 6.4)\n\n\n\nRNA hairpin. Figure from Wikimedia Commons\n\n\n(Liphardt et al. 2001)\nThe RNA hairpin is a statistical mechanical system that should correspond to a thermodynamic system satisfying \\[dE = TdS + F dx\\]\nLooking at this, we see that the correct canonical ensemble to use is the free entropy with respect to constant temperature and constant pulling force:\n\\[\\rho = Z(\\beta, \\beta F)^{-1} e^{-\\beta E + \\beta F x}\\]\nLet \\(x_0\\) be the length of a single RNA unit, \\(n\\) be the number of zipped RNA basepairs, \\(N\\) be the total number of RNA basepairs, and \\(-E_0\\) be the bonding energy of a basepair (assume that the two kinds of basepairs have the same bonding energy).\nThis gives us \\[\\rho(n) =Z^{-1}e^{\\beta( E_0 - 2Fx_0)n}, \\quad 0 \\leq 2n \\leq N\\]\nwhere\n\\[Z= \\frac{1-a^{floor(N/2)}}{1-a}, \\quad a = e^{-\\beta |E_0 - 2Fx_0|}\\]\nWe see that when \\(F = E_0/2x_0\\), the bonding force and the pulling force are exactly balanced, and the hairpin is equally likely to be in any state. When the pulling force is larger, then it tends to be unzipped. When \\(F \\gg E_0/2x_0\\), then \\(\\rho(n)\\)\n\n\nHungry hungry bacteria\nThis example came from (Howard C. Berg 1993, chap. 6)\nConsider a bacteria swimming in water. A typical one, such as E coli, is roughly a rod with length \\(a = 10^{-6}m\\) and swimming at \\(v = 2\\times 10^{-5} m/s\\). That is, it swims 20 body-lengths a second.\nWater has density \\(\\rho = 10^3 kg/m^3\\) and viscosity \\(\\eta = 10^{-3} kg/m\\cdot s\\). As is well-known, the Reynold number of the system is\n\\[Re = \\frac{\\rho v a}{\\eta} = 2 \\times 10^{-5}\\]\nmeaning that as soon as the bacteria stops powering itself, its motion ceases after coasting for a length of \\(\\approx 0.1 a \\cdot Re = 2\\times 10^{-12}m\\), less than the length of an atom! Thus, the bacteria lives in an essentially inertia-less world.\nAssuming that a bacteria is a sphere, then its swimming dissipates power at\n\\[P = Fv = 6\\pi \\eta a v^2 \\approx 8\\times 10^{-18}W \\approx 2000 k_BT/s\\]\nwhere \\(T = 300 K\\) is the standard temperature for biology. Since each glucose at complete metabolism produces \\(686 kcal/mol \\approx 1000 k_BT\\), the bacteria just needs to eat 2 molecules of glucose per second to power its swimming.\nBy the FDR, the diffusion constant for the bacteria is\n\\[D = \\frac{k_BT}{6\\pi \\eta a} = 2 \\times 10^{-13} m^2/s\\]\nmeaning that within 1 second, the bacteria diffuses a distance of \\(\\sim \\sqrt{2 D \\Delta t} \\sim a\\), about 1 body length. This shows that the swimming motion is 20 times faster than its thermal motion.\nHowever, if we look at the trajectory of a bacterium under the microscope, we would notice its direction jumping around rapidly. This is due to the thermal motion of its orientation on the rotational group \\(SO(3)\\). Just like how each translational degree of freedom gets \\(\\frac 12 k_BT\\) of thermal energy, each rotational degree of freedom gets it as well, although, because the space of rotations is not a vector space, making this precise is tricky.\nStill, once we make it precise, the FDR is still the same, gives us \\(D_{rotational} = \\frac{k_B T}{\\gamma_{rotational}}\\). Assuming the bacterium is still a sphere of radius \\(a\\), then \\(\\gamma_{rotational} = 8\\pi \\eta a^3\\), giving \\(D_{rotational} = 0.2 rad^2/s\\).\nNow, a swimming bacterium does not care about rotation around its velocity axis (“longitude”), but does care about the other two rotational freedoms. The variation in “latitude” angle is the sum:\n\\[\\braket{\\theta^2} = 2 \\times (2D_{rotational}\\Delta t)\\]\nfor small time \\(\\Delta t\\). This is false for large \\(\\Delta t\\), because otherwise we would get \\(\\braket{\\theta^2} &gt; \\pi^2\\), which is absurd because \\(\\theta \\in [0, \\pi]\\).\nThis implies that the bacteria veers by about \\(\\braket{\\theta^2} \\approx (50^\\circ)^2\\) after a second of swimming. The actual observation gives \\(\\braket{\\theta^2} \\approx (30^\\circ)^2\\). In particular, the bacteria cannot keep a direction for more than about 3 seconds, as at that point its velocity vector would have diffused by about 90 degrees. As the bacteria has no “vestibular organ”, it essentially “forgets its heading” within a few seconds. Any sense of direction must come from the outside world, such as a chemical gradient.\nThe standard strategy for E coli hunting is the “run and tumble” strategy. During the “run” phase, the bacterium keeps swimming forward for 1 – 10 seconds, curving gently due to rotational diffusion. At a random point it stops and tumbles for about 0.1 seconds, changing its direction randomly. Tumbling essentially samples a random direction for the next run, close but not quite uniformly on the sphere. It remembers the average chemical concentration during the first second and last second of each run. If the chemical concentration seem to increase, it would tumble less often (thus going on longer runs). Otherwise, it would tumble more often. The net effect is an extremely simple homing missile that diffuses up the chemical gradient.\nThere is no point for it to swim more than 10 seconds, because it would have totally lost its heading due to diffusion. There is no point in swimming less than 1 second, because it needs to average the chemical concentration over at least a second to overcome the statistical noise. And so, by simple physics, we have explained an E coli’s hunting strategy.\nSee (H. C. Berg and Purcell 1977; Howard C. Berg 2000) for a detailed look at how bacteria optimally forage for food molecules.\n\n\n\nA spherical bacterium is propelled by spinning flagella. (Howard C. Berg 1993, fig. 6.6)\n\n\nThe spherical bacterium swims along the \\(x\\) -axis. Its rotational state diffuses along three possible degrees of freedom. Of those, the one around \\(x\\)-axis is irrelevant, and the ones around \\(y\\)- and \\(z\\)-axes are relevant.\n\n\n\nA single 3D trajectory of a single bacterium, projected into the \\(xy, yz, xz\\) planes. Gently winding runs alternate with the tight tumbles. (Howard C. Berg and Brown 1972, fig. 1)\n\n\nTrajectory of an E coli in a liquid that is 3 times more viscous than water. The bacterium alternates between swimming and tumbling. During the swimming phase, the angular diffusion has a diffusion constant of \\(0.06 rad^2/s\\), as theoretically predicted. (Howard C. Berg and Brown 1972)"
  }
]