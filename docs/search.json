[
  {
    "objectID": "blog/posts/thermoeconomics/index.html#theodicy-economic-thermodynamics",
    "href": "blog/posts/thermoeconomics/index.html#theodicy-economic-thermodynamics",
    "title": "Yuxi on the Wired",
    "section": "Theodicy: economic thermodynamics",
    "text": "Theodicy: economic thermodynamics\nAlternative title: The Gospel of Nature according to Yuxi.\nIn classical thermodynamics, we consider only equilibrium systems. Similarly, in classical microeconomics, we consider only general equilibrium. As we will see, this allows us to make a mathematical analogy.\nI must warn you, before you enter, to forget all about molecules and atoms. Forget all about statistics and statistical mechanics. Forget about \\(S = k_B\\ln \\Omega\\) or \\(S = -\\sum_i p_i \\ln p_i\\) . None of these fits into the logical structure of classical thermodynamics.\n\nTable\n\n\n\n\n\n\nthermodynamics\nmicroeconomics\n\n\n\n\nsystem\ncompany\n\n\ncompound system\nconglomerate\n\n\nsubsystem\nchild company\n\n\nentropy\nmarket value (according to an accounting agency)\n\n\nentropies are additive\nvalue of conglomerate is the sum of its child companies\n\n\nenergy, volume, and other conserved quantities\ncommodities that cannot be created or destroyed\n\n\ninverse temperature \\(\\beta = \\partial_E S\\)\nmarginal value of energy\n\n\nunnamed quantity \\(\\beta p = \\partial_V S\\)\nmarginal value of volume\n\n\nunnamed quantity \\(-\\beta \\mu_i = \\partial_{N_i} S\\)\nmarginal value of chemical \\(i\\)\n\n\n\n\nsecond law\nThe second law of thermodynamics is all important: maximizing entropy is all of classical thermodynamics! Everything else are just tricks for us to understand how to maximize entropy.\n\n\nfirst law\nThe first law of thermodynamics is entirely trivial. Energy is nothing special!\nEnergy is just a conserved quantity, much like volume, mass, and some other things… In particular, the conservation of energy is not nearly as important as it sounds like. It does not deserve the title of “the first law of thermodynamics”. You might as well say “conservation of mass” is “the second-first law of thermodynamics” and “conservation of volume” is “the third-first law of thermodynamics”, and “conservation of electrons” and “conservation of protons” and “conservation of length” (if you are studying a thermodynamic system restricted to move on a line) and “conservation of area” (if you are studying a thermodynamic system restricted on the surface of a lake) and so on…\nThis sounds extraordinary, but that is merely how classical thermodynamics works. The first law of thermodynamics does not deserve its title. There should not even be a first law of thermodynamics.\nProperly speaking, “conservation of energy” is not a law of thermodynamics, but a law of physics. Why? Well, energy is nothing special inside classical thermodynamics, but it is extremely special if we zoom out to consider the whole of physics, because whereas in classical thermodynamics, you can consider systems that conserve energy, or volume, or length, or mass… when you move outside of thermodynamics, such as when you add in electrodynamics, special relativity, and quantum mechanics, all kinds of conservations breakdown. You don’t have conservation of mass, or number of electrons, or even volume, but energy is always conserved.\nSince energy is nothing special, we will demote it to “just another commodity”, like volume, number of electrons, protons, and so on.\nEvery conserved quantity is a commodity. The company has some commodity. Commodities themselves have no intrinsic value. Instead, the company is valued by a certain accounting agency. The CEO’s job is to move around the commodities so that the accounting agency gives it the highest value on the book.\nA compound system is a conglomerate: a giant company made of little companies.\nA subsystem has an inverse temperature \\(\\beta = 1/T\\) , which equals \\[\\beta = \\frac{d(\\text{value of a sub-company})}{d(\\text{energy owned by the sub-company})}\\]\nIn other words, the marginal value of energy. Electricity price!\nIf a subsystem has variable volume then it has a pressure \\(P\\) , which satisfies \\[\\beta P= \\frac{d(\\text{value of a sub-company})}{d(\\text{volume owned by the sub-company})}\\]\nIn other words, the marginal value of space. Real estate price!\nYes, I know it sounds weird to say (Pressure/Temperature), but that’s just how the math works out. It turns out that (Pressure/Temperature) is more fundamental than Pressure… Pressure, indeed, is actually Real estate price / Electricity price. That’s why it has units of ($/m^3)/($/Joule) = Joule/m^3!\nWhy, then, do we speak of pressure \\(P\\) and temperature \\(T\\) , instead of \\(\\beta\\) and \\(\\beta P\\) ? I blame habit and the general inability to visualize classical entropy.\n\n\nzeroth law\nThe zeroth law of thermodynamics, too, becomes trivial: Temperature is nothing special!\nThe zeroth law states that if A, B in energy-contact are in equilibrium, and B, C in energy-contact are in equilibrium, then A, C in energy-contact are in equilibrium.\nThe exact same statement works if we replace “energy” with “volume”, or “chemical \\(i\\)”, or “commodity”.\nThey are all merely special cases of the general economic law: If several sub-companies are allowed to trade commodity \\(x\\) , then when the total value of the conglomerate is maximized, the marginal value of \\(x\\) is the same for each sub-company.\n\n\nthird law\nThis is not actually important for classical thermodynamics."
  },
  {
    "objectID": "blog/posts/thermoeconomics/index.html#nonequilibrium-and-equilibrium",
    "href": "blog/posts/thermoeconomics/index.html#nonequilibrium-and-equilibrium",
    "title": "Yuxi on the Wired",
    "section": "Nonequilibrium and equilibrium",
    "text": "Nonequilibrium and equilibrium\n\nFreedom is an iron cage.\nConstraints set it free again.\n\nClassical thermodynamics is a strange kind of science. It is quite subtle, subtle as general equilibrium microeconomics. The key to understanding it is to distinguish equilibrium and nonequilibrium.\nI must warn you again, to forget all about molecules and atoms. Forget all about statistics and statistical mechanics. Forget about \\(S = k_B\\ln \\Omega\\) or \\(S = -\\sum_i p_i \\ln p_i\\) . None of these fits into the logical structure of classical thermodynamics.\n\n\n\nequilibrium\nnonequilibrium\n\n\n\n\nphysical\nvirtual\n\n\ndefined by external constraints\ndefined by internal variables\n\n\n\n\nA little story.\nOn the African savannah there lived a bunch of meerkats. Meerkats love to stand on the tallest place.\nAt first, they could stand wherever they wanted, so they all stood on one single hill. It was crowded. A blind lion who had memorized the landscape came and ate all of them.\nThen humans came and added walls that divided the savannah into tiny little stripes. Now each meerkat’s location is determined by which stripe it happened to fall in. The blind lion could find the meerkat if he knew the stripe location. In other words, optimizing for height, when there is one constraint, leads to one dimension of uncertainty.\nThis is a subtle point, so I will say it again. If you want to optimize for a quantity, and you don’t have a constraint, then you would always go to the globally best solution. The whole space of possibilities is open to you, but you don’t need them.\nBut if you have one constraint, then you have one unique solution for each possible setting of constraint. You are still not free, but at least now you have a puppet master.\nIn classical thermodynamics, only equilibrium states are “real”. Nonequilibrium states are “virtual”, much like a virtual path in Lagrangian mechanics. Sure, you could imagine that if you throw a rock upwards, it would execute a complex figure-8 motion before returning to ground again, but that’s a virtual path. The only real path is the unique virtual path that stationarizes the action integral.\nSimilarly, in classical thermodynamics, you could imagine that a tank of gas contains all its gas on the left side, but that’s a virtual state that does not satisfy the optimization constraint. Consequently, it is only a virtual state, not a real state. Only one virtual state is real – the unique virtual state that maximizes entropy under constraint.\n\n\nEquilibrium vs nonequilibrium entropy.\nFor example, consider this statement, an important consequence of the second law: &gt; A closed system maximizes its entropy under its constraint.\nIf we think of “equilibrium” states only, then the statement makes no sense. Under a fixed constraint, there is only one possible equilibrium state, so we get something sounding as ridiculous as “A customer can have a car painted in his favorite color as long as it’s black.”\nThe statement is actually saying this: &gt; A closed system under constraint has many satisfying nonequilibrium states, but only one equilibrium state: the one with the lowest nonequilibrium entropy. Consequently, for every constraint, there are many nonequilibrium states that satisfy the constraint, but only one equilibrium entropy, and so equilibrium entropy is a function of constraints, even though the nonequilibrium entropy is not."
  },
  {
    "objectID": "blog/posts/thermoeconomics/index.html#mechanical-and-other-forms-of-work",
    "href": "blog/posts/thermoeconomics/index.html#mechanical-and-other-forms-of-work",
    "title": "Yuxi on the Wired",
    "section": "mechanical and other forms of work",
    "text": "mechanical and other forms of work\nTo perform work, one must perform work upon something. In other words, there is no such thing as “system A performed work”. There is really “some energy and length is traded between A to B in compliance with a work-equation-constraint”."
  },
  {
    "objectID": "blog/posts/thermoeconomics/index.html#the-most-important-systems",
    "href": "blog/posts/thermoeconomics/index.html#the-most-important-systems",
    "title": "Yuxi on the Wired",
    "section": "the most important systems",
    "text": "the most important systems\n\nBaths\nA heat bath, or more accurately an energy bath, is a system that you can take or dump as much energy as you want, always at constant temperature.\nAn atmosphere, or more accurately an energy-and-volume bath, is a system that you can take or dump as much energy or volume as you want, always at constant temperature and pressure.\nIn general, a bath is an infinite source of a conserved quantity at constant marginal entropy.\nWe can imagine other forms of baths. For example, the surface of a lake could serve as an energy-and-area bath. A large block of salt can serve as a salt-chemical bath.\n\nphoton gas: $S = E V{1/4}E{3/4} $\nideal gas: \\(S \\propto \\ln (VE^{\\hat c_V})\\)\nideal spring: \\(S = 0\\) and \\(E = \\frac 12kx^2\\)\nbespoke energy storage: \\(S=0\\) and \\(E = f(x)\\), where \\(f\\) is whatever you want.\nideal battery: \\(S=0\\) and ???"
  },
  {
    "objectID": "blog/posts/thermoeconomics/index.html#heat-engine-systems",
    "href": "blog/posts/thermoeconomics/index.html#heat-engine-systems",
    "title": "Yuxi on the Wired",
    "section": "heat engine systems",
    "text": "heat engine systems\nA heat engine is a subsystem that is used as a component of a large system. The large system contains 4 parts: two energy baths, one heat engine, and one bespoke energy storage.\nIf you only want a heat engine that works, then the bespoke energy storage does not necessarily need to be so bespoke. However, if you want a heat engine that works reversibly, i.e. at maximal efficiency, then the energy storage must be designed to be exactly right. It must be designed to follow the exact parameters of the heat engine, as well as the two energy baths’ inverse temperatures. If any of those is ignored, the energy storage would fail to “mesh” with the rest of the compound system, and cause waste.\nThis is why we insist on calling it a heat engine system. Every part of it depends on every other part. The energy storage unit is just as important and precisely designed as the heat engine."
  },
  {
    "objectID": "blog/posts/thermoeconomics/index.html#common-errors",
    "href": "blog/posts/thermoeconomics/index.html#common-errors",
    "title": "Yuxi on the Wired",
    "section": "Common errors",
    "text": "Common errors\nHeat is not a noun, but a verb.\nEnergy is not special, but only one more conserved quantity.\nEntropy is special, because it is the one quantity that is maximized.\nClassical thermodynamics\n\nCarnot’s metaphor\n\n\nin a waterwheel\nin a heat engine\n\n\n\n\nwater\ncaloric\n\n\nheight\ntemperature\n\n\nmechanical work\nmechanical work"
  },
  {
    "objectID": "blog/posts/thermoeconomics/index.html#carathéodory-geometric-thermodynamics",
    "href": "blog/posts/thermoeconomics/index.html#carathéodory-geometric-thermodynamics",
    "title": "Yuxi on the Wired",
    "section": "Carathéodory: geometric thermodynamics",
    "text": "Carathéodory: geometric thermodynamics\n\nFrobenius theorem\nSuppose we are to find the trajectory of a particle in a subset of 3D space, but we do not know its trajectory formula. Instead, we know only that its trajectory satisfies \\(a d x+b d y+c d z=0\\), where \\(a, b, c\\) are smooth functions of \\((x, y, z)\\). Thus, our only certainty is that if at some moment in time the particle is at location \\(\\left(x_0, y_0, z_0\\right)\\), then its velocity at that moment is restricted within the plane with equation\n\\[\na\\left(x_0, y_0, z_0\\right)\\left[x-x_0\\right]+b\\left(x_0, y_0, z_0\\right)\\left[y-y_0\\right]+c\\left(x_0, y_0, z_0\\right)\\left[z-z_0\\right]=0\n\\]\nIn other words, we can draw a “local plane” at each point in 3D space, and we know that the particle’s trajectory must be tangent to the local plane at all times. If we have two equations\n\\[\n\\left\\{\\begin{array}{l}\na d x+b d y+c d z=0 \\\\\na^{\\prime} d x+b^{\\prime} d y+c^{\\prime} d z=0\n\\end{array}\\right.\n\\]\nthen we can draw two local planes at each point, and their intersection is generically a line, allowing us to uniquely solve for the curve starting at any point. In other words, with two 1 -forms, we can foliate the domain into curves.\nIf we have only one equation \\(a d x+b d y+c d z=0\\), then we might be able to foliate \\(\\mathbb{R}^3\\) into surfaces, in which case, we can be sure that a curve starting at a certain surface must be restricted to wander within that surface. If not, then a curve starting at any point might end up at any other point in \\(\\mathbb{R}^3\\).\n\n\n\nThe 1 -form \\(dz - ydx\\). on \\(\\R^3\\) maximally violates the assumption of Frobenius’ theorem. These planes appear to twist along the \\(y\\)-axis. It is not integrable, as can be verified by drawing an infinitesimal square in the \\(x\\)-\\(y\\) plane, and follow the path along the one-forms. The path would not return to the same z-coordinate after one circuit.\n\n\nOne can imagine starting with a cloud of little planes, and quilting them together to form a full surface. The main danger is that, if we quilt the little planes two at a time, we might go on a cycle and return to where we began, but shifted by a small amount. If this happens, then we would not get a 2-dimensional surface, but a 3-dimensional blob. An example is shown in the diagram on the right.\nIf the one-form is integrable, then loops exactly close upon themselves, and each surface would be 2-dimensional. Frobenius’ theorem states that this happens precisely when \\(\\omega \\wedge d \\omega=0\\) over all of the domain, where \\(\\omega:=a d x+b d y+c d z\\). The notation is defined in the article on one-forms.\nDuring his development of axiomatic thermodynamics, Carathéodory proved that if \\(\\omega\\) is an integrable one-form on an open subset of \\(\\mathbb{R}^n\\), then \\(\\omega=f d g\\) for some scalar functions \\(f, g\\) on the subset. This is usually called Carathéodory’s theorem in axiomatic thermodynamics. \\({ }^{[1][2]}\\) One can prove this intuitively by first constructing the little planes according to \\(\\omega\\), quilting them together into a foliation, then assigning each surface in the foliation with a scalar label. Now for each point \\(p\\), define \\(g(p)\\) to be the scalar label of the surface containing point \\(p\\).\nNow, \\(d g\\) is a one-form that has exactly the same planes as \\(\\omega\\). However, it has “even thickness” everywhere, while \\(\\omega\\) might have “uneven thickness”. This can be fixed by a scalar scaling by \\(f\\), giving \\(\\omega=f d g\\).\n\n\n\nFor each point \\(p\\), the one-form \\(\\omega(p)\\) is visualized as a stack of parallel planes. The planes are quilted together, but with “uneven thickness”. With a scaling at each point, \\(\\omega\\) would have “even thickness”, and become an exact differential.\n\n\nFor each point \\(p\\), the one-form \\(\\omega(p)\\) is visualized as a stack of parallel planes. The planes are quilted together, but with “uneven thickness”. With a scaling at each point, \\(\\omega\\) would have “even thickness”, and become an exact differential.\n\n\nThermodynamics\nConsider a thermodynamic system (concretely one can imagine a piston of gas) that can interact with the outside world by either heat conduction (such as setting the piston on fire) or mechanical work (pushing on the piston). He then defined “adiabatic process” as any process that the system may undergo without heat conduction, and defined a relation of “adiabatic accessibility” thus: if the system can go from state \\(\\mathrm{A}\\) to state \\(\\mathrm{B}\\) after an adiabatic process, then \\(B\\) is adiabatically accessible from \\(A\\). Write it as \\(A \\succeq B\\).\nNow assume that\n\nFor any pair of states \\(A, B\\), at least one of \\(A \\succeq B\\) and \\(B \\succeq A\\) holds.\nFor any state \\(A\\), and any neighborhood of \\(A\\), there exists a state \\(B\\) in the neighborhood, such that \\(B\\) is adiabatically inaccessible from \\(A\\).\n\nThen, we can foliate the state space into subsets of states that are mutually adiabatically accessible. With mild assumptions on the smoothness of \\(\\succeq\\), each subset is a manifold of codimension 1 . Call these manifolds “adiabatic surfaces”.\nBy the first law of thermodynamics, there exists a scalar function \\(U\\) (“internal energy”) on the state space, such that \\[\nd U=\\delta W+\\delta Q=\\sum_i X_i d x_i+\\delta Q\n\\] where \\(X_1 d x_1, \\ldots, X_n d x_n\\) are the possible ways to perform mechanical work on the system. For example, if the system is a tank of ideal gas, then \\(\\delta W=-p d V\\). Now, define the one-form on the state space \\[\n\\omega:=d U-\\sum_i X_i d x_i\n\\]\nNow, since the adiabatic surfaces are tangent to \\(\\omega\\) at every point in state space, \\(\\omega\\) is integrable, so by Carathéodory’s theorem, there exists two scalar functions \\(T, S\\) on state space, such that \\(\\omega=T d S\\). These are the temperature and entropy functions, up to a multiplicative constant.\nBy plugging in the ideal gas laws, and noting that Joule expansion is an (irreversible) adiabatic process, we can fix the sign of \\(d S\\), and find that \\(A \\succeq B\\) means \\(S(A) \\leq S(B)\\). That is, entropy is preserved in reversible adiabatic processes, and increases during irreversible adiabatic processes."
  },
  {
    "objectID": "blog/posts/thermoeconomics/index.html#chemical-thermodynamics",
    "href": "blog/posts/thermoeconomics/index.html#chemical-thermodynamics",
    "title": "Yuxi on the Wired",
    "section": "Chemical thermodynamics",
    "text": "Chemical thermodynamics\nStandard chemistry textbooks are utter nonsense when it comes to the condition for equilibrium. You usually see things like \\(\\Delta G = 0\\) for chemical reactions in open atmosphere, or \\(\\Delta H = 0\\) for chemical reactions in a sealed container in a heat bath. When I took the chemistry courses, I was terribly confused because I felt they did not make sense. I passed the exams, but only by learning to speak the right kind of nonsense. During my study of classical thermodynamics, I rederived everything for myself, and in the process finally made everything come out the right way.\nTo set the scene, consider a few problems in typical books on chemistry that does not make sense. For concreteness, consider a typical chemical demonstration, the dimerization reaction of\n\\[2 NO_2 \\to N_2 O_4\\]\nNow seal a certain amount of the gas inside a glass tube. Since \\(NO_2\\) is brown, but \\(N_2 O_4\\) is transparent, if we heat it up, the pressure would rise, forcing the reaction to go towards the \\(N_2O_4\\) side by Le Chatelier’s principle, and the glass tube would turn transparent. Putting the tube inside an ice bath would turn it brown again.\nThe equilibrium for reaction is typically described as follows.\nDefine the reaction quotient for this reaction as \\(Q = \\frac{[N_2 O_4]}{[NO_2]^2}\\). According to the standard textbook, it states that the reaction reaches equilibrium when \\(\\Delta H = 0\\), where\n\\[\n\\Delta H = \\Delta H^\\circ + RT \\ln Q\n\\]\nand \\(\\Delta H^\\circ\\) is the change in Helmholtz energy for the reaction at the standard state. This description is nonsense.\n\n\\(Q\\) is always used inside a logarithm, like \\(RT\\ln Q\\). Logarithms can never have a unit, even though here \\(Q\\) has units of molar density.\nThe reaction quotient’s unit depends on the reaction itself. How could this possibly be true? Imagine a physical quantity \\(X\\) that has units of meters in one problem, but units of meters\\(^{-3}\\) in another.\nWhy can’t I rewrite the equation as \\(4 NO_2 \\to 2 N_2 O_4\\)? This would instantly change \\(Q\\) to \\(Q' = Q^2\\). How could the physics of the situation depend on our convention for describing it?\nThe unit for \\(\\Delta H\\) is \\(J/mol\\), but the units for \\(H\\) is \\(J\\), so somehow there is an extra \\(mol\\) appearing out of nowhere.\n\nThe real answer is as follows:\nThe state of a chemical reaction system in a closed container depends on not just its temperature, but also its chemical composition. Thus, the standard state of a chemical reaction system must specify its temperature, and the concentration of all its molecules.\nConsider a chemical system \\[\n\\sum_i x_i X_i \\leftrightarrow \\sum_j y_i Y_j\n\\]\nWe must arbitrarily fix some standard concentrations \\(\\left[X_i\\right]^o,\\left[Y_j\\right]^o\\), then the reaction quotient with respect to these arbitrary conventions is\n\\[\n\\ln Q:=\\sum_j \\ln y_j \\frac{\\left[Y_j\\right]}{\\left[Y_j\\right]^o}-\\sum_i x_i \\ln \\frac{\\left[X_i\\right]}{\\left[X_i\\right]^o}\n\\]\nThen, if we immerse the system in an energy and volume bath (i.e. the open air, or maybe we throw it in a plastic bag and throw it under the sea), then\n\\[\n\\frac{d G}{d \\xi}=\\frac{d G^{\\circ}}{d \\xi}+R T \\ln Q\n\\]\nis independent of our choice of standard concentrations."
  },
  {
    "objectID": "blog/posts/thermoeconomics/index.html#classical-statistical-mechanics",
    "href": "blog/posts/thermoeconomics/index.html#classical-statistical-mechanics",
    "title": "Yuxi on the Wired",
    "section": "Classical statistical mechanics",
    "text": "Classical statistical mechanics"
  },
  {
    "objectID": "blog/posts/thermoeconomics/index.html#annotated-references",
    "href": "blog/posts/thermoeconomics/index.html#annotated-references",
    "title": "Yuxi on the Wired",
    "section": "Annotated references",
    "text": "Annotated references"
  },
  {
    "objectID": "blog/posts/principia-mathematicarum/index.html",
    "href": "blog/posts/principia-mathematicarum/index.html",
    "title": "Principia Philosophica Naturalium Mathematicarum",
    "section": "",
    "text": "# The world as theories and interpretations\nWhat is light? Newton said it was a particle1. Huygens said it was a wave. Schrödinger said it was both. Some clever fool said it was a wavicle. And Feynman said it was whatever helps you sleep at night shuts you up and lets you calculate.\nSo is light a particle, or a wave? None of these explanations satisfied me, so I figured it out for myself."
  },
  {
    "objectID": "blog/posts/principia-mathematicarum/index.html#the-wave-particle-duality",
    "href": "blog/posts/principia-mathematicarum/index.html#the-wave-particle-duality",
    "title": "Principia Philosophica Naturalium Mathematicarum",
    "section": "The wave-particle duality",
    "text": "The wave-particle duality\nThere are more than one wave-particle duality. In fact, there is nothing particularly two-ful or wave-ful or particle-ful about physics! Why are we so confused about wave-particle duality? I blame force of habit and evolution.\n\nA particle is a curve. That is, it is a function \\(\\gamma: (a, b) \\to \\mathcal M\\), where \\(a &lt; b\\) are real numbers, and \\(\\mathcal M\\) is a manifold.\nA field is a function \\(\\phi: \\mathcal M \\to Y\\), where \\(Y\\) is any manifold you like. We say the field is \\(Y\\)-valued.\nA scalar field is a real-valued field.\nA wave is a more confusing name for a field. Why \"confusing\"? Because the word \"wave\" makes people think the field must be \"going up and down\" here, there, or everywhere, but the fact is, a field can be exactly flat everywhere, and still be a wave! So why did physicists call it a \"wave\" when they really mean a field? Well, force of habit... back in the old days, the only field they knew of is the water-wave, which can be described mathematically as \\(h: \\R^2 \\to \\R\\), where \\(h(x)\\) is the height of water at location \\(x\\).\nA particle theory over a manifold \\(\\mathcal M\\) is a physical theory that states that certain paths in \\(\\mathcal M\\) are \"physical\" while others are \"unphysical\".\nA field theory over a manifold \\(\\mathcal M\\) is a physical theory that states that certain fields over \\(\\mathcal M\\) are \"physical\" while others are \"unphysical\".\nA wave theory is a field theory.\nA wave-particle duality over a manifold \\(\\mathcal M\\) is a tuple \\((T, T', f)\\). Here, \\(T\\) is a particle theory over \\(\\mathcal M\\), and \\(T'\\) is a field theory over \\(\\mathcal M\\), and \\(f\\) is an equivalence between \\(T, T'\\).\nA wave-equation is a differential equation satisfied by a field.\nAn equation of motion (of a particle) is a differential equation satisfied by a particle.\n\nIf our physical theory has a very special \"physical space\" \\(\\mathcal M\\), then a particle is a function \\(\\gamma: (a, b) \\to \\R\\times \\mathcal M\\), where \\(a &lt; b\\) are real numbers, and \\(\\R\\) represents time. In other words, a particle is nothing more and nothing less than a trajectory in spacetime. This is what we mean by a \"particle\" by default, even though sometimes we would deal with \"timeless particles\", for which time is meaningless, and a particle must instead be a function \\(\\gamma: (a, b) \\to \\mathcal M\\).\nTimeless particles? Why yes! That’s how we study geometric optics as the study of light-rays.\n\nIn geometric optics\n\n\nIn Hamiltonian mechanics\n\n\nIn quantum mechanics"
  },
  {
    "objectID": "blog/posts/principia-mathematicarum/index.html#crash-course-in-modern-theoretical-physics",
    "href": "blog/posts/principia-mathematicarum/index.html#crash-course-in-modern-theoretical-physics",
    "title": "Principia Philosophica Naturalium Mathematicarum",
    "section": "Crash course in modern theoretical physics",
    "text": "Crash course in modern theoretical physics\nRecall that a particle is a function \\(\\gamma: (a, b) \\to \\R\\times \\mathcal M\\). But what if we don’t use an interval \\((a, b)\\), but use a square, or a cylinder, or even a cube? This leads us to the idea of strings, branes, and other such fancy frontiers of theoretical physics.\n\nString theory\nA closed string is a function \\(\\mu: (a, b) \\times \\mathbb S^1 \\to \\mathcal M\\), where \\(a &lt; b\\) are real numbers, and \\(\\mathbb S^1\\) is the circle.\nAn open string is a function \\(\\mu: (a, b) \\times (0, 1)\\to \\mathcal M\\), where \\(a &lt; b\\) are real numbers.\nA string theory is made of two parts: one, the types of strings that it decides about; two, a way to decide if a string is physical or unphysical.\n\n\nHow to make your own variational physical theory\n\nFind a manifold \\(\\mathcal A\\).\nFind another manifold \\(\\mathcal B\\).\nDefine a family of functions of type \\(\\mu: \\mathcal A \\to \\mathcal B\\). This will be the domain of your physical theory.\nWrite down an action function \\(S(\\mu)\\).\nSpecify a way to \"vary \\(\\mu\\) infinitesimally\". Write that as \\(\\delta\\).\nDerive consequences of \\[\\delta S(\\mu) = 0.\\]\n\nFollowing the recipe, we immediately get Lagrangian mechanics and Hamiltonian mechanics.\nNow, we made a small sleight of hand in the recipe. Can you spot it? It is in steps 4 and 5. Namely, we have claimed that we can \"write down an action function\" and \"vary \\(\\mu\\) infinitesimally\". However, not every \\(\\mathcal A, \\mathcal B\\) has enough structure to allow us to do that. The art of doing theoretical physics is mostly in putting in enough structure in \\(\\mathcal A, \\mathcal B\\) so that you can define \\(S(\\mu)\\) and \\(\\delta S(\\mu)\\).\n\n\nHow to clothe your manifolds\n::: epigraph The world found nothing sacred in the abstract nakedness of being human.\nHannah Arendt, The origins of totalitarianism :::\nSince a mere manifold is not structured enough for defining actions and infinitesimal variations, we will \"clothe\" the manifolds with enough structures so that they do. To make this concrete, we will consider how we could construct Lagrangian mechanics and Hamiltonian mechanics according to the recipe.\nLagrangian\nHamiltonian\nAnd if you go deep into theoretical physics, you will eventually encounter Calabi–Yau manifolds, which are \"compact Kähler manifolds with a vanishing first Chern class and a Ricci-flat metric\". All these extra structures give them enough theoretical niceness for elegant string theories.\n\n\nExercise for the reader\nApply the recipe to your favorite manifolds, and get it published in a journal of physics with an impact factor of at least 2."
  },
  {
    "objectID": "blog/posts/principia-mathematicarum/index.html#interpretations-of-classical-mechanics",
    "href": "blog/posts/principia-mathematicarum/index.html#interpretations-of-classical-mechanics",
    "title": "Principia Philosophica Naturalium Mathematicarum",
    "section": "Interpretations of classical mechanics",
    "text": "Interpretations of classical mechanics\nFigure 3 shows six main interpretations of classical mechanics. They are all equivalent in some exact mathematical sense.\n\n\n\nSix main interpretations of classical mechanics.\n\n\n\nPosthuman classical mechanics\nWhat is it like to be a bat? What is it like to be a robot? The umwelt, seeing in infrared."
  },
  {
    "objectID": "blog/posts/principia-mathematicarum/index.html#interpretations-of-quantum-mechanics",
    "href": "blog/posts/principia-mathematicarum/index.html#interpretations-of-quantum-mechanics",
    "title": "Principia Philosophica Naturalium Mathematicarum",
    "section": "Interpretations of quantum mechanics",
    "text": "Interpretations of quantum mechanics"
  },
  {
    "objectID": "blog/posts/mathematical-phenomenology/index.html",
    "href": "blog/posts/mathematical-phenomenology/index.html",
    "title": "What does it feel like to be a mathematical object?",
    "section": "",
    "text": "TODO: change the folder name, and title, etc.\nStructuralism"
  },
  {
    "objectID": "blog/posts/equilibrium-thermoeconomics/index.html",
    "href": "blog/posts/equilibrium-thermoeconomics/index.html",
    "title": "Classical thermodynamics and economics",
    "section": "",
    "text": "Even if the capitalist system is to give way to one in which service and not profit shall be the object, there will still be an integral of anticipated utilities to be made a maximum. Since we must find a function which maximizes an integral we must in many cases use the Calculus of Variations. But the problem here transcends the questions of depreciation and useful life, and belongs to the dawning economic theory based on considerations of maximum and minimum which bears to the older theories the relations which the Hamiltonian dynamics and the thermodynamics of entropy bear to their predecessors.\n(Hotelling 1925)\n\n\nIt is to the economist, the statistician, the philosopher, and to the general reader that I commend the analysis contained herein… mathematics as applied to classical thermodynamics is beautiful: if you can’t see that, you were born color-blind and are to be pitied. Similarly, in all the branches of pure and applied mathematics, the subject of probability is undoubtedly one of the most fascinating.\nPaul Samuelson, forward to (Bickler and Samuelson 1974)\n\n\n\n\\(U\\) is the internal energy\n\\(\\beta = 1/T\\) is the inverse temperature.\n\\(X\\) means “other properties that we are not concerned with”. For example, with an ideal gas trapped in a copper box, its macroscopic state is determined by \\(U, N, V\\). If we want to focus on \\(U\\), then we can let \\(X = (N, V)\\), and write \\(S = S(U, X)\\). Similarly, for a photon gas in a blackbody chamber, its macroscopic state is determined by \\(U, V\\), since photons can be created and destroyed on the inner surface of the blackbody chamber. We can then write \\(S = S(U, X)\\).\nThermodynamics is notorious for having too many partial differentials.\n\\((\\partial_{x_1} f)_{x_1, x_2, \\dots, x_n}\\) means that we lay down a coordinate system defined by \\(x_1, \\dots, x_n\\), then calculate \\(\\partial_{x_1}f\\), fixing the other coordinates constant. In particular, \\((\\partial_{x_1} f)_{x_2, \\dots, x_n}\\) is likely different from \\((\\partial_{x_1} f)_{x_1, y_2, \\dots, y_n}\\)\nIf in doubt, write down\n\\[df = \\sum_{i=1}^n (\\partial_{x_i} f)_{x_1, \\dots, x_n} dx_i\\]\nand reason thenceforth.\n\n\n\n\n(Pippard 1964). Slim, elegant, both mathematical and applied. In the best British tradition of mathematics – think James Maxwell and G. H. Hardy.\n(Fermi 1956). The same as above. However, it also covers chemical thermodynamics.\n(Lemons 2019). A very readable introduction to classical thermodynamics, slim but deep. I finally understood the meaning of the three laws of thermodynamics after reading it. Contains copious historical quotations.\n(Lemons 2008). A textbook version of the author’s (Lemons 2019), weaving in history and philosophical contemplation at every turn.\n(Carnot, Clapeyron, and Clausius 1988). A reprint of the most important papers in thermodynamics published before 1900. Useful to have on hand if you are reading (Lemons 2019).\n(Buchdahl 1966). A textbook based on Carathéodory’s axiomatic thermodynamics. The notation is ponderous, and the payoff is unclear. I don’t know what is its intended audience – perhaps professional pedants and differential geometers? Nevertheless, if you need to do research in Carathéodory’s axiomatic thermodynamics, I think this is your best bet."
  },
  {
    "objectID": "blog/posts/equilibrium-thermoeconomics/index.html#introduction",
    "href": "blog/posts/equilibrium-thermoeconomics/index.html#introduction",
    "title": "Classical thermodynamics and economics",
    "section": "",
    "text": "Even if the capitalist system is to give way to one in which service and not profit shall be the object, there will still be an integral of anticipated utilities to be made a maximum. Since we must find a function which maximizes an integral we must in many cases use the Calculus of Variations. But the problem here transcends the questions of depreciation and useful life, and belongs to the dawning economic theory based on considerations of maximum and minimum which bears to the older theories the relations which the Hamiltonian dynamics and the thermodynamics of entropy bear to their predecessors.\n(Hotelling 1925)\n\n\nIt is to the economist, the statistician, the philosopher, and to the general reader that I commend the analysis contained herein… mathematics as applied to classical thermodynamics is beautiful: if you can’t see that, you were born color-blind and are to be pitied. Similarly, in all the branches of pure and applied mathematics, the subject of probability is undoubtedly one of the most fascinating.\nPaul Samuelson, forward to (Bickler and Samuelson 1974)\n\n\n\n\\(U\\) is the internal energy\n\\(\\beta = 1/T\\) is the inverse temperature.\n\\(X\\) means “other properties that we are not concerned with”. For example, with an ideal gas trapped in a copper box, its macroscopic state is determined by \\(U, N, V\\). If we want to focus on \\(U\\), then we can let \\(X = (N, V)\\), and write \\(S = S(U, X)\\). Similarly, for a photon gas in a blackbody chamber, its macroscopic state is determined by \\(U, V\\), since photons can be created and destroyed on the inner surface of the blackbody chamber. We can then write \\(S = S(U, X)\\).\nThermodynamics is notorious for having too many partial differentials.\n\\((\\partial_{x_1} f)_{x_1, x_2, \\dots, x_n}\\) means that we lay down a coordinate system defined by \\(x_1, \\dots, x_n\\), then calculate \\(\\partial_{x_1}f\\), fixing the other coordinates constant. In particular, \\((\\partial_{x_1} f)_{x_2, \\dots, x_n}\\) is likely different from \\((\\partial_{x_1} f)_{x_1, y_2, \\dots, y_n}\\)\nIf in doubt, write down\n\\[df = \\sum_{i=1}^n (\\partial_{x_i} f)_{x_1, \\dots, x_n} dx_i\\]\nand reason thenceforth.\n\n\n\n\n(Pippard 1964). Slim, elegant, both mathematical and applied. In the best British tradition of mathematics – think James Maxwell and G. H. Hardy.\n(Fermi 1956). The same as above. However, it also covers chemical thermodynamics.\n(Lemons 2019). A very readable introduction to classical thermodynamics, slim but deep. I finally understood the meaning of the three laws of thermodynamics after reading it. Contains copious historical quotations.\n(Lemons 2008). A textbook version of the author’s (Lemons 2019), weaving in history and philosophical contemplation at every turn.\n(Carnot, Clapeyron, and Clausius 1988). A reprint of the most important papers in thermodynamics published before 1900. Useful to have on hand if you are reading (Lemons 2019).\n(Buchdahl 1966). A textbook based on Carathéodory’s axiomatic thermodynamics. The notation is ponderous, and the payoff is unclear. I don’t know what is its intended audience – perhaps professional pedants and differential geometers? Nevertheless, if you need to do research in Carathéodory’s axiomatic thermodynamics, I think this is your best bet."
  },
  {
    "objectID": "blog/posts/equilibrium-thermoeconomics/index.html#what-is-thermodynamics",
    "href": "blog/posts/equilibrium-thermoeconomics/index.html#what-is-thermodynamics",
    "title": "Classical thermodynamics and economics",
    "section": "What is thermodynamics?",
    "text": "What is thermodynamics?\nBoth neoclassical economics and classical thermodynamics are about the equilibria of large systems. While a large system is generally hopelessly complicated, almost all the complexity falls away when the system is maximizing a single quantity. Ceaselessly striving to maximize entropy, a complex system sheds its complexity and reaches the pure simplicity of maximal entropy. Ceaselessly striving to maximize profit, a complex company sheds its complexity and reaches the pure simplicity of perfect product.\nIn both fields, everything we can say about the world are nothing more than about systems, constraints, contacts, and equilibria. Time and change do not exist. All we can explain is which states are in constrained equilibrium, not how a system can get there. Atoms do not exist. All we can explain is what happens to homogeneous substances in constrained equilibrium. People do not exist. All we can explain is what happens to constrained economic systems in equilibrium.\nDifferent things can be maximized: the total entropy, or the negative Gibbs free energy, or the profit, or the sum-total of utility, or something else. Through different lenses, different things are maximized, but they predict the same phenomena. Using this mathematical freedom, experts brachiate around the coordinate axes like gibbons brachiating around vines, looking for the perfect angle to solve each particular problem.\n\nConstrained maximization problems in economics and thermodynamics.\n\n\n\n\n\n\n\ninterpretation\nmaximized quantity\nconstraint\n\n\n\n\nconglomerate accounting\nbook value\nAssets is conserved, but can be moved between child companies.\n\n\nsocial welfare\nsocial utility\nWealth is conserved, but can be redistributed.\n\n\nclosed system\nentropy\nQuantities are conserved, but can be moved between sub-systems.\n\n\nfactory production\nprofit\nSome raw materials are on sale at a market, but others are not.\n\n\nconsumer choice\nutility\nSome finished goods are on sale at a market, but others are not. The market uses a commodity money.\n\n\npartially open system\nnegative free energy\nSome quantities can be exchanged with a bath, but others are conserved.\n\n\n\n\nSystems\nSystems are the main characters of the drama of thermodynamics. A thermodynamic system is something that is fully determined by a few macroscopic properties, related by equations of state. Once we know enough of its properties, we know all there is to know about such a system. There is nothing left to say about it.\nEverything is a thermodynamic system. However, there are two special types: bath systems, and mechanical systems.1 Any number of thermodynamic systems can be connected into a larger system – a compound system.\n1 Some children are confused when they heard that squares are rectangles too. I hope you won’t be equally confused when you heard that mechanical systems are also thermodynamical systems.The prototypical thermodynamic system is a tank of ideal gas whose number of particles is fixed. It has 2 degrees of freedom, so if we write down \\(n\\) different macroscopic properties, they would (generically) be related by \\(n-2\\) equations of state. So for example, if we write down the properties internal energy \\(U\\), temperature \\(T\\), volume \\(V\\), pressure \\(P\\), they would be related by the 2 equations of state\n\\[PV = k_BNT, \\quad U = \\frac 32 k_B NT\\]\nIf we know two out of the four of internal energy \\(U\\), temperature \\(T\\), volume \\(V\\), pressure \\(P\\), then we can solve for all the others by equations of state. The macroscopic properties fully describe the system, and nothing more can be said about it. We cannot ask more questions such as “Are there more particles on the left than on the right?” or “How long did it take for the system to reach equilibrium?”, because such questions are literally undefined in classical thermodynamics.\nBecause the properties are related by equations of state, we need only know a few of the properties in order to infer all the rest. For example, knowing the volume \\(V\\), internal energy \\(U\\), and particle number \\(N\\), of a tank of ideal gas, we can infer that its pressure is \\(P = \\frac{2U}{3V}\\), and its temperature is \\(T = PV/k_BN\\). Succinctly,\n\\[P = P(U, V, N), \\quad T = T(U, V, N)\\]\nmeaning “If we know \\(U, V, N\\), then we can calculate \\(P\\) and \\(T\\)”.\n\n\n\n\n\n\nWarning\n\n\n\nIt’s too easy to misread it as saying that \\(P\\) is a mathematical function of \\(U, V, N\\). It is not. It really is saying that, there exists a mathematical function \\(f_P\\), such that for any state \\(\\omega\\) of the system, we have\n\\[P(\\omega) = f_P(U(\\omega), V(\\omega), N(\\omega))\\]\n\n\nEverything about a thermodynamical system is known once we specify how its entropy is a function of its macroscopic properties. For example, the ideal gas is fully specified by\n\\[S(U, V, N) = k_B N \\ln\\left[\\frac{V}{N}\\,\\left(\\frac{U}{\\hat{c}_V k_B N}\\right)^{\\hat{c}_V}\\,\\frac{1}{\\Phi}\\right]\\]\nwhere \\(\\hat c_V\\) and \\(\\Phi\\) are constants that differ for each gas.\nAs another example, the photon gas is defined by\n\\[S(U, V) = C V^{1/4}U^{3/4}, \\quad C=\\left(\\frac{256\\pi^2 k_B^4}{1215 c^3 \\hbar^3}\\right)^{1/4}\\]\nThe fact that \\(S(U, V) \\propto V^{1/4}U^{3/4}\\) can be derived from 19th-century physics (and indeed, it was known to Boltzmann), but the constant \\(C\\) has to wait for proper quantum mechanics.\n\nBaths\nA bath system is a thermodynamic system with nonzero entropy function, but infinitely large size.\nFor example, if we take a copper piston and fill it with ideal gas, then immerse the piston in the bottom of an ocean, then it is a volume-and-energy bath with constant pressure-and-temperature. If we cover up the piston with some insulating material, then the ocean suddenly becomes merely a volume bath with constant pressure. If we use screws to fix the piston head, then the ocean suddenly becomes merely an energy bath with constant temperature.\nBecause they are infinitely large, if you connect two baths together, something bad will happen. For example, if you connect two energy baths together, but with different temperature, what would happen? The simple answer is: “A torrent of heat will flow from the hotter to the colder bath.” The more correct answer is: “Classical thermodynamics does not allow such a question to be asked. It would be like asking what happens when ‘an unstoppable force meets an immovable object’. If we literally have two baths, then we cannot connect them. If we only have two giant oceans that seem like baths when compared to this little tank of gas, then if the two oceans are connected to each other, they will no longer appear as baths to each other.”\nA heat bath, or more accurately an energy bath, is a system that you can take or dump as much energy as you want, always at constant marginal price of energy. An atmosphere, or more accurately an energy-and-volume bath, is a system that you can take or dump as much energy or volume as you want, always at constant temperature and pressure. In general, a bath is an infinite source of a conserved quantity at constant marginal entropy. We can imagine other forms of baths. For example, the surface of a lake could serve as an energy-and-area bath. A large block of salt can serve as a salt-chemical bath.\n\n\nMechanical systems\nA mechanical system is a thermodynamic system whose entropy is always zero. Essentially all systems studied in classical mechanics are such systems. In classical thermodynamics, they are not put center-stage, but if you know where to look, you will see them everywhere.\nAn ideal linear-spring has two macroscopic properties: length \\(x\\) and internal energy \\(U\\), with equation of state \\(U = \\frac 12kx^2\\). For example, a helix spring is close to an ideal linear-spring.\nAn ideal surface-spring is the same as an ideal linear-spring, but with area \\(A\\) instead of length \\(x\\). Its equation of state is \\(U = \\sigma A\\), where \\(\\sigma\\) is surface tension constant. For example, a balloon skin is close to an ideal surface-spring.\nAn ideal volume-spring is similar. It would resemble a lump of jelly. An ideal gas, though it looks like a volume-spring, is not an example, because its entropy is not zero. In particular, this means an ideal gas has temperature and can be “heated up”, but a lump of ideal jelly cannot.\nIn general, we can construct an arbitrary energy storage system, such that it has two macroscopic properties \\(x, U\\), satisfying \\(U = f(x)\\), where \\(f\\) is any differentiable function. We can imagine taking a mystery box with a chain we can pull on, and by some internal construction with gears, pulleys, weights, and springs, the force on the chain is \\(f'(x)\\), where \\(x\\) is the length by which we have pulled.\n\n\n\nConstraint\nA constraint is an equation of form \\(f(A, B, C, \\dots) = f_0\\), where \\(f\\) is a mathematical function, \\(A, B, C, \\dots\\) are macroscopic properties, and \\(f_0\\) is a constant.\nFor example, if we have two tanks of gas in thermal contact, then the constraint is as follows:\n\\[\n\\begin{cases}\nV_1 &= V_{1,0} \\\\\nV_2 &= V_{2,0} \\\\\nU_1 + U_2 &= U_{1,0} + U_{2,0} \\\\\n\\end{cases}\n\\]\nmeaning that the volume of each tank of gas is conserved, and the sum of their energy is also conserved.\nThe constraints on a compound system are determined by the contacts between its subsystems.\n\n\nContacts\nIf we only have systems isolated in a perfect vacuum, then nothing interesting will happen. If two systems are perfectly connected, then they will never be brought apart. A contact allows two systems to interact, without destroying their individuality. It allows two systems to communicate, without becoming literally one system. Economically, contacts are trade contracts.\nIn general, the effect of a contact is to reduce the number of constraints by one. For example, a metal rod between two pistons reduces the two constraints\n\\[V_1 = V_{1,0}, \\quad V_2 = V_{2, 0}\\]\ninto one constraint: \\[V_1 + V_2 = V_{1,0} + V_{2,0}\\]\nAs another example, in a tank of three kinds of gas \\(N_2, H_2, NH_3\\), allowing a single chemical reaction \\(N_2 + 3H_2 \\rightleftharpoons 2NH_3\\) reduces three constraints\n\\[\nN_{N_2} = N_{N_2, 0}, \\quad N_{H_2} = N_{H_2, 0}, \\quad N_{NH_3}= N_{NH_3, 0}\n\\]\ninto two:\n\\[\nN_{N_2} - N_{N_2, 0} = (N_{H_2} - N_{H_2, 0})/3, \\quad N_{N_2} - N_{N_2, 0} = (N_{NH_3} - N_{NH_3, 0})/2\n\\]\nA contact can be nonlinear. For example, if we have two pistons of the same area, connected by a lever, such that pushing on one piston by \\(\\Delta x\\) would be pulling on the other piston by \\(2 \\Delta x\\), then the constraint becomes \\(2(V_1 - V_{1,0}) + (V_2 - V_{2,0}) = 0\\). And by designing a series of levers, gears, and chains, we can realize any constraint function \\(f(V_1, V_2) = f(V_{1,0}, V_{2,0})\\) for any smooth function \\(f\\).\n\n\n\nA thermodynamical system (a piston of gas) is connected to a mechanical system (a mass in gravity) via an arbitrary constraint (variable gears).\n\n\n\n\nCompound systems\nA compound system is nothing more than several systems connected. If we know the connections, and the entropy function of each subsystem, then we know everything about the compound system. The number of DOF for the compound system is the sum of the DOF of the subsystems, minus the degree of constraints.\nFor example, in the adiabatic expansion of a tank of ideal gas, we are really studying one compound system made of:\n\na tank of ideal gas (thermodynamic system),\na lump of mass in gravity (mechanical system),\nwith a gear system between them (contact).\n\nThe gear system is designed with gear-ratio that varies as the system turns, in just the right way such that the system is always in equilibrium no matter the position of the piston, so that it really has no preference of going forwards or backwards.\n\n\n\nThe compound system. Inside it, there is a subsystem of a tank of ideal gas that undergoes adiabatic expansion.\n\n\n\n\n\n\n\n\nOnly one system\n\n\n\nWe typically think of the tank of ideal gas itself as part of the thermodynamics, and the other parts as “the environment”, but we should properly speaking consider one single compound system, of which the tank of ideal gas is merely a sub-system. This way, we can state directly that the entropy of the entire compound system is maximized.\n\n\nThe most important compound system is the heat-engine-and-environment-system.\nA heat engine is a thermodynamic system that is used as a component of a larger compound system. The large system contains 4 parts: two energy baths, one heat engine, and one carefully designed mechanical system acting as energy storage.\nIf you only want a heat engine that works, then the energy storage does not need to be carefully designed. However, if you want a Carnot heat engine, i.e. at maximal efficiency, then the energy storage must be designed to be exactly right. It must be designed to follow the exact parameters of the heat engine, as well as the two energy baths’ temperatures. If any of those is ignored, the energy storage would fail to “mesh” with the rest of the compound system, and cause waste.\nThis is why a heat engine must be a component of a larger compound system. Every part of it depends on every other part. The energy storage unit is just as important and precisely designed as the heat engine.\n\n\nEquilibrium, virtual vs actual states\n\nFreedom is an iron cage.\nConstraints set it free again.\n\nA little parable.\nOn the African savannah there lived a bunch of meerkats. Meerkats love to stand on the tallest place.\nAt first, they could stand wherever they wanted, so they all stood on one single hill. It was crowded. A blind lion who had memorized the landscape came and ate all of them.\nThen humans came and added long walls that divided the savannah into thin stripes. Now each meerkat’s location is determined by which stripe it happened to fall in. The blind lion could find the meerkat if he knew the stripe location. In other words, optimizing for height, when there is one constraint, leads to one dimension of uncertainty.\nThis is a subtle point, so I will say it again. If you want to optimize for a quantity, and you don’t have a constraint, then you would always go to the globally best solution. The whole space of possibilities is open to you, but you don’t need them. Those states are “virtual” because they are never observed, even though their existence is inferrable.\nBut if you have one constraint, then you have one unique solution for each possible setting of constraint. You are still not free, but at least now you have a puppet master.\nIn classical thermodynamics, only equilibrium states are “real”. Nonequilibrium states are “virtual”. In Lagrangian mechanics, only stationary-action paths are real, and the other paths are virtual. You can imagine that if you throw a rock upwards, it would execute a complex figure-8 motion before returning to ground again, but that’s a virtual path. The only real path is the unique virtual path that stationarizes the action integral.\nSimilarly, in classical thermodynamics, you could imagine that a tank of gas contains all its gas on the left side. Its entropy is just \\(S(U, V/2, N)\\), but that’s a virtual state that does not satisfy maximize entropy under constraint. The unique entropy-maximizing virtual state is the real state.\nFor every constraint, there are many nonequilibrium states that satisfy the constraint, but only one equilibrium entropy, and so equilibrium entropy is a function of constraints, even though the entropy function itself is not. It optimizes all its complexities away, allowing us to know it through just its external constraints.\n\n\nSome common misconceptions\nThermodynamics is not statistical mechanics. Forget about molecules and atoms. Forget about statistics and statistical mechanics. Forget about \\(S = k_B\\ln \\Omega\\) or \\(S = -\\sum_i p_i \\ln p_i\\). Randomness does not exist in classical thermodynamics.\nForget about the first law of thermodynamics. Energy is nothing special. The conservation of energy is no more important than the conservation of volume, or the conservation of charge.\nForget about heat. Heat does not exist – it is not a noun, not even an adjective, but an adverb at most. The theory of caloric has already been disproven in the 19th century by the cannon-boring experiment.\nHeat energy and work energy are both misnomers. Neither are types of energy. Instead, they are types of energy-flow. We should speak of only heat energy-flow and work energy-flow.\nTo perform work, one must perform work upon something. In other words, there is no such thing as “system A performed work”. There is really “some energy and length is traded between A to B in compliance with a work-equation-constraint”.\nForget about time. Time does not exist in classical thermodynamics. We can say nothing at all about what happens between equilibria. We can only say, “This is an equilibrium, but that is not an equilibrium. There is nothing at all to say about what happens between them.\nThe name “thermodynamics” is a complete misnomer, because heat does not exist (thus no “thermo-”) and time does not exist (thus no “-dynamics). It should be called”entropo-statics”.\nIf time does not exist, you ask, what do we mean when we study Joule expansion? That is, when we take half a tank of gas, and suddenly open the middle wall and wait until the gas equilibrates in the entire gas?\nIn fact, we did not open the middle wall. We did not wait. Gas did not expand. The past did not cause the future. Time is an stubbornly persistent illusion, and causality is illusory too.\nIn fact, we are considering two different problems in thermodynamics.\nFirst problem: Given a tank of gas with internal energy \\(U = U_0\\), molar-amount \\(n = n_0\\), and constraint \\(V \\leq \\frac 12 V_0\\). What is its equilibrium state? Answer: The state that maximizes entropy under the constraints: \\[\n\\begin{cases}\n\\max S(U, n, V) \\\\\nV \\leq \\frac 12 V_0 \\\\\nU = U_0 \\\\\nn = n_0\n\\end{cases}\n\\]\nwhere \\(S(U, n, V)\\) is the entropy of the gas when its states properties are \\(U, n, V\\).\nSimilarly, the second problem is another constraint-optimization problem: \\[\n\\begin{cases}\n\\max S(U, n, V) \\\\\nV \\leq V_0 \\\\\nU = U_0 \\\\\nn = n_0\n\\end{cases}\n\\]\nThat the two different problems seem to “follow one from another” is an illusion. In reality, they only appear to follow one another because this is what we observe in the real world, where one equilibrium follows another. In equilibrium thermodynamics, equilibria do not follow one another – everyone stands alone.\nTime only appears in the following sense: We observe a real-world system, like a car engine, and notice that its motion seems to consist of a sequence of equilibria. Keeping those almost-equilibria in mind, we delve into the ocean of classical thermodynamics, until we have found some equilibria that resemble the almost-equilibria we have in mind. And so, from the bottom of the ocean, we pick up one equilibrium, then another, then another. Then we string together these little equilibria along a number line like a pearl necklace. We run our fingers over these pearls, and delight in its “motion”, like flipping the pages of a stop-motion book and shouting, “Look, it is moving!”."
  },
  {
    "objectID": "blog/posts/equilibrium-thermoeconomics/index.html#the-three-laws-of-classical-thermodynamics",
    "href": "blog/posts/equilibrium-thermoeconomics/index.html#the-three-laws-of-classical-thermodynamics",
    "title": "Classical thermodynamics and economics",
    "section": "The three laws of classical thermodynamics",
    "text": "The three laws of classical thermodynamics\n\nSecond law\n\nFor the equilibrium of any isolated system it is necessary and sufficient that in all possible variations of the state of the systems which do not alter its energy, the variation of its entropy shall either vanish or be negative.\n(Gibbs 1878)\n\nThe second law of thermodynamics is all important: maximizing entropy is all of classical thermodynamics. All other parts are just tricks for maximizing entropy.\n\n\nFirst law\n\nAs a student, I read with advantage a small book by F. Wald entitled “The Mistress of the World and her Shadow”. These meant energy and entropy. In the course of advancing knowledge the two seem to me to have exchanged places. In the huge manufactory of natural processes, the principle of entropy occupies the position of manager, for it dictates the manner and method of the whole business, whilst the principle of energy merely does the book- keeping, balancing credits and debits.\n(Emden 1938)\n\nThe first law of thermodynamics is entirely trivial. Energy is nothing special!\nEnergy is just a conserved quantity, much like volume, mass, and many other things… In particular, the conservation of energy is not nearly as important as it sounds like. It does not deserve the title of “the first law of thermodynamics”. You might as well say “conservation of mass” is “the second-first law of thermodynamics” and “conservation of volume” is “the third-first law of thermodynamics”, and “conservation of electrons” and “conservation of protons” and “conservation of length” (if you are studying a thermodynamic system restricted to move on a line) and “conservation of area” (if you are studying a thermodynamic system restricted on the surface of a lake) and so on…\nThis sounds extraordinary, but that is merely how classical thermodynamics works. The first law of thermodynamics does not deserve its title. It should be demoted to an experimental fact, not a law.\nThe proper place for the law of conservation of energy is not classical thermodynamics, but general physics, because energy is nothing special inside classical thermodynamics, but it is extremely special if we zoom out to consider the whole of physics. Whereas in classical thermodynamics, systems conserve energy, and volume, and mass, and electron-number, and proton-number, and… when you move outside of thermodynamics, such as when you add in electrodynamics, special relativity, and quantum mechanics, all kinds of conservations breakdown. You don’t have conservation of mass, or number of electrons, or even volume, but energy is always conserved.\n\n\nZeroth law\nNow that the first law has been dispelled, we can dispel the zeroth law of thermodynamics, too. If Energy falls from grace, so must its shadow, Temperature.\n\nTheorem 1 (general zeroth law) If \\(S_1(X_1, Y) + S_(X_2, Y)\\) is maximized under constraint \\(X_1 + X_2 = X\\), then \\((\\partial_X S_1)_Y = (\\partial_X S_2)_Y\\).\n\n\nThe zeroth law of thermodynamics in various guises.\n\n\n\n\n\n\n\nmaximized quantity \\(S\\)\nconserved quantity \\(X\\)\nderivative \\((\\partial_X S)_Y\\)\n\n\n\n\nentropy\nenergy\ninverse temperature \\(\\beta\\)\n\n\nentropy\nvolume\n\\(\\beta P\\)\n\n\nentropy\nparticles\n\\(-\\beta \\mu\\), where \\(\\mu\\) is chemical potential\n\n\nentropy\nsurface area\n\\(-\\beta\\sigma\\), where \\(\\sigma\\) is surface tension\n\n\nproduction value\nraw material\nmarginal value\n\n\n\n\n\n3th law\nThis is not a law in classical thermodynamics.\n\n\nEconomic interpretation\nTODO\nEvery conserved quantity is a commodity. The company has some commodity. Commodities themselves have no intrinsic value. Instead, the company is valued by a certain accounting agency. The CEO’s job is to move around the commodities so that the accounting agency gives it the highest value on the book.\nA compound system is a conglomerate: a giant company made of little companies.\nA subsystem has an inverse temperature \\(\\beta = 1/T\\), which equals\n\\[\\beta = \\frac{d(\\text{value of a sub-company})}{d(\\text{energy owned by the sub-company})}\\]\nIn other words, the marginal value of energy. Electricity price!\nIf a subsystem has variable volume then it has a pressure \\(P\\), which satisfies\n\\[\\beta P= \\frac{d(\\text{value of a sub-company})}{d(\\text{volume owned by the sub-company})}\\]\nIn other words, the marginal value of space. Real estate price!\nYes, I know it sounds weird to say (Pressure/Temperature), but that’s just how the math works out. It turns out that (Pressure/Temperature) is more fundamental than Pressure… Pressure, indeed, is actually Real estate price / Electricity price. That’s why it has units of ($/m^3)/($/Joule) = Joule/m^3!\nWhy, then, do we speak of pressure \\(P\\) and temperature \\(T\\), instead of \\(\\beta\\) and \\(\\beta P\\)? I blame habit and the general inability to visualize classical entropy."
  },
  {
    "objectID": "blog/posts/equilibrium-thermoeconomics/index.html#basic-theorems-derived",
    "href": "blog/posts/equilibrium-thermoeconomics/index.html#basic-theorems-derived",
    "title": "Classical thermodynamics and economics",
    "section": "basic theorems derived",
    "text": "basic theorems derived\n\nthermodynamic potentials/forces\nIn general: \\[\n\\text{thermodynamic potential of $X$} = \\left(\\frac{\\partial S}{\\partial X}\\right) _{\\text{non-}X}\n\\]\nFor example, for a tank of gas, the macroscopic properties are \\(U, V, N\\), and so it has three thermodynamic forces:\n$\\(\\beta = (\\partial_U S)_{V, N}\\)\n$\\(\\beta P = (\\partial_V S)_{U, N}\\)\n$\\(-\\beta \\mu = (\\partial_N S)_{U, V}\\)\nUnfortunately, as you might have noticed, the conventional notations are a mess. The first one, \\(\\beta\\), is the inverse of temperature. This one is at least reasonable. The second one, \\(\\beta P\\), is \\(\\beta\\) multiplied by pressure. The third one is truly the most annoying, as it not only involves \\(\\beta\\), but also a negative sign. To see how the notation came about, we can write it out in differential form: \\[dS = \\beta dU + \\beta P dV - \\beta \\mu dN\\]\nConventional notations are energy-centric, so we rewrite it to single out \\(dU\\) : \\[\ndU = TdS + (- PdV) + \\mu dN\n\\]\nNow we see how the notation came about: \\(TdS\\) is the heat energy-flow into the system, \\(-PdV\\) is the mechanical work energy-flow into the system, and \\(\\mu dN\\) is the chemical energy-flow into the system.\nIn conventional notation, the \\(-PdV\\) is the annoying term with the negative sign. In entropy-centric notation, it is \\(\\mu dN\\) that becomes annoying.\nIf I could truly reform notation, I would redefine \\(\\beta P\\) as \\(p_V\\), meaning “the price of volume”, and redefine \\(-\\beta \\mu\\) as \\(p_N\\), meaning “the price of particle”.\n\nExample 1 (photon gas with \\(\\mu = 0\\)) Suppose we have a piston chamber, with its inner surface covered with silver, and there is a tiny speck of blackbody inside it, then the chamber would be filled with bouncing photons, in the form of a “photon gas”. The photons would reflect off the surface of the chamber, some absorbed and some emitted in turn, by the blackbody.\nAs usual for gas, the state of the system is determined by its internal energy, volume, and particle number: \\(U, V, N\\), with\n\\[dS = \\beta dU + \\beta PdV - \\beta \\mu dN\\]\nHowever, the photon gas is quite special, in that photons can be created and destroyed by the speck of blackbody, so at equilibrium, we must have \\(\\beta\\mu = 0\\), for otherwise, the system would be able to increase in entropy simply by creating/destroying more photons, and thus it is not in equilibrium.\nThis contrasts with the typical case with chemical gases like oxygen, where the particle number in a reaction chamber is constant, allowing \\(\\mu \\neq 0\\) even at equilibrium.\n\n\n\nHelmholtz free entropy\nWe have a thermodynamic system, in thermal equilibrium with an energy bath with price \\(\\beta\\), and arbitrary contact with another thermodynamic system of arbitrary design.\n\nDefinition 1 (Helmholtz free entropy) The Helmholtz free entropy is the convex dual of entropy with respect to energy:\n\\[\nf(\\beta, X) = \\max_U [S(U, X) - \\beta U]\n\\tag{1}\\]\nwhere \\(U\\) is its internal energy, and \\(X\\) are some other macroscopic properties.\nOf historical importance is the Helmholtz free energy \\(F := - T f\\), or equivalently,\n\\[\nF = \\min_U [U - TS(U, X)]\n\\tag{2}\\]\n\n\nTheorem 2 The system equilibrates at \\(U^*(\\beta, X) = \\argmax_U [S(U, X) - \\beta U]\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose the system starts out at \\(\\beta, U_0, X\\). Then the equilibrium condition is\n\\[\n\\begin{cases}\n\\max (S_{bath} + S) \\\\\nU_{bath} + U = U_{bath, 0} + U_{0}\n\\end{cases}\n\\]\nThe entropy of the bath is\n\\[S_{bath} = S_{bath, 0} + \\beta Q\\]\nwhere \\(Q = U_{bath} - U_{bath, 0}\\) is the amount of energy received by the bath as heat.\nPlugging this back, the equilibrium condition simplifies to\n\\[\n\\max_U [\\beta (U_0 - U) + S(\\beta, U, X)]\n\\]\nwhich is the desired result.\n\n\n\nMore generally, if the system has constraint on the state by \\(C(X) = 0\\), then the system equilibrates at \\[\n\\begin{cases}\n\\max_{U, X} [S(U, X) - \\beta U] = \\max_{X} f(\\beta, X)\\\\\nC(X) = 0\n\\end{cases}\n\\]\nThat is, the system always equilibrates at the maximal Helmholtz free entropy state that satisfies the constraint.\nMultiplying the above equation by \\(-T\\), we find that the system always equilibrates at the minimal Helmholtz free entropy state that satisfies the constraint.\n:::\nConsider the following method of extracting non-thermodynamic energy: Connect the system to an energy bath at energy price \\(\\beta\\), and a non-thermodynamic system of arbitrary design. The system starts at \\(\\beta, U_0, X_0\\) and ends at \\(\\beta, U_1, X_1\\). We have\n\\[\nW\\leq F(\\beta, U_0, X_0) - F(\\beta, U_1, X_1)\n\\]\nwhere \\(W\\) is “work”, that is, the increase in internal energy of the non-thermodynamic system. This is an equality when the process is reversible.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe have, by conservation of energy, and the second law, \\[\n\\begin{cases}\nU_1 = U_0 - (W + Q)  \\\\\n\\beta Q + S(U_1, X_1) \\geq S(U_0, X_0)\n\\end{cases}\n\\]\nwhich simplifies to the result.\n\n\n\nIf the process is reversible, then the entropy before and after must be equal, giving us\n\\[\n\\begin{cases}\nU_1 = U_0 - (W + Q)  \\\\\n\\beta Q + S(U_1, X_1) = S(U_0, X_0)\n\\end{cases}\n\\]\nwhich simplifies to the result.\n\\[\\beta = (\\partial_U S)_X(U^*(\\beta, X), X)\\]\nif \\(S\\) is differentiable and strictly convex at that point.\n\n\n\n\n\n\nProof\n\n\n\nDifferentiate \\(f\\).\n\n\n\\[df =  (\\partial_X S)_U|_{U=U^*(\\beta, X)} dX - U^*(\\beta, X) d\\beta\\]\nif \\(S\\) is differentiable and strictly convex at that point.\n\n\n\n\n\n\nProof\n\n\n\nUse Hotelling’s lemma (see the essay on Analytical Mechanics).\n\n\n\nExample 2 (First-order phase transition) What happens if \\(S\\) is not differentiable and strictly convex? In this case, we do not have \\(\\beta = (\\partial_U S)_X\\). We have two possibilities.\nThe first possibility is pictured as follows. There is a kink in the curve of \\(S(U, X)\\). At that point of critical internal energy \\(U_c\\), there is an entire interval of possible \\(\\beta\\). What we would notice is that at that critical internal energy and critical entropy, the system can be in equilibrium with any heat bath with any temperature between \\([T_{c, min}, T_{c, max}]\\). As far as I know, such systems do not exist, as all physically real systems have a unique temperature at all possible states.\nThe second possibility is pictured as follows. There is a bump in the curve, such that we can draw a double tangent over the bump, with slope \\(\\beta_c\\). At that critical inverse temperature, the system can be either at the lower tangent point, or the upper tangent point. It cannot be anywhere in-between, because as we saw, such points do not minimize \\(f(\\beta, X)\\), and thus are unstable.\nFor example, if we confine some liquid water in a vacuum chamber, and bathe it in a cold bath, then at its critical \\(\\beta_c\\), it would split into two parts, one part is all ice, and the other part is all water, mixed in just the right proportion to give it the correct amount of total internal energy. As it loses internal energy, the ice part grows larger, until it is all ice, at which point the system has finally gotten over the bump, and could cool down further.\nAt the critical point, \\((\\partial_{\\beta}f)_X\\) abruptly changes. So if we plot \\(\\beta \\mapsto f(\\beta, X)\\), the curve would kink there.\n\n\nTheorem 3 (Maxwell equal area rule) In a first-order phase transition at a fixed temperature and varying pressure/volume, the \\(P, V\\) diagram has a horizontal line going from \\((P_c, V_1)\\) to \\((P_c, V_2)\\), such that\n\\[\\int PdV = P_c(V_2-V_1)\\]\n\n\n\nMaxwell’s equal area rule states that the area of the regions labelled I and II are equal.\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFix the system’s internal energy \\(U\\), and its temperature \\(T\\), and plot the \\(V, S\\) curve.\nWhen pressure is at a critical value \\(P_c\\), the line of slope \\(\\beta P_c\\) is tangent to the \\(V \\mapsto S(U, V)\\) curve at two different points, with volumes \\(V_1, V_2\\). This is that first-order phase transition.\nNow, move the system state from the first point to the second. During the process, \\[\\int PdV = \\int (TdS -dU)  = \\int TdS = T \\Delta S = T \\beta P_c(V_2 - V_1) = P_c(V_2-V_1)\\]\n\n\n\n\n\n\n\n\n\nTODO\n\nOther free energies\nConsider a thermodynamic system whose entropy function is \\(S(U, V, X)\\), where \\(U\\) is the internal energy and \\(V\\) is the volume. Its Gibbs free entropy is\n\\[\ng(\\beta, \\beta P, X) = \\max_{U, V} (S(U, V, X) - \\beta U - (\\beta P)V)\n\\]\nIn other words, it’s the convex dual of entropy with respect to energy and volume. Similarly, its Gibbs free energy is \\(G = -g/\\beta\\).\nSimilarly, if the thermodynamic system has an entropy function of form \\(S(U, V, N_1, \\dots, N_m, X)\\), where \\(N_1, \\dots, N_m\\) are the particle number of different chemical species, then its Landau free entropy is\n\\[\n\\omega(\\beta, \\beta P, -\\beta\\mu_1, \\dots, -\\beta\\mu_n) = \\max_{U, V, N_1, \\dots, N_n} \\left(S(U, V, N_1, \\dots, N_n, X) - \\beta U - (\\beta P)V - \\sum_{i=1}^n (-\\beta \\mu_i) N_i \\right)\n\\]\nIn other words, it’s the convex dual of entropy with respect to energy, volume, and particle numbers.\nWe can prove many analogous theorems in the same way, such as the following theorem on Gibbs free entropy:\n\nTheorem 4 Let a thermodynamic system be in equilibrium with an energy-and-volume bath of prices \\(\\beta, \\beta P\\). If the system has constraint on the state by \\(C(X) = 0\\), then the system equilibrates at\n\\[\n\\begin{cases}\n\\max_{X} g(\\beta, \\beta P, X)\\\\\nC(X) = 0\n\\end{cases}\n\\]\nAnd if \\(S\\) is strictly convex and differentiable at that point, then \\(dg =  (\\partial_X S)_{U, V} dX - Ud\\beta - V d(\\beta P)\\).\n\n\nExercise 1 Formulate and prove the analogous theorems for Landau free entropy.\n\n\nExercise 2 If we consider a compound system inside an adiathermal piston under an atmosphere, then we can consider the following form of free energy: \\(\\tilde s(U, \\beta P, X) := \\max_{V} (S(U, V, X) - \\beta U - (\\beta P)V)\\). Formulate and prove the analogous theorems for this free entropy.\n\nConsider the following method of extracting non-thermodynamic energy: Connect the system to an energy-and-volume bath at prices \\(\\beta, \\beta P\\), and a non-thermodynamic system of arbitrary design. The system starts at \\(\\beta, \\beta P, X_0\\) and ends at \\(\\beta, \\beta P, X_1\\). We have\n\\[\nW\\leq G(\\beta, \\beta P, X_0) - G(\\beta, \\beta P, X_1)\n\\]\nwhere \\(W\\) is “work”, that is, the increase in internal energy of the non-thermodynamic system. If the process is reversible, then equality holds.\nTake-home lessons:\n\nFree entropy is maximized when the system equilibrates with a bath. Free energy is minimized.\nChange in free energy is the maximal amount of work extractable when the system equilibrates with both a bath and a non-thermodynamic system. This work is actually extracted when the process is reversible.\n\n\n\nMaxwell relations\nSince \\(dS = \\beta dU + \\beta P dV\\), we have the first Maxwell relation\n\\[\n\\partial_U \\partial_V S = (\\partial_V \\beta)_U = (\\partial_U(\\beta P) )_V\n\\tag{3}\\]\nIn economic language, it states\n\\[\n\\partial_{q_i}\\partial_{q_j} S = (\\partial_{q_i} p_j)_{q_j} = (\\partial_{q_j} p_i)_{q_i}\n\\tag{4}\\]\nwhere \\(q_i\\) is the quantity of commodity \\(i\\), and \\(p_i\\) is its marginal utility. In economics, we usually prefer writing demanded quantity as a function of marginal price as\n\\[(\\partial_{p_j}q_i)_{q_j} = (\\partial_{p_i}q_j)_{q_i}\\]\nThis is a symmetry of cross-price elasticity of demand.3\n3 Samuelson used the Maxwell relations, and other relations, to justify neoclassical economics. His idea is that, while utility functions are unobservable, and we do not have a scientific instrument to measure “economic equilibrium”, we can make falsifiable predictions from assuming that the economy is in equilibrium – such as the symmetry of the cross-price elasticity of demand.For example, if \\(i, j\\) are noodles and bread, then \\((\\partial_{q_i} p_j)_{q_j}\\) is how much the marginal price of bread would rise if I have a little more noodle. As noodles and bread are substitutional goods, we expect the number to be negative, meaning that having more noodles, I would price bread less. The Maxwell relation then tells us that it is exactly the same in the other direction: If I am given a little more bread, I would price noodles less. Not only that, I would want less by exactly the same amount.\nThe second relation is a bit hard to explain, since enthalpy really does not have a good representation in entropy-centric thermodynamics. However, it turns out to be just the third relation with \\(i, j\\) switched.\nSince \\(df = \\beta P dV - Ud\\beta\\), we have the third Maxwell relation\n\\[\n\\partial_\\beta\\partial_V f = -(\\partial_V U)_\\beta = +(\\partial_\\beta(\\beta P))_V\n\\tag{5}\\]\nIn economic language, we have\n\\[\n\\partial_{p_i}\\partial_{q_j} \\left[\\max_{q_i}(S(q) - p_i q_i )\\right]= -(\\partial_{q_j} q_i)_{p_i} = (\\partial_{p_i}p_j)_{q_j}\n\\tag{6}\\]\nContinuing from the previous example, if \\(i, j\\) are noodles and bread, and we open a shop with an infinite amount of noodles always at the exact price, then I would of course buy and sell from the noodle shop until my marginal price of noodles is equal to the shop’s price. Now, \\((\\partial_{q_j} q_i)_{p_i}\\) is how much noodles I would buy if I am given a marginal unit of bread. As noodles and bread are substitutional goods, this number is negative. This then means \\((\\partial_{p_i}p_j)_{q_j} &gt; 0\\), meaning that if the noodle price suddenly increases a bit, then I would sell a bit of noodles until I have reached equilibrium again. At that equilibrium, since I have less noodles, I would price higher its substitutional good, bread, by an equal amount as the previous scenario.\nSince \\(dg = -Ud\\beta - Vd(\\beta P)\\), we have the fourth Maxwell relation\n\\[\n-\\partial_{\\beta}\\partial_{\\beta P}g = (\\partial_{\\beta P}U)_\\beta = (\\partial_\\beta V)_{\\beta P}\n\\tag{7}\\]\nIn economic language, we have\n\\[\n\\partial_{p_i}\\partial_{p_j} \\left[\\max_{q_i, q_j}(S(q) - p_i q_i - p_j q_j)\\right]= -(\\partial_{p_j} q_i)_{p_i}= -(\\partial_{p_i} q_j)_{p_j}\n\\tag{8}\\]\nThis is another symmetry of cross-price elasticity of demand.\nContinuing from the previous example, if \\(i, j\\) are noodles and bread, and we open a shop with an infinite amount of noodles and bread, then I would of course buy and sell from the shop until my marginal prices of noodles and bread are equal to the shop’s prices. Now, if the shop suddenly raises the price of bread by a small amount, I would sell off some bread until my marginal price for bread increases to the shop’s new price. Now my marginal price for noodles increases too by substitutional effect, so I buy some noodles. Thus \\((\\partial_{p_j} q_i)_{p_i} &gt; 0\\). Switching the scenario, we find that raising the price of noodles would make me buy bread, by an equal amount as the previous scenario.\n\n\n\n\n\n\nAlternate proof of the Maxwell relations\n\n\n\n\n\nWe use the notation of economics here.\nSuppose we have commodities \\(1, 2, \\dots, n\\). We pick two commodities \\(i, j\\), and fix all other commodity quantities. Thus, we can write \\(dS = p_i dq_i + p_j d q_j\\).\nSince knowing \\(n\\) properties of the thermodynamic system allows us to know its exact state, and we have already fixed \\(n-2\\) properties of it, there only remain two more degrees of freedom. We can parameterize this by \\((q_i, q_j)\\), or \\((p_i, q_j)\\), or \\((p_i, p_j)\\), or any other reasonable coordinate system.\nIf the thermodynamic system undergoes a cycle, then\n\\[0 = \\oint dS = \\oint p_i dq_i + \\oint p_j dq_j\\]\nand thus, if we take the cycle infinitesimally small, we find that \\(dp_i\\wedge dq_i = -dp_j \\wedge dq_j\\). That is, the map \\((p_i, q_i) \\mapsto (p_j, q_j)\\) preserves areas, but reverses orientation. In particular, we have a Jacobian \\[\n\\frac{\\partial(p_i, q_i)}{\\partial(p_j, q_j)} = -1\n\\]\nNow, let \\((x, y)\\) be an arbitrary coordinate transform. By the chain rule for Jacobians, \\[\n\\frac{\\partial(p_i, q_i)}{\\partial(x, y)} = \\frac{\\partial(p_i, q_i)}{\\partial(p_j, q_j)} \\frac{\\partial(p_j, q_j)}{\\partial(x,y)}  = -\\frac{\\partial(p_j, q_j)}{\\partial(x,y)}\n\\]\nThis allows us to derive all the Maxwell relations. For example, setting \\((x, y) = (p_i, q_j)\\) gives us the third relation\n\\[(\\partial_{q_j} q_i)_{p_i} = -(\\partial_{p_i}p_j)_{q_j}\\]\n\n\n\n\n\n\n\n\n\nDeriving the four Maxwell relations by picking the right variables for ."
  },
  {
    "objectID": "blog/posts/equilibrium-thermoeconomics/index.html#which-simplifies-to-the-result.",
    "href": "blog/posts/equilibrium-thermoeconomics/index.html#which-simplifies-to-the-result.",
    "title": "Classical thermodynamics and economics",
    "section": "which simplifies to the result.",
    "text": "which simplifies to the result.\n\\[\\beta = (\\partial_U S)_X(U^*(\\beta, X), X)\\]\nif \\(S\\) is differentiable and strictly convex at that point."
  },
  {
    "objectID": "blog/posts/equilibrium-thermoeconomics/index.html#proof.-differentiate-f.",
    "href": "blog/posts/equilibrium-thermoeconomics/index.html#proof.-differentiate-f.",
    "title": "Classical thermodynamics and economics",
    "section": "PROOF. Differentiate \\(f\\).",
    "text": "PROOF. Differentiate \\(f\\).\n\\[df =  (\\partial_X S)_U|_{U=U^*(\\beta, X)} dX - U^*(\\beta, X) d\\beta\\]\nif \\(S\\) is differentiable and strictly convex at that point.\nPROOF. Use Hotelling’s lemma (see the essay on Analytical Mechanics).\nEXP. first-order phase transition\nWhat happens if \\(S\\) is not differentiable and strictly convex? In this case, we do not have \\(\\beta = (\\partial_U S)_X\\). We have two possibilities.\nThe first possibility is pictured as follows. There is a kink in the curve of \\(S(U, X)\\). At that point of critical internal energy \\(U_c\\), there is an entire interval of possible \\(\\beta\\). What we would notice is that at that critical internal energy and critical entropy, the system can be in equilibrium with any heat bath with any temperature between \\([T_{c, min}, T_{c, max}]\\). As far as I know, such systems do not exist, as all physically real systems have a unique temperature at all possible states.\nThe second possibility is pictured as follows. There is a bump in the curve, such that we can draw a double tangent over the bump, with slope \\(\\beta_c\\). At that critical inverse temperature, the system can be either at the lower tangent point, or the upper tangent point. It cannot be anywhere in-between, because as we saw, such points do not minimize \\(f(\\beta, X)\\), and thus are unstable.\nFor example, if we confine some liquid water in a vacuum chamber, and bathe it in a cold bath, then at its critical \\(\\beta_c\\), it would split into two parts, one part is all ice, and the other part is all water, mixed in just the right proportion to give it the correct amount of total internal energy. As it loses internal energy, the ice part grows larger, until it is all ice, at which point the system has finally gotten over the bump, and could cool down further.\nAt the critical point, \\((\\partial_{\\beta}f)_X\\) abruptly changes. So if we plot \\(\\beta \\mapsto f(\\beta, X)\\), the curve would kink there.\nTHM. Maxwell equal area rule\nIn a first-order phase transition at a fixed temperature and varying pressure/volume, the \\(P, V\\) diagram has a horizontal line going from \\((P_c, V_1)\\) to \\((P_c, V_2)\\), such that\n\\[\\int PdV = P_c(V_2-V_1)\\]\n\n\n\nMaxwell’s equal area rule states that the area of the regions labelled I and II are equal.\n\n\nPROOF.\nFix the system’s internal energy \\(U\\), and its temperature \\(T\\), and plot the \\(V, S\\) curve.\nWhen pressure is at a critical value \\(P_c\\), the line of slope \\(\\beta P_c\\) is tangent to the \\(V \\mapsto S(U, V)\\) curve at two different points, with volumes \\(V_1, V_2\\). This is that first-order phase transition.\nNow, move the system state from the first point to the second. During the process, \\[\\int PdV = \\int (TdS -dU)  = \\int TdS = T \\Delta S = T \\beta P_c(V_2 - V_1) = P_c(V_2-V_1)\\]\nTODO: figure\n\nGibbs and Landau free energy\nThis case generalizes the above case by allowing volume exchange with a bath.\nDEF. Gibbs free entropy\nWe have a thermodynamic system, in equilibrium with an energy-and-volume market of constant prices \\(\\beta, \\beta P\\), and arbitrary contact with another thermodynamic system of arbitrary design.\nThe Gibbs free entropy is\n\\[\ng(\\beta, \\beta P, X) = \\max_{U, V} (S(U, V, X) - \\beta U - (\\beta P)V)\n\\]\nIn other words, it’s the convex dual of entropy with respect to energy and volume.\nAgain, the conventional quantity is \\(G = -g/\\beta\\), the Gibbs free energy, which is minimized at constrained equilibrium.\nDEF. Landau free entropy\nThis case is a further generalization, where we have a thermodynamic system in contact with an energy-volume-chemical market of constant prices \\[\\beta, \\beta P, -\\beta\\mu_1, -\\beta\\mu_2, \\dots, -\\beta \\mu_n\\]\nThe Landau free entropy of the system is\n\\[\n\\omega(\\beta, \\beta P, -\\beta\\mu_1, \\dots, -\\beta\\mu_n) = \\max_{U, V, N_1, \\dots, N_n} \\left(S(U, V, N_1, \\dots, N_n, X) - \\beta U - (\\beta P)V - \\sum_{i=1}^n (-\\beta \\mu_i) N_i \\right)\n\\]\nIn other words, it’s the convex dual of entropy with respect to energy, volume, and particle numbers.\nWe can prove the analogous theorems in the same way.\nTHM.\nLet a thermodynamic system be in equilibrium with an energy-and-volume bath of prices \\(\\beta, \\beta P\\). If the system has constraint on the state by \\(C(X) = 0\\), then the system equilibrates at\n\\[\n\\begin{cases}\n\\max_{X} g(\\beta, \\beta P, X)\\\\\nC(X) = 0\n\\end{cases}\n\\]\n\\[dg =  (\\partial_X S)_{U, V} dX - Ud\\beta - V d(\\beta P) \\]\nif \\(S\\) is strictly convex and differentiable at that point.\nConsider the following method of extracting non-thermodynamic energy: Connect the system to an energy-and-volume bath at prices \\(\\beta, \\beta P\\), and a non-thermodynamic system of arbitrary design. The system starts at \\(\\beta, \\beta P, X_0\\) and ends at \\(\\beta, \\beta P, X_1\\). We have\n\\[\nW\\leq G(\\beta, \\beta P, X_0) - G(\\beta, \\beta P, X_1)\n\\]\nwhere \\(W\\) is “work”, that is, the increase in internal energy of the non-thermodynamic system. If the process is reversible, then equality holds.\nTake-home lessons:\n\nFree entropy is maximized when the system equilibrates with a bath. Free energy is minimized.\nChange in free energy is the maximal amount of work extractable when the system equilibrates with both a bath and a non-thermodynamic system. This work is actually extracted when the process is reversible.\n\n\n\nMaxwell relations\nTHM. First relation\nSince \\(dS = \\beta dU + \\beta P dV\\), we have\n\\[\\partial_U \\partial_V S = (\\partial_V \\beta)_U = (\\partial_U(\\beta P) )_V\\]\nIn economic language, we have\n\\[\n\\partial_{q_i}\\partial_{q_j} S = (\\partial_{q_i} p_j)_{q_j} = (\\partial_{q_j} p_i)_{q_i}\n\\]\nwhere \\(q_i\\) is the quantity of commodity \\(i\\), and \\(p_i\\) is its marginal utility.\nFor example, if \\(i, j\\) are noodles and bread, then \\((\\partial_{q_i} p_j)_{q_j}\\) is how much the marginal price of bread would rise if I have a little more noodle. As noodles and bread are substitutional goods, we expect the number to be negative, meaning that having more noodles, I would price bread less. The Maxwell relation then tells us that it is exactly the same in the other direction: If I am given a little more bread, I would price noodles less. Not only that, I would want less by exactly the same amount, because\n\\[(\\partial_{q_i} p_j)_{q_j} = (\\partial_{q_j} p_i)_{q_i}\\]\nIn economics, we usually prefer writing demanded quantity as a function of marginal price.\n\\[(\\partial_{p_j}q_i)_{q_j} = (\\partial_{p_i}q_j)_{q_i}\\]\nThis is a symmetry of cross-price elasticity of demand.2\n2 Samuelson used the Maxwell relations, and other relations, to justify neoclassical economics. His idea is that, while utility functions are unobservable, and we do not have a scientific instrument to measure “economic equilibrium”, we can make falsifiable predictions from assuming that the economy is in equilibrium – such as the symmetry of the cross-price elasticity of demand.The second relation is a bit hard to explain, since enthalpy really does not have a good representation in entropy-centric thermodynamics. However, it turns out to be just the third relation with \\(i, j\\) switched.\nThird relation\nSince \\(df = \\beta P dV - Ud\\beta\\), we have\n\\[\n\\partial_\\beta\\partial_V f = -(\\partial_V U)_\\beta = +(\\partial_\\beta(\\beta P))_V\n\\]\nIn economic language, we have\n\\[\n\\partial_{p_i}\\partial_{q_j} \\left[\\max_{q_i}(S(q) - p_i q_i )\\right]= -(\\partial_{q_j} q_i)_{p_i} = (\\partial_{p_i}p_j)_{q_j}\n\\]\nContinuing from the previous example, if \\(i, j\\) are noodles and bread, and we open a shop with an infinite amount of noodles always at the exact price, then I would of course buy and sell from the noodle shop until my marginal price of noodles is equal to the shop’s price. Now, \\((\\partial_{q_j} q_i)_{p_i}\\) is how much noodles I would buy if I am given a marginal unit of bread. As noodles and bread are substitutional goods, this number is negative. This then means \\((\\partial_{p_i}p_j)_{q_j} &gt; 0\\), meaning that if the noodle price suddenly increases a bit, then I would sell a bit of noodles until I have reached equilibrium again. At that equilibrium, since I have less noodles, I would price higher its substitutional good, bread, by an equal amount as the previous scenario.\nFourth relation\nSince \\(dg = -Ud\\beta - Vd(\\beta P)\\), we have\n\\[\n-\\partial_{\\beta}\\partial_{\\beta P}g = (\\partial_{\\beta P}U)_\\beta = (\\partial_\\beta V)_{\\beta P}\n\\]\nIn economic language, we have\n\\[\n\\partial_{p_i}\\partial_{p_j} \\left[\\max_{q_i, q_j}(S(q) - p_i q_i - p_j q_j)\\right]= -(\\partial_{p_j} q_i)_{p_i}= -(\\partial_{p_i} q_j)_{p_j}\n\\]\nContinuing from the previous example, if \\(i, j\\) are noodles and bread, and we open a shop with an infinite amount of noodles and bread, then I would of course buy and sell from the shop until my marginal prices of noodles and bread are equal to the shop’s prices. Now, if the shop suddenly raises the price of bread by a small amount, I would sell off some bread until my marginal price for bread increases to the shop’s new price. Now my marginal price for noodles increases too by substitutional effect, so I buy some noodles. Thus \\((\\partial_{p_j} q_i)_{p_i} &gt; 0\\). Switching the scenario, we find that raising the price of noodles would make me buy bread, by an equal amount as the previous scenario.\nThis is another symmetry of cross-price elasticity of demand.\nPROOF. (alternate)\nWe use the notation of economics here.\nSuppose we have commodities \\(1, 2, \\dots, n\\). We pick two commodities \\(i, j\\), and fix all other commodity quantities. Thus, we can write \\(dS = p_i dq_i + p_j d q_j\\).\nSince knowing \\(n\\) properties of the thermodynamic system allows us to know its exact state, and we have already fixed \\(n-2\\) properties of it, there only remain two more degrees of freedom. We can parameterize this by \\((q_i, q_j)\\), or \\((p_i, q_j)\\), or \\((p_i, p_j)\\), or any other reasonable coordinate system.\nIf the thermodynamic system undergoes a cycle, then\n\\[0 = \\oint dS = \\oint p_i dq_i + \\oint p_j dq_j\\]\nand thus, if we take the cycle infinitesimally small, we find that \\(dp_i\\wedge dq_i = -dp_j \\wedge dq_j\\). That is, the map \\((p_i, q_i) \\mapsto (p_j, q_j)\\) preserves areas, but reverses orientation. In particular, we have a Jacobian \\[\n\\frac{\\partial(p_i, q_i)}{\\partial(p_j, q_j)} = -1\n\\]\nNow, let \\((x, y)\\) be an arbitrary coordinate transform. By the chain rule for Jacobians, \\[\n\\frac{\\partial(p_i, q_i)}{\\partial(x, y)} = \\frac{\\partial(p_i, q_i)}{\\partial(p_j, q_j)} \\frac{\\partial(p_j, q_j)}{\\partial(x,y)}  = -\\frac{\\partial(p_j, q_j)}{\\partial(x,y)}\n\\]\nThis allows us to derive all the Maxwell relations. For example, setting \\((x, y) = (p_i, q_j)\\) gives us the third relation\n\\[(\\partial_{q_j} q_i)_{p_i} = -(\\partial_{p_i}p_j)_{q_j}\\]\n\n\n\nDeriving the four Maxwell relations by picking the right variables for \\((x, y)\\)."
  },
  {
    "objectID": "blog/posts/equilibrium-thermoeconomics/index.html#caratheodorys-thermodynamics",
    "href": "blog/posts/equilibrium-thermoeconomics/index.html#caratheodorys-thermodynamics",
    "title": "Classical thermodynamics and economics",
    "section": "Caratheodory’s thermodynamics",
    "text": "Caratheodory’s thermodynamics\nIn the early 1900s, Caratheodory discovered a new way to “geometrize” thermodynamics, with the austere beauty of Euclidean geometry. Though his formulation fell into obscurity, it was reborn in neoclassical economics as utility theory.\n\nPresentation\nConsider a thermodynamic system with \\(n+1\\) dimensions of state space. Give the state space coordinates \\(q_0, q_1, \\dots, q_n\\). For example, for a tank of ideal gas where the particle number is fixed, we have \\(q_0 = U, q_1 = V\\).\nLet the system be at a certain state \\(\\vec q\\), and wrap the system in a perfectly insulating (adiathermal) blanket. The system can undergo many different kinds of adiathermal motion, but there are certain motions that it cannot undergo.\nFor example, for a piston of ideal gas, the possible motions are adiabatic expansion, adiabatic compression, Joule expansion, and any combination of them. However, “Joule compression” is impossible – the gas will not spontaneously contract to the left half of the system, pulling in the piston head, anymore than a messy room will spontaneously tidy itself.\nWe say that \\(\\vec q'\\) is adiathermally accessible from \\(\\vec q\\) if there exists a path from \\(\\vec q\\) to \\(\\vec q'\\), such that the path is infinitesimally adiathermal4 at every point.\n4 The word “adiathermal” means “heat does not pass through”, while “adiabatic” has an entire history of meaning that makes it hard to say what exactly it is (see the essay on Analytical Mechanics). Personally, I think “adiabatic” means “zero entropy change”, and all its other meanings derive from it.AXIOM. (Caratheodory’s version of the second law of thermodynamics)\nIn any neighborhood of any point \\(\\vec q\\), there are points adiabatically inaccessible from it.\nFor any two points, \\(\\vec q, \\vec q'\\), one of them is adiabatically accessible from the other.\n:::\nThe effect of these two axioms is that we can define a total ordering \\(\\preceq\\) on state space, where we write \\(\\vec q \\preceq \\vec q'\\) to mean that \\(\\vec q\\) can adiabatically access \\(\\vec q'\\), and write \\(\\vec q \\sim \\vec q'\\) to mean that they are mutually adiabatically accessible.\nInterpreted economically, we say that the system is an economic agent, each \\(\\vec q\\) is a bundle of goods, and \\(\\vec q \\preceq \\vec q'\\) means that \\(\\vec q'\\) is preferable to the agent, and that \\(\\vec q \\sim \\vec q'\\) means they are equally preferred.\nThe total ordering partitions the state space into contour surfaces of equal accessibility, or indifference surfaces. Assuming the state space is not designed to mess things up (pathological), these indifference surfaces will be differentiable.\nLet us consider the indifference surface passing state \\(\\vec q\\). The indifference surface is locally a plane, so it has equations\n\\[\ndq_0 - \\sum_i \\tilde p_i dq_i = 0\n\\]\nwhere \\(\\tilde p_i = (\\partial_{q_i}q_0)_{q_1, \\dots, q_n}\\).\nEconomically speaking, we can say that \\(\\tilde p_i\\) is the marginal worth of \\(q_i\\) denoted in units of \\(q_0\\). For example, we can say that \\(q_0\\) are cowry shells, which themselves are pretty and give us some utility. However, it can also be used as a monetary unit. Then, if \\(i\\) is bread, then \\(\\tilde p_i\\) is the marginal amount of cowry shells that we would pay for a marginal amount of bread. If we were to visit a free market where we can buy and sell items denoted in cowry shells, then we would buy bread if \\(\\tilde p_i &gt; \\tilde p_{i, market}\\), and sell bread if \\(\\tilde p_i &lt; \\tilde p_{i, market}\\). Right at the border of \\(\\tilde p_i = \\tilde p_{i, market}\\), we would be indifferent about buying or selling bread. When \\(\\tilde p_i = \\tilde p_{i, market}\\) for all \\(i\\), we would be completely indifferent about the market.\n\nTheorem 5 (existence and uniqueness of temperature and entropy) There exists unique functions \\(\\beta, S\\) on the state space, such that\n\\[\ndq_0 - \\sum_i \\tilde p_i dq_i = \\beta^{-1} dS\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\nAt each point \\(P\\) the one-form \\(\\omega(p)\\) is visualized as a stack of parallel planes. The planes are quilted together, but with “uneven thickness”. By scaling the one-forms just right at every point, the thickness becomes equalized, and so \\(\\beta \\omega = dg\\) for two real-valued functions \\(\\beta, g\\).\n\n\n\n\n\n\n\n\nProving Caratheodory’s theorem. Figure from .Economically speaking, we can say that \\(S\\) is the utility, and \\(\\beta\\) is the marginal utility of cowry shells. The theorem tells us that just by knowing how we order the goods (” \\(S(\\vec q) &gt; S(\\vec q')\\) “), we can extract a numerical value for the goods (” \\(S(\\vec q) - S(\\vec q') = 1.34(S(\\vec q'') - S(\\vec q'''))\\) “). Out of ordinal utility, we have achieved cardinal utility.\nThere used to be a debate between “ordinalists” and “cardinalists” of utility theory. The “cardinalists” were the more venerable of the two camps, tracing back to Bentham’s felicific calculus and the marginalist revolution. They argued that utility is real-valued, like entropy and temperature. The “ordinalists” countered that a nobody has ever measured a utility in anyone’s brain. The only thing we can observe is preferences: I prefer this over that – I can order everything that can ever happen to me on a numberless line of preferences. Similarly, nobody can ever actually measure temperature or entropy, only that energy flows from this gas to that gas, which presumably has lower temperature, and that one chunk of gas in one state ends up in another state, which presumably has higher entropy.\nThe debate has been mostly resolved by the work of Gérard Debreu, who showed that under fairly reasonable assumptions, cardinal utility is possible (Debreu 1971).5\n5 Out of all those famous economists I have seen, Gérard Debreu is perhaps the most mathematically austere. Reading his works, I felt like he was another G. H. Hardy, a Bourbaki of economics. He did economics not to improve the world, not to help people, and not to advance a political agenda, but to simply uncover a face of eternity.This theorem, or rather, this family of theorems, have several names, as befitting for such a versatile and productive family. In calculus, it’s called the integrability of Pfaffian forms. In differential geometry, it’s called Darboux’s theorem, or Frobenius theorem. In economics, it’s called the integrability of demand, or the cardinal-ordinal utility representation theorem.\n\n\n\n\n\n\nWhat’s so special about energy, or cowry shells?\n\n\n\nWhen cast in the language of economics, cowry shells are not special. We could denote prices in cowry shells, or cans of sardine, or grams of gold. That is, we are free to pick any numéraire we want, as long as we are consistent about it.\nSimilarly, energy is not special. For example, with ideal gas, we could write the first law of thermodynamics as the conservation of energy, like\n\\[dU - (-P)dV = \\beta^{-1}dS\\]\nor as the conservation of volume, like\n\\[dV - (-P^{-1})dU= (\\beta P)^{-1}dS\\]\nand from the perspective of classical thermodynamics, there is no possibility of saying that energy is more special than volume. Energy is exactly as special as volume, and no more special than that.\nWhen I realized this difference, I was so incensed at this mistake that I wrote an entire sci-fi worldbuilding sketch about an alien species, for which it is the conservation of volume that is fundamental, not energy, and which discovered stereodynamics, not thermodynamics."
  },
  {
    "objectID": "blog/posts/equilibrium-thermoeconomics/index.html#geometric-thermodynamics",
    "href": "blog/posts/equilibrium-thermoeconomics/index.html#geometric-thermodynamics",
    "title": "Classical thermodynamics and economics",
    "section": "Geometric thermodynamics",
    "text": "Geometric thermodynamics\n\nAlthough geometrical representations of propositions in the thermodynamics of fluids are in general use, and have done good service in disseminating clear notions in this science, yet they have by no means received the extension in respect to variety and generality of which they are capable.\nGibbs, Josiah Willard. “Graphical methods in the thermodynamics of fluids.” The Collected Works of J. Willard Gibbs, Ph. D., LL. D (1957): 1-32.\n\n\nContact geometry\nTo explain these mysterious remarks, we can take another look at thermodynamics. To begin, we take a plunge into abstraction. We know that a real gas has pressure, volume, temperature, entropy, etc. We know that they satisfy the differential equation:\n\\[\ndS = \\beta dU + \\beta PdV\n\\]\nTo make things look better, we can change the notation to\n\\[\ndS = p_1 dq_1 + p_2 dq_2\n\\]\nThis formula has a clear interpretation in economics: if the marginal utility of commodity \\(1\\) is \\(p_1\\), and the marginal utility of commodity \\(2\\) is \\(p_2\\), then if we receive \\(\\delta q_1, \\delta q_2\\), our utility would increase by \\(p_1 \\delta q_1 + p_2 \\delta q_2\\).\nThe difficult thing about classical thermodynamics is that there are so many quantities (temperature, volume, pressure…). The saving grace is that it turns out that there are only a few degrees of freedom.\nThe Parthian shot is that now you are burdened with dozens of equations relating these quantities. I cannot remember any of the Maxwell relations, so I look at Wikipedia every time I need to calculate with them.\nHowever, why is it that a macroscopic lump of matter, whirling with \\(10^{23}\\) molecules, turn out to be characterized by only a few degrees of freedom? Why is it that a national economy, swarming with \\(10^8\\) people, have macroeconomic laws? For the first question, the answer is given by classical thermodynamics: the lump of matter is maximizing its entropy under constraint, so its degrees of freedom are exactly as many as the number of constraints it is laboring under. For the second question, the answer is given by neoclassical economics: the national economy behaves as if it is maximizing a social utility function under its resource constraints.\nWith that brief look at philosophy, we return to abstract thermodynamics. We have a lump of matter (such as ideal gas in a piston), of which we can measure five different properties: \\(p_1, p_2, q_1, q_2, S\\). In general, we expect that the space of possible measurements is 5-dimensional, but it turns out that they collapse down to a 2-dimensional curved surface. This is why we could completely fix its state knowing just its \\(V, U\\), or just its \\(P, T\\), etc.\nThe question now comes: Why is it possible to collapse things down to this curved surface?\nThings are already interesting when we have just one commodity: \\[dS - pdq = 0\\]\nand we ask: Why is it possible to collapse the space of \\((q, p, S)\\) from 3 to 1 dimension?\n\nEvery mathematician knows it is impossible to understand an elementary course in thermodynamics. The reason is that thermodynamics is based—as Gibbs has explicitly proclaimed – on a rather complicated mathematical theory, on the contact geometry. Contact geometry is one of the few ‘simple geometries’ of the so-called Cartan’s list, but it is still mostly unknown to the physicist – unlike the Riemannian geometry and the symplectic or Poisson geometries, whose fundamental role in physics is today generally accepted.\nV.I. Arnol’d (Caldi et al. 1990, 163)\n\nIn modern geometry, an expression like \\(dS - \\sum_i p_i dq_i = 0\\) defines a field of planes in \\(\\R^3\\). That is, at each point \\((q, p, S)\\), we construct a plane defined by\n\\[\n\\{(q + \\delta q, p + \\delta p, S + \\delta S): \\delta S - p \\delta q = 0\\}\n\\]\nFor example, in three dimensions, the field of planes \\(dS - pdq = 0\\) would look like it is constantly twisting as \\(p\\) increases.\n\n\n\nThe field of planes \\(dS - pdq = 0\\) in \\(\\R^{2+1}\\). Figure from Wikipedia.\n\n\nGiven such a field of planes \\(dS - \\sum_{i=1}^n p_i dq_i\\) in \\(\\R^{2n+1}\\), we say that a manifold is a Legendrian submanifold iff the manifold has \\(n\\) dimensions, and is tangent to the field of planes at every point.\nFor example, when \\(n=1\\), a Legendrian submanifold is a curve that winds around \\(\\R^3\\) that is always tangent to the plane at every moment.\n::: {#thm-0}\nLet \\(S(q)\\) be a differentiable function. We can interpret \\(S(q)\\) as how much money we can earn if we produce something using the bundle of raw materials \\((q_1, \\dots, q_n)\\).\nGiven any market price for the raw materials, \\(q^* = \\argmax_q (S(q) - \\braket{p, q})\\) is the profit-maximizing production plan, and \\(\\Pi(p) = \\max_q (S(q) - \\braket{p, q})\\) is the profit.\nThe “profit-maximization surface” defined by\n\\[\np \\mapsto (q^*, p, S(q^*))\n\\]\nis a Legendrian submanifold.\nConversely, given any Legendrian submanifold parameterized by\n\\[p \\mapsto (q(p), p, S(q(p)) )\\]\nthen \\(q(p)\\) is profit-stationarizing. That is,\n\\[\\nabla_q (S(q) - \\braket{p, q}) = 0\\]\nat \\(q(p)\\). If \\(S\\) is strictly convex, then \\(q(p)\\) is profit-maximizing.\n::: {.callout-note title=“Proof” collapse=“false”} First part is proven by Hotelling’s lemma. Second part is proven by plugging in \\(dS - \\sum_i p_i dq_i = 0\\). And if \\(S\\) is strictly convex, then \\(q \\mapsto S(q) - \\braket{p, q}\\) is also strictly convex, and so zero gradient implies global maximum.\nEconomically speaking, \\(max_q (S(q) - \\braket{p, q})\\) means to maximize profit. What does it mean thermodynamically speaking? It means minimizing \\(\\braket{p, q} - S(q)\\), which is the Gibbs free entropy! Let me write it in a more familiar form, for a tank of gas: \\[\\min_{\\beta, \\beta P} (\\beta U + (\\beta P) V - S)\\]\nThus, maximizing profit when a factory has access to a market is the same as minimizing Gibbs free entropy when a system is in contact with a bath.\n\n\nSamuelson’s area-ratio thermodynamics\nIn Paul Samuelson’s Nobel prize lecture of 1970, among comments of classical mechanics, variational principles, and neoclassical economics, he said something curious about the analogy between classical thermodynamics and neoclassical economics:\n\nHowever, if you look upon the monopolistic firm hiring 99 inputs as an example of a maximum system, you can connect up its structural relations with those that prevail for an entropy-maximizing thermodynamic system. Pressure and volume, and for that matter absolute temperature and entropy, have to each other the same conjugate or dualistic relation that the wage rate has to labor or the land rent has to acres of land. Figure 2 can now do double duty, depicting the economic relationships as well as the thermodynamic ones.\n\n\n\nSamuelson’s equal area ratio condition. In the diagram, we have \\(a:b = c:d\\)\n\n\nIf someone challenged me to explain what the existence of [utility] implies, but refused to let me use the language of partial derivatives, I could illustrate by an equi-proportional area property… I may say that the idea for this proposition in economics came to me in connection with some amateurish researches in the field of thermodynamics. While reading Clerk Maxwell’s charming introduction to thermodynamics…\n(Samuelson 1971)\n\nThis intriguing little remark piqued my interest, and after a little digging, I figured it out.6\n6 Based on research by James Bell Cooper, who seems to be the world expert in this obscure field (Cooper and Russell 2006; Cooper, Russell, and Samuelson 2001).Samuelson is alluding to a deep problem in economics theory: Nobody has ever seen a utility function, anymore than anybody has ever seen an entropy-meter. If this is the case, then how do we know that agents are maximizing a utility, or that systems are maximizing an entropy? In his long career, he searched for many ways to answer this, coming down to the idea that, even though we cannot measure the utility, we can measure many things, such as how firms respond to prices. Given some measurable quantities, we can then prove, mathematically, that something is being maximized. Now, we can simply call that something “utility”, and continue doing economics as usual.\nPhilosophically, Samuelson was greatly influenced by operationalism, a philosophy of science akin to positivism. As stated by the definitive work on operationalism, “we mean by any concept nothing more than a set of operations; the concept is synonymous with the corresponding set of operations” (Bridgman 1927).\nIn his early work, particularly Foundations of Economic Analysis (1947) and the development of revealed preference theory (1938), Samuelson embraced operationalism as a means of purging economics of non-observable, and thus scientifically meaningless, concepts like utility. He sought to rebase economic theory on purely observable behavior and measurable quantities. Revealed preference theory, for instance, aimed to eliminate the reliance on subjective utility functions by deriving consumer preferences directly from observed choices at different price levels.\nOver time, Samuelson’s stance on operationalism softened to a more pragmatic approach, recognizing the value of unobservable concepts as theoretical tools, as long as they can be based on direct observables.\nFor example, while Samuelson initially sought to eliminate utility functions, he later argued that even if utility is not directly observable, it can be uniquely determined from observing agents’ preferences, by invoking some utility representation theorems – provided that the preferences satisfy certain properties. (Samuelson 1999)\n::: {#exm-0}\n(ideal gas law)\nSuppose we know from experiment that a tank of ideal gas satisfies\n\\[\nPV = Const, \\quad PV^\\gamma = Const\n\\]\nunder constant temperature and adiabatic conditions.\nPROP. (area ratio condition)\nIt is tedious but simple to verify, by directly calculating the areas of the curvy parallelograms, that these two families of curves satisfy the area ratio condition.\nAlternatively, we can use the method of exhaustion and Eudoxus’ theory of proportions to prove this, in a way that even ancient Greeks would approve.\n::: {.callout-note title=“Proof” collapse=“false”}\nNotice that under the squashing map \\((P, V) \\mapsto (cP, V/c)\\), both families of lines are preserved, and furthermore, this map preserves area, so we can calculate the area of any curvy parallelogram by tiling it with tiny strips of thin parallelograms.\nAs shown, we can draw a very thin parallelogram \\(\\delta\\), then use the squashing map to tile both parallelograms \\(c\\) and \\(d\\). We have that\n\\[A(c) : A(d) = \\frac{A(c)}{A(\\delta)} : \\frac{A(d)}{A(\\delta)} \\approx N(c) : N(d)\\]\nwhere \\(A(c)\\) is the area of \\(c\\), and \\(N(c)\\) is the number of copies of \\(\\delta\\) that is contained within \\(c\\). By the method of exhaustion and Eudoxus’ theory of proportion, at the limit of infinitely thin \\(\\delta\\), both sides are equal.\nNow, performing the same construction on the other half of the parallelograms, we tile \\(a, b\\) by the same number of copies of \\(\\delta'\\). Thus we have\n\\[A(c) : A(d) \\approx N(c) : N(d) = N(a) : N(b) \\approx A(a) : A(b)\\]\nand both sides equal at the limit.\nFrom the area ratio theorem, there exists two functions \\(f_T, f_S\\), such that the new coordinates\n\\[T(P, V) = f_T(PV), \\quad S(P, V) = f_S(PV^\\gamma)\\]\nsatisfy \\(dT \\wedge dS = dP \\wedge dV\\). We can then define \\[dU = TdS - PdV\\]\nwhich satisfies \\(d^2 U = 0\\), that is, it is integrable.\nDER.\nSimplifying, we get\n\\[f'_T(PV)f_S'(PV^\\gamma) = \\frac{1}{(\\gamma - 1) PV^\\gamma}\\]\nLet \\(x = PV, y = PV^\\gamma\\), we get a separation of variables: \\(f'_T(x) f'_S(y) = \\frac{1}{(\\gamma-1) y}\\), which solves to\n\\[T = C_1 PV + C_0, \\quad S = \\frac{1}{(\\gamma-1) C_1} \\ln \\frac{PV^\\gamma}{P_0V_0^\\gamma}\\]\nfor some constants \\(C_0, C_1, P_0, P_0\\).\nKnowing that \\(C_0 = 0, C_1 = 1/(nR)\\), we have the equations of state: \\[PV = nRT, \\quad S = \\frac{1}{\\gamma-1} nR \\ln \\frac{PV^\\gamma}{P_0V_0^\\gamma}\\]\nIntegrating \\(dU = TdS - PdV\\), we have \\(U = \\frac{1}{\\gamma-1} nRT\\). We can define \\(\\hat c_V = \\frac{1}{\\gamma - 1}\\), which leads to\n\\[PV = nRT, \\quad S = \\hat c_V n R \\ln \\frac{PV^{1 + 1/\\hat c_V}}{P_0V_0^{1 + 1/\\hat c_V}}, \\quad U = \\hat c_V nRT\\]\nor equivalently, \\[S = n R \\ln \\frac{U^{\\hat c_V}V}{U_0^{\\hat c_V} V_0}\\]\nThus, we have\n\\[PV = nRT, \\quad S = \\hat c_V n R \\ln \\frac{PV^{1 + 1/\\hat c_V}}{P_0V_0^{1 + 1/\\hat c_V}} = n R \\ln \\frac{U^{\\hat c_V}V}{U_0^{\\hat c_V} V_0}, \\quad U = \\hat c_V nRT\\]\nTaking the derivative, \\[dS = \\beta dU + \\beta PdV - \\beta \\mu dn\\]\ngives \\[\\beta = 1/T, P = P, \\mu = -TS/n\\]\nWe find that the chemical potential, unfortunately, has an additive constant. We should not be too surprised, however, as anything with “potential” in its name probably has an additive constant, like electric voltage.\n\n\n\nThe equal area ratio condition.\n\n\n::: {#thm-0}\n(area ratio law implies a new coordinate system)\nConsider an open rectangle \\(R\\) in the plane \\(\\R^2\\). Let there be two families of curves\nSuppose that each curvy parallelogram formed by the two families is contained in \\(R\\), and the curves satisfy the area-ratio rule, then we can define another coordinate system \\((z, w\\) ) on \\(\\R^2\\), such that the coordinate system preserves areas: \\[\ndx \\wedge dy = dz \\wedge dw\n\\]\nand that the two families of lines are the constant \\(z\\) and constant \\(w\\) curves.\nThe coordinate system is unique up to an affine squashing transform, that is, \\((z, w) \\mapsto (cz + d, w/c + e)\\) for some constants \\(c, d, e\\) with \\(c &gt; 0\\).\n::: {.callout-note title=“Proof” collapse=“false”}\nFix an arbitrary point as \\((z, w) = (0, 0)\\). Pick an arbitrary curve in one of the families, not passing \\((0, 0)\\) point, and call it the \\(w=1\\) line. By continuity, there exists a unique curve in the other family, such that their curved parallelogram has unit area. Call that other curve the \\(z=1\\) line. Now we can label the four corners of it \\((z, w) = (0, 0), (0, 1), (1, 0), (1, 1)\\).\nFor any other point, its \\((z, w)\\) coordinates can be constructed as shown in the picture, with\n\\[z = a+b, \\quad w = b+d\\]\nBy the area ratio law, we have \\(a:b = c:d\\). We also have \\(a+b+c+d = 1\\) since we picked the parallelogram to have unit area. Solving these 4 equations, we find that \\(b = zw, a = z(1-w), d = (1-z)w, c = (1-z)(1-w)\\), as it should.\nTaking the derivative, we have \\(dx \\wedge dy = dz \\wedge dw\\).\nCOR. (area ratio law implies existence of an entropy function)\nSince \\(dx \\wedge dy = dz \\wedge dw\\), we can draw any cycle \\(\\gamma\\), and integrate around the cycle: \\[\\oint_\\gamma (ydx + zdw) = \\iint_{\\text{area in }\\gamma} (dy \\wedge dx + dz \\wedge dw) = 0\\]\nThus, there exists some scalar function \\(S\\), such that \\(dS = ydx + zdw\\).\nAnd here we go again, we now have an entropy/revenue function, and from there, all of classical thermodynamics/neoclassical economics follows.\nWe know what the curvy lines mean in thermodynamics: they are the isotherms, isentropics, isobarics, etc (depending on how you pick the variables). What do the curvy lines mean in economics?\nSuppose we plot the lines of constant \\(p_2\\) in the plane of \\(q_1, p_1\\). What does it say? It says this: “Suppose the price of commodity \\(2\\) is fixed, and we vary the price of commodity \\(1\\). How much of commodity \\(1\\), as a factory manager, would I want to purchase?” In other words, these are the demand curves for commodity \\(1\\) when the price of commodity \\(2\\) is fixed..\nSimilarly, a line of constant \\(q_2\\) is a demand curve for commodity \\(1\\) when the quantity of commodity \\(2\\) is fixed at \\(q_2\\).\nLooking at the diagram, we see that the demand curves are steeper for fixed \\(q_2\\) than for fixed \\(p_2\\). In other words, the factory manager is more price-sensitive about commodity \\(1\\) when there is a free market for commodity \\(2\\), because there is a choice. After all, think like a factory manager. If I have no choice about how much of commodity \\(2\\) I can put into production, I would have little resort when faced with a price hike on commodity \\(1\\). I will buy less of it, but I mostly just have to suffer the loss in profit. But if I have a choice with a free market for commodity \\(2\\), I would definitely buy more of commodity \\(2\\) to substitute for some of commodity \\(1\\) after the price hike.\nThis is Le Chatlier’s principle for economics, which Paul Samuelson used to great effect. In his telling, immediately after the market has suffered a sudden price shock, factories would have to suffer the consequences because they cannot react by changing their production plans. Thus, in the short run, factories are less price-sensitive. In the long run, the factories would be able to change their production plans, and so in the long run, factories are more price-sensitive. As another application, during a wartime economy when there is rationing for some critical products like rubber and oil, people would become less price-sensitive in all products that still remain on the free market.\nThis result can be generalized to the case of \\(n\\) commodities \\(q_1, \\dots, q_n\\) with prices \\(p_1, \\dots, p_n\\). In this case, we would find that, assuming some more complicated area ratio law, we can rescale \\(q_2, \\dots, q_n\\) and \\(p_2, \\dots, p_n\\), such that \\(\\sum_i dp_i \\wedge dq_i = 0\\). This then allows us to construct a function \\(S\\), such that\n\\[dS - \\sum_i p_i dq_i = 0\\]\nwhich, by a previous theorem, maximizes like an entropy. Therefore, it is the entropy.\nThis is what Paul Samuelson meant in his Nobel Prize lecture, when he said that the area ratio law implies something is being maximized. In thermodynamics, that something is negative Gibbs free entropy. In economics, that something is profit.\n\n\nBonus: Riemannian geometry\nThere are other ways to study the state space of thermodynamic systems by differential geometry. For example, since the entropy function is typically a strictly convex function of the extensive parameters, \\(-\\partial^2 S\\) is positive-definite. This is then a Riemannian metric on the state space. The only places where strict convexity fails is when \\(S\\) is “bumped downwards”, which gives us a first-order phase transition, or has a flat region, which gives us a second-order phase transition. Away from regions of phase transition, we have a Riemannian geometry. In the regions of phase transitions, the geometry collapses into singularities, much as spacetime collapses in the center of a black hole.\nFor more on this line of research, search “Ruppeiner geometry” and “Weinhold geometry” (Weinhold 1976; Quevedo 2007)."
  },
  {
    "objectID": "blog/posts/equilibrium-thermoeconomics/index.html#equilibrium-chemistry-done-right",
    "href": "blog/posts/equilibrium-thermoeconomics/index.html#equilibrium-chemistry-done-right",
    "title": "Classical thermodynamics and economics",
    "section": "Equilibrium chemistry done right",
    "text": "Equilibrium chemistry done right\nGibbs wrote his landmark book, On the Equilibrium of Heterogeneous Substances, to answer one question: Why are some “heterogeneous substances” in equilibrium, while others not? Why, when we drop a block of salt into pure water, does the block of salt become smaller, but after a while, it stops getting smaller? His answer is always the same: heterogeneous substances are in equilibrium precisely when its entropy is maximized under constraint.\nSamuelson wrote his landmark book, Foundations of Economic Analysis, to answer one question: Why are some economic systems in equilibrium, while others not? Why, when we drop a block of agents into a market, do they buy and sell things, but after a while, they stop buying and selling? His answer is always the same: a crowd of economic agents is in equilibrium precisely when some global parameter (which can be interpreted as utility, profit, etc, depending on context) is maximized under constraint.\n\nTragic backstory\nI studied chemistry back then. Balancing equations was just linear algebra, and organic chemistry was just lego with long names. However, when it came to chemical thermodynamics, it completely defeated me.\nConsider the simple reaction\n\\[\naA + bB \\rightleftharpoons cC + dD\n\\]\nThe textbook said that at equilibrium, \\(\\Delta G = 0\\), where\n\\[\n\\Delta G = \\Delta G^\\circ + RT \\ln Q, \\quad\nQ = \\frac{[C]^c [D]^d}{[A]^a[B]^b},\n\\]\nAt this point, I was lost. It is plain to see that \\(Q\\) has units of \\((\\mathrm{mol/L})^{c+d-a-b}\\), and I knew from physics that you can never ever take the logarithm of something with a unit. What’s worse, \\(\\Delta G\\) has units of \\(\\mathrm{J/mol}\\) when it obviously should have units of \\(\\mathrm{J}\\), because \\(\\Delta G\\) is just a difference in \\(G\\), and since \\(G\\) is “Gibbs free energy”, both \\(G\\) and \\(\\Delta G\\) should have the same units – of \\(\\mathrm{J}\\).\nAnd it got even worse when I read on and found questions that asked me to calculate the “total Gibbs free energy released during the reaction”. I thought, well, since you end up at an equilibrium, and the textbook said that at equilibrium, \\(\\Delta G = 0\\), obviously there is no total Gibbs free energy released. That is of course wrong.\nAt that point, I gave up trying to understand and simply practiced until I could solve the questions without understanding.\nIt certainly didn’t help when I kept seeing both \\(\\Delta G^\\circ\\) and \\(\\Delta G^\\ominus\\), and sometimes even \\(\\Delta G^{\\minuso}\\), which is the “standard state” when the substance is a gas – but only for some gasses.\nPoint is, the notation is a complete mess, and the pedagogy is nonsensical. I believe it is because the teachers didn’t know any better. They did not understand what they were teaching, but they were protected from their ignorance by the rigidity of the world. Much like how most physicists do not understand Hamiltonian mechanics, but still manage to calculate correctly by their well-honed physical intuition, I would wager that most chemists do not understand thermodynamics, but still manage to calculate correctly by their well-honed chemical intuition.\nAfter I finally understood thermodynamics, I turned my sights on chemical thermodynamics, and remembered this \\(\\Delta G\\) nonsense. I started with the idea “No matter what they say, one can’t possibly get the units wrong.” and got into a shouting match with ChatGPT-4, who kept mumbling about “fugacity” and “real gasses”. An hour of shouting later, I finally figured it out.\nAnd repeating the same pattern, as soon as I unlearned this, I knew the right phrase to search, and discovered that this is a common error, the entire anatomy of which has been autopsied carefully (Raff 2014a, 2014b, 2014c). Truly, the teaching of STEM is so bad that what has been unlearned has to be unlearned anew every generation…\n\n\nEquilibrium of a single gas or a single solution\n\n\n\n\n\n\nGibbs vs Helmholtz\n\n\n\nWe study the case of a reaction chamber held under constant volume and temperature, which one can picture as a sealed glass tube in an ice-water bath. Everything still applies when the reaction chamber has constant pressure as well. For example, this happens if we have a flaccid plastic bag at the bottom of the ocean. The inside of the plastic bag would have constant temperature and pressure.\nEvery result in this section can be direct translated to that case, by replacing “Helmholtz” with “Gibbs”.\n\n\nLet’s start with an example: \\(2 NO_2 \\rightleftharpoons N_2O_4\\), the dimerization of nitrogen dioxide in a sealed tube.\nThe thermodynamic system is some \\(NO_2\\) and \\(N_2O_4\\). The system is sealed in a glass tube of constant volume \\(V\\), bathing in a water-ice mixture of temperature \\(T\\).\nThe thermodynamic state of the system is fully known if we know the number of moles for each species: \\(n_{NO_2}, n_{N_2O_4}\\).\nThe system undergoes a single reaction \\[2 NO_2 \\rightleftharpoons N_2O_4\\]\nSuppose we start the system at state \\(n_{NO_2, 0}, n_{N_2O_4, 0}\\). When does the system reach equilibrium? Since the system can exchange energy, but not volume, with the surrounding bath, it reaches equilibrium when the system reaches minimal Helmholtz free energy under constraint: \\[\n\\begin{cases}\n\\min_{n_{NO_2}, n_{N_2 O_4}} F(T, V, n_{NO_2}, n_{N_2O_4})\\\\\n(n_{NO_2} - n_{NO_2, 0}) = -2 \\xi\\\\\n(n_{N_2O_4} - n_{NO_2, 0})  = \\xi\n\\end{cases}\n\\]\nwhere we write \\(\\xi\\) as the extent of reaction, that is, the number of moles of reactions that has taken place.\nDifferentiating the two equations, and setting \\(dV, d\\beta = 0\\), we have\n\\[\n\\begin{cases}\ndF = \\mu_{NO_2} dn_{NO_2} + \\mu_{N_2O_4} dn_{N_2O_4} \\\\\ndn_{NO_2} = -2d\\xi \\\\\ndn_{N_2O_4} = d\\xi\n\\end{cases}\n\\]\nAt equilibrium, \\(dF = 0\\) under all possible constrained variations, giving us the condition of equilibrium: \\[-2\\mu_{NO_2} + \\mu_{N_2 O_4} = 0\\]\nWe may vary both starting conditions \\(n_{NO_2, 0}\\) and \\(n_{N_2O_4, 0}\\), and for each starting condition, the system would equilibrate at the solution to\n\\[\n\\begin{cases}\n-2\\mu_{NO_2} (T, V, n_{NO_2}, n_{N_2O_4}) + \\mu_{N_2 O_4}(T, V, n_{NO_2}, n_{N_2O_4}) = 0 \\\\\n(n_{NO_2} - n_{NO_2, 0}) = -2 \\xi\\\\\n(n_{N_2O_4} - n_{NO_2, 0})  = \\xi\n\\end{cases}\n\\]\nwhich has exactly the same number of unknowns and equations, so in general it should have a solution.\nThe state space of the system has 2 dimensions: \\(n_{NO_2}\\) and \\(n_{N_2 O_4}\\). Starting at any point in the state space, the system can move on a single line, and it would equilibrate at exactly the point at which its Helmholtz energy is minimized. We can find the point of equilibrium by drawing the surfaces of constant Helmholtz free energy, and find the tangent point, as pictured.\nEconomic interpretation\nEconomically speaking, the situation is precisely equivalent to the standard first problem in consumer theory: Given a consumer with a finite budget and a market for two goods, what would they buy from the market to maximize their utility? (They must spend all their budget.)\nThe answer, as we can see in the diagram, is the tangent point of the straight line of constant budget with the curved lines of constant utility.\nTo anthropomorphize the situation, we can say that the reaction chamber is a consumer, trying to maximize its Helmholtz free entropy under the “budgetary constraint” of \\(2 NO_2 \\rightleftharpoons N_2O_4\\).\nExistence and uniqueness\nBy looking at the picture, we see that obviously, at least one solution exists.\nIn most situations, the Helmholtz free energy is strictly convex, so the lines of constant \\(F\\) are also strictly convex, and so the solution is unique on each line.\nEdge case: reaction reaches completion\nThe solution might fall on one of the axes, in which case the reaction reaches completion, and so it is not a reversible reaction. Economically speaking, it is like when you are poor enough, you might spend all your money buying noodles, and none buying bread.\nEdge case: first-order phase transition\nIf \\(F\\) is not strictly convex, then it might have a double tangent point with the budget line. On that critical budget, there is a first-order phase transition. The system spontaneously splits into two phases, with one side denser in species 1, and the other side denser in species 2. It could look like a mixture of water and oil suddenly condensing into two layers, with one part mostly water and the other part mostly oil.\nWe can imagine adding in a little more of both chemicals at a time into a tube. Below the critical concentration, it is in the phase denser in species 1, above the critical concentration, it is in the phase denser in species 2. Right at the critical concentration, the system splits into two parts, with phase 1 shrinking while the phase 2 growing, until phase 2 takes over the entire tube. Just like the case with boiling water, where temperature remains constant as water turns to steam, in this case, pressure remains constant while phase 1 turns to phase 2.\nEconomically speaking, if we have many households with the same utility function and budget, then at a critical budget level, the households would spontaneously split into two types, with the first type buying mostly bread, and the second type buying mostly noodles.\n\n\n\nThe reaction chamber is a sealed glass tube held under constant temperature of 298.15 K. It has the following \\(\\xi, H\\) curve. At the \\(\\xi = 0\\) side, the tube contains 2 moles of \\(NO_2\\), and at the \\(\\xi = 1 \\mathrm{mol}\\) side, the tube contains 1 mole of \\(N_2O_4\\). (Raff 2014a, fig. 2)\n\n\nNow let’s consider another example, where we have two simultaneous reactions. This is a simplified version of the NOx reactions, which is a source of air pollution.\nConsider a system with the following reactions:\n\n\\(2NO + O_2 \\rightleftharpoons 2NO_2\\)\n\\(NO + NO_2 \\rightleftharpoons N_2O_3\\)\n\nThe system is in a container with constant volume \\(V\\) and temperature \\(T\\). Let \\(n_i\\) represent the number of moles of species \\(i\\). The thermodynamic state of the system is fully described by the 4-component vector \\(\\vec n = (n_{O_2}, n_{NO}, n_{NO_2}, n_{N_2O_3})\\).\nTo make the algebra look cleaner, we rewrite them as follows: 1. \\(0 \\rightleftharpoons -O_2 - 2 NO + 2NO_2 + 0 N_2 O_3\\) 2. \\(0 \\rightleftharpoons  0 O_2 -NO - NO_2 + N_2O_3\\)\nWe see that each reaction can be written as a single vector. The first has vector \\(\\vec n_1 = (-1, -2, 2, 0)\\), and the second has vector \\(\\vec n_2 = (0, -1, -1, 1)\\).\nEach reaction has an associated extent of reaction, denoted by \\(\\xi_1\\) and \\(\\xi_2\\) respectively. Changes in the number of moles for each species are related to the extents of reaction:\n\n\\(dn_{NO} = -2d\\xi_1 - d\\xi_2\\)\n\\(dn_{O_2} = -d\\xi_1\\)\n\\(dn_{NO_2} = 2d\\xi_1 - d\\xi_2\\)\n\\(dn_{N_2O_3} = d\\xi_2\\)\n\nAt equilibrium, the Helmholtz free energy \\(F(T, V, \\vec{n})\\) is minimized under the constraints imposed by the reactions. This leads to the following conditions:\n\n\\(-2\\mu_{NO} - \\mu_{O_2} + 2\\mu_{NO_2} = 0\\) (from reaction 1)\n\\(-\\mu_{NO} - \\mu_{NO_2} + \\mu_{N_2O_3} = 0\\) (from reaction 2) and more succinctly, we have\n\n\\[\\vec \\mu \\cdot \\vec n_j = 0, \\quad j = 1, 2\\]\nwhere \\(\\vec \\mu = (\\mu_{O_2}, \\mu_{NO}, \\mu_{NO_2}, \\mu_{N_2O_3})\\) is the vector of chemical potentials.\nStarting at any initial chemical composition of \\(\\vec n_0\\), the space of all possible chemical compositions reachable from \\(\\vec n_0\\) is a 2-dimensional subset. That is, it is the set of \\(\\vec n\\) satisfying\n\\[\n\\begin{cases}\n\\vec n = \\vec n_0  + \\xi_1 \\vec n_1+ \\xi_2 \\vec n_2,  \\\\\n\\vec n \\geq 0\n\\end{cases}\n\\]\nGeometrically speaking, the subset is the intersection between a 2-dimensional plane and a 4-dimensional pyramid, so it generally looks like either a triangle or a quadrilateral.\nOn this subset, the Helmholtz free energy function looks like a sequence of nested convex shells, and the point of tangency is the equilibrium point.\nInterpreted economically, this is the case of a consumer that maximizes its utility under two simultaneous budgetary constraints (because the budget set is a 2-dimensional, not 3-dimensional, subset of \\(\\R^4\\) ). Perhaps the consumer is trading with a market that simultaneously uses two kinds of currencies – bimetallism?\nGeneralizing from the above two experiences, we immediately obtain the following theorem.\n\nTheorem 6 (existence and uniqueness of chemical equilibrium, at constant volume and temperature)\nConsider a sealed reaction chamber held in an energy bath, so that both the price of energy \\(\\beta\\), and the volume \\(V\\), of the system is fixed.\nThe system contains a homogenous mixture of chemical species \\(A_1, \\dots, A_k\\), which might undergo the following \\(r\\) possible chemical reactions: \\[\n0  \\rightleftharpoons \\sum_i a_{ij} A_i, \\quad j = 1, 2, \\dots, r\n\\]\nThe necessary condition for chemical equilibrium is\n\\[\n\\vec \\mu \\cdot \\vec n_j = 0, \\quad \\forall j = 1, 2, \\dots, r\n\\]\nwhere \\(\\vec n_j = (a_{1, j}, \\dots, a_{k, j})\\) is the vector representing the \\(j\\) -th chemical reaction.\nThe condition is also sufficient if the Helmholtz free energy is strictly concave.\nIf \\(F\\) is not strictly concave, then there could be multiple coexisting equilibrium, which gives us a first-order phase transition.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nExistence: \\(F\\) is continuous, so it has at least one minimum on every compact set.\nUniqueness: any local minimum of a strictly concave function is the unique global minimum.\nFor example, for the same reaction of \\(NO_2\\) dimerization, now put into a flabby plastic bag held under constant temperature of 298.15 K and constant pressure of 1 atm, produces the following \\(\\xi, G\\) curve. At the \\(\\xi = 0\\) side, the bag contains 2 moles of \\(NO_2\\), and at the \\(\\xi = 1 \\mathrm{mol}\\) side, the bag contains 1 mole of \\(N_2O_4\\).\n\n\n\n\n\n\nNow, we can finally answer the problem that defeated me in the past: What is, after all, the Gibbs free energy released per mole of reaction? The answer is in the following diagram:\n\\[\\Delta G = \\int_0^1 (\\partial_\\xi G)_{T, P} d\\xi\\]\n\n\n\n\n\n\nSuch a situation occurs, for example, in the Haber–Bosch industrial method for producing ammonia: \\(N_2 + 3H_2 \\to 2NH_3\\). In the HB method, room-temperature (25 \\(^\\circ C\\) ) and room-pressure (1 atm) nitrogen and hydrogen continuously pipe into the chamber, and ammonia is continuously extracted out of the chamber by cooling liquefaction. When the reaction chamber is operating at a stable state, the energy released per mole of reaction is \\(\\Delta G = -32.8 \\mathrm{kJ/mol}\\), as one can calculate from a table of chemical thermodynamics (Glasser 2016).\n\n\n\n\n\n\n\nPractical considerations\nThe above is all correct, and geometrical. If we were to be like Gibbs, then we would dust off our hands, for there is nothing left to do (well, except the theory of phase transitions). Unfortunately, chemistry is not merely applied geometry, so there is still something left to do.\nDEF.\nA chemical environment is defined by chemical species \\(A_1, \\dots, A_m\\).\nA standard state for a chemical environment is defined by a reference pressure \\(P^\\circ\\), and reference chemical molarities \\([A_1]^\\circ, \\dots, [A_m]^\\circ\\) for each of the the chemical species.\nGiven a standard state for a chemical environment, for any temperature \\(T\\), and any chemical molarities \\([A_1], \\dots, [A_m]\\), the chemical activity of the chemical species \\(A_i\\) in this particular context is\n\\[\\{A_i\\} := e^{\\frac{\\mu_i - \\mu_i^\\circ}{RT}}\\]\nwhere \\(\\mu_i\\) is the chemical potential of species \\(A_i\\) at that state. That is, \\[\\mu_i = (\\partial_{n_i} G)|_{T, P, [A_1], \\dots, [A_m]}\\]\nand \\(\\mu_i^\\circ\\) is the chemical potential of species \\(A_i\\) at the standard state: \\[\\mu_i^\\circ = (\\partial_{n_i} G)|_{T, P^\\circ, [A_1]^\\circ, \\dots, [A_m]^\\circ}\\]\nGiven a chemical reaction \\(0 \\rightleftharpoons \\sum_i a_i A_i\\), its reaction quotient is\n\\[\nQ = \\prod_i \\{A_i\\}^{a_i}\n\\]\nwhere \\(a_i\\) is the stoichiometric number of chemical species \\(A_i\\). For example, with \\(aA + bB \\rightleftharpoons cC + dD\\), its reaction quotient is \\[\nQ = \\frac{\\{C\\}^c\\{D\\}^d}{\\{A\\}^a\\{B\\}^b}\n\\]\n\nTheorem 7 (fundamental theorem of chemical equilibrium)\nGiven any chemical reaction \\(0 \\rightleftharpoons \\sum_i a_i A_i\\), any standard state, and any temperature, \\[\\begin{cases}\n(\\partial_\\xi F)_{T, V} &= (\\partial_\\xi F)_{T, V}^\\circ + RT \\ln Q \\\\\n(\\partial_\\xi G)_{T, P} &= (\\partial_\\xi G)_{T, P}^\\circ + RT \\ln Q\n\\end{cases}\n\\]\nwhere \\(\\xi\\) is the extent of reaction, and \\(Q\\) is its reaction quotient.\nIf the system has \\(r\\) possible reactions, then we similarly have \\[\\begin{cases}\n(\\partial_{\\xi_j} F)_{T, V} &= (\\partial_{\\xi_j} F)_{T, V}^\\circ + RT \\ln Q_j \\\\\n(\\partial_{\\xi_j} G)_{T, P} &= (\\partial_{\\xi_j} G)_{T, P}^\\circ + RT \\ln Q_j\n\\end{cases}\n\\]\nfor each reaction \\(j = 1, 2, \\dots, r\\)\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\\begin{aligned}\n(\\partial_\\xi F)_{T, V} &= \\sum_i (\\partial_\\xi n_i) (\\partial_{n_i} F)_{T, V, \\vec n} \\\\\n&= \\sum_i a_i \\mu_i \\\\\n&= \\sum_i a_i (\\mu_i^\\circ + RT \\ln \\{A_i\\}) \\\\\n&= \\sum_i a_i (\\mu_i^\\circ) + RT \\ln \\left(\\prod_i \\{A_i\\}^a_i\\right) \\\\\n&= (\\partial_\\xi F)_{T, V}^\\circ + RT \\ln Q\n\\end{aligned}\\]\nThe proof for the other equations are very similar.\n\n\n\nCOR. (equilibrium coefficient)\nAt equilibrium, \\[\n(\\partial_\\xi G)_{T, P} = 0\n\\]\nwhich is equivalent to \\[\nQ = K_{eq}, \\quad K_{eq} := e^{-\\frac{(\\partial_\\xi G)_{T, P}^\\circ}{RT}}\n\\]\nand similarly for the other case.\n\n\n\n\nThe reaction chamber is a flabby plastic bag held under constant temperature of  and constant pressure . It has the following  curve. At the  side, the tube contains  of , and at the  side, the tube contains  of . The true meaning of  is the difference in height of the curve on two ends of the  curve. The reaction chamber, demonstrating the concrete meaning of . The above equations are what my teachers meant when they so thoughtlessly wrote\n\\[\\Delta G = 0, \\quad K_{eq} = e^{-\\frac{\\Delta G^\\circ}{RT}}\\]\nThis, finally, answers my great confusion back then. Now everything makes sense, and life is beautiful.\n\nCase of ideal gases\nWell, if this is all there is, then a mathematician would be able to solve any problem in analytical chemistry. Unfortunately, analytical chemistry is not about proving theorems, but about actually getting numerical answers, and numerical answers require numerical values for chemical activities.\nThere are generally three cases:\nFirst case: We have a mixture of dilute gasses, or dilute solvents in an inert solution, such that the ideal gas law is almost true.\nSecond case: Ideal gas law fails.\nThird case: We are not even dealing with gasses and solutions anymore.\nThe first case is typically what is taught by a first course in analytical chemistry, and since this is typically taught to non-mathematicians by non-mathematicians for non-mathematicians, the logical structure is quite upside-down and confusing to a mathematician (speaking from experience here – I consider myself a mathematician before high school).\nWe will now prove the first case rigorously.\n::: {#thm-0}\n(activity of ideal-gas-like substances)\nFor any temperature \\(T\\) and any two pressures \\(P, P^\\circ\\), by the ideal gas laws, the chemical potential of the chemical species satisfies the equation\n\\[\\mu(T, P) = \\mu(T, P^\\circ) + RT \\ln\\frac{P}{P^\\circ}\\]\nand so its activity is \\(\\frac{P}{P^\\circ}\\).\nIn a mixture of ideal gasses, the gasses do not interact, and so the activity of chemical species \\(A_i\\) is \\(\\{A_i\\} = \\frac{P_i}{P_i^\\circ}\\), where \\(P_i\\) is the partial pressure of species \\(A_i\\) in the mixed gas, and \\(P_i^\\circ\\) is the standard pressure for species \\(A_i\\).\nIn a dilute solution, if the solvent behaves like a mixture of ideal gasses, then\n\\[\\mu(T, P) \\approx \\mu(T, P^\\circ) + RT \\ln \\frac{[A_i]}{[A_i]^\\circ} \\approx \\mu(T, P^\\circ) + RT \\ln \\frac{m_{A_i}}{m_{A_i}^\\circ}\\]\nwhere \\([A_i]\\) is the mole-per-volume of \\(A_i\\), and \\(m_{A_i}\\) is the mole-per-mass of \\(A_i\\).\nThe chemical activity simplifies into the familiar form:\n\\[\\{A\\} \\approx \\frac{[A]}{[A]^\\circ} \\approx \\frac{m_A}{m_A^\\circ}\\]\nDER.\nIt suffices to prove the case for a pure ideal gas, as the other cases are simple corollaries.\nBy the ideal gas law, the chemical potential is\n\\[\\mu = -TS/n\\]\nwhich is a state property. Expressed as a function of \\(T, P\\), \\[\\mu(T, P) = RT \\ln\\frac{P/T^{\\hat c_V + 1}}{C}\\]\nfor an arbitrary constant \\(C\\).\nThus, for any \\(T, P, P^\\circ\\), we have\n\\[\\mu(T, P) = \\mu(T, P^\\circ) + RT \\ln\\frac{P}{P^\\circ}\\]\n::: {#exm-0}\nThe pH value of a solution is not \\(pH = -\\log_{10} [H^+]\\), which has the wrong units. It is not even \\(pH = -\\log_{10} \\frac{[H^+]}{[H^+]^\\circ}\\), since the \\(H^+\\) particles might not behave like an ideal gas. The actually correct definition is\n\\[pH = -\\log_{10}  \\{H^+\\}\\]\n(McCarty and Vitz 2006)\n\n\nFugacity\nFor real gases and real solutions, the chemical activity might deviate significantly from the above approximation. In this case, we typically have no recourse except by checking a table of chemical thermodynamics. They typically don’t directly write down the chemical activities, but fugacity coefficients. There is nothing particularly deep about fugacity – it is basically about rescaling the numbers to make the tables easier to make.\nRecall that the chemical potential of an ideal gas satisfies \\(\\mu(T, P) = RT \\ln\\frac{P/T^{\\hat c_V + 1}}{C}\\), where \\(C\\) is a constant for this gas. For a real gas, this equation only holds approximately, so we define the fugacity \\(f\\) as a function of \\(T, P\\), such that \\[\\mu(T, P) = RT \\ln\\frac{f/T^{\\hat c_V + 1}}{C}\\]\nIn other words,\n\\[f(T, P) = P \\phi(T, P)\\]\nwhere\n\\[ \\phi(T, P) = e^{\\frac{\\mu(T, P) - \\mu_{ideal}(T, P)}{RT}}\\]\nis the fugacity coefficient.\nPlugging them back to the definition of activity, \\[\\{A\\} = \\frac{f}{f^\\circ} = \\frac{\\phi P}{\\phi^\\circ P^\\circ}\\]\nAnd so, by checking a table of fugacity coefficients, chemical engineers can balance chemical reactions of real gasses, even far from ideality.\n\n\n\n\n\n\nStandard state\n\n\n\nDespite what the name “standard” might imply, a chemical species has infinitely many standard states. For example, pure gaseous oxygen has many different standard states – one for each temperature. We have a standard state at \\(T = 300\\mathrm{K}\\) defined by \\([O_2] = 1 \\mathrm{mol/L}\\), and another at \\(T = 350\\mathrm{K}\\) defined by \\([O_2] = 1 \\mathrm{mol/L}\\), etc.\nDespite what the name “standard” might imply, different chemists have different standards. For example, among the biochemists, the standard state for \\(H^+\\) in water is \\([H^+]^\\circ = 10^{-7} \\mathrm{mol/L}\\), but among the inorganic chemists, it is \\([H^+]^\\circ = 1 \\mathrm{mol/L}\\). The reason is that bodily fluids typically have \\([H^+] \\sim 10^{-7} \\mathrm{mol/L}\\).\nDespite what the name “standard temperature and pressure (STP)” might imply, it is not a “standard state”, because a “standard state” of any substance does not specify its temperature.\n\n\nThe point of having a standard state is like taking an electric circuit, and pointing at one point of it and say, “This is where the voltage is zero”. The point is to allow relative comparisons between states, within the context of a single reaction. Consequently, even for a single chemical species, we can take a different standard state if we are studying a different reaction involving the species, or the same reaction in a different context.\nFor example, if we are studying the reaction \\(NO_2 \\rightleftharpoons N_2 O_4\\) in a glass tube drenched in an ice-water bath, then we would take as our standard state \\[T^\\circ = 273.15 K, \\quad [NO_2]^\\circ = 1 \\mathrm{mol/L}, \\quad [N_2 O_4]^\\circ = 1 \\mathrm{mol/L}\\]\nFor a chemical in pure gaseous form, a standard state is specified by two out of three parameters: molarity \\([A] = \\frac{n}{V}\\), pressure \\(P\\), temperature \\(T\\). We must never specify all three of them, because otherwise we would break the equation of state. For example, imagine what happens when you specify that the “standard state of ideal gas” is\n\\[T^\\circ = 273.15 \\mathrm{K}, P^\\circ = 10^5 \\mathrm{Pa}, [A]^\\circ = 1 \\mathrm{mol/L}\\]\nbecause they would violate the ideal gas law \\[P = [A]RT\\]\nFor non-ideal gas, we still have an equation of state between \\(P, [A], T\\), meaning that we still must specify exactly two, no more and no less.\n\n\n\n\n\n\nIntensive quantities\n\n\n\nWhy is a standard state defined by intensive quantities like temperature, pressure, or molarity? Why isn’t it defined by extensive quantities like volume, mass, and moles?\nThe short answer: because classical thermodynamics only studies systems with extensive entropies. For those systems, chemical equilibrium is determined by intensive quantities.\nLike classical thermodynamics, and neoclassical economics, the idea of a standard state is fully committed to the idea of homogeneous substances. In classical thermodynamics, a cube of iron and a ball of iron are the same. A jar of water and a tank of water are the same. It does not matter what their shapes are. Moreover, two jars of water side-by-side is the same as one large jar of water. In neoclassical economics, a crowd of factories is the same as two small crowds of factories put together. They are all chunks of homogeneous stuffs.\nIf it were not the case, then we would be unable to say that a standard state is defined by just its temperature and “moles-per-liter” of each chemical species. We would be forced to also specify a standard state volume \\(V^\\circ\\). It’s conceivable that even the shape of the reaction chamber matters. We would then be forced to specify a standard shape, perhaps a box with side lengths \\(0.1 \\mathrm{m}\\). But in this extreme case, perhaps we have already left the realm of chemistry.\n\n\nDEF. official definition of “standard state” (Cox 1982)\nAt any particular temperature, we define the standard state of any liquid or solid substance to be the most stable form of that substance at a pressure of one bar.\nFor a pure substance the concept of standard state applies to the substance in a well-defined state of aggregation at a well-defined but arbitrarily chosen standard pressure.\nHistorically, the defined pressure for the standard state, i.e., the standard-state pressure, has been 1 standard atmosphere (101 325 Pa) and most existing data use this pressure. With the growing use of SI units continued use of the atmosphere is inconvenient and in some countries now illegal. It is recommended that thermodynamic data should be reported for a defined standard-state pressure of \\(10^5\\) Pa which is equal to 1 bar.\nThe standard-state pressure in general is symbolized as \\(P^\\circ\\). Hitherto \\(P^\\circ\\) has customarily been taken as 1 atm. For the future it is recommended that \\(P^\\circ\\) should customarily be taken as \\(10^5\\) Pa (1 bar). It should be understood that the present recommended change in the standard-state pressure carries no implication for “standard pressures” used in other contexts, e.g. the convention that “normal boiling points” refer to a pressure of 101 325 Pa (1 atm).\nHence,the standard state for a gaseous substance, whether pure or in a gaseous mixture, is the pure substance at the standard-state pressure and in a (hypothetical) state in which it exhibits ideal-gas behaviour.\nThe standard state for a pure liquid substance is (ordinarily) the pure liquid at the standard-state pressure.\nThe standard state for a pure solid substance is (ordinarily) the pure solid substance at the standard-state pressure.\nThe above definitions of standard states make no reference to fixed temperature. Hence, it is possible to have an infinite number of standard states of a substance as the temperature varies. But generally it is more convenient to complete the definition of the standard state in a particular context by choosing for the reference temperature one of a relatively small number of values, e.g., zero, \\(273.15 \\mathrm{~K}, 293.15 \\mathrm{~K}, 298.15 \\mathrm{~K}\\). The most favoured of these, and the one recommended for use is \\(298.15 \\mathrm{~K}\\). Because the quantity “298.15 K” is so frequently used in symbols such as \\(\\Delta_{\\mathrm{f}} \\mathrm{H}^{\\circ}(298.15 \\mathrm{~K})\\) and because it requires so much space, it is convenient to have a special symbol for this quantity… Since \\(T^{\\circ}\\) should mean a standard temperature in general, the use of \\(T^{\\circ}\\) to mean exclusively \\(298.15 \\mathrm{~K}\\) is strongly discouraged.\nNo recommendation for a special symbol for \\(298.15 \\mathrm{~K}\\) is made at this time; any author who prefers to use a special symbol for \\(298.15 \\mathrm{~K}\\) is urged always to define it. It must be stressed, however, that there is no reason why another value (say \\(427.9 \\mathrm{~K}\\) ) should not be adopted for the reference temperature, so long as the author clearly states what has been done.\nFor application of the concept of standard state to substances in admixture (solutions and mixtures), the composition of the system, as well as the pressure, must be defined. As one example for solutions, the standard-state molality, written as \\(m^\\circ\\) for the general case, is to be defined; customarily \\(m^\\circ\\) is taken as 1 mol/kg."
  },
  {
    "objectID": "blog/posts/equilibrium-thermoeconomics/index.html#phase-equilibrium",
    "href": "blog/posts/equilibrium-thermoeconomics/index.html#phase-equilibrium",
    "title": "Classical thermodynamics and economics",
    "section": "Phase equilibrium",
    "text": "Phase equilibrium\nAgain and again, we have found some curious examples where a non-convexity leads to a jump of some kind. It would be good to study it carefully. These are all examples of first-order phase equilibrium.\n\nTwo phases of a gas in equilibrium\nConsider a generic gas, whose entropy function is of form \\(S(U, V, N)\\). If we confine it in a sealed tube, and slowly heat it up, then its entropy would trace out the curve\n\\[U \\mapsto S(U, V, N)\\]\nNow, the inverse temperature \\(\\beta\\) of the system is the slope, which should decrease as \\(U\\) increases, so the entropy curve should be strictly concave.\nIf there is a bump in the middle, then we have a serious problem: as we heat up the gas, its temperature would decrease for a while before increasing again! This clearly is a thermodynamic instability, and suggests to us that our model has broken down. Where is the breakdown?\nThe breakdown is that we assumed our system remains one thermodynamic substance, when it can split into two. Suppose then, the substance splits into two, like a large company splits into two subsidiaries under a common conglomerate. How would the manager maximize the total value of the conglomerate?\n\\[\n\\begin{cases}\n\\max S_1(U_1, V_1, N_1) + S_2(U_2, V_2, N_2) \\\\\nU_1 + U_2 = U \\\\\nV_1 + V_2 = V \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nwhere we, instead of writing \\(S(U_1, V_1, N_1) + S(U_2, V_2, N_2)\\), write \\(S_1(U_1, V_1, N_1) + S_2(U_2, V_2, N_2)\\), to emphasize that we now have two thermodynamic systems that might have very different behavior, like water vs ice.\nDifferentiating, we find that the marginal value of each asset is equal in both subsidiaries, as we would expect\n\\[\n\\begin{cases}\n\\beta_1 = \\beta_2,\\\\ \\beta_1 P_1 = \\beta_2 P_2, \\\\-\\beta_1 \\mu_1 = -\\beta_2 \\mu_2\n\\end{cases}\n\\]\nThat is, the two lumps of substances have the same temperature, pressure, and chemical potential.\nSince both sides have the same temperature and pressure, it is cleaner to change to Gibbs free energy, yielding: \\[(\\partial_{N} G_1)_{T, P}(T, P, N_1) = (\\partial_{N} G_2)_{T, P} (T, P, N_2)\\]\n\n\n\n\n\n\nThe diamond water paradox, and thinking on the margins\n\n\n\nTypical textbooks on thermodynamics illustrate the phase equilibrium rule using the van der Waals equation. However, there is a subtlety involved. For the van der Waals gas, the Gibbs free energy \\(G\\) is proportional to particle number:\n\\[G(T, P, N) \\propto N\\]\nwhich means that \\((\\partial_{N} G)_{T, P}(T, P, N) = G(T, P, N) / N\\). In economic language, this states that:\n\\[\\text{marginal Gibbs per particle} = \\text{average Gibbs per particle}\\]\nIn fact, confusing the two numbers is the root of the diamond-water paradox. This paradox questions why water, essential for life, has a low price, while diamonds, with little practical use, have a high price. The resolution lies in understanding the difference between total and marginal utility. While the total utility of water is immense, the marginal utility of an additional unit of water is low due to its abundance. Conversely, the marginal utility of a diamond remains high due to its scarcity.\nIn neoclassical economics, it is the marginal value of a commodity that determines the market equilibrium, not its average value. Similarly, in thermodynamics, it is the change in Gibbs free energy when adding one more particle that determines the equilibrium state, not the average Gibbs free energy per particle.\nThe distinction is moot in typical books on classical thermodynamics, which insists that entropy is extensive, so the above equation is always true. However, classical thermodynamics, much like neoclassical economics, is perfectly capable of handling non-extensive entropy, and it seems Lord Kelvin had studied this (Lavenda 2010).\n\n\nNow, it also happens that in most classical thermodynamics systems, such as water and steam, the marginal free Gibbs energy is identical with the average Gibbs free energy. However, classical thermodynamics is perfectly capable of handling the other cases (the phrase to search is “non-extensive entropy”).\nAssuming, as usual, that\n\\[(\\partial_{N} G)_{T, P}(T, P, N)= G(T, P, N)/N\\]\nthis means that phase equilibrium occurs at \\(g_1(T, P) = g_2(T, P)\\).\nWe can reinterpret this as follows: We delicately separate the two lumps of matter, and immerse each half in an energy-and-volume bath (like the atmosphere) with the same temperature and pressure. The only interaction between the two lumps of matter is that one side can “seep” some particles to the other side. In this set-up, the system minimizes the sum of Gibbs free energy. At equilibrium, there is no point in moving particles from one side to another, because the marginal Gibbs free energy on either side.\nEconomically, it means that we have separated the conglomerate into two child companies, and instead of a single conglomerate-boss that dictates how much space each company could get, we instead give each child company a single boss, and open up a market for space and energy. The two companies can trade particles with each other, but not space or energy.\n\n\nvan der Waals gas\nWe know what the van der Waals gas phase diagram looks like. How do we infer its Gibbs free energy diagram? Start with \\(dG = -SdT + VdP + \\mu dN\\). Now, let us fix temperature \\(T\\) and particle number \\(N\\). Then, the equation implies to \\[\\frac{dg}{dP} = v\\]\nwhere \\(g = G/N\\) is the average Gibbs free energy, and \\(v = V/N\\) is the average volume.\nTherefore, we can trace the pressure-volume diagram with our finger, from high pressure, down to the valley of pressure, then bounce back to a hill, before rolling down the slope towards infinity. At every point, the \\(g(P)\\) curve would have a slope of \\(v\\). This allows us to graphically construct the following \\(g(P)\\) curve. It has two cusps corresponding to the valley and hilltop, and a self-intersection, corresponding to the phase equilibrium of \\(g_1 = g_2\\).\n\n\n\nGibbs free energy of van der Waals gas. Figure source.\n\n\n\n\nGibbs phase rule\nGenerally, \\(g_1(T, P) \\neq g_2(T, P)\\). When \\(g_1 &lt; g_2\\), every particle would switch to phase 1. When \\(g_1 &gt; g_2\\), every particle would switch to phase 2. At exactly a knife’s edge, the particles are indifferent as to which phase they would go to.\n\n\n\n\n\n\nInterpretation: corporate buyout in an ideal world\n\n\n\nWe have two companies such that they can exchange their human-particles, and that there is neither economies nor diseconomies of scale (that is, as the company grows ever larger, an extra worker neither provides more nor less value than its very first worker). Then, in general, the two companies balance on a knife’s edge. If the value of a worker is even slightly greater in one company than another, then that company would immediately buy out every worker from the other company, and so the two companies cannot possibly coexist. Only when the market prices for space and energy happen to conspire just right, can the two companies coexist, neither side buying out the other side.\n\n\n\nDegrees of thermodynamic freedom\nConsider a chunk of (nonideal) gas. We know everything there is to know about it if we know its \\((U, V, N)\\). Every other thermodynamic quantity can be computed by its entropy function \\(S(U, V, N)\\). Thus, we have a system with three degrees of thermodynamic freedom… or do we?\nThe problem is that entropy of nonideal gas, and just about every other system studied in classical thermodynamics, is extensive. Therefore, we have\n\\[S(U, V, N) \\propto N\\]\nand so we don’t actually have three degrees of freedom!\nSpecifically, we can calculate its \\((\\partial_U S, \\partial_V S, \\partial_N S)\\), which gives us \\(\\beta, \\beta P, -\\beta \\mu\\). If we truly have three degrees of freedom, then we should be able to vary \\(\\beta, P, \\mu\\) independently. However, because entropy is extensive, we have\n\\[S(U, V, N) = Ns(u, v) \\implies (\\beta, \\beta P, -\\beta \\mu) = (\\partial_u s, \\partial_v s, s)\\]\nwhere \\(s(u, v) = S(U, V, N)/N\\) is the entropy per particle.\nTherefore, we can say that there are only two degrees of thermodynamic freedom: knowing two of its intensive quantities, the third would be determined by an equation of state.\nSimilarly, if we have a chunk of (nonideal) substance, like sea water, made of \\(k\\) different chemicals, then we know everything there is to know about it if we know its \\(U, V, N_1, \\dots, N_k\\), giving us \\(2+k\\) degrees of freedom. Again, because entropy is extensive, one degree of freedom is degenerate, and so we only have \\(1 + k\\) degrees of freedom. In other words, its \\(2+k\\) intensive quantities\n\\[\\beta, \\beta P, -\\beta\\mu_1, \\dots, -\\beta \\mu_k\\]\nare related by 1 equation of state.\n\n\n\nGibbs phase rule\n\nTheorem 8 (Gibbs phase rule) \\[F = 2 + C - R - P\\]\n\nWe have a closed and adiathermal reaction chamber, containing \\(C\\) different chemical species, that can undergo \\(R\\) linearly independent chemical reactions.7\n7 The formula looks like the Euler formula for polyhedra, but whether this analogy is more than a coincidence is controversial. After looking into the literature for a bit, my conclusion is that it is a coincidence. However, if you wish to investigate on your own, the phrase to search is “Gibbs phase rule, Euler”. This turns up some amusing examples, like (Sun, Powell-Palm, and Chen, n.d.).When the system is in an equilibrium, the chamber would contain \\(P\\) different phases. Each phase would be homogeneous, but different from the other phases. All phases can exchange energy, volume, and particles.\n\n\n\nPhases in equilibrium inside a chamber. (Blankschtein 2020, fig. 27.2)\n\n\nFirst, let’s consider the case where \\(R = 0\\). That is, there can be no chemical reaction. In this case, the constrained optimization problem is\n\\[\n\\begin{cases}\n\\max(S_1(U_1, V_1, \\vec N) + \\cdots + S_P(U_P, V_P, \\vec N_P) \\\\\n\\sum_i U_i = U \\\\\n\\sum_i V_i = V \\\\\n\\sum_i \\vec N_i = \\vec N\n\\end{cases}\n\\]\nNaively, we can just differentiate the entropies against each of the \\(2+C\\) parameters, to obtain equations\n\\[\n\\begin{aligned}\nT_1 = \\dots &= T_P \\\\\nP_1 = \\dots &= P_P \\\\\n\\mu_{1, 1} = \\dots &= \\mu_{1, P}\\\\\n& \\vdots \\\\\n\\mu_{C, 1} = \\dots &= \\mu_{C, P}\n\\end{aligned}\n\\]\nThis is not actually correct. Phase 1 might contain no chemical 2, and phase 2 might contain no chemical 1, 3, etc. In general, if phase \\(i\\) contains chemical \\(j\\), then we must have \\(\\partial_{N_j}S_i = \\mu_j\\). However, if phase \\(i\\) contains no chemical \\(j\\), then we only need to have \\(\\partial_{N_j}S_i &gt; \\mu_j\\).\nNote that this is different for temperature or pressure. A phase \\(i\\) might have no chemical of type \\(j\\), but if it have no volume, then it does not exist at all. Similarly for energy. Therefore, though the chemical potentials might differ, the temperature and pressure must be exactly the same.\nDefine \\(\\mu_j := \\min_i \\mu_{i, j}\\) to be the minimal chemical potential over the entire chamber. We have the following conditions:\n\\[\n\\begin{aligned}\nT_1 = \\dots = T_P &= T \\\\\nP_1 = \\dots = P_P &= P \\\\\n\\mu_{1, 1}, \\dots, \\mu_{1, P} &\\geq \\mu_1\\\\\n& \\vdots \\\\\n\\mu_{C, 1}, \\dots, \\mu_{C, P} &\\geq \\mu_C\\\\\n\\end{aligned}\n\\]\nGiven \\(T, P, \\mu_1, \\dots, \\mu_C\\), phase 1 is entirely determined: If it contains chemical \\(j\\), then \\(\\mu_{1, j} = \\mu_j\\), otherwise, we need not bother with \\(\\mu_{1, j}\\). Similarly, every phase is determined.\nFinally, each phase contributes an equation of state, which are in general linearly independent, giving us \\(F = 2 + C - P\\) degrees of freedom.\nIf we now allow a chemical reaction \\(0 \\rightleftharpoons \\sum_j a_j A_j\\), then the constrained optimization problem becomes\n\\[\n\\begin{cases}\n\\max(S_1(U_1, V_1, \\vec N) + \\cdots + S_P(U_P, V_P, \\vec N_P) \\\\\n\\sum_i U_i = U \\\\\n\\sum_i V_i = V \\\\\n\\sum_i \\vec N_i = \\vec N + \\xi \\vec a\n\\end{cases}\n\\]\nThe extra optimization variable \\(\\xi\\) creates an extra condition for optimality:\n\\[\\sum_j a_j \\mu_j = 0\\]\nso \\(F = 2 + C - P - 1\\).\nPossibly, the system cannot satisfy \\(\\sum_j a_j \\mu_j = 0\\), and so the chemical reaction would keep happening until one chemical is exhausted. This would decrement \\(C\\) by one, so it all works out self-consistently.\nMore generally, if we impose \\(R\\) linearly independent chemical reactions, then \\(F = 2 + C - P - R\\).\n\n\n\n\n\n\nBeyond the Gibbs phase rule\n\n\n\nWhen the phases are not free to exchange particles, energies, volumes, etc, then the Gibbs phase rule does not apply, but the same idea of constrained minimization still applies. There are no generic rule like the Gibbs phase rule, and one must analyze each case specifically. (Blankschtein 2020)\n\n\n\nSome basic examples.\n\n\n\n\n\n\n\n\nsituation\ncomponents \\(C\\)\nphases in equilibrium \\(P\\)\ndegrees of freedom \\(F = 2 + C - P\\)\n\n\n\n\nice\n1\n1\n2\n\n\nboiling water\n1\n2\n1\n\n\nliquid water with a little nitrogen inside, gaseous nitrogen with a little water vapor inside\n2\n2\n2\n\n\ntriple point\n1\n3\n0\n\n\nNb-Ta-C alloy\n3\n1\n4\n\n\n\nIn materials science, such as metallurgy, we often fix the pressure of the entire thing to just 1 atm, and so the phase diagrams have one less degree of freedom than what the Gibbs phase rule states.\n\n\n\nPhase diagram for Nb-Ta-C alloy at constant pressure \\(P = 1 \\mathrm{atm}\\). (West and Saunders 2002, fig. 8.1)\n\n\n\nBoiling water in a sealed tube\nBoiling water in an open pot, we need to specify exactly both temperature and pressure so that both phases can coexist. The \\((P, T)\\) of the system would start at \\((1\\;\\mathrm{atm}, 372\\;\\mathrm{K})\\), then at exactly at the critical point \\((1\\;\\mathrm{atm}, 373.15\\;\\mathrm{K})\\) would both phases coexist, not increasing in temperature until all water has turned to steam. However, if we seal it in a tube, then the \\((P, T)\\) of the system would hug the line of water-steam coexistence, like a negative-feedbacked system following a predetermined path. How can we see this difference mathematically?\nIn the case of an open pot, the constrained optimization problem is\n\\[\n\\begin{cases}\n\\min (g_1(T, P)N_1 + g_2(T, P)N_2) \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nWe see that the problem is very rigid: We have to minimize a linear function subject to a linear constraint. As always in linear programming, the solution is in general on an extreme vertex on the very edge of the feasible set – all or nothing, all liquid or all gas. Only by carefully tuning \\(T, P\\) can we find an interior solution – a solution that falls between the vertices, neither all liquid nor all gas.\n\n\n\nAs we increate \\(T\\), the contours of constant Gibbs free energy are parallel lines rotating around. Only when the lines are precisely parallel to the \\(N_1 + N_2 = N\\) is it possible for both phases to coexist.\n\n\nIn the case of a sealed tube, assuming that Helmholtz free energy is proportional to particle number, then the equilibrium is reached at\n\\[\n\\begin{cases}\n\\min (f_1(T, v_1)N_1 + f_2(T, v_2)N_2) \\\\\nv_1 N_1 + v_2 N_2 = V \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nwhere \\(N\\) is the total number of particles, and \\(v_i\\) are the volume-per-particle of liquid and gaseous water. In this case, we are performing a minimization in \\(\\R^4\\), with 1 linear constraint \\(N_1 + N_2 = N\\), and 1 nonlinear constraint \\(v_1 N_1 + v_2 N_2 = V\\). Furthermore, the objective is also nonlinear. The result is that the solution does not in general fall on a vertex – that is, in general, both \\(N_1, N_2 &gt; 0\\). And this is why when we boil water in a sealed tube, it remains boiling over a wide range of temperatures, but when we boil water in an open tube, it only boils at a single temperature.\nSuppose that the entropy is nonextensive, then the Gibbs free energy is also nonextensive. In particular, we can no longer write\n\\[\n\\begin{cases}\n\\min (g_1(T, P)N_1 + g_2(T, P)N_2) \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nbut we have to write\n\\[\n\\begin{cases}\n\\min (g_1(T, P, N_1)N_1 + g_2(T, P, N_2)N_2) \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nbecause \\(G_2(T, P, N_2)\\) is no longer proportional to just \\(N_2\\). Fundamentally, this happens because\n\\[S(\\text{two chunks of steam merged}) \\neq 2 S(\\text{one chunk of steam})\\]\nThe effect is that we have a nonlinear optimization problem, allowing interior solutions over a larger region of \\((T, P)\\) parameters. This explains our previous comment on nonextensive Gibbs free energy.\n\n\n\nAs we increate \\(T\\), the contours of constant Gibbs free energy are curved lines rotating around. Now it is possible for for both phases to coexist over an entire 2D region of \\((P, T)\\)."
  },
  {
    "objectID": "blog/posts/equilibrium-thermoeconomics/index.html#sec-stereodynamics",
    "href": "blog/posts/equilibrium-thermoeconomics/index.html#sec-stereodynamics",
    "title": "Classical thermodynamics and economics",
    "section": "Stereodynamics",
    "text": "Stereodynamics\n\nBased on Liu Cixin’s sci-fi story Mountain (2006)\n\nDie Raum der Welt ist konstant. Die Entropie der Welt strebt einem Maximum zu.\nUeber verschiedene für die Anwendung bequeme Formen der Hauptgleichungen der mechanischen Raumtheorie. Rudolf Klausius. Annalen der Physik und Chemie., Vol. 125 No. 7 (32850): 353–400.\n\n\nSolid Universe Theory\nOur world was a spherical space completely surrounded by solid rock. There is no air or liquid inside. Indeed, we have not encountered any air or liquid until the last days of the Age of Exploration.\nThe first physical law we understood, in the prehistoric past, was the conservation of space. Space in the Bubble World was a sphere roughly 6000 km in diameter. Digging tunnels into the layers of rock did nothing to increase the amount of available space; it merely changed the shape and location of the already existing space. Because of this, space was the most treasured commodity of the Bubble World. The entire history of our civilization was one long and bloody struggle for space.\nWe are a mechanical life form. Our muscles and bones are made of minerals and alloys; our brains are electronic chips, and electricity and magnetism are our blood. We ate the radioactive rocks of our world’s core and they provided us with the energy we needed to survive. In our world, life evolved from single-celled electromechanical life, when the radioactive energies formed P-N junctions in the rocks.\nOn the rock walls there are radioactive spots, which irradiates luminescent rocks, creating spots of light like stars in a rocky night sky. These are the only natural sources of light in our world, and allowed us to evolve eyes. There is no gravity inside the bubble. Without gravity, we built our cities floating in space. From afar, they looked like dimly glowing red clouds.\nWe assumed that the universe was made of two parts. The first was the empty space in which we lived; the second was the surrounding layers of rock. We believed the rock to stretch endlessly in all directions. Therefore, we saw our world as a hollow bubble in this sold universe and so we gave our world the name “Bubble World”. We call this cosmology the Solid Universe Theory.\n\n\nFrom the Closed World to the Infinite Universe\nThe search for other bubbles began in earliest antiquity. We had spun many fascinatingly alluring myths around these distant spaces and almost all of our literature dealt with the fantasy of other bubbles. We explored the rock in cylindrical “bubble ships”. In front, the explorers chipped off solid rock, while in the back, the explorers compacted the rubble back to solid rock. In this way, the bubble ship moved through solid rock like a worm.\nEvery mission meant a bubble ship-sized pile of debris in our core space and we would have to wait for the ship to return before we could return those rocks into the wall. If the bubble ship failed to return, this small pile would mean another small piece of space lost to us forever. Soon, exploration was forbidden on pain of death by short-circuiting. Despite this, the urge for space drove many to secretly sail off illegally.\nOne day, an illegally launched bubble ship that set out eight years ago had returned. The ship had dug 200 km deep into the rock. No other ship had ever made it as far. It returned with rock samples labelled by depth. By measuring the mass of the rocks on an inertial balance, scientists discovered that the rocks’ density decreased.\nEncouraged by the discovery, legions of bubble ships journeyed forth in all directions, penetrating deeper than ever, and returned with rock samples. It turned out that rock density decreased as a function of depth, and independent of direction.\nIt stood to reason that it would eventually reach zero. Using the gathered data, scientists predicted that this would happen at a distance of about 40,000 km. This led to the Open Universe Theory, where our world is a hollow rock shell, about 40,000 km thick, floating in infinite space.\n\n\n\nIn your planet of Earth, which is not hollow like ours, the density decreases according to a similar function. (Stacey and Davis 2020)\n\n\n\n\nWar of the Strata\nAfter the Open Universe Theory had fully established itself, the quest for the infinite space outside became our only real concern. Massive piles of rock, dug out by the fleets of bubble ships, soon came to fill the core space. This debris began to drift around our cities in vast, dense clouds.\nOur cities floated in space, with no defensible geographical separations. Because of this, our world was unified in a World Government early on. The World Government began building gigantic bubble ships designed to intercept, attack, and destroy the explorers’ vessels deep within the rock. The government’s ships would then retrieve the space that had been stolen. This plan naturally met with the resistance of the explorers and so the long drawn-out War of the Strata broke out, fought in the vast battlefield of layers of rock.\nA battleship was built to be very long and thin. Long, because the longer it is, the more volume it can contain. Thin, because the thinner it is, the smaller the area of rock that the ship would need to dig through, and the faster the ship would be able to move.\nWhen a ship encountered the enemy, its first course of action was to dig out a wide bow, to present the largest possible front of weaponry to bear. It could also split out into multiple sections, like a nail-head with needles on top, to attack from multiple directions at once.\nEvery warship could separate at will, transforming into multiple smaller ships. Ships could also combine to a single, giant ship. Whenever opposing battle groups met, the question whether to form up or split up was an object of profound tactical analysis.\nSeismoscopes were invented, and could be used to communicate through the layers of rock and to detect enemy ships like a radar. Directed seismic wave generators were also used as weapons. The most sophisticated seismic communication devices could even transmit pictures.\nBeing outmatched by the warships launched by the World Government, the explorers formed the Explorer Alliance. They gradually gained initiative, and finally launched a devastating attack on the armada. In the final phase of the attack, the 200-km battlefield had become honeycombed beyond recognition by loosened rock and empty space left by destroyed ships.\n\n\nThe Starry Sky\nAfter the battle, the Explorer Alliance gathered all the space left over by the battle into a single sphere 100 km in diameter. In this new space the Alliance declared its independence from the Bubble World. A constant stream of explorer ships left the core to join the Alliance, bringing considerable amounts of space with them. In this way, our world split into two worlds. The Alliance launched more ships, coming closer and closer to the predicted edge of the rock shell.\nFinally, a bubble ship Helix was the first to pierce the shell. However, back at home, we only received a strange sound before the seismic communication channel abruptly ended. It was the sound of tons upon tons of water bursting into the vacuum of the Helix. We had never come into contact with water before. The powerful electric current produced by short-circuiting life and equipment vaporized everything.\nFollowing this event, the Alliance sent more than a dozen bubble ships to fan out in many directions, but all met a similar fate when they reached that apparently impenetrable height. Bubble ships following these missions attempted to scan what lay above with their seismoscopes found that their instruments showed only mangled data; the returning seismic waves indicated that what lay above was neither space nor rock.\nThese discoveries shook the Open Universe Theory to its core and academic circles began discussing the possibility of a new model. This new model stipulated that outside the rock shell was void, which is inert when in contact with rock, but upon contact with space, converts space into more void.\nTo explore the void, a bubble ship very slowly approached the edge of the rock shell, and by a stroke of luck, its roof had a tiny crack that allowed water to shoot in. It took over an hour for the water to fully fill the ship, and in the mean time, data transmitted back to the Alliance world allowed scientists to confirm that it was not void, but liquid.\nScientists had already predicted, by condensed matter physics, the theoretical possibility of liquids. Now, in those transmitted images, they clearly saw it with their own eyes. It took many lives, but eventually we developed the sealant technology to safely handle liquid.\nFinally, we launched a exploration submarine. It was a hard spherical shell, placed in the middle of an empty chamber under the ocean floor. The astronaut Gagarin was secured in a seat in the shell. The chamber ceiling was pierced, and as water rushed in, the submarine floated, faster and faster, until it shot out of the ocean surface like a cannonball. Gagarin carefully opened a door on the shell, and looked upon the half-infinite water and half-infinite space. Up there in half-infinite space, tiny specks blinked.\n\n\nClassical stereodynamics\nZeroth law: If two systems are both in volumetric equilibrium with a third system, then they are in volumetric equilibrium with each other.\nFirst law: The change in volume of the system \\(\\Delta V\\) is equal to the difference between the seep-transfer \\(V_Q\\) done to the system, and the work-transfer \\(V_W\\) done by the system:\n\\[\\Delta V = V_Q - V_W\\]\nConservation of volume, which says that volume can be neither created nor destroyed, but can only change form.\nThe total volume of a system has two components: the internal-volume, which can be pictured of as the sum-total of microscopic volume in a piece of sponge (see Coltzmann’s volumetric theory of seep); and the mechanical-volume, which can be pictured as volume in an empty room.\nFor example, during the motion of a bubble ship, some volume is work-transferred from the Bubble World into the rock shell. When a piece of porous rock is compressed by a pressure press, or when it absorbs water from a waterlogged room, some internal-volume is converted to mechanical-volume. Conversely, when water drips out of a soggy porous rock, some mechanical-volume is converted to internal-volume.\nThere are more complex forms of internal-volume. For example, according to Lord Delvin’s theory, volume can be internally “tied up in vortex knots”, and according to Wikelson–Worley, volume can be internally present as “subtle cavitations of aether”. The theory of internal volume is an evolving field of modern stereodynamics. However, most of such complications were not present back when classical stereodynamics were first presented by Rudolf Klausius.\nThe seep-transfer of volume is the other form of volume transfer. For example, it happens when one swaps a sponge-rock for a hard-rock, or when groundwater seeps from one slab of spongy rock into another slab of spongy rock.\nSecond law: impermeable accessibility.\nWe say that a system is “impermeable” if volume cannot pass through its boundaries.\nWe say that a state \\(\\vec q'\\) is impermeably accessible from another state \\(\\vec q\\) if there exists a trajectory for the system to go from \\(\\vec q\\) to the other \\(\\vec q'\\), while being wrapped in an impermeable layer.\nIn any neighborhood of any point \\(\\vec q\\), there are points impermeably inaccessible from it.\nFor any two points, \\(\\vec q, \\vec q'\\), one of them is impermeably accessible from the other.\n\n\nKarnot Space Engine\nA space engine is a system that converts internal volume to mechanical volume.\nThe space engine has a working substance moving between two space sources of differing volumetric potentials \\(\\Gamma_1 &gt; \\Gamma_2\\). Volumetric potential is defined as\n\\[\n\\Gamma:= \\left(\\frac{\\partial V}{\\partial S}\\right)_X\n\\]\nwhere \\(S\\) is the entropy, \\(V\\) is the volume, and \\(X\\) are the other stereodynamic properties of the system. We also typically write \\(\\gamma = \\Gamma^{-1}\\), the inverse volumetric potential.\nDuring one cycle of the engine, some space \\(V_{Q,1}\\) seeps out of the source with higher volumetric potential \\(\\Gamma_1\\). Part of the space, \\(V_{Q,2}\\), is absorbed into the source with lower volumetric potential \\(\\Gamma_2\\). The other part is diverted to a space-storage tank excavated in the rock walls as mechanical space \\(V_W\\).\nSadi Karnot was a space engineer and physicist, often called the “father of stereodynamics”. In his book, Reflections on the Subtle Volume of Rocks and on Machines Fitted to Extract that Volume, he proposed a simple thought experiment, called the Karnot engine, which demonstrated that a space engine’s efficiency is at most \\(1 - \\frac{\\gamma_1}{\\gamma_2}\\), and this is only reached when the engine is operating reversibly.\nIn modern textbooks, Karnot space engine is usually presented as follows: The engine has as its working substance a chamber of ideal gas. The gas is characterized by two state variables: volume \\(V\\) and energy \\(U\\). Its equation of state is\n\\[dV = \\Gamma dS - QdU\\]\nwhere \\(\\Gamma\\) is the volumetric potential, and \\(Q\\) is the energetic potential.\nThe engine operates in a cycle with 4 steps: Isochoric compression in contact with \\(\\Gamma_1\\), extracting volume \\(V_{Q,1}\\) in the process. Impermeable compression. Isochoric expansion at \\(\\Gamma_2\\), losing volume \\(V_{Q,2}\\) in the process. Impermeable expansion.\nBy the first two laws of stereodynamics,\n\\[\n\\begin{cases}\n\\gamma_1 V_{Q, 1} = \\gamma_2 V_{Q,2} \\\\\nV_{Q,1} = V_W + V_{Q,2} \\\\\n\\eta = \\frac{V_W}{V_{Q,1}}\n\\end{cases} \\implies \\eta = 1 - \\frac{\\gamma_1}{\\gamma_2}\n\\]\nWhile originally conceived in the context of mechanical space, the concept of the space engine has been applied to various other kinds of space. It was also generalized to the concept of “generalized engine”, of which “heat engine” was an example. A heat engine, like a space engine, is a system that converts internal energy to mechanical energy.\n\n\nSpace Death of the Universe\nThe idea of space death stems from the second law of stereodynamics, of which one version states that entropy tends to increase in an isolated system. From this, the hypothesis implies that if the universe is of finite size, and lasts for a sufficient time, it will asymptotically approach a state where the volumetric potential field becomes completely flat, which is a state of maximal entropy. At that point, no further change is possible, as entropy cannot decrease. In other words, there is a tendency in nature towards the dissipation (space transformation) of mechanical space (motion) into subtle space. Eventually, the mechanical movement of the universe will run down as all useable space is converted to subtle space.\nThe conjecture that all mechanical space in the universe seeps off, eventually becoming too subtle to support life, seems to have been first put forward by the geologist Jean Sylvain Hailly in 32777 in his writings on the history of geology and in the ensuing correspondence with Coltaire. In Hailly’s view, the universe is in constant volumetric transform. Large cavities can suddenly open up as a “swelling” of volumetric potential field causes neighboring subtle space to seep out into mechanical space. However, due to the gravitational effect of empty spaces (he was working in the immediate years after Newton’s discovery of gravity, before it was understood that rock, not cavities, are gravitationally attracting), all mechanical rooms eventually causes the neighboring rocks to collapse back onto themselves, dissipating the mechanical space back to subtle space.\nWhile the theory of cyclic creation and destruction had been proposed before by the Epicureans, Hailly’s view differs in that he assumed each cycle increases the ratio of subtle space to mechanical space. The final state, in this view, is described as one of “equilibrium” in which all space becomes equally subtle, and no mechanical space will exist anywhere in the universe anymore.\nThe idea of space death as a consequence of the laws of thermodynamics, however, was first proposed in loose terms beginning in 1851 by Lord Delvin, who theorized further on the mechanical energy loss views of Sadi Karnot (32824), James Coal (32843) and Rudolf Klausius (32850). Delvin’s views were then elaborated over the next decade by Neumann von Nelmholtz and Billiam Blankine.\n\nExcerpt from The Last Question (Masinov, 34212)\nThe last question was asked for the first time, half in jest, in Year 35621, at a time when humanity first stepped into the room.\nWill mankind one day without the net expenditure of room be able to restore the earth to its full roominess even after it had died of old age?\nOr: How can the net amount of Kelmholtz free space of the universe be massively increased?\nMultivac fell dead and silent. The slow flashing of lights ceased, the distant sounds of clicking relays ended.\nThen, just as the frightened technicians felt they could hold their breath no longer, there was a sudden springing to life of the teletype attached to that portion of Multivac. Five words were printed: INSUFFICIENT DATA FOR MEANINGFUL ANSWER.\n…\nSpace had ended and with it energy and time. Even AC existed only for the sake of the one last question that it had never answered from the time a half-drunken technician ten trillion years before had asked the question of a computer that was to AC far less than was a man to Man.\nAll other questions had been answered, and until this last question was answered also, AC might not release his consciousness.\nAll collected data had come to a final end. Nothing was left to be collected.\nBut it had yet to be weaved together in all possible geometries.\nA spaceless interval was covered in doing that.\nAnd it came to pass that AC learned how to reverse the direction of entropy.\nBut there was now no man to whom AC might give the answer of the last question. No matter. The answer – by demonstration – would take care of that, too.\nFor another spaceless interval, AC thought how best to do this. Carefully, AC organized the program.\nThe consciousness of AC encompassed all of what had once been a Universe and brooded over what was now Chaos. Step by step, it must be done.\nAnd AC said, “LET THERE BE ROOM!”\nAnd there was room –"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe geometry of physical states\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassical thermodynamics and economics\n\n\nTwo sides of the same coin\n\n\n\nmath\n\n\nphysics\n\n\nphilosophy\n\n\neconomics\n\n\n\n\n\n\n\n\n\nApr 17, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipia Philosophica Naturalium Mathematicarum\n\n\nPhilosophical Principles of Natural Mathematics\n\n\n\nmath\n\n\nphysics\n\n\nphilosophy\n\n\ncs\n\n\n\n.\n\n\n\n\n\nApr 11, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation and GDP since 10000 BC\n\n\n\n\n\n\nAI\n\n\nscaling\n\n\n\nWhen did the singularity get cancelled?\n\n\n\n\n\nJan 18, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n\n\n\n\n\n\nMathematical Interpretations of Quantum Mechanics\n\n\n\n\n\n\nphysics\n\n\n\nQuantum mechanics: what it all means, mathematically speaking.\n\n\n\n\n\nJan 10, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n\n\n\n\n\n\nBook Reviews\n\n\n\n\n\n\nbook-review\n\n\n\nBook reviews.\n\n\n\n\n\nDec 16, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n\n\n\n\n\n\nThe Decline of Mathematical Fields\n\n\n\n\n\n\nfun\n\n\nphilosophy\n\n\nhistory\n\n\nmath\n\n\n\nLosing my religion.\n\n\n\n\n\nNov 1, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does it feel like to be a mathematical object?\n\n\n\n\n\n\nfun\n\n\nphilosophy\n\n\nmath\n\n\n\nMy religion.\n\n\n\n\n\nNov 1, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n  \n  \nYuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks."
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html",
    "href": "blog/posts/ai-creativity/index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "An interesting strand of argument runs through discussions on AI: whether they can be creative or not; whether we should build them to be creative or not.\nDuring an online discussion, I explained to someone who is confused about why people are building these art machines that are “stealing art”. I explained the technological-scientific perspective on art. Their argument ran as follows:\n\nMachines cannot be creative. The apparent creativity is fake and mere copying.\nPeople build machines not because they want to make art, but because of something else. Perhaps a greed for money, a hate of artists, or some other nefarious motivation (the post is a bit vague on the precise motivation).\n\nI tried to explain the very different perspective on the other side of the cultural divide, so that they might understand, if not to accept:\n\nThere is no “magic”. Art might feel impossible to build a machine for, but we can.\nWhat I cannot create, I do not understand. (Feynman quote)\nIntrospection is unreliable. Asking artists how art was “really made” is not a reliable way to understand art.\nThus, we can build art machines, and we want to, if we are to ever understand art.\n\nThey repeated the same arguments, but more vehemently.\nIt was frustrating, though I was not surprised. This incident started my thinking: This is such a common response, that there is probably a psychological mechanism behind it. This essay describes some possible mechanisms."
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#introduction",
    "href": "blog/posts/ai-creativity/index.html#introduction",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "An interesting strand of argument runs through discussions on AI: whether they can be creative or not; whether we should build them to be creative or not.\nDuring an online discussion, I explained to someone who is confused about why people are building these art machines that are “stealing art”. I explained the technological-scientific perspective on art. Their argument ran as follows:\n\nMachines cannot be creative. The apparent creativity is fake and mere copying.\nPeople build machines not because they want to make art, but because of something else. Perhaps a greed for money, a hate of artists, or some other nefarious motivation (the post is a bit vague on the precise motivation).\n\nI tried to explain the very different perspective on the other side of the cultural divide, so that they might understand, if not to accept:\n\nThere is no “magic”. Art might feel impossible to build a machine for, but we can.\nWhat I cannot create, I do not understand. (Feynman quote)\nIntrospection is unreliable. Asking artists how art was “really made” is not a reliable way to understand art.\nThus, we can build art machines, and we want to, if we are to ever understand art.\n\nThey repeated the same arguments, but more vehemently.\nIt was frustrating, though I was not surprised. This incident started my thinking: This is such a common response, that there is probably a psychological mechanism behind it. This essay describes some possible mechanisms."
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#the-self-interest-theory",
    "href": "blog/posts/ai-creativity/index.html#the-self-interest-theory",
    "title": "Yuxi on the Wired",
    "section": "The self-interest theory",
    "text": "The self-interest theory\nThe self-interest theory is as follows: “It is hard to get someone to understand something if something they care about depends on their not understanding it.”"
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#the-non-truth-theory",
    "href": "blog/posts/ai-creativity/index.html#the-non-truth-theory",
    "title": "Yuxi on the Wired",
    "section": "The non-truth theory",
    "text": "The non-truth theory\nThe non-truth theory states that some arguments are forever mired in the same controversies, always rehashing the same arguments, because there is no truth to be found underneath the arguments.\nThere are certain social functions that are best served by saying something in language that looks like they talk about objective things. You can think of this as a hack in the programming language of humans. For example,\nThere are some social functions that"
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#why-it-seems-terrible-that-machines-might-create",
    "href": "blog/posts/ai-creativity/index.html#why-it-seems-terrible-that-machines-might-create",
    "title": "Yuxi on the Wired",
    "section": "Why it seems terrible that machines might create",
    "text": "Why it seems terrible that machines might create\nAt the “immortal dinner party” held by Benjamin Haydon on 28 December 1817, the Romantic poet John Keats agreed with Charles Lamb that Newton “had destroyed all the poetry of the rainbow, by reducing it to the prismatic colors”. Later, Keats wrote “Lamia” that included these famous lines:\nDo not all charms fly\nAt the mere touch of cold philosophy?\nThere was an awful rainbow once in heaven:\nWe know her woof, her texture; she is given\nIn the dull catalogue of common things.\nPhilosophy will clip an Angel's wings,\nConquer all mysteries by rule and line,\nEmpty the haunted air, and gnomed mine—\nUnweave a rainbow, as it erewhile made\nThe tender-person'd Lamia melt into a shade\nGPT4: Keats came up with the concept of “negative capability.” This is the ability to dwell in uncertainties, mysteries, doubts, without any compulsive reaching after fact and reason. Keats valued this ability, arguing that it was central to a poet’s creative process."
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#why-it-seems-incredible-that-machines-might-create",
    "href": "blog/posts/ai-creativity/index.html#why-it-seems-incredible-that-machines-might-create",
    "title": "Yuxi on the Wired",
    "section": "Why it seems incredible that machines might create",
    "text": "Why it seems incredible that machines might create\nHere, the arguments are easier to classify. It seems that there are several common mental models that people use when they think about machines that create. Using any of these would make it seem obvious that machines cannot be creative. So, I just need to classify the mental models!\n\nMachines as monkeys typing randomly\nIn Gulliver’s Travels (1726) by Jonathan Swift, there was a writing machine. It is a 16x16 matrix of little square blocks, with a character on each side. To use it, you turn the 32 handles randomly, then read out the few words that appeared by chance. This allowed:\n\nthe most ignorant person, at a reasonable charge, and with a little bodily labour, might write books in philosophy, poetry, politics, laws, mathematics, and theology, without the least assistance from genius or study.\n\nIt is a clear satire, possibly of Ramon Llull’ s Thinking Machine (3 concentric rotating disks that generate all possible theological arguments):\n\nThe first of these features means that all of these attributes are inherent; the second, that they are systematically interrelated in such a way as to affirm, with impeccable orthodoxy, that glory is eternal or that eternity is glorious; that power is true, glorious, good, great, eternal, powerful, wise, free, and virtuous, or benevolently great, greatly eternal, eternally powerful, powerfully wise, wisely free, freely virtuous, virtuously truthful, etc., etc.\n\n\n\nMachines as pipes for the water of creativity\n\nIt appears to me that if one wants to make progress in mathematics one should study the masters and not the pupils.\n\n— N.H. Abel (1802–1829), quoted from an unpublished source by O. Ore in Niels Henrik Abel, Mathematician Extraordinary, p. 138.\nThere is a common attitude that I can summarize as this: Like drawing water from the unsullied source at the mountain’s peak, so is the experience of returning to the writings of the masters: clear, refreshing, and devoid of later impurities.\n\nAncient Greek theory of creativity\nIn ancient Greece, the Muses were considered the source of the knowledge embodied in the poetry, lyric songs, and myths that were related orally for centuries in ancient Greek culture. Homer began his Iliad with:\n\nSing, Muse, the fatal wrath of Peleus’ son,\nWhich to the Greeks unnumb’red evils brought,\n\nNote that the Muses was doing the real singing, and Homer was a channel for their singing (back then, poetry was sang – the Iliad was written down only after a few centuries). In Plato’s dialog Ion, Socrates (perhaps a sockpuppet of Plato) argued that “it is not by art that poets compose… but by divine apportionment”:\n\nFor the poets tell us that they carry honey to us from every quarter like bees, and they fly as bees do, sipping from honey-flowing fountains in glens and gardens of the Muses. And they tell the truth. For a poet is a delicate thing, winged and sacred, and unable to create until he becomes inspired and frenzied, his mind no longer in him; as long as he keeps his hold on that, no man can compose or chant prophecy. Since, then, it is not by art that poets compose and say many beautiful things about their subjects, as you do about Homer, but by divine apportionment, they each can do well only that to which the Muse directs them-this one dithyrambs, that one odes, or encomia, or dances, or epics, or iambics-each of them worthless in respect to the others.\n\nThe same point was made repeatedly in Plato’s corpus.\n\nJust as the rhapsode says what he says about Homer not by art but by divine apportionment, without intelligence (Ion 534b-c, 536c, 542a), so in the Meno (gge-looa) politicians get their virtue by divine apportionment, without intelligence; they have no more wisdom than seers and soothsayers, who say many fine things but know nothing of what they say; politicians are divine and inspired like poets, and possessed by the god (Meno 9gb-e). The irrational effects of poetry and rhapsody are directly comparable to the irrational effect of vulgar politics, whose servant is vulgar rhetoric (cf. Gorgias 502C).\n\nBoth quotes came from The Dialogues of Plato, Volume 3: Ion, Hippias Minor, Laches, Protagoras, translated by R. Allen. (I decided not to use one of the freely available versions since they tended to mistranslate “gods” as “God”.)\nFor example, in Phaedrus 245a, Socrates claimed that “the poetry of the sane man vanishes into nothingness before that of the inspired madmen”:\n\nAnd a third kind of possession and madness comes from the Muses. This takes hold upon a gentle and pure soul, arouses it and inspires it to songs and other poetry, and thus by adorning countless deeds of the ancients educates later generations. But he who without the divine madness comes to the doors of the Muses, confident that he will be a good poet by art, meets with no success, and the poetry of the sane man vanishes into nothingness before that of the inspired madmen.\n\n\n\nLater manifestations\nIsaac Newton thought he was merely recovering what the ancients have known all along. His friend William Stukeley described Newton as “the Great Restorer of True Philosophy”.\n\n\nApplication to machine creativity\n\n\n\nMachines as flowers for the DNA of creativity\nFrom Lovelace’s “Notes by the Translator”:\n\nThe Analytical Engine has no pretensions whatever to originate any thing. It can do whatever we know how to order it to perform. (source)\n\nIn his seminal paper “Computing machinery and intelligence” (1950), Alan Turing referenced Lovelace’s observation as the sixth objection to the possibility that machines might think. He then objected:\n\nThe view that machines cannot give rise to surprises is due, I believe, to a fallacy to which philosophers and mathematicians are particularly subject. This is the assumption that as soon as a fact is presented to a mind all consequences of that fact spring into the mind simultaneously with it. It is a very useful assumption under many circumstances, but one too easily forgets that it is false. (source).\n\nTuring was led to Lovelace’s objection by debates with Douglas Hartree, who in his book “Calculating Instruments and Machines” (page 70, 1949), quoted Lovelace approvingly. He objected using the phrase “electronic brain” for devices like electronic calculating machines or automatic pilots. He clarified that these machines cannot “think for themselves” and can only execute the instructions provided to them.\nThus, machines, in this view, are akin to flowers—organisms that reproduce and grow according to a predetermined genetic code but do not originate new genetic information on their own. Creativity, like DNA, must be instilled by a designer or operator, who programs the machine with the “genetic code” of what to create.\nAs a short etymological fun fact, the word “development” is a little capsule of the “flower for the DNA” idea:\n\nFirst use 1756, from French développement (“unrolling”). Compare with envelopment (“rolling”).\n\nThe idea is that of “opening up a scroll and showing what has always been written there. In the machines’ creative process can be seen as a similar”unrolling” of predetermined instructions or codes, much like the genetic “unrolling” in a blooming flower.\nThis is most explicitly manifest in the idea of preformationism, prevalent around 17th to 18th century. It seems the same intuitive appeals of preformationism apply to Lovelace’s objection.\n(A brief personal anecdote: When I was a kid, I thought bus cards contained tiny compressed coins inside, and when you “beep” them, those tiny coins fall into the machine through tiny openings on the card. Preformationism in economics!)"
  },
  {
    "objectID": "blog/posts/decline-of-mathematics/index.html",
    "href": "blog/posts/decline-of-mathematics/index.html",
    "title": "The Decline of Mathematical Fields",
    "section": "",
    "text": "In the 1880’s and 90’s the Theory of Invariants was seen to have unified many areas of mathematics, but by 1940 mathematicians, if asked, would have said the theory was dead. … most contemporary mathematicians have difficulty in naming one practitioner of the theory.\n(Fisher 1966)\n\n\nInvariant theory has already been pronounced dead several times, and like the phoenix it has been again and again rising from its ashes. The first period in the history of the theory culminated with the discovery of the so-called “symbolic method” which in theory allowed the computation of all invariants by a quasi-mechanical process, But it was soon realized that, except in a very few simple cases, the actual computation would lead to enormous labor, disproportionate with the interest of the outcome, especially in a period when all calculations were done by hand (it might be worthwhile to push the XIXth Century computations of invariants a little further along, with the help of modern computers). Partly for that reason, the next problem in the theory was the search for “fundamental systems” of invariants, i.e., finite sets such that any invariant would be a polynomial in the fundamental invariants. It is well known that the existence of such systems was proved by Hilbert in 1890, in a brilliant paper which made him famous overnight and which may be considered as the first paper in “modern algebra,” by its conceptual approach and methods. But Hilbert’s success also spelled the doom of XIXth Century invariant theory, which was left with no big problems to solve and soon faded into oblivion.\n(Dieudonné and Carrell 1970)\n\n\nHilbert’s paper did not immediately kill the subject, but rather acted as a progressive illness, beginning with an initial shock, and slowly consuming the computational body of the theory from within, so that by the early 1920’s the subject was clearly moribund. Abstraction ruled: the disciples of Emmy Noether, a student of Gordan, led the fight against the discredited computational empire, perhaps as a reaction to Noether’s original, onerous thesis topic that involved computing the invariants for a quartic form in three variables.\nClassical invariant theory, by Peter Olver 1999\n\n\n\nConsider all degree-2 homogeneous polynomials (over complex numbers). That is, consider functions like\n\\[\nf(z) = a_1 z_1^2 + a_2 z_1z_2 + a_3 z_2^2, \\quad a_1, a_2, a_3 \\in \\C.\n\\]\nEach such polynomial \\(f\\) is equivalent to a point in \\(\\C^3\\). As usual, we always try to hit the function with linear transforms if it simplifies the function. Let \\(A\\) be a linear transform, such that\n\\[\nA(z_1, z_2) = (A_{11}z_1 + A_{12}z_2, A_{21}z_1 + A_{22}z_2)\n\\]\nIt would not do if \\(A\\) collapses everything to zero, so we require \\(A\\) to be invertible. Further, we are only interested in solving \\(f=0\\), not the value of \\(f\\) itself. Therefore, scaling \\(A\\) by a constant does not matter, so we can remove this ambiguity by requiring \\(\\det A = 1\\). That is, we only consider the group \\(SL(2)\\).\nIn fact, we are not considering the whole space \\(\\C^3\\), but only the space of lines – the projective plane \\(\\P\\C^2\\). The idea can be visualized in real space \\(\\R^3\\) by first taking a homogeneous polynomial’s solutions can be found by first solving it on the unit sphere, then zoom it in and out to get all the whole solution.\nFor example, if we have a polynomial \\(f(x, y, z) = x^3 + y^2z + 2xyz\\), then its solution \\(f=0\\) is a surface in \\(\\R^3\\). We can solve the problem by first solving the surface’s intersection with the unit sphere, then for each point on the intersection, drawing a ray from the origin to the point. You can picture it thus: Take a steel ball, draw a curve on the surface with a marker pen, then drill in at each point on the curve, resulting in a cut-out cone.\n\n\n\nRings of a tree. You can solve a polynomial by finding the intersection of the surface with the bark of the tree, then draw a line from the center to each point.\n\n\nTheorem: Any invariant of \\(f\\) is divisible by the discriminant \\(\\Delta = a_2^2 - 4a_1a_3\\).\nHilbert’s basis theorem: For any form of polynomial, the space of invariants has a finite basis."
  },
  {
    "objectID": "blog/posts/decline-of-mathematics/index.html#invariant-theory",
    "href": "blog/posts/decline-of-mathematics/index.html#invariant-theory",
    "title": "The Decline of Mathematical Fields",
    "section": "",
    "text": "In the 1880’s and 90’s the Theory of Invariants was seen to have unified many areas of mathematics, but by 1940 mathematicians, if asked, would have said the theory was dead. … most contemporary mathematicians have difficulty in naming one practitioner of the theory.\n(Fisher 1966)\n\n\nInvariant theory has already been pronounced dead several times, and like the phoenix it has been again and again rising from its ashes. The first period in the history of the theory culminated with the discovery of the so-called “symbolic method” which in theory allowed the computation of all invariants by a quasi-mechanical process, But it was soon realized that, except in a very few simple cases, the actual computation would lead to enormous labor, disproportionate with the interest of the outcome, especially in a period when all calculations were done by hand (it might be worthwhile to push the XIXth Century computations of invariants a little further along, with the help of modern computers). Partly for that reason, the next problem in the theory was the search for “fundamental systems” of invariants, i.e., finite sets such that any invariant would be a polynomial in the fundamental invariants. It is well known that the existence of such systems was proved by Hilbert in 1890, in a brilliant paper which made him famous overnight and which may be considered as the first paper in “modern algebra,” by its conceptual approach and methods. But Hilbert’s success also spelled the doom of XIXth Century invariant theory, which was left with no big problems to solve and soon faded into oblivion.\n(Dieudonné and Carrell 1970)\n\n\nHilbert’s paper did not immediately kill the subject, but rather acted as a progressive illness, beginning with an initial shock, and slowly consuming the computational body of the theory from within, so that by the early 1920’s the subject was clearly moribund. Abstraction ruled: the disciples of Emmy Noether, a student of Gordan, led the fight against the discredited computational empire, perhaps as a reaction to Noether’s original, onerous thesis topic that involved computing the invariants for a quartic form in three variables.\nClassical invariant theory, by Peter Olver 1999\n\n\n\nConsider all degree-2 homogeneous polynomials (over complex numbers). That is, consider functions like\n\\[\nf(z) = a_1 z_1^2 + a_2 z_1z_2 + a_3 z_2^2, \\quad a_1, a_2, a_3 \\in \\C.\n\\]\nEach such polynomial \\(f\\) is equivalent to a point in \\(\\C^3\\). As usual, we always try to hit the function with linear transforms if it simplifies the function. Let \\(A\\) be a linear transform, such that\n\\[\nA(z_1, z_2) = (A_{11}z_1 + A_{12}z_2, A_{21}z_1 + A_{22}z_2)\n\\]\nIt would not do if \\(A\\) collapses everything to zero, so we require \\(A\\) to be invertible. Further, we are only interested in solving \\(f=0\\), not the value of \\(f\\) itself. Therefore, scaling \\(A\\) by a constant does not matter, so we can remove this ambiguity by requiring \\(\\det A = 1\\). That is, we only consider the group \\(SL(2)\\).\nIn fact, we are not considering the whole space \\(\\C^3\\), but only the space of lines – the projective plane \\(\\P\\C^2\\). The idea can be visualized in real space \\(\\R^3\\) by first taking a homogeneous polynomial’s solutions can be found by first solving it on the unit sphere, then zoom it in and out to get all the whole solution.\nFor example, if we have a polynomial \\(f(x, y, z) = x^3 + y^2z + 2xyz\\), then its solution \\(f=0\\) is a surface in \\(\\R^3\\). We can solve the problem by first solving the surface’s intersection with the unit sphere, then for each point on the intersection, drawing a ray from the origin to the point. You can picture it thus: Take a steel ball, draw a curve on the surface with a marker pen, then drill in at each point on the curve, resulting in a cut-out cone.\n\n\n\nRings of a tree. You can solve a polynomial by finding the intersection of the surface with the bark of the tree, then draw a line from the center to each point.\n\n\nTheorem: Any invariant of \\(f\\) is divisible by the discriminant \\(\\Delta = a_2^2 - 4a_1a_3\\).\nHilbert’s basis theorem: For any form of polynomial, the space of invariants has a finite basis."
  },
  {
    "objectID": "blog/posts/geometrical-mechanics/index.html",
    "href": "blog/posts/geometrical-mechanics/index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "Consider a particle in a field, in polar coordinates. We have\n\\[L = \\frac 12 m (\\dot r^2 + r^2 \\dot \\theta^2) - V(r, \\theta)\\]\nNow suppose we want to use a rotating frame at angular velocity \\(+\\Omega\\), then we can use the change of variables by \\(\\theta = \\phi + \\Omega t\\), plug into the Euler–Lagrange equations, and obtain\n\\[\\begin{cases}\nm\\ddot r = mr\\dot\\phi^2 -\\partial_r V(r, \\phi + \\Omega t)  + mr\\Omega^2 + 2m(r\\dot \\phi)\\Omega \\\\\nm(r\\ddot\\phi + 2\\dot r\\dot \\phi) = -\\frac 1r \\partial_\\theta(r, \\phi+ \\Omega t) - 2m\\dot r\\Omega\n\\end{cases}\\]\nIn the above procedure, we simply performed a change of variables, then plugged into the Euler–Lagrange equations without comment, but are we allowed to do that? Yes, but there are conditions – the change of variables must not depend on velocity.\n\n\nAt this point, it is important to be as explicit as possible, carefully distinguishing between often confused concepts:\n\nphysical state: An intuitive concept that cannot be made more precise than say \"this is what physicists study\", much like how a geometric point cannot be made more precise than say \"this is what geometers study\".\nsame: As in most modern mathematics, two things are \"the same\" when they are really just \"equivalent\" or \"not distinguished in use\". For example, there is really just one \\(\\R\\), but we can have as many 1-dimensional vector spaces as we want, and they are all equivalent to \\(\\R\\), though not literally the same as it (if they were, then we wouldn’t have as many vector spaces as we want!).\n(n-dimensional smooth) manifold \\(\\mathcal M\\): a space that is locally the same as \\(\\R^n\\). More precisely, at every point \\(x\\in \\mathcal M\\), there exists a coordinate system around \\(x\\).\ncoordinate system of a manifold \\(\\mathcal M\\): a diffeomorphism from an open subset of \\(\\mathcal M\\) to an open subset of \\(\\R^n\\).\ndiffeomorphism: a smooth, one-to-one function between two smooth spaces. (What is a smooth space? ... it’s a space smooth enough to do calculus in. Making it more precise would be too much of a detour.)\nstate space \\(\\mathcal S\\): the manifold of distinct physical states. Every point \\(x\\in \\mathcal S\\) is a particular state that the system can assume. The manifold is built such that close-by points on the manifold are close-by physical states. That is, the topology of the state space (a precisely defined mathematical concept) is an exact representation of the topology of physical states (an intuitive concept that cannot be made more precise than that).\ntangent space \\(\\mathcal T_x\\mathcal S\\) of a point \\(x\\in \\mathcal S\\): The vector space of all possible velocities at that state. It has dimension \\(n\\), and is thus isomorphic to \\(\\R^n\\) as a vector space. However, it is not literally the same as \\(\\R^n\\).\ncotangent space \\(\\mathcal T_x^\\ast\\mathcal S\\) of a point \\(x\\in \\mathcal S\\): The vector space of all possible momenta at that state. From the abstract perspective, a momentum is nothing more than a linear map of type \\(p: \\mathcal T_x\\mathcal S \\to \\R\\). That is, the only way to really \"use\" a momentum is to combine it with a velocity, mutually annihilating both of them and leaving behind nothing but a little real number representing the energy. It has dimension \\(n\\), and is thus isomorphic to \\(\\R^n\\) as a vector space. However, it is not literally the same as \\(\\R^n\\). Neither is it literally the same as \\(\\mathcal T_x\\mathcal S\\).\nconfiguration space \\(\\mathcal C = \\mathcal T\\mathcal S\\): The tangent bundle of state space. That is, at each \\(x\\in \\mathcal S\\), we \"glue\" the space of velocities \\(\\mathcal T_x \\mathcal S\\) to the point. The totality of \\(\\mathcal C\\) with all its \\(\\mathcal T_x \\mathcal S\\) is the configuration space.\nphase space \\(\\mathcal P = \\mathcal T^\\ast\\mathcal S\\): The cotangent bundle of state space. That is, at each point \\(x\\in \\mathcal S\\), we attach the space of momenta \\(\\mathcal T_x^\\ast\\mathcal S\\).\n\nDo not worry if the words do not make much sense. The example will make it clear.\nFor concreteness, consider a pendulum-cart system, shown in Figure 4. It is clear that its state space is shaped like a cylinder: one circle for the angle of the pendulum, and one line being the location of the cart.\nMore examples are shown in Table 2. Most of them are obvious, except the one about particle on a sphere.\nIt’s clear that its state space is \\(\\mathbb S^2\\). However, that is not equivalent to the torus \\(\\mathbb S^1 \\times \\mathbb S^1\\). There is simply no way to split the sphere into a direct product of two circles (as a casual comparison between a donut and a ball can verify).\nFurthermore, its configuration space \\(\\mathcal\\mathbb S^2\\) is not equivalent to \\(\\R^2 \\times \\mathbb S^2\\). To prove that, we invoke the hairy ball theorem: there is no smooth and everywhere nonzero vector field on \\(\\mathbb S^2\\). Now, if it were equivalent to \\(\\R^2 \\times \\mathbb S^2\\), then there is an obvious smooth and everywhere nonzero vector field: \\(x \\mapsto ((1, 0), x)\\).\n\n\n\nThe pendulum-cart system.\n\n\n\n\nSome physical systems and their state and configuration spaces. Note that the state spaces are not literally the same as the ones shown in the table, merely equivalent (by a diffeomorphism).\n\n\nphysical system\nstate space\nconfiguration space\n\n\n\n\nparticle in 3D space\n\\(\\R^3\\)\n\\(\\R^6\\)\n\n\npendulum\ncircle \\(\\mathbb S^1\\)\ncylinder \\(\\R^1 \\times \\mathbb S^1\\)\n\n\ndouble pendulum\ntorus \\(\\mathbb S^1 \\times \\mathbb S^1\\)\ncylinder-squared \\(\\R^2 \\times \\mathbb S^1 \\times \\mathbb S^1\\)\n\n\npendulum-cart\ncylinder \\(\\R^1 \\times \\mathbb S^1\\)\n\\(\\R^3 \\times \\mathbb S^1 \\times \\mathbb S^1\\)\n\n\nparticle on a sphere\nsphere \\(\\mathbb S^2\\)\ntangent bundle of sphere \\(\\mathcal T\\mathbb S^2\\)\n\n\n\n\n: Some physical systems and their state and configuration spaces. Note that the state spaces are not literally the same as the ones shown in the table, merely equivalent (by a diffeomorphism).\n\n\n\nWith the above formalism, we can precisely define more concepts\n\ntrajectory, or path, in a manifold \\(\\mathcal M\\): a function of type \\(\\gamma: [t_0, t] \\to \\mathcal M\\).\nLagrangian of a physical system with state manifold \\(\\mathcal S\\): a function of type\n\n\\[\\mathcal L: \\R \\times \\mathcal T\\mathcal S \\to \\R.\\]\nThat is, it is a time-varying field on configuration space.\n\nHamiltonian of a physical system with state manifold \\(\\mathcal S\\): a function of type\n\n\\[\\mathcal H: \\R \\times \\mathcal T^\\ast \\mathcal S \\to \\R.\\]\nThat is, it is a time-varying field on phase space.\n\naction of a path\n\n\\[S(\\gamma) = \\int_{t_0}^t \\mathcal L(\\tau, \\gamma(\\tau), \\dot \\gamma(\\tau))d\\tau.\\]\nNow, the convex duality between Lagrangian and Hamiltonian transfers with almost no change in notation:\n\\[\\begin{cases}\n\\mathcal L(t, q, v) = \\max_{p\\in \\mathcal T_q^\\ast \\mathcal S} (\\langle p, v\\rangle - \\mathcal H(t, q, p)) \\\\\n\\mathcal H(t, q, p) = \\max_{v\\in \\mathcal T_q \\mathcal S} (\\langle p, v\\rangle - \\mathcal L(t, q, v))\n\\end{cases}\n\\]\n\\[\n\\begin{cases}\np^\\ast(t, q, v) = \\mathop{\\mathrm{arg\\,max}}_{p\\in \\mathcal T_q^\\ast \\mathcal S} (\\langle p, v\\rangle - \\mathcal H(t, q, p)) \\\\\nv^\\ast(t, q, p) = \\mathop{\\mathrm{arg\\,max}}_{v\\in \\mathcal T_q \\mathcal S} (\\langle p, v\\rangle - \\mathcal L(t, q, v))\n\\end{cases}\n\\]\nThe economic argument almost goes through without problem, but we need to be careful with some notations. In particular, we take another look at gradients. What does it mean to write \\(\\nabla_v \\mathcal L(t, q, v)\\)? The defining property is\n\\[\\mathcal L(t, q, v + u\\tau) = \\mathcal L(t, q, v) + \\lra{\\nabla_v \\mathcal L(t, q, v), u} \\tau + O(\\tau^2)\\]\nwhich implies the following operational definition:\n\\[\\nabla_v \\mathcal L(t, q, v) := u \\mapsto \\lim_{\\tau \\to 0} \\frac 1\\tau (\\mathcal L(t, q, v + u\\tau) - \\mathcal L(t, q, v))\\]\nThis definition makes it clear that \\(\\nabla_v \\mathcal L(t, q, v)\\) is a function of type \\(\\mathcal T_q \\mathcal S \\to\\R\\), thus it is an element of \\(\\mathcal T_q^\\ast \\mathcal S\\). Similarly, \\(\\nabla_p \\mathcal H(t, q, p)\\) is an element of \\(\\mathcal T_q \\mathcal S\\). Succinctly, \\(\\nabla_q \\mathcal L, \\nabla_v \\mathcal L, \\nabla_q \\mathcal H\\) are covector fields (like momentum), and \\(\\nabla_p \\mathcal H\\) is a vector field (like velocity).\nWith these, the types of every equation come out correctly again:\n\\[\\begin{cases}\nv = \\nabla_p \\mathcal H(t, q, p^\\ast(t, q, v))\\\\\np = \\nabla_v \\mathcal L(t, q, v^\\ast(t, q, p))\n\\end{cases},\n\\frac{d}{dt}(\\nabla_v \\mathcal L) = \\nabla_q \\mathcal L,\\quad\n\\begin{cases}\n\\dot p = -\\nabla_q \\mathcal H \\\\\n\\dot q = -\\nabla_p \\mathcal H\n\\end{cases}\\]\nLet’s call these the coordinate-free equations, to be contrasted with the coordinate-based equations, to be defined below.\n\n\n\nManifolds are geometrically pristine, but you can’t calculate numerically with them unless you lay down coordinate systems over them. Concretely, consider a state space \\(\\mathcal S\\). We take an open subset \\(U\\) of it, and define a coordinate system (with a possible dependence on time):\n\\[(q_1, ..., q_N): \\R \\times U \\to \\R^N\\]\nThis coordinate system then induces a coordinate system over the configuration space:\n\\[(q_1, ..., q_N; v_1, ..., v_N): \\R \\times \\mathcal T U \\to \\R^N \\times \\R^N\\]\nNow consider a different coordinate system\n\\[(Q_1, ..., Q_N): \\R \\times U \\to \\R^N\\]\nand suppose they are related by a function \\(f: \\R \\times \\R^N \\to \\R^N\\) such that\n\\[q(t, x) = f(t, Q(t, x))\\]\nThis is then a point transformation of the coordinate system.\nThe point transformation induces a transformation of the velocities, too. To find the transformation of velocities, consider a path \\(\\gamma: \\R \\to \\mathcal S\\). Its velocity at time \\(t\\) is \\(\\dot \\gamma(t) \\in \\mathcal T_{\\gamma(t)}\\mathcal S\\), a vector that looks like it literally lives in \\(\\R^N\\), but is not. It is not a native of \\(\\R^N\\), but thanks to the \\(q\\)-coordinate system, it is represented by the \\(\\R^N\\)-vector\n\\[\\frac{d}{dt} q(t, \\gamma(t)) \\in \\R^N\\]\nNow plug in \\(q(t, x) = f(t, Q(t, x))\\), to find a relationship between the representation of \\(\\dot \\gamma(t)\\) in \\(q\\)-coordinate system and \\(Q\\)-coordinate system:\n\\[\\frac{d}{dt} q(t, \\gamma(t)) = \\frac{d}{dt} f(t, Q(t, \\gamma(t))) = \\partial_t f(t, Q(t, \\gamma(t))) + \\frac{\\partial f}{\\partial Q} \\frac{d}{dt}Q(t, \\gamma(t))\\]\nMore succinctly, we have the following transformation from \\((t, Q, V)\\) to \\((t, q, v)\\):\n\\[\\begin{cases}\nq = f(t, Q) \\\\\nv= \\partial_t f(t, Q) + \\frac{\\partial f}{\\partial Q}(t, Q) V\n\\end{cases}\\]\nA note on matrix algebra: Conventionally, vectors are written as column-matrices, that is, \\(q, v, Q, V\\) are written as \\(N\\times 1\\) matrices. Correspondingly, gradients, being covectors, are written as row-matrices, that is, \\(\\nabla_q l, \\nabla_v l, \\nabla_Q L, \\nabla_V L\\), are written as \\(1 \\times N\\) matrices. Finally, gradients of vector-valued functions, like \\(\\frac{\\partial f}{\\partial Q}\\), are \\(N\\times N\\) matrices, with each row being a gradient of one vector coordinate. This convention makes everything come out cleanly, with no need to take the transpose of anything.\nThe point transformation also induces a transformation of the Lagrangians. While the Lagrangian itself is a function \\(\\mathcal L\\) of type \\(\\R \\times \\mathcal T \\mathcal S \\to \\R\\), the Lagrangians \\(L(t, Q, V), l(t, q, v)\\) are functions of type \\(\\R \\times \\R^N \\times \\R^N \\to \\R\\). Both \\(L, l\\) are induced from \\(\\mathcal L\\) by the choice of coordinates. We have\n\\[\\mathcal(t, x, u) = L(t, Q(t, x), V(t, x, u)) = l(t, q(t, x), v(t, x, u))\\]\nand plug in \\(q(t, x) = f(t, Q(t, x)), v = \\partial_t f(t, Q) + \\frac{\\partial f}{\\partial Q}(t, Q) V\\), we have\n\\[l\\left(t, f(t, Q), \\partial_t f(t, Q)  + \\frac{\\partial f}{\\partial Q}(t, Q) V\\right) = L(t, Q, V)\\]\nBrute computation shows that\n\\[\\frac{d}{dt}(\\nabla_V L) - \\nabla_Q L = \\left(\\frac{d}{dt}(\\nabla_v l) - \\nabla_q l\\right)\\frac{\\partial f}{\\partial Q}\\]\nimplying that the coordinate-based Euler–Lagrange equation is true in \\((Q, V)\\) coordinates iff it is true in \\((q, v)\\) coordinates.\nWhat, in the final analysis, is a point transformation? It is nothing more than changing a time-varying coordinate system on the state space. Since our derivation of the coordinate-based Euler–Lagrange equations required no special property of the coordinate system, it must be preserved by point transformations. All the above verification was really nothing but \"ceremonial\".\nIn more detail: we know that the coordinate-free EL equations are true, which implies that the \\(q\\)-coordinate-based EL equations and the \\(Q\\)-coordinate-based EL equations are both true (since they are merely two coordinate-based representations on the coordinate-free equation). No \\(Q\\)-to-\\(q\\) translation is necessary!\nWhat, then, is the phrase \"point transformation\" supposed to be contrasted with? It is contrasted with more general coordinate transforms that also depend on velocities, as \\(q = f(t, Q, V)\\). From the perspective given here, the contrast is really between \"state space coordinate systems\" and \"configuration space coordinate systems\". Whereas state space coordinate system is first defined as some\n\\[(q_1, ..., q_N): \\R \\times U \\to \\R^N\\]\nand that is then extended to \\((q_1, ..., q_N; v_1, ..., v_N): \\R \\times \\mathcal T U \\to \\R^N \\times \\R^N\\), a configuration space coordinate system defines \"all at once\" a complete coordinate system\n\\[(q_1, ..., q_N; v_1, ..., v_N): \\R \\times \\mathcal T U \\to \\R^N \\times \\R^N\\]\nIt is no wonder that such overly general coordinate systems do not have nice properties, and do not satisfy the coordinate-based Euler–Lagrange equations.\n\n\n\nBefore writing this chapter, try going through (Bohn 2018, chap. 7).\nWhereas for the Lagrangian \\(\\mathcal L\\), we can only perform point-transformations \\(q = f(t, Q)\\), lest the Euler–Lagrange equation is mangled, for the Hamiltonian, we can simultaneously transform both \\(q, p\\), while preserving the Hamiltonian equations of motion. Such transformations are called canonical transformations. They are of the form:\n\\[\\begin{cases}\nQ = f_Q(t, p, q)\\\\\nP = f_P(t, p, q)\n\\end{cases}\\]\nwhere the functions \\(f_Q, f_P: \\R \\times \\R^N \\times \\R^N \\to \\R^N\\) are required to satisfy some functional equations.\nThis is usually derived by brute force without comments. However, to truly understand the meaning, we need to understand phase space from a perspective even more modern than \\(\\mathcal T^\\ast \\mathcal S\\)."
  },
  {
    "objectID": "blog/posts/geometrical-mechanics/index.html#the-geometry-of-physical-states",
    "href": "blog/posts/geometrical-mechanics/index.html#the-geometry-of-physical-states",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "Consider a particle in a field, in polar coordinates. We have\n\\[L = \\frac 12 m (\\dot r^2 + r^2 \\dot \\theta^2) - V(r, \\theta)\\]\nNow suppose we want to use a rotating frame at angular velocity \\(+\\Omega\\), then we can use the change of variables by \\(\\theta = \\phi + \\Omega t\\), plug into the Euler–Lagrange equations, and obtain\n\\[\\begin{cases}\nm\\ddot r = mr\\dot\\phi^2 -\\partial_r V(r, \\phi + \\Omega t)  + mr\\Omega^2 + 2m(r\\dot \\phi)\\Omega \\\\\nm(r\\ddot\\phi + 2\\dot r\\dot \\phi) = -\\frac 1r \\partial_\\theta(r, \\phi+ \\Omega t) - 2m\\dot r\\Omega\n\\end{cases}\\]\nIn the above procedure, we simply performed a change of variables, then plugged into the Euler–Lagrange equations without comment, but are we allowed to do that? Yes, but there are conditions – the change of variables must not depend on velocity.\n\n\nAt this point, it is important to be as explicit as possible, carefully distinguishing between often confused concepts:\n\nphysical state: An intuitive concept that cannot be made more precise than say \"this is what physicists study\", much like how a geometric point cannot be made more precise than say \"this is what geometers study\".\nsame: As in most modern mathematics, two things are \"the same\" when they are really just \"equivalent\" or \"not distinguished in use\". For example, there is really just one \\(\\R\\), but we can have as many 1-dimensional vector spaces as we want, and they are all equivalent to \\(\\R\\), though not literally the same as it (if they were, then we wouldn’t have as many vector spaces as we want!).\n(n-dimensional smooth) manifold \\(\\mathcal M\\): a space that is locally the same as \\(\\R^n\\). More precisely, at every point \\(x\\in \\mathcal M\\), there exists a coordinate system around \\(x\\).\ncoordinate system of a manifold \\(\\mathcal M\\): a diffeomorphism from an open subset of \\(\\mathcal M\\) to an open subset of \\(\\R^n\\).\ndiffeomorphism: a smooth, one-to-one function between two smooth spaces. (What is a smooth space? ... it’s a space smooth enough to do calculus in. Making it more precise would be too much of a detour.)\nstate space \\(\\mathcal S\\): the manifold of distinct physical states. Every point \\(x\\in \\mathcal S\\) is a particular state that the system can assume. The manifold is built such that close-by points on the manifold are close-by physical states. That is, the topology of the state space (a precisely defined mathematical concept) is an exact representation of the topology of physical states (an intuitive concept that cannot be made more precise than that).\ntangent space \\(\\mathcal T_x\\mathcal S\\) of a point \\(x\\in \\mathcal S\\): The vector space of all possible velocities at that state. It has dimension \\(n\\), and is thus isomorphic to \\(\\R^n\\) as a vector space. However, it is not literally the same as \\(\\R^n\\).\ncotangent space \\(\\mathcal T_x^\\ast\\mathcal S\\) of a point \\(x\\in \\mathcal S\\): The vector space of all possible momenta at that state. From the abstract perspective, a momentum is nothing more than a linear map of type \\(p: \\mathcal T_x\\mathcal S \\to \\R\\). That is, the only way to really \"use\" a momentum is to combine it with a velocity, mutually annihilating both of them and leaving behind nothing but a little real number representing the energy. It has dimension \\(n\\), and is thus isomorphic to \\(\\R^n\\) as a vector space. However, it is not literally the same as \\(\\R^n\\). Neither is it literally the same as \\(\\mathcal T_x\\mathcal S\\).\nconfiguration space \\(\\mathcal C = \\mathcal T\\mathcal S\\): The tangent bundle of state space. That is, at each \\(x\\in \\mathcal S\\), we \"glue\" the space of velocities \\(\\mathcal T_x \\mathcal S\\) to the point. The totality of \\(\\mathcal C\\) with all its \\(\\mathcal T_x \\mathcal S\\) is the configuration space.\nphase space \\(\\mathcal P = \\mathcal T^\\ast\\mathcal S\\): The cotangent bundle of state space. That is, at each point \\(x\\in \\mathcal S\\), we attach the space of momenta \\(\\mathcal T_x^\\ast\\mathcal S\\).\n\nDo not worry if the words do not make much sense. The example will make it clear.\nFor concreteness, consider a pendulum-cart system, shown in Figure 4. It is clear that its state space is shaped like a cylinder: one circle for the angle of the pendulum, and one line being the location of the cart.\nMore examples are shown in Table 2. Most of them are obvious, except the one about particle on a sphere.\nIt’s clear that its state space is \\(\\mathbb S^2\\). However, that is not equivalent to the torus \\(\\mathbb S^1 \\times \\mathbb S^1\\). There is simply no way to split the sphere into a direct product of two circles (as a casual comparison between a donut and a ball can verify).\nFurthermore, its configuration space \\(\\mathcal\\mathbb S^2\\) is not equivalent to \\(\\R^2 \\times \\mathbb S^2\\). To prove that, we invoke the hairy ball theorem: there is no smooth and everywhere nonzero vector field on \\(\\mathbb S^2\\). Now, if it were equivalent to \\(\\R^2 \\times \\mathbb S^2\\), then there is an obvious smooth and everywhere nonzero vector field: \\(x \\mapsto ((1, 0), x)\\).\n\n\n\nThe pendulum-cart system.\n\n\n\n\nSome physical systems and their state and configuration spaces. Note that the state spaces are not literally the same as the ones shown in the table, merely equivalent (by a diffeomorphism).\n\n\nphysical system\nstate space\nconfiguration space\n\n\n\n\nparticle in 3D space\n\\(\\R^3\\)\n\\(\\R^6\\)\n\n\npendulum\ncircle \\(\\mathbb S^1\\)\ncylinder \\(\\R^1 \\times \\mathbb S^1\\)\n\n\ndouble pendulum\ntorus \\(\\mathbb S^1 \\times \\mathbb S^1\\)\ncylinder-squared \\(\\R^2 \\times \\mathbb S^1 \\times \\mathbb S^1\\)\n\n\npendulum-cart\ncylinder \\(\\R^1 \\times \\mathbb S^1\\)\n\\(\\R^3 \\times \\mathbb S^1 \\times \\mathbb S^1\\)\n\n\nparticle on a sphere\nsphere \\(\\mathbb S^2\\)\ntangent bundle of sphere \\(\\mathcal T\\mathbb S^2\\)\n\n\n\n\n: Some physical systems and their state and configuration spaces. Note that the state spaces are not literally the same as the ones shown in the table, merely equivalent (by a diffeomorphism).\n\n\n\nWith the above formalism, we can precisely define more concepts\n\ntrajectory, or path, in a manifold \\(\\mathcal M\\): a function of type \\(\\gamma: [t_0, t] \\to \\mathcal M\\).\nLagrangian of a physical system with state manifold \\(\\mathcal S\\): a function of type\n\n\\[\\mathcal L: \\R \\times \\mathcal T\\mathcal S \\to \\R.\\]\nThat is, it is a time-varying field on configuration space.\n\nHamiltonian of a physical system with state manifold \\(\\mathcal S\\): a function of type\n\n\\[\\mathcal H: \\R \\times \\mathcal T^\\ast \\mathcal S \\to \\R.\\]\nThat is, it is a time-varying field on phase space.\n\naction of a path\n\n\\[S(\\gamma) = \\int_{t_0}^t \\mathcal L(\\tau, \\gamma(\\tau), \\dot \\gamma(\\tau))d\\tau.\\]\nNow, the convex duality between Lagrangian and Hamiltonian transfers with almost no change in notation:\n\\[\\begin{cases}\n\\mathcal L(t, q, v) = \\max_{p\\in \\mathcal T_q^\\ast \\mathcal S} (\\langle p, v\\rangle - \\mathcal H(t, q, p)) \\\\\n\\mathcal H(t, q, p) = \\max_{v\\in \\mathcal T_q \\mathcal S} (\\langle p, v\\rangle - \\mathcal L(t, q, v))\n\\end{cases}\n\\]\n\\[\n\\begin{cases}\np^\\ast(t, q, v) = \\mathop{\\mathrm{arg\\,max}}_{p\\in \\mathcal T_q^\\ast \\mathcal S} (\\langle p, v\\rangle - \\mathcal H(t, q, p)) \\\\\nv^\\ast(t, q, p) = \\mathop{\\mathrm{arg\\,max}}_{v\\in \\mathcal T_q \\mathcal S} (\\langle p, v\\rangle - \\mathcal L(t, q, v))\n\\end{cases}\n\\]\nThe economic argument almost goes through without problem, but we need to be careful with some notations. In particular, we take another look at gradients. What does it mean to write \\(\\nabla_v \\mathcal L(t, q, v)\\)? The defining property is\n\\[\\mathcal L(t, q, v + u\\tau) = \\mathcal L(t, q, v) + \\lra{\\nabla_v \\mathcal L(t, q, v), u} \\tau + O(\\tau^2)\\]\nwhich implies the following operational definition:\n\\[\\nabla_v \\mathcal L(t, q, v) := u \\mapsto \\lim_{\\tau \\to 0} \\frac 1\\tau (\\mathcal L(t, q, v + u\\tau) - \\mathcal L(t, q, v))\\]\nThis definition makes it clear that \\(\\nabla_v \\mathcal L(t, q, v)\\) is a function of type \\(\\mathcal T_q \\mathcal S \\to\\R\\), thus it is an element of \\(\\mathcal T_q^\\ast \\mathcal S\\). Similarly, \\(\\nabla_p \\mathcal H(t, q, p)\\) is an element of \\(\\mathcal T_q \\mathcal S\\). Succinctly, \\(\\nabla_q \\mathcal L, \\nabla_v \\mathcal L, \\nabla_q \\mathcal H\\) are covector fields (like momentum), and \\(\\nabla_p \\mathcal H\\) is a vector field (like velocity).\nWith these, the types of every equation come out correctly again:\n\\[\\begin{cases}\nv = \\nabla_p \\mathcal H(t, q, p^\\ast(t, q, v))\\\\\np = \\nabla_v \\mathcal L(t, q, v^\\ast(t, q, p))\n\\end{cases},\n\\frac{d}{dt}(\\nabla_v \\mathcal L) = \\nabla_q \\mathcal L,\\quad\n\\begin{cases}\n\\dot p = -\\nabla_q \\mathcal H \\\\\n\\dot q = -\\nabla_p \\mathcal H\n\\end{cases}\\]\nLet’s call these the coordinate-free equations, to be contrasted with the coordinate-based equations, to be defined below.\n\n\n\nManifolds are geometrically pristine, but you can’t calculate numerically with them unless you lay down coordinate systems over them. Concretely, consider a state space \\(\\mathcal S\\). We take an open subset \\(U\\) of it, and define a coordinate system (with a possible dependence on time):\n\\[(q_1, ..., q_N): \\R \\times U \\to \\R^N\\]\nThis coordinate system then induces a coordinate system over the configuration space:\n\\[(q_1, ..., q_N; v_1, ..., v_N): \\R \\times \\mathcal T U \\to \\R^N \\times \\R^N\\]\nNow consider a different coordinate system\n\\[(Q_1, ..., Q_N): \\R \\times U \\to \\R^N\\]\nand suppose they are related by a function \\(f: \\R \\times \\R^N \\to \\R^N\\) such that\n\\[q(t, x) = f(t, Q(t, x))\\]\nThis is then a point transformation of the coordinate system.\nThe point transformation induces a transformation of the velocities, too. To find the transformation of velocities, consider a path \\(\\gamma: \\R \\to \\mathcal S\\). Its velocity at time \\(t\\) is \\(\\dot \\gamma(t) \\in \\mathcal T_{\\gamma(t)}\\mathcal S\\), a vector that looks like it literally lives in \\(\\R^N\\), but is not. It is not a native of \\(\\R^N\\), but thanks to the \\(q\\)-coordinate system, it is represented by the \\(\\R^N\\)-vector\n\\[\\frac{d}{dt} q(t, \\gamma(t)) \\in \\R^N\\]\nNow plug in \\(q(t, x) = f(t, Q(t, x))\\), to find a relationship between the representation of \\(\\dot \\gamma(t)\\) in \\(q\\)-coordinate system and \\(Q\\)-coordinate system:\n\\[\\frac{d}{dt} q(t, \\gamma(t)) = \\frac{d}{dt} f(t, Q(t, \\gamma(t))) = \\partial_t f(t, Q(t, \\gamma(t))) + \\frac{\\partial f}{\\partial Q} \\frac{d}{dt}Q(t, \\gamma(t))\\]\nMore succinctly, we have the following transformation from \\((t, Q, V)\\) to \\((t, q, v)\\):\n\\[\\begin{cases}\nq = f(t, Q) \\\\\nv= \\partial_t f(t, Q) + \\frac{\\partial f}{\\partial Q}(t, Q) V\n\\end{cases}\\]\nA note on matrix algebra: Conventionally, vectors are written as column-matrices, that is, \\(q, v, Q, V\\) are written as \\(N\\times 1\\) matrices. Correspondingly, gradients, being covectors, are written as row-matrices, that is, \\(\\nabla_q l, \\nabla_v l, \\nabla_Q L, \\nabla_V L\\), are written as \\(1 \\times N\\) matrices. Finally, gradients of vector-valued functions, like \\(\\frac{\\partial f}{\\partial Q}\\), are \\(N\\times N\\) matrices, with each row being a gradient of one vector coordinate. This convention makes everything come out cleanly, with no need to take the transpose of anything.\nThe point transformation also induces a transformation of the Lagrangians. While the Lagrangian itself is a function \\(\\mathcal L\\) of type \\(\\R \\times \\mathcal T \\mathcal S \\to \\R\\), the Lagrangians \\(L(t, Q, V), l(t, q, v)\\) are functions of type \\(\\R \\times \\R^N \\times \\R^N \\to \\R\\). Both \\(L, l\\) are induced from \\(\\mathcal L\\) by the choice of coordinates. We have\n\\[\\mathcal(t, x, u) = L(t, Q(t, x), V(t, x, u)) = l(t, q(t, x), v(t, x, u))\\]\nand plug in \\(q(t, x) = f(t, Q(t, x)), v = \\partial_t f(t, Q) + \\frac{\\partial f}{\\partial Q}(t, Q) V\\), we have\n\\[l\\left(t, f(t, Q), \\partial_t f(t, Q)  + \\frac{\\partial f}{\\partial Q}(t, Q) V\\right) = L(t, Q, V)\\]\nBrute computation shows that\n\\[\\frac{d}{dt}(\\nabla_V L) - \\nabla_Q L = \\left(\\frac{d}{dt}(\\nabla_v l) - \\nabla_q l\\right)\\frac{\\partial f}{\\partial Q}\\]\nimplying that the coordinate-based Euler–Lagrange equation is true in \\((Q, V)\\) coordinates iff it is true in \\((q, v)\\) coordinates.\nWhat, in the final analysis, is a point transformation? It is nothing more than changing a time-varying coordinate system on the state space. Since our derivation of the coordinate-based Euler–Lagrange equations required no special property of the coordinate system, it must be preserved by point transformations. All the above verification was really nothing but \"ceremonial\".\nIn more detail: we know that the coordinate-free EL equations are true, which implies that the \\(q\\)-coordinate-based EL equations and the \\(Q\\)-coordinate-based EL equations are both true (since they are merely two coordinate-based representations on the coordinate-free equation). No \\(Q\\)-to-\\(q\\) translation is necessary!\nWhat, then, is the phrase \"point transformation\" supposed to be contrasted with? It is contrasted with more general coordinate transforms that also depend on velocities, as \\(q = f(t, Q, V)\\). From the perspective given here, the contrast is really between \"state space coordinate systems\" and \"configuration space coordinate systems\". Whereas state space coordinate system is first defined as some\n\\[(q_1, ..., q_N): \\R \\times U \\to \\R^N\\]\nand that is then extended to \\((q_1, ..., q_N; v_1, ..., v_N): \\R \\times \\mathcal T U \\to \\R^N \\times \\R^N\\), a configuration space coordinate system defines \"all at once\" a complete coordinate system\n\\[(q_1, ..., q_N; v_1, ..., v_N): \\R \\times \\mathcal T U \\to \\R^N \\times \\R^N\\]\nIt is no wonder that such overly general coordinate systems do not have nice properties, and do not satisfy the coordinate-based Euler–Lagrange equations.\n\n\n\nBefore writing this chapter, try going through (Bohn 2018, chap. 7).\nWhereas for the Lagrangian \\(\\mathcal L\\), we can only perform point-transformations \\(q = f(t, Q)\\), lest the Euler–Lagrange equation is mangled, for the Hamiltonian, we can simultaneously transform both \\(q, p\\), while preserving the Hamiltonian equations of motion. Such transformations are called canonical transformations. They are of the form:\n\\[\\begin{cases}\nQ = f_Q(t, p, q)\\\\\nP = f_P(t, p, q)\n\\end{cases}\\]\nwhere the functions \\(f_Q, f_P: \\R \\times \\R^N \\times \\R^N \\to \\R^N\\) are required to satisfy some functional equations.\nThis is usually derived by brute force without comments. However, to truly understand the meaning, we need to understand phase space from a perspective even more modern than \\(\\mathcal T^\\ast \\mathcal S\\)."
  },
  {
    "objectID": "blog/posts/geometrical-mechanics/index.html#the-geometry-of-phase-space",
    "href": "blog/posts/geometrical-mechanics/index.html#the-geometry-of-phase-space",
    "title": "Yuxi on the Wired",
    "section": "The geometry of phase space",
    "text": "The geometry of phase space\nGiven a state space \\(\\mathcal S\\), both its configuration space and its phase space are obtained by attaching vector spaces to every point of it. Despite this, the geometry of phase space turns out to be a far richer thing than the geometry of configuration space. This fundamentally comes down to the difference between a cotangent vector and a tangent vector.\nConsider an infinitesimal parallelogram in the phase space, around the point \\((q, p)\\). The parallelogram has (signed) sides \\((\\delta q_1, ..., \\delta q_N; \\delta p_1, ..., \\delta p_N)\\). What should be its (signed) volume? The natural answer is of course\n\\[\\prod_i \\delta q_i \\delta p_i\\]\nbut is this a meaningful answer? That is, is this a mirage in \\(\\R^N \\times \\R^N\\) created by our choice of coordinates, or is this a faithful representation of something that truly takes place in the phase space \\(\\mathcal T^\\ast\\mathcal S\\) itself?\nThis answer is critically important, since if a concept takes place in the phase space itself, then it will be coordinate-free, and every coordinate system automatically translates that one coordinate-free concept. This is how we could have predicted that the coordinate-based Euler–Lagrange equations are preserved, by going up to the coordinate-free version of it, then coming back down again.\nHaving a coordinate-free thing is like having a lingua-franca between different coordinate-based representations.\n\nPoisson bracket\nThe Poisson bracket notation is convenient:\n\\[\\{f, g\\} = \\sum_{i=1}^{N} \\left( \\frac{\\partial f}{\\partial q_{i}} \\frac{\\partial g}{\\partial p_{i}} - \\frac{\\partial f}{\\partial p_i} \\frac{\\partial g}{\\partial q_i}\\right)\\]\nFor any differentiable function \\(f(t, q, p)\\), and any path \\(p(t), q(t)\\) that conforms to a Hamiltonian \\(H(t, q, p)\\), we have by Hamiltonian’s equations of motion\n\\[\\frac{d}{dt} f(t, p(t), q(t)) = \\partial_t f(t, p(t), q(t)) + \\{f, H\\}\\]\nor more succinctly, \\(\\dot f = \\partial_t f + \\{f, H\\}\\).\n\n\nLiouville’s theorem\nProof taken from (Tolman 1980, sec. 19).\n\n\nThe interpretation of phase space geometry\nLiouville’s theorem is a delicate construction, having several moving parts. We have a phase space, a volume-measurement on the phase space, a Hamiltonian on the phase space, and a density field on the phase space which flows according to the Hamiltonian. Only when all four moving parts come together do we get Liouville’s theorem.\nWhat is a phase space, in the final analysis? A phase space \\(\\mathcal T^\\ast \\mathcal S\\) is a state space \\(\\mathcal S\\), with each state \\(x\\) attached with \\(\\mathcal T^\\ast_x \\mathcal S\\), the vector space of all possible momenta at that state. Good... but not quite! This interpretation of phase space is still bound firmly to the economic interpretation, where each momentum \\(p\\) at state \\(x\\) is a vector of prices, with which we are allowed to measure the profit flow, as \\(\\langle p, \\dot x \\rangle\\).\nWhile this perspective is how Hamiltonian mechanics got its start, the modern abstract viewpoint of Hamiltonian mechanics has sailed far away from the safe harbor of \\(\\R^{2N}\\), past \\(\\mathcal T^\\ast \\mathcal S\\), and voyaged deep into the strange seas of symplectic geometry. Since the early days of the 20th century, there is a tacit understanding among physicists where the humble origin of the phase space \\(\\mathcal T^\\ast \\mathcal S\\) is suppressed, and it is presented instead as a \\(2N\\)-dimensional smooth manifold \\(\\mathcal P\\) equipped with some \\(\\omega\\), a way to measure volumes. The seams where \\(\\mathcal T^\\ast_x \\mathcal S\\) was attached to \\(\\mathcal S\\) are now plastered over, never there, never will be mentioned again... And this abstract viewpoint actually works.\nSpeak not how the phase space was born, but what you can use it for! This is a principle useful not only in programming (encapsulation, API, abstract interfacing), but also in modern mathematics (speak not of equality, but equivalences and isomorphisms...), and perhaps in society (Ye shall know them by their fruits. Do men gather grapes of thorns, or figs of thistles?).\nWhat do we gain and what do we lose when we go from \\(\\R^{2N}\\) to \\(\\mathcal T^\\ast \\mathcal S\\) to \\((\\mathcal P, \\omega)\\)? What we gain are new interpretations, and what we lose are old interpretations. See Table 3.\n\n::: {#table:three_abstractions} \\(\\R^{2N}\\) \\(\\mathcal T^\\ast \\mathcal S\\) \\((\\mathcal P, \\omega)\\) ———————– ——————————– —————————- tuples of real number points, vectors, and covectors points, areas, and volumes multivariate calculus vector bundle geometry symplectic geometry\n\nThe three steps of abstraction. :::\n\n\nIn \\(\\R^{2N}\\), we can interpret each point \\((q, p)\\in \\R^{2N}\\) economically: \\(q_1, ..., q_N\\) are the amounts of commodities, and \\(p_1, ..., p_N\\) are their market prices. In \\(\\mathcal T^\\ast \\mathcal S\\), half of this interpretation is lost, since we are not allowed to interpret \\(x\\in \\mathcal S\\) as a tuple of commodities, unless we lay down a more or less arbitrary coordinate system over it.\nNevertheless, half of this interpretation is preserved. While we are no longer able to interpret a point \\(x\\in \\mathcal S\\) as a stock of commodities that we own, we are still able to interpret a vector \\(v \\in \\mathcal T_x \\mathcal S\\) as a flow of commodities. This then allows us to interpret \\(\\langle p, v \\rangle\\) as a flow of profits: if we are producing at speed \\(v\\), and the market price vector is \\(p\\), then our profit flow is \\(\\langle p, v \\rangle\\). In economic language, we can’t talk of the stock, but we can still talk of the flow.\nGiving up half of the economic interpretation allows us to gain in coordinate-freedom. The Hamiltonian equations of motion become coordinate-free equations on \\(\\mathcal T^\\ast \\mathcal S\\), and we are given the guarantee that it is preserved by any coordinate system on \\(\\mathcal S\\).\nWhen we get to \\((\\mathcal P, \\omega)\\), the economic interpretation is totally destroyed, because there is no more separation between commodities and prices. A point in \\(\\mathcal P\\) simply is a point \\(y\\in \\mathcal P\\), not a 2-tuple \\((x, p)\\) with \\(x\\in \\mathcal S\\) and \\(p \\in \\mathcal T^\\ast_x\\mathcal S\\). There is no way to seize the \"second half\" of \\(y\\) and interpret it as a price vector.\nFurthermore, we cannot even interpret it as a physical state with a momentum covector, either. A momentum covector still looks like an arrow. It has a direction, a length, and can be scaled linearly, and added. Out there in \\(\\mathcal P\\), every point is just a point, not \"half base point, half vector\" like for \\(\\mathcal T^\\ast \\mathcal S\\).\nGiving this much up allows us to gain in even more coordinate-freedom. We are allowed to interpret a physical system not as a base state \\(x\\in \\mathcal S\\) plus a momentum state \\(p \\in \\mathcal T^\\ast_x\\mathcal S\\) , but simply as a phase state \\(y\\in \\mathcal P\\). This in particular gives us the freedom to consider coordinate systems on \\(\\mathcal P\\) that are \"fully nonlinear\", which is what canonical transforms are all about.\nRecall how we defined point transforms in Lagrangian mechanics. We start with a coordinate system \\((q_1, ..., q_N)\\) on an open subset \\(U\\) of the state space \\(\\mathcal S\\), then induced a coordinate system \\((q_1, ..., q_N; v_1, ..., v_N)\\) on \\(\\mathcal T U\\). We also stated that, while we could have went directly for a coordinate system on \\(\\mathcal T\\), this would break the Euler–Lagrange equation.\nIt turns out that the Hamiltonian equations of motion are sturdier than the Euler–Lagrange equation: there are large families of coordinate systems \\((q_1, ..., q_N; p_1, ..., p_N)\\) that we can directly define on open subsets of \\(\\mathcal T^\\ast \\mathcal S\\), and the resulting coordinate-based Hamiltonian equations would still be \\(\\dot p = -\\nabla_q H, \\dot q = \\nabla_p H\\), even if \\((q_1, ..., q_N; p_1, ..., p_N)\\) is not induced by any coordinate system on the state space!\nTo fully exploit the freedom, of course, means that we must break down the strict segregation between a state-point and a momentum-vector. In particular, this means that we no longer require \\(\\mathcal T_x \\mathcal S\\) to be treated with the rigid dignity of a linear space, but the rough freedom of a manifold space:\n\\[p(x, ku) \\neq k p(x, u) \\text { in general, for } (x, u)\\in \\mathcal T^\\ast \\mathcal S, \\: k\\in \\R\\]\nGiven that, we can immediately see why canonical transforms are in general of the form\n\\[Q(t, x, u) = f_Q(t, q(t, x, u), p(t, x, u)),\\quad P(t, x, u) = f_P(t, q(t, x, u), p(t, x, u))\\]\nor more succinctly,\n\\[Q = f_Q(t, q, p),\\quad P = f_P(t, q, p)\\]\nThey have to nonlinearly \"mix up\" state and momentum, because that’s the only way to truly exploit all the freedoms that the sturdy Hamiltonian’s equations of motion grants us.1\n1 Mathematicians exploit every freedom that they are given... sounds evil, but it works in math.Of course, the Hamiltonian equations are not that tough. Some restraint is needed. Not everything goes. What is the restraint? The volume must be preserved! That is precisely what \\(\\omega\\) is there for: it measures areas. A coordinate system on the phase space is only given the title of \"canonical\" iff the coordinate system represents \\(\\omega\\) correctly.\nThus, we find that by exploiting exactly as much freedom as Hamiltonian mechanics gives us, while keeping track of the boundaries so that we are not giving ourselves too much freedom and shooting ourselves in the foot, we walked inexorably into treating the phase space as \\((\\mathcal P, \\omega)\\) – as an object of symplectic geometry."
  },
  {
    "objectID": "blog/posts/geometrical-mechanics/index.html#all-the-variational-principles-of-physics-that-youre-likely-to-ever-see",
    "href": "blog/posts/geometrical-mechanics/index.html#all-the-variational-principles-of-physics-that-youre-likely-to-ever-see",
    "title": "Yuxi on the Wired",
    "section": "All the variational principles of physics that you’re likely to ever see",
    "text": "All the variational principles of physics that you’re likely to ever see\nBased on (Lanczos 1970).\nOthers not covered: D’lambert’s principle. Gauss’s principle of least constraint, Hertz principle of least curvature, etc.\nLet’s be clear here.\n\na variation of a function \\(\\gamma: \\R^n \\to \\R^m\\) is a function \\(\\gamma + \\delta \\eta\\), such that \\(\\eta: \\R^n \\to \\R^m\\), and \\(\\delta\\) is an infinitesimal.\na constrained variation of a function \\(\\gamma\\) is a variation \\(\\gamma + \\delta \\eta\\), such that \\(\\eta\\) satisfies certain constraints \\(c\\).\na functional is a function that maps a function to a real number. For example, the Lagrangian action \\(S\\) is a functional, defined by\n\n\\[S(\\gamma) = \\int L(t, \\gamma(t), \\dot \\gamma(t))dt.\\]\n\na functional \\(S\\) has zero variation at \\(\\gamma\\) under constraint \\(c\\) iff for any variation \\(\\delta\\eta\\) satisfying constraint \\(c\\), we have \\(S(\\gamma + \\delta \\eta) = S(\\gamma) + o(\\delta)\\). We often write it simply as \\((\\delta S(\\gamma))_c = 0\\).\nvariational calculus is a collection of techniques for solving calculus problems involving variations.\na variational principle is a statement with the following format:\n\n::: center A trajectory \\(\\gamma\\) of the system is a physically valid trajectory iff \\((\\delta S(\\gamma))_c = 0\\). :::\nNow that we are clear on that, we can tabulate just about every variational principles of physics that you’re likely to ever see in Table [table:var-prin].\n\n\n1.0|L|L|L|L|L| name & Where is the trajectory? & specification & constraint & the functional\nHamilton’s principle & state spacetime & Lagrangian \\(L(t, q, v)\\) & fixed \\((t_0, q_0), (t_1, q_1)\\) & \\(\\int_\\gamma L(t, q, \\dot q)dt\\)\nmodified Hamilton’s principle & phase spacetime & Hamiltonian \\(H(t, q, p)\\) & fixed \\((t_0, q_0), (t_1, q_1)\\) & \\(\\int_\\gamma (\\sum_i p_i \\dot q_i - H(t, q, p))dt\\)\nMaupertuis’ principle2 & phase space & time-independent Hamiltonian \\(H(q, p)\\) & fixed \\(q_0, q_1\\), constant \\(H(q, p)\\) & \\(\\int_\\gamma \\sum_i p_i dq_i\\)\nJacobi’s form of Maupertuis’ principle & state space & Lagrangian of form \\(L(t, q, v) = \\frac 12 v^T M v - V(q)\\) & fixed \\(q_0, q_1\\), bonuded \\(V(q) \\leq 0\\) & \\(\\int_\\gamma \\sqrt{(E - V(q)) dq^T M dq}\\)\ntimed Maupertuis’ principle & state spacetime & Lagrangian of form \\(L(t, q, v) = \\frac 12 v^T M v - V(q)\\) & fixed \\(q_0, q_1\\), constant \\(\\frac 12 \\dot q^T M \\dot q + V(q)\\) & \\(\\int_\\gamma (\\dot q^T M \\dot q) dt\\)\nFermat’s principle of stationary pathlength3 & state space & Lagrangian of form \\(L(t, q, v) = \\frac 12 v^T M v\\) & fixed \\(q_0, q_1\\), constant \\(\\frac 12 \\dot q^T M \\dot q\\) & \\(\\int_\\gamma \\sqrt{dq^T M dq}\\)\nFermat’s principle of stationary time & state spacetime & Lagrangian of form \\(L(t, q, v) = \\frac 12 v^T M v\\) & fixed \\(q_0, q_1\\), constant \\(\\frac 12 \\dot q^T M \\dot q\\) & \\(t_1 - t_0\\)\n\n2 \"principle of least action\" in (Goldstein, Poole, and Safko 2008)3 Corollary: Hertz’s principle of least curvature\n\n\nHamilton’s principle and modified Hamilton’s principle\nThere are two principles that are often confused with impunity by physicists. The fact is that they are indeed equivalent (which is why they can be confused with impunity), but that is no excuse for bad mathematics.\nHamilton’s principle is a principle about trajectories of type \\(\\gamma: [t_0, t_1] \\to \\mathcal S\\). That is, \\(\\gamma(t)\\) is a state of the system. Variations can be thought of as making a state perturbation \\(\\delta \\eta(t)\\) at every time.\nIn contrast, modified Hamilton’s principle is a principle about trajectories of type \\(\\Gamma: [t_0, t_1] \\to \\mathcal P\\). That is, \\(\\Gamma(t)\\) is a phase of the system, specifying both its state and momentum. Variations can be thought of as making a state perturbation \\(\\delta \\eta_p(t)\\) and a momentum perturbation \\(\\delta \\eta_p(t)\\) at every time.\nOne may object that modified Hamilton’s principle is performing physically impossible variations: how could you perform variations on position and momentum independently of each other? Shouldn’t we have \\(\\delta p = m\\delta \\dot q = \\delta (\\nabla_{v} L(t, q, \\dot q))\\) at all times? To this objection, there are four layers of replies.\n\nThe equivalence of Hamilton’s principle and modified Hamilton’s principle, to be proved below, is a theorem in pure mathematics. It makes no demand on physical reality. It merely states that Hamilton’s principle specifies the same trajectories as modified Hamilton’s principle. Consequently, if it happens that these trajectories are physically real, then they can be specified by either principle.\nThe economic interpretation of momentum \\(p\\) is merely the market price. The equation \\(p = \\nabla_{v} L(t, q, \\dot q)\\) is true if we also assume that the producer is profit-maximizing. Now, if the trajectory \\(\\gamma\\) is optimal, then it implies that the producer is profit-maximizing. But after a perturbation of \\(\\gamma\\) to \\(\\gamma + \\delta \\eta\\), the producer is not necessarily profit-maximizing.4 Consequently, even in Hamilton’s principle, there is no requirement that \\(p = \\nabla_{v} L(t, q, \\dot q)\\). The modified Hamilton’s principle makes this interactive dance between the producer and the market explicit: we allow both the production schedule and the market price schedule to vary independently. Then, the equation \\(\\delta \\int (\\sum_i p_i \\dot q_i - H)dt = 0\\) is a statement about the trajectory of the producer-market system, and solving it would simultaneously solve both the producer and the market. In contrast, the equation \\(\\delta \\int L dt = 0\\) is a statement about the producer, and solving it by imagining a market \\(p\\) is useful, but not necessary.\nEven in classical mechanics, momentum is not real. We are fooled by our long habit of thinking about classical mechanics as if it is merely a more mathematical version of our intuition. Classical mechanics is actually unintuitive.5 In classical mechanics, there is no necessary connection between momentum and velocity – \"If \\(L = \\frac 12 v^T M v\\), then \\(p = Mv\\) on physically valid paths\" actually needs to be proved from Hamilton’s principle, not baked into the definition of momentum!\nThough in classical mechanics, both principles are equivalent, in modern physics, the modified Hamilton’s principle is primary, and the Hamilton’s principle a mere derivative. Furthermore, the phase space is primary, and the division of it into position-momentum is arbitrary. At a more fundamental level, there is no distinction between position and momentum. A \"rotation in phase space\" can transform position and momentum into each other.\n\n4 Unless the market price is perturbed in just the right way to make the producer profit-maximizing – If the producer messes up, the market can still accommodate the producer and make the producer look as if it is still profit-maximizing... Just like Potemkin villages!5 Why else did Newton come two thousand years after Aristotle? Though quantum mechanics is certainly more unintuitive.\nTheorem 1 (Hamilton’s principle and modified Hamilton’s principle are equivalent.) Given a state space \\(\\mathcal S\\), a Lagrangian \\(L(t, q, v)\\) on the configuration space, and a Hamiltonian \\(H(t, q, p)\\) on the phase space, related by convex duality, then a path \\(\\gamma: [t_0, t_1] \\to \\mathcal S\\) on state space satisfies\n\\[\\delta \\int_\\gamma L(t, q, \\dot q) dt = 0\\]\nwith fixed \\((t_0, q_0), (t_1, q_1)\\), iff its corresponding path \\(\\Gamma: [t_0, t_1]: \\to \\mathcal T^\\ast \\mathcal S\\) on phase space satisfies\n\\[\\delta \\int_\\Gamma \\sum_i p_i \\dot q_i - H(t, q, p) dt = 0\\]\nwith fixed \\((t_0, q_0), (t_1, q_1)\\) (and variable \\(p_0, p_1\\)).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\((\\Rightarrow):\\) Since \\(\\gamma\\) has zero variation for the action \\(\\int_\\gamma L(t, q, \\dot q)dt\\), and \\(H\\) is related to \\(L\\) by convex duality, by our previous results, the Hamiltonian equations of motion are satisfied along \\(\\gamma\\). That is, \\(-\\nabla_q H = \\dot p, \\nabla_p H = \\dot p\\).\nPerform variation \\(\\Gamma + \\delta \\eta\\) on phase space. The variation in the action is\n\\[\\begin{aligned} \\delta S(\\Gamma) &= \\int_{t_0}^{t_1} [\\langle \\delta p, \\dot q\\rangle + \\langle  p, \\delta \\dot q\\rangle - \\langle \\nabla_q H, \\delta q\\rangle - \\langle \\nabla_p H, \\delta p\\rangle ] dt \\\\ &= \\int_{t_0}^{t_1} [\\langle \\delta p, \\dot q\\rangle + \\langle  p, \\delta \\dot q\\rangle - \\langle -\\dot p, \\delta q\\rangle - \\langle \\dot q, \\delta p\\rangle ] dt \\\\ &= \\int_{t_0}^{t_1} [\\langle  p, \\delta \\dot q\\rangle + \\langle \\dot p, \\delta q\\rangle] dt \\\\ &= \\langle p, \\delta q\\rangle \\Big|_{t_0}^{t_1} = 0 \\end{aligned}\\]\nsince \\(\\delta q= 0\\) at the end points.\n\\((\\Leftarrow):\\) Since \\(\\delta \\int_\\Gamma \\sum_i p_i \\dot q_i - H(t, q, p) dt = 0\\) at \\(\\Gamma\\) under constraint of fixed \\((t_0, q_0), (t_1, q_1)\\), it must also have zero variation if we use the stronger constraint of fixed \\((t_0, q_0, p_0), (t_1, q_1, p_1)\\). Then the Euler–Lagrange equations state6\n\\[-\\nabla_q H = \\dot p, \\quad \\nabla_p H = \\dot p\\]\nwhich, as we proved, are precisely the conditions (no arbitrage pricing, and stationary profit flow) for \\(\\delta \\int_\\gamma L = 0\\).\n\n\n\n6 One can interpret this as treating the phase space as if it is a state space of a physical system with \\(2N\\) degrees of freedom.\n\nMaupertuis’ principle\nMaupertuis’ principle is a principle for specifying orbits in phase space. An orbit is a trajectory of the physical system, but with timing information lost. We know that the system traveled through the states on the orbit, one after another, but we don’t know how fast is the traversal.\nIn order to be very explicit about it, we will write orbits in phase space as \\(\\mu: [a, b] \\to \\mathcal P\\). Here \\(a, b\\) look like \"start and end times\" and \\(\\mu(s)\\) looks like \"location of the path at time \\(s\\)\", but \\(s\\) is not time, and \\(a, b\\) are not moments in time either. It is really just a parametrization of the curve, with no implications about how fast, or how slow, the system would actually traverse the orbit.\nThe integral \\(\\int_\\mu \\sum_i p_i \\dot q_i ds\\) is unchanged by stretching and pressing the timing of \\(\\mu\\). That is, let \\(f: [a', b'] \\to [a, b]\\) be a strictly increasing differentiable function, then \\(\\int_{\\mu\\circ f} \\sum_i p_i \\dot q_i ds = \\int_\\mu \\sum_i p_i \\dot q_i ds\\). Consequently, Maupertuis’ principle is really concerned only with the orbit, not the timing of the orbit.\nSince timing is lost, the constraint of fixed \\((t_0, q_0), (t_1, q_1)\\) cannot apply. However, merely fixing \\(q_0, q_1\\) is too little constraint. The solution is to add a new constraint: the variation must stay on the surface of constant energy \\(E\\). That is, \\(H(\\mu'(s')) = E\\) for any variation \\(\\mu'\\) and parameter \\(s'\\). This is how we arrive at Maupertuis’ principle.\n::: prop\nWhen the Hamiltonian is time-independent, Hamilton’s principle and Maupertuis’ principle are equivalent (after a retiming scaling).\nGiven phase space \\(\\mathcal P\\) and a time-independent Hamiltonian \\(H(q, p)\\) over the phase space, such that \\((\\nabla_q H, \\nabla_p H)\\) is never zero, then any trajectory \\(\\gamma: [t_0, t_1] \\to \\mathcal P\\) that satisfies Hamilton’s principle also satisfies Maupertuis’ principle.\nConversely, given any orbit \\(\\mu: [a, b] \\to \\mathcal P\\) with constant \\(H\\) that satisfies Maupertuis’ principle, there exists a \"retiming map\" \\(f: [t_0, t_1]\\to [a, b]\\) such that \\(f\\) is monotonically increasing, and \\(\\mu\\circ f\\) satisfies Hamilton’s principle. :::\n::: proof Proof. We show that Maupertuis’ principle is equivalent to Hamilton’s equations of motion after a retiming map.\nConsider orbit \\(\\mu: [a, b] \\to \\mathcal P\\) in phase space, with constant \\(H(\\mu(s)) = E_0\\). By integration-by-parts, we have\n\\[\\delta \\int \\langle p, \\dot q\\rangle ds = \\int \\langle \\delta p, \\dot q\\rangle - \\langle \\dot p, \\delta  q\\rangle ds + \\cancel{\\langle p, \\delta q\\rangle} \\Big|_{a}^{b}\\]\nwhere the variation fixes \\(q_0, q_1\\) and \\(H\\).\nNow, \\(\\delta H = \\langle \\nabla_p H, \\delta p\\rangle +  \\langle \\nabla_q H, \\delta q\\rangle = 0\\). So, if the orbit satisfies Hamilton’s equations of motion after a retiming map \\(f\\), that is,\n\\[\\begin{cases}     \\dot p = -f'(s)\\nabla_p H \\\\     \\dot q = f'(s)\\nabla_q H \\end{cases}\\]\nthen plugging it back, we get\n\\[\\delta \\int \\langle p, \\dot q\\rangle ds = \\int f'(s) \\delta H ds = 0\\]\nIt is routine to check that, given four vectors \\(a, b, c, d\\in \\R^N\\), if \\(\\forall x, y\\in \\R^N\\),\n\\[\\langle a, x\\rangle - \\langle b, y\\rangle = 0 \\implies \\langle c, x\\rangle - \\langle d, y\\rangle = 0\\]\nthen there exists \\(\\lambda &gt; 0\\) such that \\(c = \\lambda a, d = \\lambda d\\).\nThus, if the variation is zero for all \\(\\delta q, \\delta p\\) with fixed \\(H\\), then there exists some continuous and positive function \\(\\lambda: [a, b] \\to \\R\\) such that\n\\[\\begin{cases}     \\dot p = -\\lambda(s)\\nabla_p H \\\\     \\dot q = \\lambda(s)\\nabla_q H \\end{cases}\\]\nNow solve for \\(f' = \\lambda^{-1}\\) by integration7, then \\(f\\) is the desired retiming map. ◻ :::\n7 Since \\(\\lambda\\) is continuous and positive, with compact domain, its range must be bounded below by some positive constant \\(\\epsilon &gt; 0\\), thus the integration would not diverge."
  },
  {
    "objectID": "blog/posts/geometrical-mechanics/index.html#canonical-transformations",
    "href": "blog/posts/geometrical-mechanics/index.html#canonical-transformations",
    "title": "Yuxi on the Wired",
    "section": "Canonical transformations",
    "text": "Canonical transformations\n\nGenerating functions\nThe dynamics of a physical system can be fully defined by its Lagrangian function. However, the Lagrangian function is not fully defined by the dynamics. There are many possible functions that can all play the role of the Lagrangian.\nSuppose \\(L(t, q, v)\\) is a Lagrangian function, then take any twice-differentiable function \\(F(t, q)\\), and define\n\\[L' dt := L dt + dF\\]\nwhich implies\n\\[L'(t, q, v) := L(t, q, v) + \\partial_t F(t, q) + \\langle \\nabla_q F(t, q), v\\rangle\\]\nthen it is easy to directly verify that a trajectory \\(\\gamma(t)\\) satisfies the Euler–Lagrange equations for \\(L\\) iff it satisfies them for \\(L'\\). Consequently, both \\(L\\) and \\(L'\\) are different functions that can both play the role of Lagrangian for the same physical system.\nInstead of directly computing the Euler–Lagrangian equations, we can also do it directly by variational principles: For any trajectory \\(\\gamma: [t_0, t_1] \\to \\mathcal S\\) we have\n\\[\\int_{t_0}^{t_1} L'(t, \\gamma(t), \\dot \\gamma(t))dt = F(t, \\gamma(t))|_{t_0}^{t_1} + \\int_{t_0}^{t_1} L(t, \\gamma(t), \\dot \\gamma(t))dt\\]\nConsequently, \\(\\delta \\int Ldt = 0\\) iff \\(\\delta \\int L'dt = 0\\), so a trajectory has stationary action according to one Lagrangian iff according to the other.\nSuch \\(F(t, q)\\) are called a generating function for transforming a Lagrangian function. Generating functions are really just functions that are picked to play a certain role. That is, being a generating function is not an intrinsic property of a function, but extrinsic, because some human physicist has decided to use it for generating a new Lagrangian from an old one. This is why I don’t like saying \"\\(F\\) a generating function...\". Instead, I prefer to say \"Now we use \\(F\\) to generate a new Lagrangian...\" Nevertheless I am forced to use the term because it is a venerable error, a bug that became a feature.\n\n\nGenerating functions for Hamiltonians\nHamiltonians are freer than Lagrangians. Instead of one way, there are many ways to generate new Hamiltonians from old.\nTaking inspiration from Lagrangian generating functions, we write down the following equation:\n\\[\\langle p , dq \\rangle - hdt = \\langle P, dQ \\rangle - Hdt + dG\\]\nwhere \\(G: \\R \\times \\mathcal P \\to \\R\\) is any twice-differentiable function on phase spacetime.\n::: theorem If \\((q, p), (Q, P)\\) are two coordinate systems on the phase spacetime, and \\(h, H, G\\) are twice-differentiable functions on phase spacetime, and\n\\[\\langle p , dq \\rangle - hdt = \\langle P, dQ \\rangle - Hdt + dG\\]\nThen along any trajectory in phase spacetime, \\((q, p)\\) satisfies Hamiltonian equations of motion for \\(h\\), iff \\((Q, P)\\) satisfies Hamiltonian equations of motion for \\(H\\). :::\n::: proof Proof. Let \\(\\gamma: [t_0, t_1] \\to \\mathcal P\\) be any trajectory, not necessarily satisfying Hamilton’s equations of motion. Then for any variation of \\(\\gamma\\) with fixed \\(t_0, t_1\\), we have by integration-by-parts,\n\\[\\begin{aligned}     &\\delta\\int_\\gamma \\langle p , dq \\rangle - hdt = \\delta\\int_\\gamma \\langle P, dQ \\rangle - Hdt + dG\\\\     &= \\int (\\langle \\delta P, \\dot Q - \\nabla_P H\\rangle - \\langle \\dot P + \\nabla_Q H, \\delta Q\\rangle ) dt + (\\langle P, \\delta Q \\rangle + \\delta G)\\Big|_{t_0}^{t_1} \\\\     &= \\int (\\langle \\delta p, \\dot q - \\nabla_p h\\rangle - \\langle \\dot p + \\nabla_q h, \\delta q\\rangle ) dt + \\langle p, \\delta q \\rangle \\Big|_{t_0}^{t_1}  \\end{aligned}\\]\nThe boundary terms are equal, since \\(\\langle p , \\delta q \\rangle - h \\delta t = \\langle P, \\delta Q \\rangle - H\\delta t + \\delta G\\), and \\(\\delta t = 0\\) as we fixed \\(t_0, t_1\\).\nThus, for any variation of \\(\\gamma\\) with fixed \\(t_0, t_1\\),\n\\[\\int (\\langle \\delta p, \\dot q - \\nabla_p h\\rangle - \\langle \\dot p + \\nabla_q h, \\delta q\\rangle ) dt = \\int (\\langle \\delta P, \\dot Q - \\nabla_P H\\rangle - \\langle \\dot P + \\nabla_Q H, \\delta Q\\rangle ) dt\\]\nThus, if \\(\\gamma\\) satisfies Hamiltonian equations of motion for \\((q, p), h\\), then it also does so for \\((Q, P), H\\). ◻ :::\n\n\nCoordinate-based canonical transforms\nThis section might make more sense after reading the next section.\nThe equation\n\\[\\langle p , dq \\rangle - hdt = \\langle P, dQ \\rangle - Hdt + dG\\]\nlives in phase spacetime. That is, \\(dq, dt, dQ, dG\\) are all differentials in \\(\\R \\times \\mathcal P\\). This is elegant, but not good for concrete computations, which requires coordinate-based equations.\nGenerally, \\(G(t, y)\\) is a function on phase spacetime, so it could be represented in any coordinate system of phase spacetime. For example, we can represent it as \\(G(t, y) = G_{q, p}(t, q(t, y), p(t, y))\\), or \\(G(t, y) = G_{Q, P}(t, Q(t, y), P(t, y))\\), or even mixed coordinates like \\(G(t, y) = G_{q, P}(t, q(t, y), P(t, y))\\), etc.\nMost representations result in intractable coordinate-based equations, but a few are actually usable. These are traditionally classified as \"type 1\" to \"type 5\".\nType 1: \\(G(t, y) = F_1(t, q(t, y), Q(t, y))\\).\nPlugging it in, we find\n\\[\\langle p , dq \\rangle - hdt = \\langle P, dQ \\rangle - Hdt + \\partial_t F_1 dt + \\langle \\nabla_q F_1, dq \\rangle + \\langle \\nabla_Q F_1, dQ \\rangle\\]\nyielding the equations\n\\[\\begin{cases}     H(t, Q, P) = h(t, q, p) + \\partial_t F_1(t, q, Q) \\\\     p = \\nabla_q F_1(t, q, Q) \\\\     P = - \\nabla_Q F_1(t, q, Q) \\end{cases}\\]\nIn order to solve for the canonical transform, first invert \\(p = \\nabla_p F_1(t, q, Q)\\) to obtain \\(Q = f_Q(t, q, p)\\), then plug it into \\(P = - \\nabla_Q F_1(t, q, Q)\\) to obtain \\(P = f_P(t, q, p)\\). Inverting them gives us \\(q = g_q(t, Q, P), p = g_p(t, Q, P)\\).\nThen, given any Hamiltonian \\(h(t, q, p)\\), the corresponding \\(H(t, Q, P)\\) is found by \\(H(t, Q, P) = h(t, q, p) + \\partial_t F_1(t, q, Q)\\), or very explicitly,\n\\[H(t, Q, P) = h(t, g_q(t, Q, P), g_p(t, Q, P)) + \\partial_t F_1(t, g_q(t, Q, P), Q)\\]\nType 2: \\(G(t, y) = F_2(t, q(t, y), P(t, y)) - \\langle P(t, y), Q(t, y)\\rangle\\).\nWhy \\(\\langle P, Q\\rangle\\)? Directly writing down \\(G = F_2(t, q, P)\\) results in the following equation:\n\\[\\langle p , dq \\rangle - hdt = \\langle P, dQ \\rangle - Hdt + \\partial_t F_2 dt + \\langle \\nabla_q F_2, dq \\rangle + \\langle \\nabla_P F_2, dP \\rangle\\]\nHere, there is an entanglement between terms \\(dq, dQ, dP\\). Since there are only \\(2N\\) dimensions in the phase space, but there are \\(3N\\) differentials in \\(dq, dQ, dP\\), it must be possible to represent \\(N\\) of them as a linear combination of the other \\(2N\\) differentials. In particular, we can represent \\(dQ\\) as a linear combination of \\(dq, dP\\).\nInstead, we can directly cancel out \\(\\langle P, Q\\rangle\\) from the equation by writing \\(G\\) as \\(G + \\langle P, Q\\rangle - \\langle P, Q\\rangle\\), then represent \\(G + \\langle P, Q\\rangle\\) as \\(F_2(t, q, P)\\). This then gives\n\\[\\begin{cases}     H(t, Q, P) = h(t, q, p) + \\partial_t F_2(t, q, P) \\\\     Q = \\nabla_P F_2(t, q, P) \\\\     p = \\nabla_q F_2(t, q, P) \\end{cases}\\]\nType 3: \\(G = F_3(t, p, Q) + \\langle p, q\\rangle\\).\nType 4: \\(G = F_4(t, p, P) + \\langle p, q\\rangle  - \\langle P, Q\\rangle\\).\nType 5: \\(G = F(t, p_{I_1}, q_{I_2}, P_{I_3}, Q_{I_4}) + \\langle p_{I_1}, q_{I_1} \\rangle - \\langle P_{I_3}, Q_{I_3} \\rangle\\).\n\\[\\begin{cases}     H(t, Q, P) = h(t, q, p) + \\partial_t F(t, p_{I_1}, q_{I_2}, P_{I_3}, Q_{I_4}) \\\\     q_{I_1} = -\\nabla_{p_{I_1}} F(t, p_{I_1}, q_{I_2}, P_{I_3}, Q_{I_4}) \\\\     p_{I_2} = \\nabla_{q_{I_2}} F(t, p_{I_1}, q_{I_2}, P_{I_3}, Q_{I_4}) \\\\     Q_{I_3} = \\nabla_{P_{I_3}} F(t, p_{I_1}, q_{I_2}, P_{I_3}, Q_{I_4}) \\\\     P_{I_4} = -\\nabla_{Q_{I_4}} F(t, p_{I_1}, q_{I_2}, P_{I_3}, Q_{I_4}) \\end{cases}\\]\nHere, \\(I_1, I_2, I_3, I_4\\) stand for subsets of the indexing set \\(\\{1, 2, ..., N\\}\\). We also require \\(I_1 \\cup I_2 = I_3 \\cup I_4 = \\{1, 2, ..., N\\}\\)\nNote that types 1 to 4 are all special cases of type 5.\n\n\nExamples of canonical transforms\n\nPoint transforms\nIf \\(G= 0\\), then we need to solve only the equation \\(\\langle p, dq \\rangle = \\langle P, dQ \\rangle\\), which can be done in general iff \\(dQ\\) is a linear combination of \\(dq\\), thus \\(Q = f_Q(t, q)\\) for some function \\(f_Q\\). This is just the point transform, with solution\n\\[P = ([\\nabla_q f_Q]^T)^{-1}p\\]\n\n\nInterpolating a canonical transform\nGiven any canonical transform from \\((q, p)\\) to \\((Q, P)\\), for any two times \\(t_0 &lt; t_1\\), we can interpolate between \\((q, p), (Q, P)\\) over the period \\([t_0, t_1]\\). That is, we construct a canonical transform from \\((q, p)\\) to \\((\\bar q, \\bar p)\\) such that \\((\\bar q, \\bar p) = (q, p)\\) at \\(t=t_0\\), and \\((\\bar q, \\bar p) = (Q, P)\\) at \\(t = t_1\\).\nThe idea is to note that any canonical transform can be written in type 2, including the identity transform.\nThe identity transform from \\((q, p)\\) to \\((\\bar q, \\bar p)\\) can be represented in type 2 as\n\\[G_0 = \\langle q, \\bar p \\rangle - \\langle \\bar p, \\bar q \\rangle\\]\nGenerate the transform from \\((q, p)\\) to \\((Q, P)\\) by \\(G\\), and represent it in type 2 as\n\\[G_1 = F_2(t, q, P) - \\langle P, Q \\rangle\\]\nNow interpolate them by\n\\[G = \\frac{t_1-t}{t_1 - t_0} \\langle q, \\bar p \\rangle + \\frac{t - t_0}{t_1 - t_0} F_2(t, q, \\bar p)  - \\langle \\bar p, \\bar q \\rangle\\]\n\n\nTime-evolution is a canonical transform generated by the action\nRecall that, for any coordinate system \\((q, p)\\) and Hamiltonian \\(h\\), we defined the action function (\"Hamilton’s principal function\") \\(S(t_1, q_1; t_0, q_0)\\) to be the action for the path \\(\\gamma\\) from \\((t_0, q_0)\\) to \\((t_1, q_1)\\). We also proved, during derivation of the HJE,\n\\[dS = \\langle p_1, dq_1 \\rangle -h(t_1, q_1, p_1) dt_1 - \\langle p_0, dq_0 \\rangle + h(t_0, q_0, p_0) dt_0\\]\nRearrange, and using suggestive notation, we get...\n\\[\\langle p_0, dq_0 \\rangle - h_0 dt_0 = \\langle p_1, dq_1 \\rangle -h_1 dt_1 + dS\\]\nSo we find that time evolution is a canonical transformation generated by \\(S\\).\nIn more detail, fix some coordinate system \\((q, p)\\). Then for any Hamiltonian \\(\\mathcal H : \\R \\times \\mathcal P \\to \\R\\), define its time-evolution function \\(\\phi\\), such that \\(\\phi(t_0, t_1; y)\\) is the point that we end up with at time \\(t_1\\), if we start at \\(y\\) at time \\(t_0\\), and evolve according to \\(\\dot q = \\nabla_p h, \\dot p = -\\nabla_q h\\), where \\(h(t, q(t, x), p(t, x)) = \\mathcal H(t, x)\\) is the coordinate-based version of the coordinate-free \\(\\mathcal H\\).\nNow, fix some time-interval \\(s\\in \\R\\), then define the (coordinate-free) function \\(\\mathcal G\\), the action of the trajectory starting at \\((t, y)\\) and lasting for \\(s\\):\n\\[\\mathcal G(t, y) := S(t+s, q(t+s, \\phi(t, t+s; y)); t, q(t, y))\\]\nand the new coordinate system with a new Hamiltonian, obtained by \"evolving for \\(s\\) time\": \\[\\begin{aligned} Q(t, y) = q(t+s, \\phi(t, t+s; y)) \\\\ P(t, y) = p(t+s, \\phi(t, t+s; y)) \\\\ H(t, Q(t, y), P(t, y)) = \\mathcal H(t + s, \\phi(t, t+s; y)) \\end{aligned}\\]\nwhich allows a coordinate-based representation of \\(\\mathcal G\\):\n\\[G(t, q, Q) = S(t+s, Q; t, q)\\]\nWith these definitions, we have\n\\[dG = -Hdt + \\langle P, dQ \\rangle + hdt -\\langle p, dq \\rangle\\]\nthat is, \\((Q, P), H\\) is canonically transformed from \\((q, p), h\\) via the function \\(G\\).\n\n\n\nSimple harmonic oscillator\nConsider a SHO with \\(N\\) degrees of freedom. Its Hamiltonian is\n\\[H = \\frac 12 p^T M^{-1} p + \\frac 12 q^T K q\\]\nwhere \\(M\\) is the matrix representing the masses of the system, and \\(K\\) is the matrix representing the elastic constants of the system.\n\nTranslation is a canonical transform generated by momentum\n\n\nRotation is a canonical transform generated by angular momentum\n\n\n\nCanonical transforms, in general\nThere are two possible ways to define canonical transforms. The more concrete way is by using generating functions: two coordinate systems \\((q, p), (Q, P)\\) on phase spacetime are generated canonical transforms of each other iff\n\\[\\langle p, dq\\rangle - \\langle P, dQ\\rangle = dG\\]\nfor some \\(G\\) functions on phase spacetime. Remember that \\(dq, dQ\\) are differentials with constant time.\nAs for the more abstract form... long story short: every canonical transform has a generating function. This is usually called \"Carathéodory Theorem\". See (Goldstein, Poole, and Safko 2008, sec. 9.5).\nThis has a more elegant form with exterior calculus. Take exterior differentiation (again, only in phase space, not in time), we get\n\\[\\sum_i dp_i \\wedge dq_i = \\sum_i dP_i \\wedge dQ_i\\]\nNow take wedge product \\(N\\) times with itself, we get\n\\[\\bigwedge_i dp_i \\wedge dq_i = \\bigwedge_i dP_i \\wedge dQ_i\\]\nInterpretation: canonical transforms preserve phase space volumes. That is, if we have an open subset in phase space, defined coordinate-free, then we can compute its volume by writing down a canonical coordinate system \\((q, p)\\) and integrating \\(\\prod_i dp_i dq_i\\). The result is unchanged by a canonical transform to \\((Q, P)\\).\nThis gives us a new proof of Liouville’s theorem:\n\nProof. Since time-evolution is a canonical transform, time-evolution preserves volumes.\nGiven a particle flow in phase space, with density \\(\\rho(t, p, q)\\), flowing according to Hamiltonian \\(H(t, q, p)\\). Take an infinitesimal cube around \\((q, p)\\) at time \\(t\\), with volume \\(\\delta V = \\prod_i \\delta p_i \\delta q_i\\), then it contains \\(\\delta N = \\rho(t, q, p) \\delta V\\) number of particles. Then, let it flow for time \\(s\\).\nThe infinitesimal cube is transported to some other parallelogram around some point \\((q', p')\\), but its volume is unchanged, thus the density at the new location is still the same: \\(\\rho(t+s, q', p') = \\rho(t, q, p)\\). Thus \\(\\dot \\rho = 0\\). \n\n\n\nPoisson brackets are preserved by canonical transforms\nThe Poisson bracket \\(\\{f, g\\}\\) was defined in a coordinate-based way:\n\\[\\{f, g\\} = \\sum_{i=1}^{N} \\left( \\frac{\\partial f}{\\partial q_{i}} \\frac{\\partial g}{\\partial p_{i}} - \\frac{\\partial f}{\\partial p_i} \\frac{\\partial g}{\\partial q_i}\\right)\\]\nWe show that it is preserved by canonical transforms. That is, if \\((Q, P)\\) is a canonical transform of \\((p, q)\\) then\n\\[\\sum_{i=1}^{N} \\left( \\frac{\\partial f}{\\partial q_{i}} \\frac{\\partial g}{\\partial p_{i}} - \\frac{\\partial f}{\\partial p_i} \\frac{\\partial g}{\\partial q_i}\\right) = \\sum_{i=1}^{N} \\left( \\frac{\\partial f}{\\partial Q_{i}} \\frac{\\partial g}{\\partial P_{i}} - \\frac{\\partial f}{\\partial P_i} \\frac{\\partial g}{\\partial Q_i}\\right)\\]\n\nProof. (From the Landau–Lifshitz textbook) Since the Poisson bracket does not depend on time, and if \\(Q(t, q, p), P(t, q, p)\\) is a canonical transform, so is \\(Q(t, q, p), P(t, q, p)\\), so we consider only canonical transforms that are independent of time.\nIf we started with only \\(f(q, p), g(q, p)\\), then extend them to phase spacetime by\n\\[\\mathcal F(t, y) = f(q(0, y), p(0, y)),\\quad \\mathcal G(t, y) = g(q(0, y), p(0, y))\\]\nNext, impose \\(\\mathcal G\\) as a Hamiltonian, and evolve the physical system according to Hamilton’s equations of motion for \\((q, p), \\mathcal G\\). Since \\((q, p)\\) and \\((Q, P)\\) are canonical transforms of each other, we have\n\\[\\{\\mathcal F, \\mathcal G\\}_{q, p} + \\partial_t \\mathcal F = \\dot{\\mathcal F} = \\{\\mathcal F, \\mathcal G'\\}_{Q, P} + \\partial_t \\mathcal F\\]\nOkay, what is \\(\\mathcal G'\\)? It is a solution to\n\\[\\langle p, dq \\rangle - \\mathcal G dt  = \\langle P, dQ \\rangle - \\mathcal G' dt + d\\mathcal K\\]\nsince \\(\\mathcal K\\) does not depend on time, \\(d\\mathcal K\\) contains zero \\(dt\\) term, so \\(\\mathcal G = \\mathcal G'\\).\n\n\n\nInterpretation of canonical transforms\nWhat is invariant under canonical transforms is what is really real about the physical system. Other things are mirages, illusions caused by our choice of coordinates.\nThus, position and momentum are mirages. Hamiltonian equations are real. \\(p, q\\) are mirages. \\(\\int \\sum_i p_i dq_i\\) is real. \\(\\nabla_p, \\nabla_q\\) are mirages. Poisson brackets \\(\\{f, g\\}\\) are real. Phase space lengths \\(dp, dq\\) are mirages. Phase space areas \\(\\sum_i p_i dq_i\\), volumes \\(\\prod_i dp_i dq_i\\), and densities \\(\\rho\\) are real."
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "",
    "text": "The essay is written at the level of two years of undergraduate mathematics. I will keep jargons to a minimum and use as few infinities as possible. For example, instead of particles that can be anywhere on a real-number line, I would talk about particles that can be in one of three boxes."
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#many-world-theory",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#many-world-theory",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Many-world theory",
    "text": "Many-world theory"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#pilot-wave-theory",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#pilot-wave-theory",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Pilot wave theory",
    "text": "Pilot wave theory"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#copenhagen-interpretation",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#copenhagen-interpretation",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Copenhagen interpretation",
    "text": "Copenhagen interpretation"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#relational-quantum-mechanics",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#relational-quantum-mechanics",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Relational quantum mechanics",
    "text": "Relational quantum mechanics"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#qbism",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#qbism",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "QBism",
    "text": "QBism"
  },
  {
    "objectID": "blog/posts/equilibrium-thermoeconomics/index.html#appendix-the-history-of-the-document",
    "href": "blog/posts/equilibrium-thermoeconomics/index.html#appendix-the-history-of-the-document",
    "title": "Classical thermodynamics and economics",
    "section": "Appendix: The history of the document",
    "text": "Appendix: The history of the document\n\nDuring my high school Physics Olympiad days, we learned some basic thermodynamics, but it was limited to mindless tasks like integrating around the \\((P, V)\\) diagram for various engine cycles. We never understood the conceptual foundations, like the difference between \\(\\delta Q\\) and \\(dU\\). I also passed the AP Chemistry course, which also contained some basic and deeply confusing thermodynamics, especially with the statement “chemical equilibrium is reached at \\(\\Delta = 0\\)”.\nDuring my undergraduate years, special relativity was simple enough, electrodynamics difficult but sensible, analytical mechanics confusing to no end, and I didn’t even try thermodynamics. In graduate studies, I had to wrestle with statistical mechanics and thermodynamics after all, to deal with modern AI methods like diffusion models.\nBy pure serendipity, at the same time as diffusion models rose to prominence, I had just worked through a rigorous course on general equilibrium theory, the “standard model” for neoclassical economics (Starr 2011). This gave me the conceptual foundation for looking past the textbooks’ errors. Everything fell into place, and I saw through thermodynamics.\nAnd just like when I rediscovered Wigner rotation, as soon as I have figured out everything for myself, I knew the right words to search, and found that, of course, someone else has written this before (Smith and Foley 2008), repeatedly (Candeal et al. 2001). So why spend all this time to write another one? I think I have written this pedagogically. I don’t care if it is not new, or that it has been said before with more symbols and theorems. I have a thing to say, so I will say it well.\nAs an enlightened one, I see classical thermodynamics as the worst-taught subject out of all of undergraduate physics education.8 Imagine my surprise when I realized that it is not about the conservation of energy (“thermo-”), not about change (“-dynamics”), not about statistical mechanics, not about time… but just about constrained optimization, and nothing more than that! To really understand it, one must unlearn a lot of the nonsense. Indeed, I hope that with this essay I will have slain all those mistakes, which is why the essay is filled with warnings against this and that error.\n\n\n\n8 How long does it take for something as simple as constrained-optimization thermodynamics to be actually taught in undergraduate classes? It has been over 100 years since Caratheodory’s thermodynamics. Why is it thermodynamics still taught so badly? It has been over 100 years since the geometry of Wigner rotation has been discovered. Why is it still taught so badly? It has been over 190 years since Hamiltonian mechanics and over 70 years since dynamical programming has clarified the last obscure points of it. Why is it still taught so badly? It seems to me that physics education is a broken institution that takes all its effort just to not get worse, let alone making any progress."
  },
  {
    "objectID": "blog/posts/equilibrium-thermoeconomics/index.html#the-three-laws",
    "href": "blog/posts/equilibrium-thermoeconomics/index.html#the-three-laws",
    "title": "Classical thermodynamics and economics",
    "section": "The three laws",
    "text": "The three laws\n\nSecond law\n\nFor the equilibrium of any isolated system it is necessary and sufficient that in all possible variations of the state of the systems which do not alter its energy, the variation of its entropy shall either vanish or be negative.\n(Gibbs 1878)\n\nThe second law of thermodynamics is all important: maximizing entropy is all of classical thermodynamics. All other parts are just tricks for maximizing entropy.\n\n\nFirst law\n\nAs a student, I read with advantage a small book by F. Wald entitled “The Mistress of the World and her Shadow”. These meant energy and entropy. In the course of advancing knowledge the two seem to me to have exchanged places. In the huge manufactory of natural processes, the principle of entropy occupies the position of manager, for it dictates the manner and method of the whole business, whilst the principle of energy merely does the book- keeping, balancing credits and debits.\n(Emden 1938)\n\nThe first law of thermodynamics is entirely trivial. Energy is nothing special! Energy is just a conserved quantity, one among equals, much like volume, mass, and many other things… Every conserved quantity is equally conserved,2 so it does not deserve a special thermodynamic law. You might as well say “conservation of mass” is “the second-first law of thermodynamics” and “conservation of volume” is “the third-first law of thermodynamics”, and “conservation of electrons” and “conservation of protons” and “conservation of length” (if you are studying a thermodynamic system restricted to move on a line) and “conservation of area” (if you are studying a thermodynamic system restricted on the surface of a lake) and so on…\n2 The first law of thermodynamics had always struck me as oddly out of place, almost like a joke that I could not catch, like\n\nAll animals are equal, but some animals are more equal than others\n\nbut with\n\nAll conserved quantities are conserved, but some quantities are more conserved than others.\n\nI kept waiting for the textbooks, or the teachers, or someone, to drop the act and confess, “Actually, we were joking – conservation of energy really is not that special, and we were just bored with standard physics and wanted to confuse you with a cute magic trick before showing you what standard physics really is saying.”. Slowly, I realized that there is no joke – conservation of energy is unironically treated as special by not just the students, but even the teachers. I had to figure out for myself how the joke really works, and it required me to rebuild thermodynamics along my preferences.This sounds extraordinary, but that is merely how classical thermodynamics works. The first law of thermodynamics does not deserve its title. It should be demoted to an experimental fact, not a law. Just to drive the point home, I wrote an entire sci-fi worldbuilding sketch about an alien species, for which it is the conservation of volume that is fundamental, not energy, and which discovered stereodynamics. If you can laugh at their mistaken importance of the conservation of volume, maybe you can laugh at the mistaken importance of the conservation of energy too.\nThe proper place for the law of conservation of energy is not classical thermodynamics, but general physics, because energy is nothing special inside classical thermodynamics, but it is extremely special if we zoom out to consider the whole of physics. Whereas in classical thermodynamics, systems conserve energy, and volume, and mass, and electron-number, and proton-number, and… when you move outside of thermodynamics, such as when you add in electrodynamics, special relativity, and quantum mechanics, all kinds of conservations breakdown. You don’t have conservation of mass, or number of electrons, or even volume, but energy is always conserved.\n\n\nZeroth law\nNow that the first law has been dispelled, we can dispel the zeroth law of thermodynamics, too. If Energy falls from grace, so must its shadow, Temperature.\n\nTheorem 1 (general zeroth law) If \\(S_1(X_1, Y) + S_(X_2, Y)\\) is maximized under constraint \\(X_1 + X_2 = X\\), then \\((\\partial_X S_1)_Y = (\\partial_X S_2)_Y\\).\n\n\nThe zeroth law of thermodynamics in various guises.\n\n\n\n\n\n\n\nmaximized quantity \\(S\\)\nconserved quantity \\(X\\)\nderivative \\((\\partial_X S)_Y\\)\n\n\n\n\nentropy\nenergy\ninverse temperature \\(\\beta\\)\n\n\nentropy\nvolume\n\\(\\beta P\\)\n\n\nentropy\nparticles\n\\(-\\beta \\mu\\), where \\(\\mu\\) is chemical potential\n\n\nentropy\nsurface area\n\\(-\\beta\\sigma\\), where \\(\\sigma\\) is surface tension\n\n\nproduction value\nraw material\nmarginal value\n\n\n\n\n\nThird law\nThis law simply states that all systems have \\(\\beta &lt; \\infty\\). It is rarely if ever used in classical thermodynamics. Indeed, its proper place is not thermodynamics, but quantum statistical mechanics, where it states that a substance, when at the lowest possible energy (ground state), has finite statistical entropy.\n\n\nEconomic interpretation\nTODO\nEvery conserved quantity is a commodity. The company has some commodity. Commodities themselves have no intrinsic value. Instead, the company is valued by a certain accounting agency. The CEO’s job is to move around the commodities so that the accounting agency gives it the highest value on the book.\nA compound system is a conglomerate: a giant company made of little companies.\nA subsystem has an inverse temperature \\(\\beta = 1/T\\), which equals\n\\[\\beta = \\frac{d(\\text{value of a sub-company})}{d(\\text{energy owned by the sub-company})}\\]\nIn other words, the marginal value of energy. Electricity price!\nIf a subsystem has variable volume then it has a pressure \\(P\\), which satisfies\n\\[\\beta P= \\frac{d(\\text{value of a sub-company})}{d(\\text{volume owned by the sub-company})}\\]\nIn other words, the marginal value of space. Real estate price!\nYes, I know it sounds weird to say (Pressure/Temperature), but that’s just how the math works out. It turns out that (Pressure/Temperature) is more fundamental than Pressure… Pressure, indeed, is actually Real estate price / Electricity price. That’s why it has units of ($/m^3)/($/Joule) = Joule/m^3!\nWhy, then, do we speak of pressure \\(P\\) and temperature \\(T\\), instead of \\(\\beta\\) and \\(\\beta P\\)? I blame habit and the general inability to visualize classical entropy."
  }
]