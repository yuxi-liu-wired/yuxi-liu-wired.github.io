[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "If you thought the Stroop test is hard, just wait until you have tried this.\nPlay online | Play offline"
  },
  {
    "objectID": "projects/index.html#double-stroop",
    "href": "projects/index.html#double-stroop",
    "title": "Projects",
    "section": "",
    "text": "If you thought the Stroop test is hard, just wait until you have tried this.\nPlay online | Play offline"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n  \n  \nYuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Introduction\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPopulation and GDP since 10000 BC\n\n\n\n\n\n\n\nAI\n\n\nscaling\n\n\n\n\nWhen did the singularity get cancelled?\n\n\n\n\n\n\nJan 18, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nMathematical Interpretations of Quantum Mechanics\n\n\n\n\n\n\n\nphysics\n\n\n\n\nQuantum mechanics: what it all means, mathematically speaking.\n\n\n\n\n\n\nJan 10, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nBook Reviews\n\n\n\n\n\n\n\nbook-review\n\n\n\n\nBook reviews.\n\n\n\n\n\n\nDec 16, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nThe Decline of Mathematical Fields\n\n\n\n\n\n\n\nfun\n\n\nphilosophy\n\n\nhistory\n\n\nmath\n\n\n\n\nLosing my religion.\n\n\n\n\n\n\nNov 1, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nWhat does it feel like to be a mathematical object?\n\n\n\n\n\n\n\nfun\n\n\nphilosophy\n\n\nmath\n\n\n\n\nMy religion.\n\n\n\n\n\n\nNov 1, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nNeural scaling law by data manifold dimensions\n\n\n\n\n\n\n\nAI\n\n\nscaling\n\n\n\n\nNeural networks scale they way they do, purely because of data.\n\n\n\n\n\n\nNov 1, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "Yuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks."
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#history",
    "href": "blog/posts/mixture-of-experts/index.html#history",
    "title": "Mixture of Experts",
    "section": "History",
    "text": "History"
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#future",
    "href": "blog/posts/mixture-of-experts/index.html#future",
    "title": "Mixture of Experts",
    "section": "Future",
    "text": "Future\n\nAnd then it finally worked. And I think the biggest difference was the computing power. Definitely there were advances in data. So we could do image net because Fei-Fei Li and others gathered this large database, and that was really important. There are certainly differences in the algorithm, right? We’ve got a slightly different squashing function. Instead of shaped like this, it’s shaped like this. I mean, I don’t know how big a deal that was, but we learned how to do stochastic gradient dissent a little bit better. We figured that dropout gave you a little bit better robustness.\nSo there were small things, but I think probably the biggest was the computing power. And I mean, I certainly remember Geoff Hinton came to Berkeley when I was a grad student in 1981, I think, when he talked about these neural nets. And we fellow grad students thought that was so cool. So we said, “Let’s go back into the lab and implement it.\nAnd of course, there was absolutely nothing you could download, so we had to build it all from scratch. And we got it to do exclusive or, and then we got it to do something a little bit more complicated. And it was exciting. And then we gave it the first real problem, and it ran overnight, and it didn’t converge, and we let it run one more day, and it still didn’t converge. And then we gave up, and we went back to our sort of knowledge-based systems approach. But if we had the computing power of today, it probably would have converged after five seconds. (Norvig 2021)"
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#a-toy-model",
    "href": "blog/posts/mixture-of-experts/index.html#a-toy-model",
    "title": "Mixture of Experts",
    "section": "A toy model",
    "text": "A toy model\nTo make this concrete, I coded up system in Python in a Jupyter notebook. See\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate the dataset\ndef generate_dataset(num_samples, sharpness=5):\n    X = np.random.randn(num_samples, 2)\n    p = np.minimum(np.exp(sharpness * np.minimum(X[:, 0], X[:, 1])), 1)\n    y = np.random.binomial(1, p, size=num_samples)\n    return X, y\n\n# Generate the dataset\nnum_samples = 1000\nX, y = generate_dataset(num_samples)\n\n# Split the dataset manually\nsplit_ratio = 0.8\nsplit_index = int(split_ratio * num_samples)\n\nX_train, X_test = X[:split_index], X[split_index:]\ny_train, y_test = y[:split_index], y[split_index:]\n\n# Plot the dataset\ndef plot_dataset(X, y, ax):\n    ax.scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], label='Class 0', marker='o', c='blue')\n    ax.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], label='Class 1', marker='x', c='red')\n    ax.set_title('Dataset Scatter Plot')\n    return ax\n\nfig, ax = plt.subplots(figsize=(8, 6))\nplot_dataset(X_test, y_test, ax)\nplt.show()"
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html",
    "href": "blog/posts/mixture-of-experts/index.html",
    "title": "Mixture of Experts",
    "section": "",
    "text": "The code for the post is available at moe.ipynb."
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#theory",
    "href": "blog/posts/mixture-of-experts/index.html#theory",
    "title": "Mixture of Experts",
    "section": "Theory",
    "text": "Theory\nMixture of Experts is an old technique dating back to 1991, but it has become a vital component of modern deep learning to get around the memory bottleneck. There is not much theory to speak of, because this is honestly a very simple technique. Let’s say you have a few predictive models. Each model is an expert. Now you take all of them and combine their predictions in some way – that’s mixture of experts.\n\nMixing\nConsider a simple example. Suppose we are to classify points on the \\(\\mathbb{R}^2\\) plane into 2 classes. Suppose that we can only use a single linear-logistic function \\(f(x) = \\frac{1}{1 + e^{w^T x + b}}\\), then we can write down this classifier:\n\\[\n\\hat y := \\begin{cases}\n1, \\quad & \\text{if }f(x) &gt; 0 \\\\\n0 , & \\text{otherwise}\n\\end{cases}\n\\]\nIn other words, we have a logistic regression model.\n\n\n\nAn example of a logistic regression model. The curve shows the estimated probability of passing an exam versus hours studying. If we have to do a binary prediction, then we predict \\(\\hat y = 1\\) iff \\(x \\geq 2.7\\), that is, we predict the student would pass the exam iff they had studied more than 2.7 hours. Figure from Wikipedia\n\n\nLike perceptrons, logistic regression is simple, fast, and has a very elegant theory – and like perceptrons, logistic regression does not work if the underlying system is not linearly separable.\nNow, consider the simplest example that is not linearly separable: a binary classification on the plane. One class falls into the first quadrant, and the other into the other 3 quadrants. There is some noise, so the points near the edges do not always fall into their respective classes. There is no way to perform this task well with just one logistic classifier, but with two, we should be able to perform this task well enough.\n\n\n\nA scatterplot of points that fall into 2 classes that are not linearly separable.\n\n\nLet’s design the 2 experts manually, and somehow combine them. The 2 experts should each handle one of the edges:\n\\[\nf_1(x, y) = \\frac{1}{e^{10 x}+1}, \\quad f_2(x, y) = \\frac{1}{e^{10 y}+1}\n\\]\nIn words, \\(f_1\\) is a smooth approximation to the 0-1 function that sends \\((x, y)\\) to \\(1\\) iff \\(x &gt; 0\\), and similarly, \\(f_2\\) is a smooth approximation to the 0-1 function that sends \\((x, y)\\) to \\(1\\) iff \\(y &gt; 0\\). How do we combine them?\nWe can add another “manager” which is an expert at picking experts. It would pick \\(f_1\\) if the point \\((x, y)\\) falls above the diagonal line \\(x=y\\), and pick \\(f_2\\) otherwise. This would then give us\n\\[\nf(x, y) = \\begin{cases}\nf_1(x, y), \\quad &\\text{if } y-x &gt; 0 \\\\\nf_2(x, y), \\quad &\\text{if } y-x &lt; 0\n\\end{cases}\n\\]\nThis is the simplest example of sparsely-gated MoE. For each point, the manager picks the right expert to call, and call that expert. The other expert does not ever need to be activated, saving half the compute, the manager’s computation is so simple that it does not cost anything compared to the expert’s computation, which contains an exponential.\nWe can also combine the experts by a linear function, as in\n\\[\nf(x, y) = \\sum_{i = 1}^2 p_i(x, y) f_i(x, y)\n\\]\nwhere \\((p_1, p_2)\\) is a probability distribution over the experts that depends on \\((x, y)\\), such as \\(\\sm(A(x, y))\\) where \\(A\\) is a linear operator, that is, a matrix. For lack of a better word, I call this dense MoE.\n\n\nSparsifying\nGiven a MoE, there are two ways to use it. One can use it as-is, but then every expert must be consulted on every query, defeating the main purpose of MoE in the age of large models: conditional computing. Therefore, the model should be sparsified.\nIn the first MoE paper (Jacobs et al. 1991), they manually inspected the weights (the matrix \\(A\\) in our notation), and found that some experts would never be called on any input. Then they just removed those experts. This can be understood as sparsification “at compile time”.\nIn the toy model, I trained 6 logistic regression experts to classify 2-dimensional points, so the matrix \\(A\\) has 6 rows and 2 columns. To sparsify the model at compile time to only \\(k\\) experts, I took the matrix \\(A\\) and ranked them according to their L2-norm, found the top-\\(k\\) rows of them, then mask out all the other experts. The resulting heat maps at various levels of \\(k \\in 1:6\\) are as follows.\n\n\n\nHeat maps of the compile-time sparsified MoE at various levels of sparsity.\n\n\nAs expected, when \\(k=1\\), we have only one expert taking care of everything, and end up with a linear classifier. When \\(k=2\\), the sparsified MoE looks much closer to the correct classifier. When \\(k \\geq 3\\), it becomes indistinguishable.\nNow, for the sparsely-gated MoE, the sparsification is done “at runtime”. That is, for each input \\(x\\), we find the top-\\(k\\) experts for this specific \\(x\\), and use those experts:\n\\[w(x) = \\sm(\\mathrm{top}_k(Ax))\\]\nwhere \\(\\mathrm{top}_k(v)\\) preserves the top-k entries of \\(v\\), but set all other entries to \\(-\\infty\\). This means we have to keep all experts at runtime, since each expert might be needed for some specific input point, but every input point would only activate a few experts. The key is that the activated experts depend on \\(x\\), unlike the MoE sparsified at compile time, which always activates the same few experts. This means we can achieve a lower sparsity, and less compute. We trade memory for performance and compute.\nIn the same toy model, the resulting heat maps at various levels of \\(k \\in 1:6\\) are as follows.\n\n\n\nHeat maps of the sparsely-gated MoE at various levels of sparsity.\n\n\nCompared with the compile-time sparsified MoE, the sparsely-gated MoE is already usable when \\(k=1\\), and it looks like a piecewise-linear classifier. When \\(k=2\\), it already becomes indistinguishable from the correct classifier."
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#before-deep-learning",
    "href": "blog/posts/mixture-of-experts/index.html#before-deep-learning",
    "title": "Mixture of Experts",
    "section": "Before deep learning",
    "text": "Before deep learning\nIn the beginning was the gaussian. The gaussian is a beautiful distribution, with linearity, the central limit theorem, fast inference, least squares regression, and so on. The problem is that it has just one peak.\nIf one wants to model a complicated distribution with several bumps, one can make one step up the staircase of complexity, and build distributions from a linear sum of several gaussians. This is the mixture of gaussians. More generally, simple statistical models like the Poisson distribution, the Bernoulli distribution, and so on, can be added together to create mixture models.\n\n\n\nA mixture of three gaussian bumps. Figure from Wikipedia.\n\n\nA mixture of experts is then a simple generalization, and training a mixture of experts, back in the old days, was mostly thought of as statistical inference. The main problem was simply modelling complex data with a larger family of statistical distribution. Their main worry was that the experts would overfit.\nThey had little data (enough to fit onto a floppy disk), and each expert was usually just a gaussian distibution or a logistic classifier (any more complex and they wouldn’t know how to calculate the integrals and derivatives). Consequently, what they ended up trying to solve was to fit a few thousand datapoints using tens of very simple experts.\nIt is a general fact of classical machine learning that they were very worried about overfitting, and it is reasonable back then to worry, since they had such small datasets (MNIST was in 1994). This, combined with their inability to hand-design learning algorithms for complex machine learning architectures and the slowness of pure gradient descent, meant that machine learning algorithms back then were simple ones fitted onto small datasets.\nThe overall effect is:\n\ngetting training data: expensive (you have to do it yourself)\ndesigning the algorithm: expensive (cheaper if you have graduate students)\ntraining compute: moderate to high (though a few pioneers have bravely pushed to the “very expensive” regime, and failed1)\ninference compute: very cheap (since that you wouldn’t be able to train anything large)\n\n1 Peter Norvig, coauthor of the most popular AI textbook, recalls:\n\nI certainly remember Geoffrey Hinton came to Berkeley when I was a grad student in 1981, I think, when he talked about these neural nets. And we fellow grad students thought that was so cool. So we said, “Let’s go back into the lab and implement it. And of course, there was absolutely nothing you could download, so we had to build it all from scratch. And we got it to do XOR, and then we got it to do something a little bit more complicated. And it was exciting. And then we gave it the first real problem, and it ran overnight, and it didn’t converge, and we let it run one more day, and it still didn’t converge. And then we gave up, and we went back to our sort of knowledge-based systems approach. But if we had the computing power of today, it probably would have converged after five seconds. (Norvig 2021)\n\nThis should be compared to the very different situation with deep learning:\n\ngetting training data: cheap (just download it online)\ndesigning the algorithm: cheap (make a standard network, add a few decorations, then use backprop with Adam optimizer)\ntraining compute: as expensive as you want\ninference compute: as expensive as you want"
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#after-deep-learning",
    "href": "blog/posts/mixture-of-experts/index.html#after-deep-learning",
    "title": "Mixture of Experts",
    "section": "After deep learning",
    "text": "After deep learning\nWhile classical statistics and machine learning was mainly constrained by how many partial derivatives and integrals the statistician can calculate confidently on paper,2 deep learning is mainly constrained by memory and compute budget.2 If you want a taste of the old days, look at the formulas inside (Jordan and Jacobs 1994). They explicitly calculated the expectation-maximization algorithms for learning a hierarchy of linear experts.\nSo when the deep learning era came circa 2012, people immediately started looking into how to perform conditional computing: save computing cost by only calling a small portion of the model. The idea is that you would have different portions of the model be specialized for different forms of input, and for each input, the model would first cheaply find out which expert should handle it, then call upon only the few specialized experts to handle this particular input.\nDeep learning came with AlexNet (2012), and the first paper on applying MoE to deep learning was “Learning Factored Representations in a Deep Mixture of Experts” (2013). Things really started heating up though with sparsely-gated MoE (2017)."
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#why-moe-for-deep-learning",
    "href": "blog/posts/mixture-of-experts/index.html#why-moe-for-deep-learning",
    "title": "Mixture of Experts",
    "section": "Why MoE for deep learning?",
    "text": "Why MoE for deep learning?\nGenerally, one uses a MoE on the frontier, because:\n\nYou really need to push the metric up by a few points.\nYou can’t train a dense model larger than the frontier model, because it simply fails to converge, or the hyperparameter settings for the small models don’t work for the larger one (and you can’t just run a grid search to find it because it costs a million dollars to do a single run).\nYou can train around 10 copies of the frontier model, because while you don’t have the money to do grid search beyond the current frontier, you have the money to train 10 at the frontier.\nYou can’t infer a dense model larger than the frontier one, because one dense model \\(N\\) times as wide would cost you \\(N^2\\) amount of storage and compute, while if you just train \\(N\\) experts, each with roughly the same architecture as the dense model, it would cost you about \\(N\\) amount of storage and about \\(2\\) amount of compute (if only 2 experts are called per question).\nIndeed, if there are too many parameters, then it can’t even be fit onto a good GPU and must be split across GPUs, and then the GPU–GPU communication becomes a serious problem (the “von Neumann bottleneck”).\n\n\n\n\nThe storage hierarchy. Figure from Harvard CS 61: Systems Programming and Machine Organization (2018), Storage 2: Cache model.\n\n\nAll of which are satisfied by Microsoft, Google, etc. This explains why GPT-4 is a MoE made by multiple GPT-3–like models.\nA quick scan of the recent literature shows this, all from Google.\n\nWe present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. (Shazeer et al. 2017)\n\n\nCombining expert, model and data parallelism, we design two large Switch Transformer models, one with 395 billion and 1.6 trillion parameters, respectively. (Fedus, Zoph, and Shazeer 2022)\n\n\nwe demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet. (Riquelme et al. 2021)\n\n(Shazeer et al. 2017) is not the first paper on MoE in the deep learning era, but it is the most important one. It was applied to between “stacked LSTM layers”, because it was published back when neural language models were stacks of LSTM. Nowadays, of course, MoE usually means MoE layers within Transformers, because only with Transformers do people regularly train models with more than 10 billion parameters."
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "",
    "text": "This is a theory of neural scaling law, proposed by (Bahri et al. 2021; Sharma and Kaplan 2022)\nAccording to this theory, a neural network, when trained to convergence, allocates its \\(N\\) parameters in two parts: * A fixed number of parameters that map the data to an intrinsic data manifold of dim \\(d\\). * All other parameters that handle pieces of this manifold. Loss \\(\\propto\\) the volume of each manifold piece.\nThey argued that the loss function should scale as \\(L \\propto N^{-4/d}\\) for cross-entropy and mean-square losses."
  },
  {
    "objectID": "blog/posts/neural-network-scrapbook/index.html",
    "href": "blog/posts/neural-network-scrapbook/index.html",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "",
    "text": "Marvin Minsky, on how he gave up on neural networks after the 1950s because he could not afford a few million neurons.\n\nI had the naive idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks. Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. (Bernstein 1981)\n\nPeter Norvig, on how he quickly gave up on neural networks in the 1980s due to lack of compute.\n\nAnd then it finally worked. And I think the biggest difference was the computing power. Definitely there were advances in data. So we could do image net because Fei-Fei Li and others gathered this large database, and that was really important. There are certainly differences in the algorithm, right? We’ve got a slightly different squashing function. Instead of shaped like this, it’s shaped like this. I mean, I don’t know how big a deal that was, but we learned how to do stochastic gradient dissent a little bit better. We figured that dropout gave you a little bit better robustness.\nSo there were small things, but I think probably the biggest was the computing power. And I mean, I certainly remember Geoffrey Hinton came to Berkeley when I was a grad student in 1981, I think, when he talked about these neural nets. And we fellow grad students thought that was so cool. So we said, “Let’s go back into the lab and implement it.\nAnd of course, there was absolutely nothing you could download, so we had to build it all from scratch. And we got it to do exclusive or, and then we got it to do something a little bit more complicated. And it was exciting. And then we gave it the first real problem, and it ran overnight, and it didn’t converge, and we let it run one more day, and it still didn’t converge. And then we gave up, and we went back to our sort of knowledge-based systems approach. But if we had the computing power of today, it probably would have converged after five seconds. (Norvig 2021)"
  },
  {
    "objectID": "blog/posts/neural-network-scrapbook/index.html#the-scaling-hypothesis",
    "href": "blog/posts/neural-network-scrapbook/index.html#the-scaling-hypothesis",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "",
    "text": "Marvin Minsky, on how he gave up on neural networks after the 1950s because he could not afford a few million neurons.\n\nI had the naive idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks. Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. (Bernstein 1981)\n\nPeter Norvig, on how he quickly gave up on neural networks in the 1980s due to lack of compute.\n\nAnd then it finally worked. And I think the biggest difference was the computing power. Definitely there were advances in data. So we could do image net because Fei-Fei Li and others gathered this large database, and that was really important. There are certainly differences in the algorithm, right? We’ve got a slightly different squashing function. Instead of shaped like this, it’s shaped like this. I mean, I don’t know how big a deal that was, but we learned how to do stochastic gradient dissent a little bit better. We figured that dropout gave you a little bit better robustness.\nSo there were small things, but I think probably the biggest was the computing power. And I mean, I certainly remember Geoffrey Hinton came to Berkeley when I was a grad student in 1981, I think, when he talked about these neural nets. And we fellow grad students thought that was so cool. So we said, “Let’s go back into the lab and implement it.\nAnd of course, there was absolutely nothing you could download, so we had to build it all from scratch. And we got it to do exclusive or, and then we got it to do something a little bit more complicated. And it was exciting. And then we gave it the first real problem, and it ran overnight, and it didn’t converge, and we let it run one more day, and it still didn’t converge. And then we gave up, and we went back to our sort of knowledge-based systems approach. But if we had the computing power of today, it probably would have converged after five seconds. (Norvig 2021)"
  },
  {
    "objectID": "blog/posts/neural-network-scrapbook/index.html#neural-networks-want-to-work",
    "href": "blog/posts/neural-network-scrapbook/index.html#neural-networks-want-to-work",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "Neural networks want to work",
    "text": "Neural networks want to work\nMarvin Minsky’s SNARC (1951). Designed to simulate one mouse escaping a maze, it ended up simulating multiple mice due to design bugs – which were never debugged. Though the machine had only 40 neurons, and its parts failed all the time, the whole network continued to work.\n\nIt turned out that because of an electronic accident in our design we could put two or three rats in the same maze and follow them all. The rats actually interacted with one another. If one of them found a good path, the others would tend to follow it. We sort of quit science for a while to watch the machine. We were amazed that it could have several activities going on at once in its little nervous system. Because of the random wiring, it had a sort of fail-safe characteristic. If one of the neurons wasn’t working, it wouldn’t make much of a difference—and, with nearly three hundred tubes and the thousands of connections we had soldered, there would usually be something wrong somewhere. In those days, even a radio set with twenty tubes tended to fail a lot. I don’t think we ever debugged our machine completely, but that didn’t matter. By having this crazy random design, it was almost sure to work, no matter how you built it. (Bernstein 1981)\n\nBernard Widrow once built a MADALINE I (circa 1962) in a rush to present at a technical meeting. Despite that only 1/4 of its circuits were defective, it still worked at reduced capacity.\n\nWe discovered the inherent ability of adaptive computers to ignore their own defects while we were rushing through construction of a system called Madaline I for presentation at a technical meeting. The machine was finished late the night before the meeting and the next day we showed some very complex pattern discriminations. Later we discovered that about a fourth of the circuitry was defective. Things were connected backward, there were short circuits, and poor solder joints. We were pretty unhappy until it dawned on us that this system has the ability to adapt around its own internal flaws. The capacity of the system is diminished but it does not fail. (Widrow 1963)\n\nAndrej Karpathy, on how neural network program bugs are very hard to find, because bugged neural networks do not fail, merely degrade.\n\n… perhaps you forgot to flip your labels when you left-right flipped the image during data augmentation. Your net can still (shockingly) work pretty well because your network can internally learn to detect flipped images and then it left-right flips its predictions. Or maybe your autoregressive model accidentally takes the thing it’s trying to predict as an input due to an off-by-one bug. Or you tried to clip your gradients but instead clipped the loss, causing the outlier examples to be ignored during training. Or you initialized your weights from a pretrained checkpoint but didn’t use the original mean. Or you just screwed up the settings for regularization strengths, learning rate, its decay rate, model size, etc. Therefore, your misconfigured neural net will throw exceptions only if you’re lucky; Most of the time it will train but silently work a bit worse. (Karpathy 2019)\n\nResearchers at OpenAI (2018) reported that fixing RL bugs is as important as better algorithms.\n\nBig-picture considerations like susceptibility to the noisy-TV problem are important for the choice of a good exploration algorithm. However, we found that getting seemingly-small details right in our simple algorithm made the difference between an agent that never leaves the first room and an agent that can pass the first level. To add stability to the training, we avoided saturation of the features and brought the intrinsic rewards to a predictable range. We also noticed significant improvements in performance of RND every time we discovered and fixed a bug (our favorite one involved accidentally zeroing an array which resulted in extrinsic returns being treated as non-episodic; we realized this was the case only after being puzzled by the extrinsic value function looking suspiciously periodic). Getting such details right was a significant part of achieving high performance even with algorithms conceptually similar to prior work. This is one reason to prefer simpler algorithms where possible. (Burda and Edwards 2018)\n\nAround 2019, Gwern, Shawn Presser, and others, trained \\(512\\times 512\\) image generation models using the BigGAN architecture. However, they used compare_gan, which had a multiply-by-zero bug. Somehow it still worked, but not well enough compared to the original BigGAN.\n\nOur primary goal was to train & release 512px BigGAN models on not just ImageNet but all the other datasets we had like anime datasets. The compare_gan BigGAN implementation turned out to have a subtle +1 gamma bug which stopped us from reaching results comparable to the model; while we beat our heads against the wall trying to figure out why it was working but not well enough (figuring it out far too late, after we had disbanded) … “Neural nets want to work” – even if they start out being effectively multiplied by zero. (Branwen 2022)\n\nPersonal story at the Berkeley CS 285, Deep Reinforcement Learning, 2022 Fall.\nFor Homework 3, we were asked to implement the soft actor-critic algorithm. We would implement the agent, run the agent on the Half Cheetah environment, and submit the trajectories to Gradescope, where an autograder would check the trajectories and see if the agent achieved a final score above 300. For the Half Cheetah, score means the distance it travels per episode, averaged over several episodes.\nI noticed that the algorithm I implemented did learn, but the learning curve looked like a rollercoaster, jumping up and down around the range of 250 – 300. After many fruitless and paranoid programming sessions I managed to pass the autograder by trying enough random seeds and just submitting the best seeds. The professor, Sergey Levine, offered little help, admitting that RL agents are extremely hard to debug.\nOne day after the assignment deadline, the professor announced that there was a critical one-line bug in the starter code: The correct algorithm should train the model with past game frames in a random order, but the given code always give them in the FIFO order. With the fix, the learning curve would smoothly sigmoid to 350.\n\nThe Neural Net Tank Urban Legend\nA large list of examples in The Neural Net Tank Urban Legend · Gwern.net. I have a few more.\nAccording to Sejnowski, Takeo Kanade did work on detecting tanks in images. This is unconfirmed. I have looked for “Artificial Intelligence Vision: Progress and Non-Progress”, but it is not available online. I looked for your doctoral dissertation of 1974, but it contains only facial recognition. I also cannot find anything about detecting tanks in his publication list.\n\nIn his talk “Artificial Intelligence Vision: Progress and Non-Progress,” Takeo Kanade (from Carnegie Mellon) noted that computer memories back in the 1960s were tiny by today’s standards and could hold only one image at a time. For his doctoral dissertation in 1974, Takeo had shown that, though his program could find a tank in one image, it was too difficult for it to do so in other images where the tank was in a different position and the lighting was different. But, by the time his early students graduated, the programs they designed could recognize tanks under more general conditions because computers were more powerful. Today his students’ programs can recognize tanks in any image. The difference is that today we have access to millions of images that sample a wide range of poses and lighting conditions, and computers are millions of times more powerful. (Sejnowski 2018, 256)\n\nThere was not a lot of actual research on tank recognition. (Kanal and Randall 1964) contains some good pictures. The network was a two-layered perceptron network, of type \\(\\mathbb{R}^{N \\times N} \\to \\{0, 1\\}^{32\\times 32} \\to \\{0, 1\\}^{24} \\to \\{0, 1\\}\\). It works as follows:\n\nThe grayscale photo is down-scaled and binarized by convolution with a discrete Laplace filter: \\(\\mathbb{R}^{N \\times N} \\to \\{0, 1\\}^{32\\times 32}\\).\nThe weights for the 24 hidden perceptrons are constructed by linear discriminant analysis: \\(\\{0, 1\\}^{32\\times 32} \\to \\{0, 1\\}^{24}\\)\nThe output perceptron is learned by the perceptron learning rule: \\(\\{0, 1\\}^{24} \\to \\{0, 1\\}\\).\n\n\nFigure 1: Images from (Kanal and Randall 1964).\n\n\n\n\n\n\n(a) Grayscale photos, some containing tanks, and some not.\n\n\n\n\n\n\n\n(b) A picture of a tank after convolution with a discrete Laplace filter.\n\n\n\n\n\n\n\n\n\n(c) The architecture of the network."
  },
  {
    "objectID": "blog/posts/neural-network-scrapbook/index.html#the-second-neural-network-winter",
    "href": "blog/posts/neural-network-scrapbook/index.html#the-second-neural-network-winter",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "The second neural network winter",
    "text": "The second neural network winter\nThe first neural network winter started around 1965, when the main research centers pivoted away from neural networks: the Stanford Research Institute group turned to symbolic AI; the Bernard Widrow group turned to using single neurons as adaptive filters; the Frank Rosenblatt group died from lack of funds and then the literal death of Rosenblatt in 1971. It rose again around 1985, when backpropagation and improved compute allowed researchers to train neural networks on the order of \\(10^4\\) parameters and \\(4\\) layers.\nSomething strange happened during the 1990 – 2010 period: the neural network research community silently disappeared again for another 20 years. Unlike the previous case, there was no great mythology or drama about this winter, no Perceptron controversy.\nI would like to find out why.\n\nLukas: So I remember Daphne Koller telling me, maybe 2003, that the kind of state-of-the-art handwriting systems were neural nets, but that it was such an ad hoc kind of system that we shouldn’t focus on it. And I wonder if maybe I should have paid more attention to that and tried harder to make neural nets work for the applications I was doing.\nPeter: Yeah, me too. And certainly Yann LeCun had success with the digit database, and I think that was over-engineered in that they looked at exactly the features they needed for that set of digitizations of those digits. And in fact, I remember researchers talking about, “Well, what change are we going to do for sample number 347?” Right?\nLukas: Oh, really? Okay.\nPeter: There were individual data points that they would perform theories on, so that was definitely over-tuning to the data. And it should have been an indication that was a good approach. It was better than other approaches at the time.\nLukas: I guess so. Although that does sound like damming level of over-fitting the data, I suppose.\nPeter: Right. There was only a couple thousand data points. I forget exactly how many. Maybe it was 10,000. Maybe it was even 100,000, but it wasn’t many. (Norvig 2021)"
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html",
    "href": "blog/posts/ai-creativity/index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "An interesting strand of argument runs through discussions on AI: whether they can be creative or not; whether we should build them to be creative or not.\nDuring an online discussion, I explained to someone who is confused about why people are building these art machines that are “stealing art”. I explained the technological-scientific perspective on art. Their argument ran as follows:\n\nMachines cannot be creative. The apparent creativity is fake and mere copying.\nPeople build machines not because they want to make art, but because of something else. Perhaps a greed for money, a hate of artists, or some other nefarious motivation (the post is a bit vague on the precise motivation).\n\nI tried to explain the very different perspective on the other side of the cultural divide, so that they might understand, if not to accept:\n\nThere is no “magic”. Art might feel impossible to build a machine for, but we can.\nWhat I cannot create, I do not understand. (Feynman quote)\nIntrospection is unreliable. Asking artists how art was “really made” is not a reliable way to understand art.\nThus, we can build art machines, and we want to, if we are to ever understand art.\n\nThey repeated the same arguments, but more vehemently.\nIt was frustrating, though I was not surprised. This incident started my thinking: This is such a common response, that there is probably a psychological mechanism behind it. This essay describes some possible mechanisms."
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#introduction",
    "href": "blog/posts/ai-creativity/index.html#introduction",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "An interesting strand of argument runs through discussions on AI: whether they can be creative or not; whether we should build them to be creative or not.\nDuring an online discussion, I explained to someone who is confused about why people are building these art machines that are “stealing art”. I explained the technological-scientific perspective on art. Their argument ran as follows:\n\nMachines cannot be creative. The apparent creativity is fake and mere copying.\nPeople build machines not because they want to make art, but because of something else. Perhaps a greed for money, a hate of artists, or some other nefarious motivation (the post is a bit vague on the precise motivation).\n\nI tried to explain the very different perspective on the other side of the cultural divide, so that they might understand, if not to accept:\n\nThere is no “magic”. Art might feel impossible to build a machine for, but we can.\nWhat I cannot create, I do not understand. (Feynman quote)\nIntrospection is unreliable. Asking artists how art was “really made” is not a reliable way to understand art.\nThus, we can build art machines, and we want to, if we are to ever understand art.\n\nThey repeated the same arguments, but more vehemently.\nIt was frustrating, though I was not surprised. This incident started my thinking: This is such a common response, that there is probably a psychological mechanism behind it. This essay describes some possible mechanisms."
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#the-self-interest-theory",
    "href": "blog/posts/ai-creativity/index.html#the-self-interest-theory",
    "title": "Yuxi on the Wired",
    "section": "The self-interest theory",
    "text": "The self-interest theory\nThe self-interest theory is as follows: “It is hard to get someone to understand something if something they care about depends on their not understanding it.”"
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#the-non-truth-theory",
    "href": "blog/posts/ai-creativity/index.html#the-non-truth-theory",
    "title": "Yuxi on the Wired",
    "section": "The non-truth theory",
    "text": "The non-truth theory\nThe non-truth theory states that some arguments are forever mired in the same controversies, always rehashing the same arguments, because there is no truth to be found underneath the arguments.\nThere are certain social functions that are best served by saying something in language that looks like they talk about objective things. You can think of this as a hack in the programming language of humans. For example,\nThere are some social functions that"
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#why-it-seems-terrible-that-machines-might-create",
    "href": "blog/posts/ai-creativity/index.html#why-it-seems-terrible-that-machines-might-create",
    "title": "Yuxi on the Wired",
    "section": "Why it seems terrible that machines might create",
    "text": "Why it seems terrible that machines might create\nAt the “immortal dinner party” held by Benjamin Haydon on 28 December 1817, the Romantic poet John Keats agreed with Charles Lamb that Newton “had destroyed all the poetry of the rainbow, by reducing it to the prismatic colors”. Later, Keats wrote “Lamia” that included these famous lines:\nDo not all charms fly\nAt the mere touch of cold philosophy?\nThere was an awful rainbow once in heaven:\nWe know her woof, her texture; she is given\nIn the dull catalogue of common things.\nPhilosophy will clip an Angel's wings,\nConquer all mysteries by rule and line,\nEmpty the haunted air, and gnomed mine—\nUnweave a rainbow, as it erewhile made\nThe tender-person'd Lamia melt into a shade\nGPT4: Keats came up with the concept of “negative capability.” This is the ability to dwell in uncertainties, mysteries, doubts, without any compulsive reaching after fact and reason. Keats valued this ability, arguing that it was central to a poet’s creative process."
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#why-it-seems-incredible-that-machines-might-create",
    "href": "blog/posts/ai-creativity/index.html#why-it-seems-incredible-that-machines-might-create",
    "title": "Yuxi on the Wired",
    "section": "Why it seems incredible that machines might create",
    "text": "Why it seems incredible that machines might create\nHere, the arguments are easier to classify. It seems that there are several common mental models that people use when they think about machines that create. Using any of these would make it seem obvious that machines cannot be creative. So, I just need to classify the mental models!\n\nMachines as monkeys typing randomly\nIn Gulliver’s Travels (1726) by Jonathan Swift, there was a writing machine. It is a 16x16 matrix of little square blocks, with a character on each side. To use it, you turn the 32 handles randomly, then read out the few words that appeared by chance. This allowed:\n\nthe most ignorant person, at a reasonable charge, and with a little bodily labour, might write books in philosophy, poetry, politics, laws, mathematics, and theology, without the least assistance from genius or study.\n\nIt is a clear satire, possibly of Ramon Llull’ s Thinking Machine (3 concentric rotating disks that generate all possible theological arguments):\n\nThe first of these features means that all of these attributes are inherent; the second, that they are systematically interrelated in such a way as to affirm, with impeccable orthodoxy, that glory is eternal or that eternity is glorious; that power is true, glorious, good, great, eternal, powerful, wise, free, and virtuous, or benevolently great, greatly eternal, eternally powerful, powerfully wise, wisely free, freely virtuous, virtuously truthful, etc., etc.\n\n\n\nMachines as pipes for the water of creativity\n\nIt appears to me that if one wants to make progress in mathematics one should study the masters and not the pupils.\n\n— N.H. Abel (1802–1829), quoted from an unpublished source by O. Ore in Niels Henrik Abel, Mathematician Extraordinary, p. 138.\nThere is a common attitude that I can summarize as this: Like drawing water from the unsullied source at the mountain’s peak, so is the experience of returning to the writings of the masters: clear, refreshing, and devoid of later impurities.\n\nAncient Greek theory of creativity\nIn ancient Greece, the Muses were considered the source of the knowledge embodied in the poetry, lyric songs, and myths that were related orally for centuries in ancient Greek culture. Homer began his Iliad with:\n\nSing, Muse, the fatal wrath of Peleus’ son,\nWhich to the Greeks unnumb’red evils brought,\n\nNote that the Muses was doing the real singing, and Homer was a channel for their singing (back then, poetry was sang – the Iliad was written down only after a few centuries). In Plato’s dialog Ion, Socrates (perhaps a sockpuppet of Plato) argued that “it is not by art that poets compose… but by divine apportionment”:\n\nFor the poets tell us that they carry honey to us from every quarter like bees, and they fly as bees do, sipping from honey-flowing fountains in glens and gardens of the Muses. And they tell the truth. For a poet is a delicate thing, winged and sacred, and unable to create until he becomes inspired and frenzied, his mind no longer in him; as long as he keeps his hold on that, no man can compose or chant prophecy. Since, then, it is not by art that poets compose and say many beautiful things about their subjects, as you do about Homer, but by divine apportionment, they each can do well only that to which the Muse directs them-this one dithyrambs, that one odes, or encomia, or dances, or epics, or iambics-each of them worthless in respect to the others.\n\nThe same point was made repeatedly in Plato’s corpus.\n\nJust as the rhapsode says what he says about Homer not by art but by divine apportionment, without intelligence (Ion 534b-c, 536c, 542a), so in the Meno (gge-looa) politicians get their virtue by divine apportionment, without intelligence; they have no more wisdom than seers and soothsayers, who say many fine things but know nothing of what they say; politicians are divine and inspired like poets, and possessed by the god (Meno 9gb-e). The irrational effects of poetry and rhapsody are directly comparable to the irrational effect of vulgar politics, whose servant is vulgar rhetoric (cf. Gorgias 502C).\n\nBoth quotes came from The Dialogues of Plato, Volume 3: Ion, Hippias Minor, Laches, Protagoras, translated by R. Allen. (I decided not to use one of the freely available versions since they tended to mistranslate “gods” as “God”.)\nFor example, in Phaedrus 245a, Socrates claimed that “the poetry of the sane man vanishes into nothingness before that of the inspired madmen”:\n\nAnd a third kind of possession and madness comes from the Muses. This takes hold upon a gentle and pure soul, arouses it and inspires it to songs and other poetry, and thus by adorning countless deeds of the ancients educates later generations. But he who without the divine madness comes to the doors of the Muses, confident that he will be a good poet by art, meets with no success, and the poetry of the sane man vanishes into nothingness before that of the inspired madmen.\n\n\n\nLater manifestations\nIsaac Newton thought he was merely recovering what the ancients have known all along. His friend William Stukeley described Newton as “the Great Restorer of True Philosophy”.\n\n\nApplication to machine creativity\n\n\n\nMachines as flowers for the DNA of creativity\nFrom Lovelace’s “Notes by the Translator”:\n\nThe Analytical Engine has no pretensions whatever to originate any thing. It can do whatever we know how to order it to perform. (source)\n\nIn his seminal paper “Computing machinery and intelligence” (1950), Alan Turing referenced Lovelace’s observation as the sixth objection to the possibility that machines might think. He then objected:\n\nThe view that machines cannot give rise to surprises is due, I believe, to a fallacy to which philosophers and mathematicians are particularly subject. This is the assumption that as soon as a fact is presented to a mind all consequences of that fact spring into the mind simultaneously with it. It is a very useful assumption under many circumstances, but one too easily forgets that it is false. (source).\n\nTuring was led to Lovelace’s objection by debates with Douglas Hartree, who in his book “Calculating Instruments and Machines” (page 70, 1949), quoted Lovelace approvingly. He objected using the phrase “electronic brain” for devices like electronic calculating machines or automatic pilots. He clarified that these machines cannot “think for themselves” and can only execute the instructions provided to them.\nThus, machines, in this view, are akin to flowers—organisms that reproduce and grow according to a predetermined genetic code but do not originate new genetic information on their own. Creativity, like DNA, must be instilled by a designer or operator, who programs the machine with the “genetic code” of what to create.\nAs a short etymological fun fact, the word “development” is a little capsule of the “flower for the DNA” idea:\n\nFirst use 1756, from French développement (“unrolling”). Compare with envelopment (“rolling”).\n\nThe idea is that of “opening up a scroll and showing what has always been written there. In the machines’ creative process can be seen as a similar”unrolling” of predetermined instructions or codes, much like the genetic “unrolling” in a blooming flower.\nThis is most explicitly manifest in the idea of preformationism, prevalent around 17th to 18th century. It seems the same intuitive appeals of preformationism apply to Lovelace’s objection.\n(A brief personal anecdote: When I was a kid, I thought bus cards contained tiny compressed coins inside, and when you “beep” them, those tiny coins fall into the machine through tiny openings on the card. Preformationism in economics!)"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "",
    "text": "The essay is written at the level of two years of undergraduate mathematics. I will keep jargons to a minimum and use as few infinities as possible. For example, instead of particles that can be anywhere on a real-number line, I would talk about particles that can be in one of three boxes."
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#many-world-theory",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#many-world-theory",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Many-world theory",
    "text": "Many-world theory"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#pilot-wave-theory",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#pilot-wave-theory",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Pilot wave theory",
    "text": "Pilot wave theory"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#copenhagen-interpretation",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#copenhagen-interpretation",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Copenhagen interpretation",
    "text": "Copenhagen interpretation"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#relational-quantum-mechanics",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#relational-quantum-mechanics",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Relational quantum mechanics",
    "text": "Relational quantum mechanics"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#qbism",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#qbism",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "QBism",
    "text": "QBism"
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html",
    "href": "blog/posts/perplexity-turing-test/index.html",
    "title": "When will AI pass the Turing Test?",
    "section": "",
    "text": "Alternative title: How much would it cost to train the first AI scientist?"
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html#turing-test-as-statistical-hypothesis-testing",
    "href": "blog/posts/perplexity-turing-test/index.html#turing-test-as-statistical-hypothesis-testing",
    "title": "When will AI pass the Turing Test?",
    "section": "Turing test as statistical hypothesis testing",
    "text": "Turing test as statistical hypothesis testing\n\nSequential hypothesis test\nWe need the following equality:\n\\[\\frac 1n E_{X \\sim P(\\cdot | H_0)}\\left[ \\ln\\frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)}\\right] = \\frac 1n\nE_{X \\sim P(\\cdot | H_0)}\\left[ \\ln\\frac{1}{P(X_{1:n}|H_1)}\\right] - \\frac 1n  E_{X \\sim P(\\cdot | H_0)}\\left[ \\frac{1}{\\ln P(X_{1:n}|H_0)}\\right]\\]\nExplain what each of the three terms of the equality mean. Suppose the machine is a perfect replica of the human, what would each term be?\n\\[\\underbrace{\\frac 1n E_{X \\sim P(\\cdot | H_0)}\\left[ \\ln\\frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)}\\right]}_{\\frac 1n D_{KL}(P(\\cdot | H_0)\\| P(\\cdot | H_1)) } = \\underbrace{\\frac 1n\nE_{X \\sim P(\\cdot | H_0)}\\left[ \\ln\\frac{1}{P(X_{1:n}|H_1)}\\right]}_{\\text{negative log-likelihood loss per token}} - \\underbrace{\\frac 1n  E_{X \\sim P(\\cdot | H_0)}\\left[ \\frac{1}{\\ln P(X_{1:n}|H_0)}\\right]}_{\\text{entropy rate of the human itself}}\\]\nThe first term is the KL-divergence between the machine and the human, as two sources of symbolic strings, averaged over sequence length. Roughly speaking, it is how different they are, per symbol emitted. It is an information-theoretic quantity.\nThe second term is negative log-likelihood loss per token. This is what the language model is trained to minimize.\nThe third term is the entropy rate of the human. It is how random the human is, as a source of symbols.\nIf the machine is a perfect replica of the human, then the second term is zero, and the first term equals the third term.\n\\[T^* \\approx \\frac{\\ln 10}{L - L_\\infty}\\]\nHere, each token on average moves the log-probability-ratio away from 0 by another \\((L-L_\\infty)\\). Decision is triggered when it finally moves out of the interval \\([-\\ln 10, \\ln 10]\\).\nWe are not able to simply look at a few tokens, draw a straight line, and call it a day, because the trajectory of log-probability-ratio is much closer to a random walk with drift. Subjectively, if you were a judge and watching the log-probability-ratio moving, you’d see ups and downs, keeping you in suspense, until it finally crosses the decision boundaries.}\n\n\nSlowdown factor\nHumans are not perfect judges. However, as a crude approximation, we can model them as a slowed-down version of the perfect judge. Specifically, if it takes about \\(T\\) tokens for the perfect judge to reach a likelihood ratio of \\(r\\), it would take about \\(sT\\) tokens for a human judge. Here, \\(s\\) is a number larger than one.\nWe do not have good data on what \\(s\\) is, or whether it is even consistently measurable. However, for our current question, let’s assume \\(s=10\\).\nWhat should the language model’s \\(L-L_\\infty\\) be, before it can pass the Turing test against a human judge for 1000 tokens?\n\\[10 \\times \\ln 10 / 1000 = 0.023\\]\nHow do we estimate the slowdown factor?\n(Jannai et al. 2023)\n\n\nEntropy of natural languages\nWe found that \\(L_\\infty\\) should be interpreted as the intrinsic entropy of the source material. In this case, it is the entropy of natural English. Now, the intrinsic entropy of English is not very easy to estimate, but there had been several attempts.\nThe earliest attempt is by Shannon himself, in 1951. He estimated that the entropy of English is about 0.6 – 1.3 bits per character. Now, we cannot use this number directly, because it is not in the right units – loglikelihood loss is in units of nat/token.\nThe conversion between nat and bit is known exactly: \\(1 \\;\\mathrm{nat} = \\ln(2)\\;\\mathrm{nat}= 0.693\\;\\mathrm{nat}\\). The conversion between character and token can be estimated by running a tokenizer over a large natural English corpus. I have estimated this by running the GPT-2 tokenizer on WikiText2 corpus, and found that on average, 0.22 tokens/character, 1.17 tokens/word.\nWhat is Shannon’s estimated entropy of English, in units of nat/token?\n\\[\\ln 2 \\times [0.6, 1.3] / 0.22 = [1.89, 4.09]\\;\\mathrm{nat/token}\\]\nAnother way to estimate is by attempting to compress a large natural English corpus. It is known in basic information theory that the entropy rate is the lower bound on compression rate. In more detail, if you have a source of information emitting symbols, and its symbol stream has an entropy rate of \\(x \\;\\mathrm{bit/symbol}\\), then it takes \\(xl\\) bits to encode a long segment \\(l\\) symbols long.\nThe Hutter prize is a competition for compressing a \\(10^9\\)-byte segment of the English Wikipedia as much as possible. The competition has been ongoing since 2005.\nThe zipped file is only about 300 Mb in size, meaning that the total entropy in the corpus is no more than \\(3\\times 10^8\\) bytes.\nOver the years, the progress has been slow but somewhat steady. If we extrapolate the prize-winning entries over the years, we see that the best possible compression ratio seems to be about 10x.\nI ran the GPT-2 tokenizer through 1/100 of the dataset, and got 2.96 million tokens. So the entire dataset should have about 300 million tokens. Assuming this, what is the entropy of English, in units of nat/token?\nIf we assume the maximal compression ratio of 10x, then the corpus contains entropy\n\\[10^8\\;\\mathrm{byte} = 8\\times 10^8 \\;\\mathrm{bit} = 5.55\\times 10^8 \\;\\mathrm{nat}\\]\n\\[\\frac{5.55\\times 10^8}{3\\times 10^8}= 1.85 \\;\\mathrm{nat/token}\\]\nAnd the third way is to look at what the scaling laws for the largest language models imply what \\(L_\\infty\\) is. According to page 25 of “Training Compute-Optimal Large Language Models” (2022) (hereafter “Chinchilla scaling law”), \\(L_\\infty = 1.69\\;\\mathrm{nat/token}\\).\nThe estimate by compression and language modelling are remarkably close.\nShannon’s estimate of entropy is above the other two estimates by about 2x. Considering that he measured this by asking volunteers to just look at a small passage of text and keep guessing the next character, we can consider his estimate a noisy estimate biased to be too high (since the volunteers would not be able to extract the very last predictabilities of English by only looking at a small preceding passage).\nCompute scaling law\nThe “Chinchilla scaling law” paper reported a series of training runs on language models, trained by Google DeepMind researchers. According to them, if we have a fixed amount of computing budget \\(C\\) (in units of FLOP), by choosing the model and dataset size correctly, the minimal reducible loss achievable is\n\\[L - L_\\infty = \\frac{1070}{C^{0.154}}\\]\nAssuming slowdown factor \\(s=10\\), and Chinchilla scaling law, then, we have a direct method to predict how long a language model can pass a Turing test, according to how many FLOPs it cost to compute:\n\\[T^* \\sim \\frac{10\\ln 10}{1070}C^{0.154}\\]\nThis gives, as a rule of thumb, 100x compute means 2x length of Turing test.\nIf GPT-4 costs 2e25 FLOP in compute, for how many words can it pass the Turing test? Assume 1 word is 1.2 tokens, as described previously.\n\\[T^* \\approx 170 \\text{ tokens} \\approx 150 \\text{ words}\\] meaning it has a good chance of passing the Turing test if limited to only 150 words. For context, the Attention is All You Need paper has an abstract that’s 200 tokens long.\nA typical scientific paper is about 4000 words long. How much training compute would it cost, in units of of GPT-4? Assuming GPT-4 cost 10 million USD to train, how much money would this model cost to train?\n4000 words is 27x more than 150 words, so it would need \\(27^{1/0.153} = 2e9\\) amount of compute. Assuming 1 GPT-4 cost 10 million USD, that would cost 2e16 USD, or 200 years of global GDP (2023).\nSo it seems like it would take a very long time to train the first AI scientist by just scaling up the GPT-like models. We need more than pure language modelling.\nI leave you with the inspirational quote from Edward “the Bomb” Teller:\n\nThe possibilities of developing an atomic weapon and the desirability of doing it secretly were discussed at a Princeton University conference in which I participated in March 1939… Bohr said this rare variety could not be separated from common uranium except by turning the country into a gigantic factory. Bohr was worried that this could be done and that an atomic bomb could be developed—but he hoped that neither could be accomplished. Years later, when Bohr came to Los Alamos, I was prepared to say, “You see…” But before I could open my mouth, he said: “You see, I told you it couldn’t be done without turning the whole country into a factory. You have done just that.” (Teller and Brown 1975)"
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html#sequential-hypothesis-test",
    "href": "blog/posts/perplexity-turing-test/index.html#sequential-hypothesis-test",
    "title": "When will AI pass the Turing Test?",
    "section": "Sequential hypothesis test",
    "text": "Sequential hypothesis test\nWe need the following equality:\n\\[\\frac 1n \\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[ \\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\right] = \\frac 1n\n\\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[ \\ln\\frac{1}{Pr(X_{1:n}|H_1)}\\right] - \\frac 1n  \\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[ \\frac{1}{\\ln Pr(X_{1:n}|H_0)}\\right]\\]\nExplain what each of the three terms of the equality mean. Suppose the machine is a perfect replica of the human, what would each term be?\n\\[\\underbrace{\\frac 1n E_{X \\sim Pr(\\cdot | H_0)}\\left[ \\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\right]}_{\\frac 1n D_{KL}(Pr(\\cdot | H_0)\\| Pr(\\cdot | H_1)) } = \\underbrace{\\frac 1n\nE_{X \\sim Pr(\\cdot | H_0)}\\left[ \\ln\\frac{1}{Pr(X_{1:n}|H_1)}\\right]}_{\\text{negative log-likelihood loss per token}} - \\underbrace{\\frac 1n  E_{X \\sim Pr(\\cdot | H_0)}\\left[ \\frac{1}{\\ln Pr(X_{1:n}|H_0)}\\right]}_{\\text{entropy rate of the human itself}}\\]\nThe first term is the KL-divergence between the machine and the human, as two sources of symbolic strings, averaged over sequence length. Roughly speaking, it is how different they are, per symbol emitted. It is an information-theoretic quantity.\nThe second term is negative log-likelihood loss per token. This is what the language model is trained to minimize.\nThe third term is the entropy rate of the human. It is how random the human is, as a source of symbols.\nIf the machine is a perfect replica of the human, then the second term is zero, and the first term equals the third term.\n\\[T^* \\approx \\frac{\\ln 10}{L - L_\\infty}\\]\nHere, each token on average moves the log-probability-ratio away from 0 by another \\((L-L_\\infty)\\). Decision is triggered when it finally moves out of the interval \\([-\\ln 10, \\ln 10]\\).\nWe are not able to simply look at a few tokens, draw a straight line, and call it a day, because the trajectory of log-probability-ratio is much closer to a random walk with drift. Subjectively, if you were a judge and watching the log-probability-ratio moving, you’d see ups and downs, keeping you in suspense, until it finally crosses the decision boundaries.}\n\nSlowdown factor\nHumans are not perfect judges. However, as a crude approximation, we can model them as a slowed-down version of the perfect judge. Specifically, if it takes about \\(T\\) tokens for the perfect judge to reach a likelihood ratio of \\(r\\), it would take about \\(sT\\) tokens for a human judge. Here, \\(s\\) is a number larger than one.\nWe do not have good data on what \\(s\\) is, or whether it is even consistently measurable. However, for our current question, let’s assume \\(s=10\\).\nWhat should the language model’s \\(L-L_\\infty\\) be, before it can pass the Turing test against a human judge for 1000 tokens?\n\\[10 \\times \\ln 10 / 1000 = 0.023\\]\nHow do we estimate the slowdown factor?\n(Jannai et al. 2023)"
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html#entropy-of-natural-languages",
    "href": "blog/posts/perplexity-turing-test/index.html#entropy-of-natural-languages",
    "title": "When will AI pass the Turing Test?",
    "section": "Entropy of natural languages",
    "text": "Entropy of natural languages\nIn Equation 1, we argued that \\(L_\\infty\\) should be interpreted as the entropy rate of the source, usually human-generated English. Unfortunately, unlike that of coin flips or Markov chains, the entropy rate of English cannot be calculated, only estimated. Fortunately, it can be estimated in several ways, and we can check their agreement.\nSince tokenizers are temporary, but English is permanent, we convert all units to \\(\\;\\rm{bit/character}\\) for easy comparison.\n\nChinchilla scaling\nIn the Chinchilla scaling law paper, the authors trained many language models with various sizes from a single architecture family, and fitted a statistical law to the data, giving \\(L_\\infty = 1.69 \\;\\rm{ nat/token}\\) (without error bars, unfortunately) (Hoffmann et al. 2022, 25).\nTo find the effective \\(\\;\\rm{bit/character}\\) for the Chinchilla scaling law, we need to convert \\(\\rm{nat}\\) to \\(\\rm{bit}\\), and \\(\\rm{token}\\) to \\(\\rm{character}\\). The first is easy: \\(1 \\;\\mathrm{bit} = \\ln(2)\\;\\mathrm{nat}\\). The second can be estimated by running a tokenizer over a large natural English corpus. I have estimated this by running the GPT-2 tokenizer on the WikiText-2 corpus, and found that on average,\n\\[\n1 \\;\\rm{token} = 4.5 \\;\\rm{character} = 0.85 \\;\\rm{word}\n\\]\nThus, \\(L_\\infty \\approx \\frac{1.69}{4.5\\times \\ln 2} = 0.54 \\;\\rm{bit/character}\\).\n\n\nGuessing game\nThe earliest attempt to measure the entropy rate of English is by Shannon himself (Shannon 1951): \\([0.6, 1.3] \\;\\rm{bit/character}\\). He obtained the estimate by presenting human subjects \\(n-1\\) characters from a text, and ask them to guess the next character repeatedly, until they got it right. In this case, the optimal strategy is to construct the \\(n\\)-gram table, and pick the argmax character for the given \\((n-1)\\)-gram, then the arg-next-max, and so on.\nLet \\(N\\) be the total number of characters allowed – Shannon’s experiment used \\(N = 27\\), with 26 lowercase letters and one white space. Let \\(p_k\\) be the frequency that the subject makes exactly \\(k\\) guesses – including the correct guess, so that \\(\\sum_{k=1}^N p_k = 1\\). By convention, \\(p_{N+1} := 0\\). Shannon derived both an upper and a lower bound for the entropy per character:\n\\[\n\\sum_{k=1}^N k(p_k - p_{k+1}) \\ln k \\leq H \\leq - \\sum_{k=1}^N p_k \\ln p_k\n\\]\nThe upper bound is proved by Shannon’s source coding theorem. Taking a human subject, copy it, then they can be used as an encoder-decoder pair.4 The lower bound is not only tricky to prove, but also wrong in general. It is only correct when the human subject is the optimal \\(N\\)-gram predictor.5 Because of this, I do not recommend using this lower bound, but will quote it anyway.4 It still works even if the humans are pseudorandom. We just have to whisper the same RNG seed into both humans’ ears, and then they would behave in the same pseudorandom way.5 The simplest counterexample: Suppose the source is binary, and satisfies \\(X_{n+1} = X_{n} + 1 \\mod 2\\), so it has zero entropy. Nevertheless, the human intentionally guesses wrong the first time. Therefore, we have \\(p_2 = 1\\), and we have violated the lower bound by \\(2\\ln 2 &gt; 0\\).\nThis source can be made ergodic by adding an \\(\\epsilon\\) amount of coin-flip noise: \\(X_{n+1} = X_{n} + 1 \\mod 2\\) with probability \\(1-\\epsilon\\). This would still give us \\(2\\ln 2 + O(\\epsilon) &gt; O(\\epsilon \\ln \\epsilon)\\).\nOver the years, others devised other methods to estimate this entropy. For example, (Cover and King 1978) used a gambling game estimation, in the style of the Kelly criterion. Subjects were required to divide their entire bankroll into 27 differently-sized bets over 27 possibilities (26 letters and 1 whitespace). The right bet pays back 27-fold, and the other bets are lost. Let \\(S_n\\) be the size of bankroll after \\(n\\) rounds of betting, then\n\\[\nH \\leq \\ln 27 - \\limsup_n \\frac 1n \\ln S_n\n\\]\nThey found that \\(H \\leq 1.3 \\;\\rm{bit/character}\\).\nThe guesser does not have to be a human. It can very well be a language model. (Brown et al. 1992) made a simple trigram model over the Brown corpus (600 million words), and found that it gives \\(H \\leq 1.75 \\;\\rm{bit/character}\\). (Behr Jr et al. 2002) used a model that combines multiple n-gram models, giving \\(H \\leq 1.46 \\;\\rm{bit/character}\\).\n\n\nLossless compression\nAnother way to estimate is by lossless compression of a large corpus, since the entropy rate is the lower bound on compression rate. In more detail, if you have a source of information emitting symbols, and its symbol stream has an entropy rate of \\(x \\;\\mathrm{bit/symbol}\\), then it takes at least \\(\\sim xl\\) bits to encode a long segment with \\(l\\) symbols. Furthermore, this lower bound is approachable using the entropy encoding.\nThe Hutter prize is a competition for compressing a \\(10^9\\)-byte corpus from the English Wikipedia (enwik9). For the size of the finished product, both the algorithm and the compressed data must be counted. In particular, if a neural network is used, then the size of the neural network weights must be counted as well.\nThe enwik9 dataset is in XML format, and thus contains a lot of non-English content like &lt;timestamp&gt;2005-12-27T18:46:47Z&lt;/timestamp&gt;. It has \\(10^9\\) bytes. It is tricky to decide how to clean it up to remove all the XML formatting. As a simple estimate, we counted its words and characters directly with Linux command wc without any preprocessing, which gives us\n\\[\n13,147,025 \\text{ words} =  129,347,857 \\text{ characters} = 1,000,000,000 \\text{ bytes}\n\\]\nTherefore, the entropy rate is\n\\[\n\\frac{8\\times 10^8 / 13,147,025}{\\text{compression ratio}} = \\frac{6.15}{\\text{compression ratio}}\\;\\rm{bit/character}\n\\tag{2}\\]\nThe standard zip algorithm can compress it down to about 300 Mb in size, a compression ratio of \\(\\sim 3\\times\\). Over the years, the progress has been slow but somewhat steady. The current winning entry (Saurabh Kumar, 2023) has a compression ratio of \\(8.76\\times\\). If we extrapolate the prize-winning entries over the years, it seems that the best possible compression ratio is \\(\\sim 10\\times\\).\nSimilar to the Hutter prize, the Large Text Compression Benchmark also asks for compressing the enwik9 dataset. However, there is no limit to the algorithm runtime or size, so the compression ratio for this benchmark is always higher. Currently (2024-01-19), the maximal compression rate reached is \\(9.35\\times\\) with nncp v3.2, which uses a small Transformer model.\n(Grassberger 2002) used a substitutional compression algorithm with increasingly large codebooks. When the codebook had 6000 codes, the algorithm gave \\(h \\leq 1.82 \\;\\rm{bit/character}\\). By extrapolating the {codebook size}-{entropy rate} curve to an infinitely large codebook, they estimated that English has entropy rate \\(0.7 \\pm 0.2 \\;\\rm{bit/character}\\).\n\n\nSummary\n\n\n\nestimate\nmethod\nraw number\neffective entropy rate (bit/char)\n\n\n\n\n(Grassberger 2002)\ncompression, extrapolation\n\\(0.7 \\pm 0.2 \\;\\rm{bit/character}\\)\n\\(\\sim[0.5, 0.9]\\)\n\n\nHutter prize (Saurabh Kumar, 2023)\ncompression\ncompression ratio \\(\\geq 8.76\\)\n\\(\\leq 0.70\\)\n\n\nHutter prize extrapolated\ncompression, extrapolation\ncompression ratio \\(\\sim 10\\)\n\\(\\sim 0.62\\)\n\n\nLarge Text Compression Benchmark (nncp v3.2, 2023)\ncompression\ncompression ratio \\(\\geq 9.35\\)\n\\(\\leq 0.66\\)\n\n\n(Shannon 1951)\nguessing game\n\\(\\in [0.6, 1.3] \\;\\rm{bit/character}\\)\n\\(\\in [0.6, 1.3]\\)\n\n\n(Cover and King 1978)\nguessing game\n\\(\\leq 1.3 \\;\\rm{bit/character}\\)\n\\(\\leq 1.3\\)\n\n\n(Brown et al. 1992)\n3-gram language model\n\\(\\leq 1.75 \\;\\rm{bit/character}\\)\n\\(\\leq 1.75\\)\n\n\n(Behr Jr et al. 2002)\nn-gram language model\n\\(\\leq 1.46 \\;\\rm{bit/character}\\)\n\\(\\leq 1.46\\)\n\n\n(Hoffmann et al. 2022)\nTransformer language model, extrapolation\n\\(L_\\infty = 1.69 \\;\\rm{nat/token}\\)\n\\(\\sim 0.54\\)\n\n\n\nNotably, the above table has mostly upper bounds, and only one dubious lower bound (by Shannon) from 1951. Perhaps lower bounds can be established by using randomness extractors on a large corpus, and checking that the output from the extractor passes pseudorandomness tests."
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html#compute-scaling",
    "href": "blog/posts/perplexity-turing-test/index.html#compute-scaling",
    "title": "When will AI pass the Turing Test?",
    "section": "Compute scaling",
    "text": "Compute scaling\nThe “Chinchilla scaling law” paper reported a series of training runs on language models, trained by Google DeepMind researchers. According to them, if we have a fixed amount of computing budget \\(C\\) (in units of FLOP), by choosing the model and dataset size correctly, the minimal reducible loss achievable is\n\\[L - L_\\infty = \\frac{1070}{C^{0.154}}\\]"
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html#forecasting-agi",
    "href": "blog/posts/perplexity-turing-test/index.html#forecasting-agi",
    "title": "When will AI pass the Turing Test?",
    "section": "Forecasting AGI",
    "text": "Forecasting AGI\nAccording to the Chinchilla scaling law (Hoffmann et al. 2022), if we have a fixed amount of computing budget \\(C\\), by choosing the model and dataset size correctly, the minimal reducible loss achievable is\n\\[\nL - L_\\infty = \\frac{1070}{(C/\\;\\rm{FLOP})^{0.154}} \\;\\rm{nat/token}\n\\tag{3}\\]\nAssuming a slowdown factor \\(s\\), that the judge decides when the odds ratio is \\(r:1\\), and the Chinchilla scaling law, we have a direct method to predict how long a language model can survive in a Turing test, according to the cost of training compute \\(C\\):\n\\[T^* \\sim \\frac{s\\ln r}{1070}(C/\\;\\rm{FLOP})^{0.154} \\;\\rm{token}\\]\nThis gives, as a rule of thumb, \\(100\\times\\) compute means \\(2 \\times\\) length of survival in a Turing test.\nFor example, assuming a slowdown factor of \\(s=10\\), and that the judge decides when the odds ratio is \\(10:1\\), for a language model to survive for 1000 tokens, it needs\n\\[L - L_\\infty \\leq 10 \\times \\ln 10 / 1000 = 0.023 \\;\\rm{nat/token}\\]\nIf GPT-4 costs \\(2\\times 10^{25} \\;\\rm{FLOP}\\) in compute, and \\(1 \\;\\rm{word} \\approx 1.2 \\;\\rm{token}\\), then\n\\[T^* \\approx 170 \\text{ tokens} \\approx 150 \\text{ words}\\]\nmeaning it has a good chance of passing the Turing test if limited to only 150 words. For context, the Attention is All You Need paper has an abstract that’s 200 tokens long.\nA typical scientific paper is about 4000 words long, which is \\(27\\times\\) that of 150 words, so it would need \\(27^{1/0.153} = (2\\times 10^9)\\times\\) that of compute. Assuming that GPT-4 cost 10 million USD to train, this hypothetical AI would cost \\(2\\times 10^{16}\\) USD, or 200 years of global GDP2023.\nThis implies that the first AGI will not be a scaled-up GPT – autoregressive transformer generatively pretrained on a lightly filtered text dataset. It has to include something else, perhaps multimodal data, high-quality data, better architecture, etc. Even if we were to attempt to merely scale it up, turning earth into a GPT-factory,6 with even 50% of global GDP devoted,7 and with 2% growth rate forever, it would still take 110 years,8 arriving at year 2133. Whole brain emulation would likely take less time.96 Consider this anecdote from Edward Teller:\n\nThe possibilities of developing an atomic weapon and the desirability of doing it secretly were discussed at a Princeton University conference in which I participated in March 1939… Bohr said this rare variety could not be separated from common uranium except by turning the country into a gigantic factory. Bohr was worried that this could be done and that an atomic bomb could be developed–but he hoped that neither could be accomplished. Years later, when Bohr came to Los Alamos, I was prepared to say, “You see…” But before I could open my mouth, he said: “You see, I told you it couldn’t be done without turning the whole country into a factory. You have done just that.” (Teller and Brown 1975)\n\n7 Only in a life-or-death situation does 50% of GDP get devoted to one purpose. For example, that is about the level of GDP devoted to war production during WWII in the major combatant countries. The USA spent 4 trillion USD2011 over 6 years out of an annual GDP of 1.3 trillion USD2011.8 Solve for \\(x\\) in \\(200 = \\sum_{k=0}^x 0.5 \\times 1.02^k\\).9"
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html#generating-and-measuring-data-manifold-dimensions",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html#generating-and-measuring-data-manifold-dimensions",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Generating and measuring data manifold dimensions",
    "text": "Generating and measuring data manifold dimensions\nConsider the space of all MNIST images. Each MNIST image is a 28x28 grayscale, so the total space is \\(\\mathbb R^{28\\times 28} = \\mathbb R^{784}\\).\nHowever, as you may have seen in experiments with the VAE, most of the MNIST dataset “collapses” onto a much smaller subset of \\(\\mathbb R^{784}\\). This is the “(intrinsic) data manifold”, with a dimension much smaller than \\(784\\). Indeed, the very fact that the intrinsic dimension is small allows us to have meaningful “2D slices” of the dataset:\n\n\n\nFigure from Convolutional Variational Autoencoder - Tensorflow Core Tutorials. More images can be found in Visualizing the Variational Autoencoder, and can be regenerated in about 10 minutes with a Google Colab GPU.\n\n\nReal data can be expensive, though, which is why we often use “toy” datasets with known dimensions, generated by a known random process. For example, the following is the “Swiss roll” dataset. It is generated by first populating a 2D square \\([0, 1]^2\\), then use a function \\(f: \\mathbb R^2 \\to \\mathbb R^3\\) to “roll up” the square into 3D space.\n\n\n\nThe Isomap algorithm (Tenenbaum, Silva, and Langford 2000), popular for constructing data manifolds. Figure from Wikipedia."
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html#generating-data-manifold-by-random-neural-networks",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html#generating-data-manifold-by-random-neural-networks",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Generating data manifold by random neural networks",
    "text": "Generating data manifold by random neural networks\nConsider the simplest data manifold: \\(\\mathbb R^d\\), affinely transformed, then embedded in \\(\\mathbb R^n\\), with \\(n &gt; d\\).\nTo synthesize such a data manifold, we randomly initialize a teacher network, so-called because it implements the function that a student network will fit to by supervised training. Each teacher network is constructed thus:\n\nThe number of neurons in each layer are: \\([d, 9, 600, 600, 1]\\)\nIt has 0 bias.\nThe weights between layers are sampeled from \\(\\mathcal N(0, m^{-1/2})\\) , where \\(m\\) is the input size of the layer (a form of He initialization).\nThe activation function at the second (with 9 neurons) and last layers are identity. All other activation functions are ReLU.\n\nOnce we have constructed a teacher network, we use it to generate a dataset \\(\\{(x_i, y_i)\\}_i\\) thus:\n\nGenerate random gaussian vectors \\(\\{t_i\\}_i\\) in \\(\\mathbb R^d\\), with mean \\(0\\) and std \\(I_{d\\times d}\\).\nFor each \\(t \\in \\{t_i\\}_i\\), push \\(t\\) through the teacher network.\nLet \\(x \\in \\mathbb R^9\\) be the teacher network activation at the second layer, with 9 neurons.\nLet \\(y \\in \\mathbb R\\) be the teacher network output."
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html#learning-data-manifold-by-neural-networks",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html#learning-data-manifold-by-neural-networks",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Learning data manifold by neural networks",
    "text": "Learning data manifold by neural networks\nAccording to the theory, if the data manifold has dimension \\(d\\), then as we scale up a neural network with \\(N\\) parameters, the MSE loss of a fully-trained network would scale like \\(L \\sim N^{-\\alpha}\\), where \\(\\alpha \\approx 4/d\\).\nLet’s test this.\nFirst, we define the “student” neural network architecture:\n\nThe number of neurons in each layer are: \\([9, n, n, 1]\\).\nThe biases are initialized to 0.\nThe weights between layers are sampeled from \\(\\mathcal N(0, m^{-1/2})\\), where \\(m\\) is the input size of the layer (a form of He initialization).\nAll activation functions are ReLU.\n\nA simple computation shows that the network has exactly \\(N = n^2+12n + 1\\) parameters11 \\(N = \\underbrace{(n+n+1)}_{\\text{first layer}} + \\underbrace{(9n + n^2 + n)}_{\\text{second layer}}\\)\n\n\n\nAn example teacher network architecture with \\([9, 5, 5, 1]\\) neurons."
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html#the-manifold-hypothesis",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html#the-manifold-hypothesis",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "The manifold hypothesis",
    "text": "The manifold hypothesis\nConsider the space of all MNIST images. Each MNIST image is a 28x28 grayscale, so the total space is \\(\\mathbb R^{28\\times 28} = \\mathbb R^{784}\\).\nHowever, as you may have seen in experiments with the VAE, most of the MNIST dataset “collapses” onto a much smaller subset of \\(\\mathbb R^{784}\\). This is the “(intrinsic) data manifold”, with a dimension much smaller than \\(784\\). Indeed, the very fact that the intrinsic dimension is small allows us to have meaningful “2D slices” of the dataset:\n\n\n\nFigure from Convolutional Variational Autoencoder - Tensorflow Core Tutorials. More images can be found in Visualizing the Variational Autoencoder, and can be regenerated in about 10 minutes with a Google Colab GPU.\n\n\nReal data can be expensive, though, which is why we often use “toy” datasets with known dimensions, generated by a known random process. For example, the following is the “Swiss roll” dataset. It is generated by first populating a 2D square \\([0, 1]^2\\), then use a function \\(f: \\mathbb R^2 \\to \\mathbb R^3\\) to “roll up” the square into 3D space.\n\n\n\nThe Isomap algorithm (Tenenbaum, Silva, and Langford 2000), popular for constructing data manifolds. Figure from Wikipedia."
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html#synthetic-data-manifolds",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html#synthetic-data-manifolds",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Synthetic data manifolds",
    "text": "Synthetic data manifolds\nConsider the simplest data manifold: \\(\\mathbb R^d\\), affinely transformed, then embedded in \\(\\mathbb R^n\\), with \\(n &gt; d\\).\nTo synthesize such a data manifold, we randomly initialize a teacher network, so-called because it implements the function that a student network will fit to by supervised training. Each teacher network is constructed thus:\n\nThe number of neurons in each layer are: \\([d, 9, 600, 600, 1]\\)\nIt has 0 bias.\nThe weights between layers are sampeled from \\(\\mathcal N(0, m^{-1/2})\\) , where \\(m\\) is the input size of the layer (a form of He initialization).\nThe activation function at the second (with 9 neurons) and last layers are identity. All other activation functions are ReLU.\n\nOnce we have constructed a teacher network, we use it to generate a dataset \\(\\{(x_i, y_i)\\}_i\\) thus:\n\nGenerate random gaussian vectors \\(\\{t_i\\}_i\\) in \\(\\mathbb R^d\\), with mean \\(0\\) and std \\(I_{d\\times d}\\).\nFor each \\(t \\in \\{t_i\\}_i\\), push \\(t\\) through the teacher network.\nLet \\(x \\in \\mathbb R^9\\) be the teacher network activation at the second layer, with 9 neurons.\nLet \\(y \\in \\mathbb R\\) be the teacher network output.\n\n\nSome proofs\nAssuming that we have Lipschitz continuity, we can make some proofs."
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html#experiments",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html#experiments",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Experiments",
    "text": "Experiments\nAccording to the theory, if the data manifold has dimension \\(d\\), then as we scale up a neural network with \\(N\\) parameters, the MSE loss of a fully-trained network would scale like \\(L \\sim N^{-\\alpha}\\), where \\(\\alpha \\approx 4/d\\). We test this in two ways, once with synthetic datasets, where we know that the data manifold has the desired number of dimensions, and once with the CIFAR-10 dataset, where we do not have the dimension of the data manifold, and must estimate it.\nAll code for generating the dataset, and for analyzing the dataset, are in a GitHub repo: yuxi-liu-wired/scaling-law-by-data-manifold.\n\nSynthetic data manifolds\nSince Consider the simplest data manifold: \\(\\mathbb R^d\\), affine-transformed, then embedded in \\(\\mathbb R^n\\), with \\(n &gt; d\\).\nTo synthesize such a data manifold, we randomly initialize a teacher network, so-called because it implements the function that a student network will fit to by supervised training. Each teacher network is constructed thus:\n\nThe number of neurons in each layer are: \\([d, 9, 600, 600, 1]\\)\nIt has 0 bias.\nThe weights between layers are sampled from \\(\\mathcal N(0, m^{-1/2})\\) , where \\(m\\) is the input size of the layer (a form of He initialization).\nThe activation function at the second (with 9 neurons) and last layers are identity. All other activation functions are ReLU.\n\nOnce we have constructed a teacher network, we use it to generate a dataset \\(\\{(x_i, y_i)\\}_i\\) in this way:\n\nGenerate random gaussian vectors \\(\\{t_i\\}_i\\) in \\(\\mathbb R^d\\), with mean \\(0\\) and std \\(I_{d\\times d}\\).\nFor each \\(t \\in \\{t_i\\}_i\\), push \\(t\\) through the teacher network.\nLet \\(x \\in \\mathbb R^9\\) be the teacher network activation at the second layer, with 9 neurons.\nLet \\(y \\in \\mathbb R\\) be the teacher network output.\n\nFirst, we define the “student” neural network architecture:\n\nThe number of neurons in each layer are: \\([9, n, n, 1]\\).\nThe biases are initialized to 0.\nThe weights between layers are sampled from \\(\\mathcal N(0, m^{-1/2})\\), where \\(m\\) is the input size of the layer (a form of He initialization).\nAll activation functions are ReLU.\n\n\n\n\nAn example teacher network architecture with \\([9, 5, 5, 1]\\) neurons.\n\n\nThe parameter count is\n\\[\nN = \\underbrace{(n+n+1)}_{\\text{first layer}} + \\underbrace{(9n + n^2 + n)}_{\\text{second layer}}\n\\]\nWith these settings, I ran the experiment many times, for \\(N\\) ranging from \\(500\\) to \\(10000\\), and \\(d\\) from \\(2\\) to \\(18\\). The results do not look as clean as given in the paper, despite that I have tried my best to match the experimental design as specified in the paper.\n\n\n\nExperimental data for various synthetic dataset dimensions and student network sizes.\n\n\n\n\nCIFAR-10\nThe CIFAR-10 dataset is a popular benchmark, consisting of 32-by-32 RGB images in 10 different image classes, with 6,000 images per class. While the images live in a space of dimension \\(32^2 \\times 3 = 3072\\), (Sharma and Kaplan 2022) reports that the CIFAR-10 images lies in a data manifold with dimension of only around 16–18.\nTo fit the dataset, I trained a family of convolutional networks with 3 convolution layers and 2 fully connected layers on CIFAR-10. In order to run a controlled experiment, I varied as few parameters as possible, with the following designs:\n\nThe network architecture is fixed, and the network parameter count is changed by changing a single number: the number of channels in the convolutional layers.\nThe experiment is run with 20 different network sizes, from 5408 to 115114.\nEach training run lasts 50 epochs, with batch size 128.\nThe optimizer is AdamW with lr=5e-4.\n\nWith these settings, I generated all the data and logged them into TensorBoard log files, then cleaned them up for quantile regression. Plotting in log-log scale, with the x-axis being the model parameter count, and the y-axis being the cross-entropy loss, we would get a downward sloping line. Our hope is that the line should have a slope of close to \\(-4/d\\), where \\(d \\approx 17\\).\nThis is exactly what I have found. Not only is it true for cross-entropy loss, it is also true for classification accuracy (0-1 loss), except the slope is \\(+4/d\\).\n\n\n\nExperimental data for the train/validation splits of CIFAR-10, and with two different criteria: cross entropy loss and accuracy. We see that in all 4 cases, the scaling exponent is close to the theoretical prediction."
  },
  {
    "objectID": "blog/posts/mathematical-phenomenology/index.html",
    "href": "blog/posts/mathematical-phenomenology/index.html",
    "title": "What does it feel like to be a mathematical object?",
    "section": "",
    "text": "TODO: change the folder name, and title, etc.\nStructuralism"
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html#ergodic-theory",
    "href": "blog/posts/perplexity-turing-test/index.html#ergodic-theory",
    "title": "When will AI pass the Turing Test?",
    "section": "Ergodic theory",
    "text": "Ergodic theory\nThis section is foundational, but the full complexity is not necessary. In the next section we will build the theory at two levels of generality, once with ergodic theory and once with just a working knowledge in probability theory.\n\nMeasure-theoretic POV\nI know, you know too, nobody really likes measure theory any more than pianists like practicing scales hundreds of times. Still, it is at the right level of abstraction for many theories, including probability.\nWe omit all mentions of “almost-everywhere”, “except on a set of measure zero”, and similar annoying phrases. As long as you never make a union of uncountable many subsets, you will not be hurt by this omission.\nA probability space is a measurable space with a measure of \\(1\\). We write it as \\((\\Omega, \\mathcal B, Pr)\\), where \\(\\mathcal B\\) is the sigma-algebra of measurable sets, and \\(Pr\\) is the probability measure. We also write \\(\\mu\\) for the measure.22 Pronounced “mu” – it is a pun because both “mu” and “measure” starts with “m”.\nWe consider a single measurable function \\(T : \\Omega \\to \\Omega\\), and call it the shift map.\nWe demand that \\(T\\) must preserve measure. That is, \\(\\forall S \\in \\mathcal B\\), we have \\(Pr(T^{-1}(S)) = Pr(S)\\).\nA subset is measurable iff it is an element of \\(\\mathcal B\\). A measurable set is also called an event.\nA subset \\(S \\in \\mathcal B\\) is \\(T\\)-invariant iff \\(T^{-1}(S) = S\\) almost everywhere.3 Let \\(\\mathcal I\\) be the set of all \\(T\\)-invariant subsets:3 That is, except on a subset of measure zero: \\(Pr(T^{-1}(S) - S) = 0\\) and \\(Pr(S - T^{-1}(S)) = 0\\). This is the last time we will measure this.\n\\[\n\\mathcal I := \\{S \\in \\mathcal B : T^{-1}(S) = S\\}\n\\]\nNow, obviously any set of measure zero or one are \\(T\\)-invariant. We say that those are trivially \\(T\\)-invariant. We say that \\(T\\) is ergodic iff \\(\\mathcal I\\) has only such trivial subsets. In other words, \\(T\\) is ergodic iff it cannot be factored into two nontrivial chunks:\n\\[\nS, S' \\text{ partitions } \\Omega,\\quad \\text{such that } T^{-1}(S) = S ,\\; T^{-1}(S') = S',\\; Pr(S) &gt; 0 ,\\; Pr(S') &gt; 0\n\\]\nWe usually ask \\(T\\) to also be ergodic, though sometimes we don’t need that.\nErgodic maps have many very good properties. We will use the following one. For the theorem, you can picture it as the real space \\(\\mathbb{R}^n\\) with the gaussian probability distribution, but in fact, it applies for just about everything we would care about, such as the space of English texts, queuing jobs, random walks, etc.44 Except pathological examples constructed by logicians who have nothing better to do than to care about the continuum hypothesis, large cardinals, and the arithmetic hierarchy. Those who desire the rigor-mortis of logic, let them have it.\n\nTheorem 1 (Dense orbits) If the state space is a topological space with a countable basis, and any nonempty open set has positive measure, then almost any \\(X\\in\\Omega\\) has a dense orbit.\n\n\nProof. Let \\(U\\) be a nonempty open set.\n\\(\\Omega - \\cup_{i \\geq 0} T^{-i}U\\) is \\(T\\)-invariant, and since it excludes \\(U\\), it does not have the full measure. Since \\(T\\) is ergodic, the set actually has zero measure.\nNow, \\(\\cup(\\Omega - \\cup T^{-i}U)\\) is a union of countably many zero-measure sets, so it still has zero measure. By expanding the definition, this is the set of all points with non-dense orbit.\n\nFinally, there is a common theme in ergodic theory. There are rigorous versions of it, but instead of going for rigor, the spirit is more important:\n\nTheorem 2 (ergodic decomposition) Any interesting map is a partition/sum/integral of ergodic maps.\n\nFor example, the shear map on the unit square \\([0, 1]^2\\) defined by\n\\[\n(x, y) \\mapsto (x, x+y \\mod 1)\n\\]\ncan be thought of as an integral over rotations: For each \\(x \\in [0, 1]\\), we have \\(T_x : y \\mapsto x+y\\mod 1\\). For almost all \\(x\\in [0, 1]\\), we have \\(T_x\\) an irrational rotation, thus ergodic.\n\n\nSequence POV\nWe must interpret the language of measure theory, which is dead like chalk dust, back into the language of sequence predictions, which is alive like reinforced concrete.\nEach point in the state space \\(X\\in \\Omega\\) is a text: a stream of tokens infinite both forwards and backwards. The state space \\(\\Omega\\) is the all possible texts \\((X_n)_n\\). We assume that all tokens come from the same finite-size alphabet, for example, the 128 ASCII symbols.\nThe shift map on the state space \\(T : \\Omega \\to \\Omega\\) is defined by moving the origin to the right by one:\n\\[\nT(\\dots, X_{-1}, X_0, X_1, \\dots) := (\\dots, X_0, X_1, X_2, \\dots)\n\\]\nThe shift map is measure-preserving, meaning that the process is stationary: We could have started reading at any point, and we would still expect to see the same kind of probability distribution. It would not be like “Sorry, the word ‘cat’ appears with zero probability when \\(n \\geq 1000\\).”. It would be like “No matter where we start reading, we should expect to the first three tokens to be ‘cat’ with probability \\(10^{-4}\\).”.\nRepeatedly applying the shift map \\(T\\) is just reading through the stream, one token at a time:\n\\[\n\\text{...Lorem ipsum ...} \\mapsto \\text{...orem ipsum d...} \\mapsto \\text{...rem ipsum do...} \\mapsto \\cdots\n\\]\nA periodic point of \\(T\\) is a text that repeats itself like a broken record. For example, \\(X := \\text{... and and and ...}\\) satisfies \\(T^4X = X\\).\nA \\(T\\)-invariant set \\(S\\subset \\Omega\\) is a set of texts, such that if we take any text \\(X\\) from \\(S\\), and jump either forwards or backwards for an arbitrary amount, we get another set in \\(S\\). In other words, \\(S\\) is a set of token streams where there is no origin: you can start reading from any token.\nA probability distribution over \\(\\Omega\\) describes the probability of observing various kinds of text streams.\nIf we can partition \\(\\Omega\\) into two subsets \\(P, Q\\), with probabilities \\(\\epsilon &gt; 0, 1-\\epsilon &gt; 0\\), then it means that any text from \\(P\\) is different from any text from \\(Q\\), after any shift. It is as if there are two languages, and each text can be exclusively written in one language only.\nWe wish to consider only texts created by some imaginary “universal English speaker”. In particular, we do not want it to get stuck in one sub-language of English, then never escape from it. That is, we assume the universal speaker is ergodic.\nNow imagine that we randomly sample two pieces of text generated by the universal speaker, and we shift the first text around to match it against the second. By Theorem 1, the orbit of the first text is dense in the space of all possible English texts spoken by the universal speaker. We can gamify this situation thus:\n\nProver: “I take one piece of text \\(x\\), then another piece \\(x'\\).”.\nChallenger: “I challenge you to find a stretch of text from \\(x\\) that matches the \\(-1000:1000\\) stretch in \\(x'\\).”.\nProver asks a team of immortal monkeys to do the task. A million years later: “At \\(49134819\\).”.\nChallenger verifies that \\(T^{49134819}(x)_{-1000:1000} = (x')_{-1000:1000}\\).\n\n\n\nShannon–McMillan–Breiman\nIf someone has created an infinite sequence of coin flips \\(X_{-\\infty:+\\infty}\\), then revealed it to us one by one, then each reveal would give us \\(1 \\rm{bit} = \\ln 2 \\rm{nat}\\). The long-term average obtained per reveal is still \\(\\ln 2 \\rm{nat}\\), a rather boring situation.\nHow do we measure the entropy of an English speaker? It speaks token by token, and we have to measure the average information we obtain per token. The problem is that there are two senses of “average”. It could be the time-average: we listen to the speaker speak for a very long time, and calculate the entropy in the speech. It could be the ensemble-average: we listen to the speaker speak for a very long time, then do it again, then again, etc, then average together the time-averages.\nIf the speaker is ergodic, then the speaker essentially has just one speech, and any two samples of its speech are just translations of each other. Consequently, it is intuitively clear that with probability 1, the time-average of the entropy of one speech equals the ensemble-average of the entropy of all speeches. Intuitively, with probability 1,\n\\[\n\\frac{1}{n} \\ln Pr(X_{1:n}) \\to \\mathbb{E}\\left[\\frac{1}{n} \\ln Pr(X_{1:n})\\right]\n\\]\nFor non-ergodic speakers. We simply decompose the speaker into an ensemble of ergodic speakers, then apply the SMB theorem to each one. It is like the strong law of large numbers. Intuitively, it that with probability 1,\n\\[\n\\frac{1}{n} \\ln Pr(X_{1:n}| X \\text{ is type }i)\\to \\mathbb{E}\\left[\\frac{1}{n} \\ln Pr(X_{1:n}) | X \\text{ is type }i\\right]\n\\]\nThis is the Shannon–McMillan–Breiman theorem.\nIn textbooks and Wikipedia, the SMB theorem is stated rigorously, but you have already understood the idea of SMB, and the rigorous versions are simply paraphrases of the idea."
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html#turing-test",
    "href": "blog/posts/perplexity-turing-test/index.html#turing-test",
    "title": "When will AI pass the Turing Test?",
    "section": "Turing test",
    "text": "Turing test\n\nTuring test as statistical hypothesis test\nIn the Turing test, there are three players: one judge and two players. The first player is a human, and the second is a machine. The judge asks each player text questions and receives text answers. The judge must decide who is the human.\nWe consider a simplified Turing test. In this test, the judge does not ask, and simply receives one stream of text \\(X_{1:\\infty}\\). The judge must decide whether the stream is produced by the human or the machine, and do so quickly.\nCast in the language of statistical hypothesis testing, we have two hypotheses:\n\n\\(H_0\\) “the stream is produced by the human”\n\\(H_1\\) “the stream is produced by the machine”\n\nThe judge would read from the stream \\(X_{1:\\infty}\\), o-n-e- -t-o-k-e-n at a time, and at each token, decide whether to take another one, or announce its judgment: \\(H_0\\) or \\(H_1\\).\nAs the organizers of the Turing test, we would start the test by flipping a fair coin to decide whether to use the human or the machine. Therefore, \\(Pr(H_0) = Pr(H_1)\\), and by Bayes, the posterior log-probability ratio is\n\\[\n\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)} = \\ln\\frac{Pr(H_0|X_{1:n})}{Pr(H_1|X_{1:n})}\n\\]\nThis allows us to use the sequential probability ratio test (SPRT). The judge would decide on two decision boundaries, and calculate \\(\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\) at each token. It would stop and announce the decision as soon as the quantity exceeds one of the boundaries.\nFor example, suppose the judge wants to decide when the odds ratio is 10 to 1, then it would make the decision boundaries to be \\([-\\ln 10, + \\ln 10]\\). If \\(\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\) goes above \\(+\\ln 10\\) when \\(n = 60\\), then the judge would announce “\\(H_0\\)” at that point.\nThe \\(\\ln 10\\) is a good rule of thumb, which we will use for the remainder of the essay.\n\n\nSequential hypothesis testing\nConsider the following simple equation:\n\\[\n\\underbrace{\\frac 1n \\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[ \\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\right]}_{\\text{$\\frac 1n D_{KL}(Pr(\\cdot | H_0)\\| Pr(\\cdot | H_1))$}} = \\underbrace{\\frac 1n\n\\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[\\ln\\frac{1}{Pr(X_{1:n}|H_1)}\\right]}_{\\text{negative log-likelihood loss per token}} - \\underbrace{\\frac 1n  \\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[\\frac{1}{\\ln Pr(X_{1:n}|H_0)}\\right]}_{\\text{entropy rate of the human itself}}\n\\tag{1}\\]\nThe first term is the KL-divergence per token between the machine and the human. Roughly speaking, it is how different they are, per token emitted. It is an information-theoretic quantity.\nThe second term is negative log-likelihood loss per token. This is what language models are trained to minimize. We write it as \\(L\\).\nThe third term is the entropy rate of the human. It is how random the human is. We write it as \\(L_\\infty\\), because it is the theoretical minimal loss that the language model can reach.\nIf the machine is a perfect replica of the human, then the second term is zero, and the first term equals the third term.\nAssuming that the human is an ergodic speakers of English,2 we can sample an infinite stream \\(X_{1:\\infty}\\) from the human, then call up the Shannon–McMillan–Breiman theorem and find that2 In short, an ergodic speaker is someone who has only one speech. If you hear it speak once for a very long time, then hear it speak again for a very long time, then you can take the first and shift it around, so that it looks like the second over a very long sub-segment. Ergodic speakers allow you to take the average over a single very long speech, and be assured that it is close to the average over all possible speeches.\nIn long, see the appendix on ergodic theory.\n\\[\n\\frac 1n \\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)} \\to L - L_\\infty\n\\]\nOn the other hand, if the machine is also an ergodic speaker of English, then we can sample an infinite stream \\(X_{1:\\infty}\\) from the machine, then call up the SMB theorem and find that\n\\[\n\\frac 1n \\ln\\frac{Pr(X_{1:n}|H_1)}{Pr(X_{1:n}|H_0)} \\to L' - L_\\infty'\n\\]\nwhere unfortunately, we have the odd \\(L'\\) and \\(L_\\infty'\\), defined by\n\\[\nL' := \\lim_n \\frac 1n\n\\mathbb{E}_{X \\sim Pr(\\cdot | H_1)}\\left[\\ln\\frac{1}{Pr(X_{1:n}|H_0)}\\right], \\quad L_\\infty' := \\lim_n \\frac 1n\n\\mathbb{E}_{X \\sim Pr(\\cdot | H_1)}\\left[\\ln\\frac{1}{Pr(X_{1:n}|H_1)}\\right]\n\\]\nWe can interpret them as the loss of the human at imitating the machine, and the entropy rate of the machine itself. When the machine is close enough to the human, we can take the approximation \\(L' \\approx L, L_\\infty' \\approx L_\\infty\\).\nNow, define the log-ratio at step \\(n\\) to be \\(r_n := \\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\). During a Turing test, the judge calculates\n\\[\n\\begin{aligned}\nr_0 &= 1 \\\\\nr_1 &= r_0 + \\frac{Pr(X_{1:1}|H_0)}{Pr(X_{1:1}|H_1)} \\\\\nr_2 &= r_1 + \\frac{Pr(X_{1:2}|X_{1:1}, H_0)}{Pr(X_{1:2}|X_{1:1}, H_1)} \\\\\n&\\cdots\n\\end{aligned}\n\\]\nSo, imagine that such a perfect judge is going through a Turing test, upon receiving “my cat is technically”, and we are listening on its thoughts:\n\n“If it were a human, then it would start with ‘my’ with probability \\(0.01\\). If it were a machine, then \\(0.05\\). Therefore, the odds ratio is 2 to 1.”\n“If it were a human, then it would follow ‘my’ with ‘cat’ with probability \\(0.01\\). If it were a machine, then \\(0.033\\). Therefore, the odds ratio is 3 to 1.”\n“If it were a human, then it would follow ‘is’ with ‘my cat’ with probability… I do not know. However, I do know that the odds ratio is 2 to 1. Now the total odds ratio is 12 to 1, I can decide: \\(H_0\\).”\n\nWe see that the judge does not have to know the probabilities \\(Pr(X_{1:n}|H_0)\\) and \\(Pr(X_{1:n}|H_1)\\), only their ratio. This might be a minor point, but this idea of likelihood ratio is quite important. It is like “I don’t know how often you say ‘cat’ but I know that you say it twice as often than I do!”.\nLet \\(T^*\\) be the time it takes for the judge to decide.\n\\[T^* \\approx \\frac{\\ln 10}{L - L_\\infty}\\]\nIntuitively, each token on average moves the log-probability-ratio away from 0 by another \\((L-L_\\infty)\\). Decision is triggered when it finally moves out of the interval \\([-\\ln 10, +\\ln 10]\\).\nWe are not able to simply look at a few tokens, draw a straight line, and call it a day, because the trajectory of log-probability-ratio is much closer to a random walk with drift. Subjectively, if you were a judge and watching the log-probability-ratio moving, you’d see ups and downs, keeping you in suspense, until it finally crosses the decision boundaries.\n\n\nSlowdown factor\nTo perform the SPRT as described, the judge must know intimately the difference between a human and a machine. Can the judge do that? Can anyone know, with certainty, that I would start my speech with “Forty cats …” with a probability that is exactly 32.42 times that of GPT-3?\nAs a crude approximation, we can model real-world judges as slowed-down version of the perfect judge. We can imagine that at each step, instead of updating the log-ratio by\n\\[\n\\ln r_{n+1} \\leftarrow \\ln r_n + \\ln \\frac{Pr(X_{n+1}|H_0, X_{1:n})}{Pr(X_{n+1}|H_1, X_{1:n})}\n\\]\nwe update it by\n\\[\n\\ln r_{n+1} \\leftarrow \\ln r_n + \\frac 1s \\ln \\frac{Pr(X_{n+1}|H_0, X_{1:n})}{Pr(X_{n+1}|H_1, X_{1:n})}\n\\]\nwhere \\(s &gt; 1\\) is the slowdown factor. This implies that if it takes \\(\\sim T\\) tokens for the perfect judge to reach a likelihood ratio of \\(r\\), it would take \\(\\sim sT\\) tokens for a human judge.\n\n\nMeasuring the slowdown factor\nThe slowdown factor \\(s\\) is unknown.\n\nInformed by an internal poll, we enforce a lognormal distribution with a median of 53.1, a 15th percentile estimate of 9.84, and an 85th percentile of 290. (Atkinson 2023)\n\nThe original paper (Barnett and Besiroglu 2023a) contains no estimate of \\(s\\). They did propose to measure it experimentally by running the Turing test with a human judge and two language models. One model \\(H_0\\) “perfectly imitates humans” by simply sampling a random text segment from a corpus, and the other model \\(H_1\\) is a trained language model, finetuned to imitate the same corpus. They claimed that for any piece of text \\(X_{1:n}\\), they can calculate the log-ratio \\(\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\), but I found it difficult: Suppose \\(X_{1:n} = \\text{ technically fork}\\), which is unlikely but possible, yet the phrase never appears in the corpus, what should be \\(Pr(X_{1:n}|H_0)\\)? We can use one of the many smoothing tricks (Jurafsky and Martin 2023, chap. 3), but this gets complicated.\nWhat I think would work well is if both \\(H_0\\)and \\(H_1\\) are language models, perhaps even the same model with different sampling temperatures, then the human judge only has to distinguish the two models.\nThere was one large-scale attempt at the Turing test in early 2023, in a game called “Human or Not?” (Jannai et al. 2023). Human participants took 2-minute conversations, and at the end, had to decide whether they were talking to a human or a bot.33 There was no mention of whether the bots had to decide the same question.\n\nThe conversations have a “ping-pong” structure that prevents players from sending two consecutive messages without a response, in order to ensure a balanced and dynamic exchange. Each message, limited to a maximum of 100 characters, has to be composed and sent within a 20-second window, and the chat ends after 2 minutes, usually consisting of 4-5 messages from each side. This ensures that players don’t have to wait for too long, so they can remain engaged with the game and a constant suspense is kept. Once the conversation is over, players are prompted to guess whether their conversational partner was a fellow human or an AI bot. (Jannai et al. 2023)\n\nI counted that during a typical message, each side sends \\([20, 40]\\) English words in total, or \\([30, 50]\\) tokens. In \\([60\\%, 70\\%]\\) of trials, the human participant judged correctly. This suggests that the log-ratio achieved after \\([30, 50]\\) tokens is around the range of \\([\\pm \\ln 6/4, \\pm \\ln 7/3]\\). In other words, the average log-ratio per token is\n\\[\n\\frac{[\\ln 6/4, \\ln 7/3]}{[30, 50]} \\in [0.01, 0.03] \\;\\rm{ nat/token}\n\\]\nThey used several different AI, ranging between Jurassic-2, GPT-4, and Cohere. None of them have published their training compute or loss curves. The only good estimate is for GPT-4, which has training cost \\(C = 2\\times 10^{25}\\rm{FLOP}\\).\nAssuming that Chinchilla scaling holds, average log-ratio per token that an ideal judge should achieve is \\(L - L_\\infty = \\frac{1070}{C^{0.154}} = 0.14 \\;\\rm{ nat/token}\\). Therefore,\n\\[s \\in [5, 14]\\]\nI did not expect the estimate to be nearly symmetric around \\(10\\)."
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html#sec-ergodic-theory",
    "href": "blog/posts/perplexity-turing-test/index.html#sec-ergodic-theory",
    "title": "When will AI pass the Turing Test?",
    "section": "Appendix: Ergodic theory",
    "text": "Appendix: Ergodic theory\nSince we used ergodic theory during the essay, we should quickly explain what it is about. This section is foundational, but the full complexity is not necessary.\n\nMeasure-theoretic POV\nI know, you know too, nobody really likes measure theory any more than pianists like practicing scales hundreds of times. Still, it is at the right level of abstraction for many theories, including probability.\nWe omit all mentions of “almost-everywhere”, “except on a set of measure zero”, and similar annoying phrases. As long as you never make a union of uncountable many subsets, you will not be hurt by this omission.\nA probability space is a measurable space with a measure of \\(1\\). We write it as \\((\\Omega, \\mathcal B, Pr)\\), where \\(\\mathcal B\\) is the sigma-algebra of measurable sets, and \\(Pr\\) is the probability measure. We also write \\(\\mu\\) for the measure.1010 Pronounced “mu” – it is a pun because both “mu” and “measure” starts with “m”.\nWe consider a single measurable function \\(T : \\Omega \\to \\Omega\\), and call it the shift map.\nWe demand that \\(T\\) must preserve measure. That is, \\(\\forall S \\in \\mathcal B\\), we have \\(Pr(T^{-1}(S)) = Pr(S)\\).\nA subset is measurable iff it is an element of \\(\\mathcal B\\). A measurable set is also called an event.\nA subset \\(S \\in \\mathcal B\\) is \\(T\\)-invariant iff \\(T^{-1}(S) = S\\) almost everywhere.11 Let \\(\\mathcal I\\) be the set of all \\(T\\)-invariant subsets:11 That is, except on a subset of measure zero: \\(Pr(T^{-1}(S) - S) = 0\\) and \\(Pr(S - T^{-1}(S)) = 0\\). This is the last time we will measure this.\n\\[\n\\mathcal I := \\{S \\in \\mathcal B : T^{-1}(S) = S\\}\n\\]\nNow, obviously any set of measure zero or one are \\(T\\)-invariant. We say that those are trivially \\(T\\)-invariant. We say that \\(T\\) is ergodic iff \\(\\mathcal I\\) has only such trivial subsets. In other words, \\(T\\) is ergodic iff it cannot be factored into two nontrivial chunks:\n\\[\nS, S' \\text{ partitions } \\Omega,\\quad \\text{such that } T^{-1}(S) = S ,\\; T^{-1}(S') = S',\\; Pr(S) &gt; 0 ,\\; Pr(S') &gt; 0\n\\]\nWe usually ask \\(T\\) to also be ergodic, though sometimes we don’t need that.\nErgodic maps have many very good properties. We will use the following one. For the theorem, you can picture it as the real space \\(\\mathbb{R}^n\\) with the gaussian probability distribution, but in fact, it applies for just about everything we would care about, such as the space of English texts, queuing jobs, random walks, etc.1212 Except pathological examples constructed by logicians who have nothing better to do than to care about the continuum hypothesis, large cardinals, and the arithmetic hierarchy. Those who desire the rigor-mortis of logic, let them have it.\n\nTheorem 1 (Dense orbits) If the state space is a topological space with a countable basis, and any nonempty open set has positive measure, then almost any \\(X\\in\\Omega\\) has a dense orbit.\n\n\nProof. Let \\(U\\) be a nonempty open set.\n\\(\\Omega - \\cup_{i \\geq 0} T^{-i}U\\) is \\(T\\)-invariant, and since it excludes \\(U\\), it does not have the full measure. Since \\(T\\) is ergodic, the set actually has zero measure.\nNow, \\(\\cup(\\Omega - \\cup T^{-i}U)\\) is a union of countably many zero-measure sets, so it still has zero measure. By expanding the definition, this is the set of all points with non-dense orbit.\n\nFinally, there is a common theme in ergodic theory. There are rigorous versions of it, but instead of going for rigor, the spirit is more important:\n\nTheorem 2 (ergodic decomposition) Any interesting map is a partition/sum/integral of ergodic maps.\n\nFor example, the shear map on the unit square \\([0, 1]^2\\) defined by\n\\[\n(x, y) \\mapsto (x, x+y \\mod 1)\n\\]\ncan be thought of as an integral over rotations: For each \\(x \\in [0, 1]\\), we have \\(T_x : y \\mapsto x+y\\mod 1\\). For almost all \\(x\\in [0, 1]\\), we have \\(T_x\\) an irrational rotation, thus ergodic.\n\n\nSequence POV\nWe must interpret the language of measure theory, which is dead like chalk dust, back into the language of sequence predictions, which is alive like reinforced concrete.\nEach point in the state space \\(X\\in \\Omega\\) is a text: a stream of tokens infinite both forwards and backwards. The state space \\(\\Omega\\) is the all possible texts \\((X_n)_n\\). We assume that all tokens come from the same finite-size alphabet, for example, the 128 ASCII symbols.\nThe shift map on the state space \\(T : \\Omega \\to \\Omega\\) is defined by moving the origin to the right by one:\n\\[\nT(\\dots, X_{-1}, X_0, X_1, \\dots) := (\\dots, X_0, X_1, X_2, \\dots)\n\\]\nThe shift map is measure-preserving, meaning that the process is stationary: We could have started reading at any point, and we would still expect to see the same kind of probability distribution. It would not be like “Sorry, the word ‘cat’ appears with zero probability when \\(n \\geq 1000\\).”. It would be like “No matter where we start reading, we should expect to the first three tokens to be ‘cat’ with probability \\(10^{-4}\\).”.\nRepeatedly applying the shift map \\(T\\) is just reading through the stream, one token at a time:\n\\[\n\\text{...Lorem ipsum ...} \\mapsto \\text{...orem ipsum d...} \\mapsto \\text{...rem ipsum do...} \\mapsto \\cdots\n\\]\nA periodic point of \\(T\\) is a text that repeats itself like a broken record. For example, \\(X := \\text{... and and and ...}\\) satisfies \\(T^4X = X\\).\nA \\(T\\)-invariant set \\(S\\subset \\Omega\\) is a set of texts, such that if we take any text \\(X\\) from \\(S\\), and jump either forwards or backwards for an arbitrary amount, we get another set in \\(S\\). In other words, \\(S\\) is a set of token streams where there is no origin: you can start reading from any token.\nA probability distribution over \\(\\Omega\\) describes the probability of observing various kinds of text streams.\nIf we can partition \\(\\Omega\\) into two subsets \\(P, Q\\), with probabilities \\(\\epsilon &gt; 0, 1-\\epsilon &gt; 0\\), then it means that any text from \\(P\\) is different from any text from \\(Q\\), after any shift. It is as if there are two languages, and each text can be exclusively written in one language only.\nWe wish to consider only texts created by some imaginary “universal English speaker”. In particular, we do not want it to get stuck in one sub-language of English, then never escape from it. That is, we assume the universal speaker is ergodic.\nNow imagine that we randomly sample two pieces of text generated by the universal speaker, and we shift the first text around to match it against the second. By Theorem 1, the orbit of the first text is dense in the space of all possible English texts spoken by the universal speaker. We can gamify this situation thus:\n\nProver: “I take one piece of text \\(x\\), then another piece \\(x'\\).”.\nChallenger: “I challenge you to find a stretch of text from \\(x\\) that matches the \\(-1000:1000\\) stretch in \\(x'\\).”.\nProver asks a team of immortal monkeys to do the task. A million years later: “At \\(49134819\\).”.\nChallenger verifies that \\(T^{49134819}(x)_{-1000:1000} = (x')_{-1000:1000}\\).\n\n\n\nShannon–McMillan–Breiman\nIf someone has created an infinite sequence of coin flips \\(X_{-\\infty:+\\infty}\\), then revealed it to us one by one, then each reveal would give us \\(1 \\;\\rm{bit} = \\ln 2 \\;\\rm{nat}\\). The long-term average obtained per reveal is still \\(\\ln 2 \\;\\rm{nat}\\), a rather boring situation.\nHow do we measure the entropy of an English speaker? It speaks token by token, and we have to measure the average information we obtain per token. The problem is that there are two senses of “average”. It could be the time-average: we listen to the speaker speak for a very long time, and calculate the entropy in the speech. It could be the ensemble-average: we listen to the speaker speak for a very long time, then do it again, then again, etc, then average together the time-averages.\nIf the speaker is ergodic, then the speaker essentially has just one speech, and any two samples of its speech are just translations of each other. Consequently, it is intuitively clear that with probability 1, the time-average of the entropy of one speech equals the ensemble-average of the entropy of all speeches. Intuitively, with probability 1,\n\\[\n\\frac{1}{n} \\ln Pr(X_{1:n}) \\to \\mathbb{E}\\left[\\frac{1}{n} \\ln Pr(X_{1:n})\\right]\n\\]\nFor non-ergodic speakers. We simply decompose the speaker into an ensemble of ergodic speakers, then apply the SMB theorem to each one. It is like the strong law of large numbers. Intuitively, it that with probability 1,\n\\[\n\\frac{1}{n} \\ln Pr(X_{1:n}| X \\text{ is type }i)\\to \\mathbb{E}\\left[\\frac{1}{n} \\ln Pr(X_{1:n}) | X \\text{ is type }i\\right]\n\\]\nThis is the Shannon–McMillan–Breiman theorem.\nIn textbooks and Wikipedia, the SMB theorem is stated rigorously, but you have already understood the idea of SMB, and the rigorous versions are simply paraphrases of the idea."
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html#turing-test-as-statistical-hypothesis-test",
    "href": "blog/posts/perplexity-turing-test/index.html#turing-test-as-statistical-hypothesis-test",
    "title": "When will AI pass the Turing Test?",
    "section": "Turing test as statistical hypothesis test",
    "text": "Turing test as statistical hypothesis test\n\nTuring test\nIn the Turing test, there are three players: one judge and two players. The first player is a human, and the second is a machine. The judge asks each player text questions and receives text answers. The judge must decide who is the human.\nWe consider a simplified Turing test. In this test, the judge does not ask, and simply receives one stream of text \\(X_{1:\\infty}\\). The judge must decide whether the stream is produced by the human or the machine, and do so quickly.\nCast in the language of statistical hypothesis testing, we have two hypotheses:\n\n\\(H_0\\) “the stream is produced by the human”\n\\(H_1\\) “the stream is produced by the machine”\n\nThe judge would read from the stream \\(X_{1:\\infty}\\), o-n-e- -t-o-k-e-n at a time, and at each token, decide whether to take another one, or announce its judgment: \\(H_0\\) or \\(H_1\\).\nAs the organizers of the Turing test, we would start the test by flipping a fair coin to decide whether to use the human or the machine. Therefore, \\(Pr(H_0) = Pr(H_1)\\), and by Bayes, the posterior log-probability ratio is\n\\[\n\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)} = \\ln\\frac{Pr(H_0|X_{1:n})}{Pr(H_1|X_{1:n})}\n\\]\nThis allows us to use the sequential probability ratio test (SPRT). The judge would decide on two decision boundaries, and calculate \\(\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\) at each token. It would stop and announce the decision as soon as the quantity exceeds one of the boundaries.\nFor example, suppose the judge wants to decide when the odds ratio is 10 to 1, then it would make the decision boundaries to be \\([-\\ln 10, + \\ln 10]\\). If \\(\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\) goes above \\(+\\ln 10\\) when \\(n = 60\\), then the judge would announce “\\(H_0\\)” at that point.\nThe \\(\\ln 10\\) is a good rule of thumb, which we will use for the remainder of the essay.\n\n\nSequential hypothesis testing\nConsider the following simple equation:\n\\[\n\\underbrace{\\frac 1n \\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[ \\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\right]}_{\\text{$\\frac 1n D_{KL}(Pr(\\cdot | H_0)\\| Pr(\\cdot | H_1))$}} = \\underbrace{\\frac 1n\n\\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[\\ln\\frac{1}{Pr(X_{1:n}|H_1)}\\right]}_{\\text{negative log-likelihood loss per token}} - \\underbrace{\\frac 1n  \\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[\\frac{1}{\\ln Pr(X_{1:n}|H_0)}\\right]}_{\\text{entropy rate of the human itself}}\n\\tag{1}\\]\nThe first term is the KL-divergence per token between the machine and the human. Roughly speaking, it is how different they are, per token emitted. It is an information-theoretic quantity.\nThe second term is negative log-likelihood loss per token. This is what language models are trained to minimize. We write it as \\(L\\).\nThe third term is the entropy rate of the human. It is how random the human is. We write it as \\(L_\\infty\\), because it is the theoretical minimal loss that the language model can reach.\nIf the machine is a perfect replica of the human, then the second term is zero, and the first term equals the third term.\nAssuming that the human is an ergodic speakers of English,2 we can sample an infinite stream \\(X_{1:\\infty}\\) from the human, then call up the Shannon–McMillan–Breiman theorem and find that2 In short, an ergodic speaker is someone who has only one speech. If you hear it speak once for a very long time, then hear it speak again for a very long time, then you can take the first and shift it around, so that it looks like the second over a very long sub-segment. Ergodic speakers allow you to take the average over a single very long speech, and be assured that it is close to the average over all possible speeches.\nIn long, see the appendix on ergodic theory.\n\\[\n\\frac 1n \\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)} \\to L - L_\\infty\n\\]\nOn the other hand, if the machine is also an ergodic speaker of English, then we can sample an infinite stream \\(X_{1:\\infty}\\) from the machine, then call up the SMB theorem and find that\n\\[\n\\frac 1n \\ln\\frac{Pr(X_{1:n}|H_1)}{Pr(X_{1:n}|H_0)} \\to L' - L_\\infty'\n\\]\nwhere unfortunately, we have the odd \\(L'\\) and \\(L_\\infty'\\), defined by\n\\[\nL' := \\lim_n \\frac 1n\n\\mathbb{E}_{X \\sim Pr(\\cdot | H_1)}\\left[\\ln\\frac{1}{Pr(X_{1:n}|H_0)}\\right], \\quad L_\\infty' := \\lim_n \\frac 1n\n\\mathbb{E}_{X \\sim Pr(\\cdot | H_1)}\\left[\\ln\\frac{1}{Pr(X_{1:n}|H_1)}\\right]\n\\]\nWe can interpret them as the loss of the human at imitating the machine, and the entropy rate of the machine itself. When the machine is close enough to the human, we can take the approximation \\(L' \\approx L, L_\\infty' \\approx L_\\infty\\).\nNow, define the log-ratio at step \\(n\\) to be \\(r_n := \\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\). During a Turing test, the judge calculates\n\\[\n\\begin{aligned}\nr_0 &= 1 \\\\\nr_1 &= r_0 + \\frac{Pr(X_{1:1}|H_0)}{Pr(X_{1:1}|H_1)} \\\\\nr_2 &= r_1 + \\frac{Pr(X_{1:2}|X_{1:1}, H_0)}{Pr(X_{1:2}|X_{1:1}, H_1)} \\\\\n&\\cdots\n\\end{aligned}\n\\]\nSo, imagine that such a perfect judge is going through a Turing test, upon receiving “my cat is technically”, and we are listening on its thoughts:\n\n“If it were a human, then it would start with ‘my’ with probability \\(0.01\\). If it were a machine, then \\(0.05\\). Therefore, the odds ratio is 2 to 1.”\n“If it were a human, then it would follow ‘my’ with ‘cat’ with probability \\(0.01\\). If it were a machine, then \\(0.033\\). Therefore, the odds ratio is 3 to 1.”\n“If it were a human, then it would follow ‘is’ with ‘my cat’ with probability… I do not know. However, I do know that the odds ratio is 2 to 1. Now the total odds ratio is 12 to 1, I can decide: \\(H_0\\).”\n\nWe see that the judge does not have to know the probabilities \\(Pr(X_{1:n}|H_0)\\) and \\(Pr(X_{1:n}|H_1)\\), only their ratio. This might be a minor point, but this idea of likelihood ratio is quite important. It is like “I don’t know how often you say ‘cat’ but I know that you say it twice as often than I do!”.\nLet \\(T^*\\) be the time it takes for the judge to decide.\n\\[T^* \\approx \\frac{\\ln 10}{L - L_\\infty}\\]\nIntuitively, each token on average moves the log-probability-ratio away from 0 by another \\((L-L_\\infty)\\). Decision is triggered when it finally moves out of the interval \\([-\\ln 10, +\\ln 10]\\).\nWe are not able to simply look at a few tokens, draw a straight line, and call it a day, because the trajectory of log-probability-ratio is much closer to a random walk with drift. Subjectively, if you were a judge and watching the log-probability-ratio moving, you’d see ups and downs, keeping you in suspense, until it finally crosses the decision boundaries.\n\n\nSlowdown factor\nTo perform the SPRT as described, the judge must know intimately the difference between a human and a machine. Can the judge do that? Can anyone know, with certainty, that I would start my speech with “Forty cats …” with a probability that is exactly 32.42 times that of GPT-3?\nAs a crude approximation, we can model real-world judges as slowed-down version of the perfect judge. We can imagine that at each step, instead of updating the log-ratio by\n\\[\n\\ln r_{n+1} \\leftarrow \\ln r_n + \\ln \\frac{Pr(X_{n+1}|H_0, X_{1:n})}{Pr(X_{n+1}|H_1, X_{1:n})}\n\\]\nwe update it by\n\\[\n\\ln r_{n+1} \\leftarrow \\ln r_n + \\frac 1s \\ln \\frac{Pr(X_{n+1}|H_0, X_{1:n})}{Pr(X_{n+1}|H_1, X_{1:n})}\n\\]\nwhere \\(s &gt; 1\\) is the slowdown factor. This implies that if it takes \\(\\sim T\\) tokens for the perfect judge to reach a likelihood ratio of \\(r\\), it would take \\(\\sim sT\\) tokens for a human judge.\n\n\nMeasuring the slowdown factor\nThe slowdown factor \\(s\\) is unknown.\n\nInformed by an internal poll, we enforce a lognormal distribution with a median of 53.1, a 15th percentile estimate of 9.84, and an 85th percentile of 290. (Atkinson 2023)\n\nThe original paper (Barnett and Besiroglu 2023a) contains no estimate of \\(s\\). They did propose to measure it experimentally by running the Turing test with a human judge and two language models. One model \\(H_0\\) “perfectly imitates humans” by simply sampling a random text segment from a corpus, and the other model \\(H_1\\) is a trained language model, finetuned to imitate the same corpus. They claimed that for any piece of text \\(X_{1:n}\\), they can calculate the log-ratio \\(\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\), but I found it difficult: Suppose \\(X_{1:n} = \\text{ technically fork}\\), which is unlikely but possible, yet the phrase never appears in the corpus, what should be \\(Pr(X_{1:n}|H_0)\\)? We can use one of the many smoothing tricks (Jurafsky and Martin 2023, chap. 3), but this gets complicated.\nWhat I think would work well is if both \\(H_0\\)and \\(H_1\\) are language models, perhaps even the same model with different sampling temperatures, then the human judge only has to distinguish the two models.\nThere was one large-scale attempt at the Turing test in early 2023, in a game called “Human or Not?” (Jannai et al. 2023). Human participants took 2-minute conversations, and at the end, had to decide whether they were talking to a human or a bot.33 There was no mention of whether the bots had to decide the same question.\n\nThe conversations have a “ping-pong” structure that prevents players from sending two consecutive messages without a response, in order to ensure a balanced and dynamic exchange. Each message, limited to a maximum of 100 characters, has to be composed and sent within a 20-second window, and the chat ends after 2 minutes, usually consisting of 4-5 messages from each side. This ensures that players don’t have to wait for too long, so they can remain engaged with the game and a constant suspense is kept. Once the conversation is over, players are prompted to guess whether their conversational partner was a fellow human or an AI bot. (Jannai et al. 2023)\n\nI counted that during a typical message, each side sends \\([20, 40]\\) English words in total, or \\([30, 50]\\) tokens. In \\([60\\%, 70\\%]\\) of trials, the human participant judged correctly. This suggests that the log-ratio achieved after \\([30, 50]\\) tokens is around the range of \\([\\pm \\ln 6/4, \\pm \\ln 7/3]\\). In other words, the average log-ratio per token is\n\\[\n\\frac{[\\ln 6/4, \\ln 7/3]}{[30, 50]} \\in [0.01, 0.03] \\;\\rm{ nat/token}\n\\]\nThey used several different AI, ranging between Jurassic-2, GPT-4, and Cohere. None of them have published their training compute or loss curves. The only good estimate is for GPT-4, which has training cost \\(C = 2\\times 10^{25}\\rm{FLOP}\\).\nAssuming that Chinchilla scaling holds, average log-ratio per token that an ideal judge should achieve is \\(L - L_\\infty = \\frac{1070}{C^{0.154}} = 0.14 \\;\\rm{ nat/token}\\). Therefore,\n\\[s \\in [5, 14]\\]\nI did not expect the estimate to be nearly symmetric around \\(10\\)."
  },
  {
    "objectID": "blog/posts/vernor-vinge/index.html",
    "href": "blog/posts/vernor-vinge/index.html",
    "title": "The Coming Technological Singularity",
    "section": "",
    "text": "Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended.\nIs such progress avoidable? If not to be avoided, can events be guided so that we may survive?These questions are investigated. Some possible answers (and some further dangers) are presented."
  },
  {
    "objectID": "blog/posts/vernor-vinge/index.html#abstract",
    "href": "blog/posts/vernor-vinge/index.html#abstract",
    "title": "The Coming Technological Singularity",
    "section": "",
    "text": "Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended.\nIs such progress avoidable? If not to be avoided, can events be guided so that we may survive?These questions are investigated. Some possible answers (and some further dangers) are presented."
  },
  {
    "objectID": "blog/posts/vernor-vinge/index.html#what-is-the-singularity",
    "href": "blog/posts/vernor-vinge/index.html#what-is-the-singularity",
    "title": "The Coming Technological Singularity",
    "section": "What is The Singularity?",
    "text": "What is The Singularity?\nThe acceleration of technological progress has been the central feature of this century. I argue in this paper that we are on the edge of change comparable to the rise of human life on Earth. The precise cause of this change is the imminent creation by technology of entities with greater than human intelligence. There are several means by which science may achieve this breakthrough (and this is another reason for having confidence that the event will occur):\n\nThe development of computers that are “awake” and superhumanly intelligent. (To date, most controversy in the area of AI relates to whether we can create human equivalence in a machine. But if the answer is “yes, we can”, then there is little doubt that beings more intelligent can be constructed shortly thereafter.\nLarge computer networks (and their associated users) may “wake up” as a superhumanly intelligent entity.\nComputer/human interfaces may become so intimate that users may reasonably be considered superhumanly intelligent.\nBiological science may find ways to improve upon the natural human intellect.\n\nThe first three possibilities depend in large part on improvements in computer hardware. Progress in computer hardware has followed an amazingly steady curve in the last few decades 1. Based largely on this trend, I believe that the creation of greater than human intelligence will occur during the next thirty years. (Charles Platt 2 has pointed out the AI enthusiasts have been making claims like this for the last thirty years. Just so I’m not guilty of a relative-time ambiguity, let me more specific: I’ll be surprised if this event occurs before 2005 or after 2030.)1 Moravec, Hans, Mind Children, Harvard University Press, 1988.2 Platt, Charles, Private Communication.\nWhat are the consequences of this event? When greater-than-human intelligence drives progress, that progress will be much more rapid. In fact, there seems no reason why progress itself would not involve the creation of still more intelligent entities – on a still-shorter time scale. The best analogy that I see is with the evolutionary past: Animals can adapt to problems and make inventions, but often no faster than natural selection can do its work – the world acts as its own simulator in the case of natural selection. We humans have the ability to internalize the world and conduct “what if’s” in our heads; we can solve many problems thousands of times faster than natural selection. Now, by creating the means to execute those simulations at much higher speeds, we are entering a regime as radically different from our human past as we humans are from the lower animals.\nFrom the human point of view this change will be a throwing away of all the previous rules, perhaps in the blink of an eye, an exponential runaway beyond any hope of control. Developments that before were thought might only happen in “a million years” (if ever) will likely happen in the next century. (In 3, Greg Bear paints a picture of the major changes happening in a matter of hours.)3 Bear, Greg, “Blood Music”, Analog Science Fiction-Science Fact, June, 1983. Expanded into the novel Blood Music, Morrow, 1985\nI think it’s fair to call this event a singularity (“the Singularity” for the purposes of this paper). It is a point where our models must be discarded and a new reality rules. As we move closer and closer to this point, it will loom vaster and vaster over human affairs till the notion becomes a commonplace. Yet when it finally happens it may still be a great surprise and a greater unknown. In the 1950s there were very few who saw it: Stanislaw Ulam 4 paraphrased John von Neumann as saying:4 Ulam, S., Tribute to John von Neumann, Bulletin of the American Mathematical Society, vol 64, nr 3, part 2, May, 1958, p1-49.\n\nOne conversation centered on the ever accelerating progress of technology and changes in the mode of human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue.\n\nVon Neumann even uses the term “singularity”, though it appears he is still thinking of normal progress, not the creation of superhuman intellect. (For me, the superhumanity is the essence of the Singularity. Without that we would get a glut of technical riches, never properly absorbed 5.)5 Stent, Gunther S., The Coming of the Golden Age: A View of the End of Progress, The Natural History Press, 1969.\nIn the 1960s there was recognition of some of the implications of superhuman intelligence. I. J. Good wrote 6:6 Good, I. J., “Speculations Concerning the First Ultraintelligent Machine”, in Advances in Computers, vol 6, Franz L. Alt and Morris Rubinoff, eds, pp31-88, 1965, Academic Press.\n\nLet an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. … It is more probable than not that, within the twentieth century, an ultraintelligent machine will be built and that it will be the last invention that man need make.\n\nGood has captured the essence of the runaway, but does not pursue its most disturbing consequences. Any intelligent machine of the sort he describes would not be humankind’s “tool” – any more than humans are the tools of rabbits or robins or chimpanzees. Through the ’60s and ’70s and ’80s, recognition of the cataclysm spread 7 8 9 10. Perhaps it was the science-fiction writers who felt the first concrete impact. After all, the “hard” science-fiction writers are the ones who try to write specific stories about all that technology may do for us. More and more, these writers felt an opaque wall across the future. Once, they could put such fantasies millions of years in the future 11. Now they saw that their most diligent extrapolations resulted in the unknowable … soon. Once, galactic empires might have seemed a Post-Human domain. Now, sadly, even interplanetary ones are.7 Vinge, Vernor, “Bookworm, Run!”, Analog, March 1966, pp8-40. Reprinted in True Names and Other Dangers, Vernor Vinge, Baen Books, 1987.8 Alfvén, Hannes, writing as Olof Johanneson, The End of Man?, Award Books, 1969 earlier published as “The Tale of the Big Computer”, Coward-McCann, translated from a book copyright 1966 Albert Bonniers Forlag AB with English translation copyright 1966 by Victor Gollanz, Ltd.9 Vinge, Vernor, First Word, Omni, January 1983, p10.10 Bear, Greg, “Blood Music”, Analog Science Fiction-Science Fact, June, 1983. Expanded into the novel Blood Music, Morrow, 198511 Stapledon, Olaf, The Starmaker, Berkley Books, 1961 (but from the forward probably written before 1937).\nWhat about the ‘90s and the ’00s and the ’10s, as we slide toward the edge? How will the approach of the Singularity spread across the human world view? For a while yet, the general critics of machine sapience will have good press. After all, till we have hardware as powerful as a human brain it is probably foolish to think we’ll be able to create human equivalent (or greater) intelligence. (There is the far-fetched possibility that we could make a human equivalent out of less powerful hardware, if were willing to give up speed, if we were willing to settle for an artificial being who was literally slow 12. But it’s much more likely that devising the software will be a tricky process, involving lots of false starts and experimentation. If so, then the arrival of self-aware machines will not happen till after the development of hardware that is substantially more powerful than humans’ natural equipment.)12 Vinge, Vernor, “True Names”, Binary Star Number 5, Dell, 1981. Reprinted in True Names and Other Dangers, Vernor Vinge, Baen Books, 1987.\nBut as time passes, we should see more symptoms. The dilemma felt by science fiction writers will be perceived in other creative endeavors.(I have heard thoughtful comic book writers worry about how to have spectacular effects when everything visible can be produced by the technically commonplace.) We will see automation replacing higher and higher level jobs. We have tools right now (symbolic math programs, CAD/CAM) that release us from most low-level drudgery. Or put another way: The work that is truly productive is the domain of a steadily smaller and more elite fraction of humanity. In the coming of the Singularity, we are seeing the predictions of true technological unemployment finally come true.\nAnother symptom of progress toward the Singularity: ideas themselves should spread ever faster, and even the most radical will quickly become commonplace. When I began writing, it seemed very easy to come up with ideas that took decades to percolate into the cultural consciousness; now the lead time seems more like eighteen months. (Of course, this could just be me losing my imagination as I get old, but I see the effect in others too.) Like the shock in a compressible flow, the Singularity moves closer as we accelerate through the critical speed.\nAnd what of the arrival of the Singularity itself? What can be said of its actual appearance? Since it involves an intellectual runaway, it will probably occur faster than any technical revolution seen so far. The precipitating event will likely be unexpected – perhaps even to the researchers involved. (“But all our previous models were catatonic! We were just tweaking some parameters…”) If networking is widespread enough (into ubiquitous embedded systems), it may seem as if our artifacts as a whole had suddenly wakened.\nAnd what happens a month or two (or a day or two) after that? I have only analogies to point to: The rise of humankind. We will be in the Post-Human era. And for all my rampant technological optimism, sometimes I think I’d be more comfortable if I were regarding these transcendental events from one thousand years remove … instead of twenty."
  },
  {
    "objectID": "blog/posts/vernor-vinge/index.html#can-the-singularity-be-avoided",
    "href": "blog/posts/vernor-vinge/index.html#can-the-singularity-be-avoided",
    "title": "The Coming Technological Singularity",
    "section": "Can the Singularity be Avoided?",
    "text": "Can the Singularity be Avoided?\nWell, maybe it won’t happen at all: Sometimes I try to imagine the symptoms that we should expect to see if the Singularity is not to develop. There are the widely respected arguments of Penrose 13 and Searle 14 against the practicality of machine sapience. In August of 1992, Thinking Machines Corporation held a workshop to investigate the question “How We Will Build a Machine that Thinks” 15. As you might guess from the workshop’s title, the participants were not especially supportive of the arguments against machine intelligence. In fact, there was general agreement that minds can exist on nonbiological substrates and that algorithms are of central importance to the existence of minds. However, there was much debate about the raw hardware power that is present in organic brains. A minority felt that the largest 1992 computers were within three orders of magnitude of the power of the human brain. The majority of the participants agreed with Moravec’s estimate 16 that we are ten to forty years away from hardware parity. And yet there was another minority who pointed to 17 18, and conjectured that the computational competence of single neurons may be far higher than generally believed. If so, our present computer hardware might be as much as ten orders of magnitude short of the equipment we carry around in our heads. If this is true (or for that matter, if the Penrose or Searle critique is valid), we might never see a Singularity. Instead, in the early ’00s we would find our hardware performance curves begin to level off – this caused by our inability to automate the complexity of the design work necessary to support the hardware trend curves. We’d end up with some very powerful hardware, but without the ability to push it further. Commercial digital signal processing might be awesome, giving an analog appearance even to digital operations, but nothing would ever “wake up” and there would never be the intellectual runaway which is the essence of the Singularity. It would likely be seen as a golden age … and it would also be an end of progress. This is very like the future predicted by Gunther Stent. In fact, on page 137 of 19, Stent explicitly cites the development of transhuman intelligence as a sufficient condition to break his projections.13 Penrose, R., The Emperor’s New Mind, Oxford University Press, 1989.14 Searle, John R., “Minds, Brains, and Programs”, in The Behavioral and Brain Sciences, v.3, Cambridge University Press, 1980. The essay is reprinted in The Mind’s I, edited by Douglas R. Hofstadter and Daniel C. Dennett, Basic Books, 1981. This reprinting contains an excellent critique of the Searle essay.15 Thearling, Kurt, “How We Will Build a Machine that Thinks”, a workshop at Thinking Machines Corporation. Personal Communication.16 Moravec, Hans, Mind Children, Harvard University Press, 1988.17 Conrad, Michael et al., “Towards an Artificial Brain”, BioSystems, vol23, pp175-218, 1989.18 Rasmussen, S. et al., “Computational Connectionism within Neurons: a Model of Cytoskeletal Automata Subserving Neural Networks”, in Emergent Computation, Stephanie Forrest, ed., p428-449, MIT Press, 1991.19 Stent, Gunther S., The Coming of the Golden Age: A View of the End of Progress, The Natural History Press, 1969.\nBut if the technological Singularity can happen, it will. Even if all the governments of the world were to understand the “threat” and be in deadly fear of it, progress toward the goal would continue. In fiction, there have been stories of laws passed forbidding the construction of “a machine in the form of the mind of man” 20.In fact, the competitive advantage – economic, military, even artistic – of every advance in automation is so compelling that passing laws, or having customs, that forbid such things merely assures that someone else will get them first.20 Herbert, Frank, Dune, Berkley Books, 1985. However, this novel was serialized in Analog Science Fiction-Science Fact in the 1960s.\nEric Drexler 21 has provided spectacular insight about how far technical improvement may go. He agrees that superhuman intelligences will be available in the near future – and that such entities pose a threat to the human status quo. But Drexler argues that we can embed such transhuman devices in rules or physical confinement such that their results can be examined and used safely. This is I. J. Good’s ultraintelligent machine, with a dose of caution. I argue that confinement is intrinsically impractical. For the case of physical confinement: Imagine yourself confined to your house with only limited data access to the outside, to your masters. If those masters thought at a rate – say – one million times slower than you, there is little doubt that over a period of years (your time) you could come up with “helpful advice” that would incidentally set you free. (I call this “fast thinking” form of superintelligence “weak superhumanity”. Such a “weakly superhuman” entity would probably burn out in a few weeks of outside time. “Strong superhumanity” would be more than cranking up the clock speed on a human-equivalent mind. It’s hard to say precisely what “strong superhumanity” would be like, but the difference appears to be profound. Imagine running a dog mind at very high speed. Would a thousand years of doggy living add up to any human insight? (Now if the dog mind were cleverly rewired and then run at high speed, we might see something different….) Most speculations about superintelligence seem to be based on the weakly superhuman model. I believe that our best guesses about the post-Singularity world can be obtained by thinking on the nature of strong superhumanity. I will return to this point later in the paper.)21 Drexler, K. Eric, Engines of Creation, Anchor Press/Doubleday, 1986.\nThe other approach to Drexlerian confinement is to build rules into the mind of the created superhuman entity (Asimov’s Laws). I think that performance rules strict enough to be safe would also produce a device whose ability was clearly inferior to the unfettered versions (and so human competition would favor the development of the those more dangerous models).Still, the Asimov dream is a wonderful one: Imagine a willing slave, who has 1000 times your capabilities in every way. Imagine a creature who could satisfy your every safe wish (whatever that means) and still have 99.9% of its time free for other activities. There would be a new universe we never really understood, but filled with benevolent gods (though one of my wishes might be to become one of them).\nIf the Singularity can not be prevented or confined, just how bad could the Post-Human era be? Well … pretty bad. The physical extinction of the human race is one possibility. (Or as Eric Drexler put it of nanotechnology: Given all that such technology can do, perhaps governments would simply decide that they no longer need citizens!). Yet physical extinction may not be the scariest possibility. Again, analogies: Think of the different ways we relate to animals. Some of the crude physical abuses are implausible, yet…. In a Post-Human world there would still be plenty of niches where human equivalent automation would be desirable: embedded systems in autonomous devices, self-aware daemons in the lower functioning of larger sentients. (A strongly superhuman intelligence would likely be a Society of Mind 22 with some very competent components.) Some of these human equivalents might be used for nothing more than digital signal processing. They would be more like whales than humans. Others might be very human-like, yet with a one-sidedness, a dedication that would put them in a mental hospital in our era. Though none of these creatures might be flesh-and-blood humans, they might be the closest things in the new environment to what we call human now. (I. J. Good had something to say about this, though at this late date the advice may be moot: Good 23 proposed a “Meta-Golden Rule”, which might be paraphrased as “Treat your inferiors as you would be treated by your superiors.”It’s a wonderful, paradoxical idea (and most of my friends don’t believe it) since the game-theoretic payoff is so hard to articulate. Yet if we were able to follow it, in some sense that might say something about the plausibility of such kindness in this universe.)22 Minsky, Marvin, Society of Mind, Simon and Schuster, 1985.23 Good, I. J., [Help! I can’t find the source of Good’s Meta-Golden Rule, though I have the clear recollection of hearing about it sometime in the 1960s. Through the help of the net, I have found pointers to a number of related items. G. Harry Stine and Andrew Haley have written about metalaw as it might relate to extraterrestrials: G. Harry Stine, “How to Get along with Extraterrestrials … or Your Neighbor”, Analog Science Fact- Science Fiction, February, 1980, p39-47.]\nI have argued above that we cannot prevent the Singularity, that its coming is an inevitable consequence of the humans’ natural competitiveness and the possibilities inherent in technology. And yet … we are the initiators. Even the largest avalanche is triggered by small things. We have the freedom to establish initial conditions, make things happen in ways that are less inimical than others. Of course (as with starting avalanches), it may not be clear what the right guiding nudge really is:"
  },
  {
    "objectID": "blog/posts/vernor-vinge/index.html#other-paths-to-the-singularity-intelligence-amplification",
    "href": "blog/posts/vernor-vinge/index.html#other-paths-to-the-singularity-intelligence-amplification",
    "title": "The Coming Technological Singularity",
    "section": "Other Paths to the Singularity: Intelligence Amplification",
    "text": "Other Paths to the Singularity: Intelligence Amplification\nWhen people speak of creating superhumanly intelligent beings, they are usually imagining an AI project. But as I noted at the beginning of this paper, there are other paths to superhumanity. Computer networks and human-computer interfaces seem more mundane than AI, and yet they could lead to the Singularity. I call this contrasting approach Intelligence Amplification (IA). IA is something that is proceeding very naturally, in most cases not even recognized by its developers for what it is. But every time our ability to access information and to communicate it to others is improved, in some sense we have achieved an increase over natural intelligence. Even now, the team of a PhD human and good computer workstation (even an off-net workstation!) could probably max any written intelligence test in existence.\nAnd it’s very likely that IA is a much easier road to the achievement of superhumanity than pure AI. In humans, the hardest development problems have already been solved. Building up from within ourselves ought to be easier than figuring out first what we really are and then building machines that are all of that. And there is at least conjectural precedent for this approach. Cairns-Smith 24 has speculated that biological life may have begun as an adjunct to still more primitive life based on crystalline growth. Lynn Margulis 25 has made strong arguments for the view that mutualism is the great driving force in evolution.24 Cairns-Smith, A. G., Seven Clues to the Origin of Life, Cambridge University Press, 1985.25 Margulis, Lynn and Dorion Sagan, Microcosmos, Four Billion Years of Evolution from Our Microbial Ancestors, Summit Books, 1986.\nNote that I am not proposing that AI research be ignored or less funded. What goes on with AI will often have applications in IA, and vice versa. I am suggesting that we recognize that in network and interface research there is something as profound (and potential wild) as Artificial Intelligence. With that insight, we may see projects that are not as directly applicable as conventional interface and network design work, but which serve to advance us toward the Singularity along the IA path.\nHere are some possible projects that take on special significance, given the IA point of view:\n\nHuman/computer team automation: Take problems that are normally considered for purely machine solution (like hill-climbing problems), and design programs and interfaces that take a advantage of humans’ intuition and available computer hardware. Considering all the bizarreness of higher dimensional hill-climbing problems (and the neat algorithms that have been devised for their solution), there could be some very interesting displays and control tools provided to the human team member.\nDevelop human/computer symbiosis in art: Combine the graphic generation capability of modern machines and the esthetic sensibility of humans. Of course, there has been an enormous amount of research in designing computer aids for artists, as labor saving tools. I’m suggesting that we explicitly aim for a greater merging of competence, that we explicitly recognize the cooperative approach that is possible. Karl Sims 26 has done wonderful work in this direction.\nAllow human/computer teams at chess tournaments. We already have programs that can play better than almost all humans. But how much work has been done on how this power could be used by a human, to get something even better? If such teams were allowed in at least some chess tournaments, it could have the positive effect on IA research that allowing computers in tournaments had for the corresponding niche in AI.\nDevelop interfaces that allow computer and network access without requiring the human to be tied to one spot, sitting in front of a computer. (This is an aspect of IA that fits so well with known economic advantages that lots of effort is already being spent on it.)\nDevelop more symmetrical decision support systems. A popular research/product area in recent years has been decision support systems. This is a form of IA, but may be too focussed on systems that are oracular. As much as the program giving the user information, there must be the idea of the user giving the program guidance.\nUse local area nets to make human teams that really work (ie, are more effective than their component members). This is generally the area of “groupware”, already a very popular commercial pursuit. The change in viewpoint here would be to regard the group activity as a combination organism. In one sense, this suggestion might be regarded as the goal of inventing a “Rules of Order” for such combination operations. For instance, group focus might be more easily maintained than in classical meetings. Expertise of individual human members could be isolated from ego issues such that the contribution of different members is focussed on the team project. And of course shared data bases could be used much more conveniently than in conventional committee operations. (Note that this suggestion is aimed at team operations rather than political meetings. In a political setting, the automation described above would simply enforce the power of the persons making the rules!)\nExploit the worldwide Internet as a combination human/machine tool. Of all the items on the list, progress in this is proceeding the fastest and may run us into the Singularity before anything else. The power and influence of even the present-day Internet is vastly underestimated. For instance, I think our contemporary computer systems would break under the weight of their own complexity if it weren’t for the edge that the USENET “group mind” gives the system administration and support people!) The very anarchy of the worldwide net development is evidence of its potential. As connectivity and bandwidth and archive size and computer speed all increase, we are seeing something like Lynn Margulis’ 27 vision of the biosphere as data processor recapitulated, but at a million times greater speed and with millions of humanly intelligent agents (ourselves).\n\n26 Sims, Karl, “Interactive Evolution of Dynamical Systems”, Thinking Machines Corporation, Technical Report Series, published in Toward a Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life, Paris, MIT Press, December 1991.27 Margulis, Lynn and Dorion Sagan, Microcosmos, Four Billion Years of Evolution from Our Microbial Ancestors, Summit Books, 1986.The above examples illustrate research that can be done within the context of contemporary computer science departments. There are other paradigms. For example, much of the work in Artificial Intelligence and neural nets would benefit from a closer connection with biological life. Instead of simply trying to model and understand biological life with computers, research could be directed toward the creation of composite systems that rely on biological life for guidance or for the providing features we don’t understand well enough yet to implement in hardware. A long-time dream of science-fiction has been direct brain to computer interfaces 28 29. In fact, there is concrete work that can be done (and has been done) in this area:28 Anderson, Poul, “Kings Who Die”, If, March 1962, p8-36. Reprinted in Seven Conquests, Poul Anderson, MacMillan Co., 1969.29 Vinge, Vernor, “Bookworm, Run!”, Analog, March 1966, pp8-40. Reprinted in True Names and Other Dangers, Vernor Vinge, Baen Books, 1987.\n\nLimb prosthetics is a topic of direct commercial applicability. Nerve to silicon transducers can be made 30.This is an exciting, near-term step toward direct communication.\nSimilar direct links into brains may be feasible, if the bit rate is low: given human learning flexibility, the actual brain neuron targets might not have to be precisely selected. Even 100 bits per second would be of great use to stroke victims who would otherwise be confined to menu-driven interfaces.\nPlugging in to the optic trunk has the potential for bandwidths of 1 Megabit/second or so. But for this, we need to know the fine-scale architecture of vision, and we need to place an enormous web of electrodes with exquisite precision. If we want our high bandwidth connection to be in addition to what paths are already present in the brain, the problem becomes vastly more intractable. Just sticking a grid of high-bandwidth receivers into a brain certainly won’t do it. But suppose that the high-bandwidth grid were present while the brain structure was actually setting up, as the embryo develops. That suggests:\nAnimal embryo experiments. I wouldn’t expect any IA success in the first years of such research, but giving developing brains access to complex simulated neural structures might be very interesting to the people who study how the embryonic brain develops. In the long run, such experiments might produce animals with additional sense paths and interesting intellectual abilities.\n\n30 Kovacs, G. T. A. et al., “Regeneration Microelectrode Array for Peripheral Nerve Recording and Stimulation”, IEEE Transactions on Biomedical Engineering, v 39, n 9, pp 893-902.Originally, I had hoped that this discussion of IA would yield some clearly safer approaches to the Singularity. (After all, IA allows our participation in a kind of transcendence.) Alas, looking back over these IA proposals, about all I am sure of is that they should be considered, that they may give us more options. But as for safety … well, some of the suggestions are a little scarey on their face. One of my informal reviewers pointed out that IA for individual humans creates a rather sinister elite. We humans have millions of years of evolutionary baggage that makes us regard competition in a deadly light. Much of that deadliness may not be necessary in today’s world, one where losers take on the winners’ tricks and are coopted into the winners’ enterprises. A creature that was built de novo might possibly be a much more benign entity than one with a kernel based on fang and talon. And even the egalitarian view of an Internet that wakes up along with all mankind can be viewed as a nightmare 31.31 Swanwick Michael, Vacuum Flowers, serialized in Isaac Asimov’s Science Fiction Magazine, December(?) 1986 - February 1987. Republished by Ace Books, 1988.\nThe problem is not that the Singularity represents simply the passing of humankind from center stange, but that it contradicts some of our most deeply held notions of being. I think a closer look at the notion of strong superhumanity can show why that is."
  },
  {
    "objectID": "blog/posts/vernor-vinge/index.html#strong-superhumanity-and-the-best-we-can-ask-for",
    "href": "blog/posts/vernor-vinge/index.html#strong-superhumanity-and-the-best-we-can-ask-for",
    "title": "The Coming Technological Singularity",
    "section": "Strong Superhumanity and the Best We Can Ask for",
    "text": "Strong Superhumanity and the Best We Can Ask for\nSuppose we could tailor the Singularity. Suppose we could attain our most extravagant hopes. What then would we ask for: That humans themselves would become their own successors, that whatever injustice occurs would be tempered by our knowledge of our roots. For those who remained unaltered, the goal would be benign treatment (perhaps even giving the stay-behinds the appearance of being masters of godlike slaves).It could be a golden age that also involved progress (overleaping Stent’s barrier). Immortality (or at least a lifetime as long as we can make the universe survive 32 33) would be achievable.32 Dyson, Freeman, “Physics and Biology in an Open Universe”, Review of Modern Physics, vol 51, pp447-460, 1979.33 Barrow, John D. and Frank J. Tipler, The Anthropic Cosmological Principle, Oxford University Press, 1986.\nBut in this brightest and kindest world, the philosophical problems themselves become intimidating. A mind that stays at the same capacity cannot live forever; after a few thousand years it would look more like a repeating tape loop than a person. (The most chilling picture I have seen of this is in 34.) To live indefinitely long, the mind itself must grow … and when it becomes great enough, and looks back … what fellow-feeling can it have with the soul that it was originally? Certainly the later being would be everything the original was, but so much vastly more. And so even for the individual, the Cairns-Smith (or Lynn Margulis) notion of new life growing incrementally out of the old must still be valid.34 Niven, Larry, “The Ethics of Madness”, If, April 1967, pp82-108. Reprinted in Neutron Star, Larry Niven, Ballantine Books, 1968.\nThis “problem” about immortality comes up in much more direct ways. The notion of ego and self-awareness has been the bedrock of the hardheaded rationalism of the last few centuries. Yet now the notion of self-awareness is under attack from the Artificial Intelligence people (“self-awareness and other delusions”). Intelligence Amplification undercuts the importance of ego from another direction. The post-Singularity world will involve extremely high-bandwidth networking. A central feature of strongly superhuman entities will likely be their ability to communicate at variable bandwidths, including ones far higher than speech or written messages. What happens when pieces of ego can be copied and merged, when the size of a self-awareness can grow or shrink to fit the nature of the problems under consideration?These are essential features of strong superhumanity and the Singularity. Thinking about them, one begins to feel how essentially strange and different the Post-Human era will be – no matter how cleverly and benignly it is brought to be.\nFrom one angle, the vision fits many of our happiest dreams: a place unending, where we can truly know one another and understand the deepest mysteries. From another angle, it’s a lot like the worst case scenario I imagined earlier in this paper.\nWhich is the valid viewpoint? In fact, I think the new era is simply too different to fit into the classical frame of good and evil. That frame is based on the idea of isolated, immutable minds connected by tenuous, low-bandwidth links. But the post-Singularity world does fit with the larger tradition of change and cooperation that started long ago (perhaps even before the rise of biological life). I think there are notions of ethics that would apply in such an era. Research into IA and high-bandwidth communications should improve this understanding. I see just the glimmerings of this now, in Good’s Meta-Golden Rule, perhaps in rules for distinguishing self from others on the basis of bandwidth of connection. And while mind and self will be vastly more labile than in the past, much of what we value (knowledge, memory, thought) need never be lost. I think Freeman Dyson has it right when he says 35: “God is what mind becomes when it has passed beyond the scale of our comprehension.”35 Dyson, Freeman, Infinite in All Directions, Harper && Row, 1988."
  },
  {
    "objectID": "blog/posts/vernor-vinge/index.html#appendix-metadata",
    "href": "blog/posts/vernor-vinge/index.html#appendix-metadata",
    "title": "The Coming Technological Singularity",
    "section": "Appendix: Metadata",
    "text": "Appendix: Metadata\nModernized from The Coming Technological Singularity on the original author’s website. The original header for the is as follows:\nVernor Vinge\n\nDepartment of Mathematical Sciences San Diego State University\n\n(c) 1993 by Vernor Vinge (Verbatim copying/translation and distribution of this entire article is permitted in any medium, provided this notice is preserved.)\n\n[I wish to thank John Carroll of San Diego State University and Howard Davidson of Sun Microsystems for discussing the draft version of this paper with me.]\n\nThis article was for the VISION-21 Symposium sponsored by NASA Lewis Research Center and the Ohio Aerospace Institute, March 30-31, 1993. It is also retrievable from the NASA technical reports server as part of NASA CP-10129. A slightly changed version appeared in the Winter 1993 issue of _Whole Earth Review_.\nVISION-21 Symposium was a conference held in March 1993, described as:\n\nThe symposium Vision-21: Interdisciplinary Science and Engineering in the Era of Cyberspace was held at the NASA Lewis Research Center on March 30-31, 1993. The purpose of the symposium was to simulate interdisciplinary thinking in the sciences and technologies which will be required for exploration and development of space over the next thousand years. The keynote speakers were Hans Moravec, Vernor Vinge, Carol Stoker, and Myron Krueger. The proceedings consist of transcripts of the invited talks and the panel discussion by the invited speakers, summaries of workshop sessions, and contributed papers by the attendees. (Vision 21: Interdisciplinary Science and Engineering in the Era of Cyberspace 1993)"
  },
  {
    "objectID": "blog/posts/decline-of-mathematics/index.html",
    "href": "blog/posts/decline-of-mathematics/index.html",
    "title": "The Decline of Mathematical Fields",
    "section": "",
    "text": "In the 1880’s and 90’s the Theory of Invariants was seen to have unified many areas of mathematics, but by 1940 mathematicians, if asked, would have said the theory was dead. … most contemporary mathematicians have difficulty in naming one practitioner of the theory.\n(Fisher 1966)\n\n\nInvariant theory has already been pronounced dead several times, and like the phoenix it has been again and again rising from its ashes. The first period in the history of the theory culminated with the discovery of the so-called “symbolic method” which in theory allowed the computation of all invariants by a quasi-mechanical process, But it was soon realized that, except in a very few simple cases, the actual computation would lead to enormous labor, disproportionate with the interest of the outcome, especially in a period when all calculations were done by hand (it might be worthwhile to push the XIXth Century computations of invariants a little further along, with the help of modern computers). Partly for that reason, the next problem in the theory was the search for “fundamental systems” of invariants, i.e., finite sets such that any invariant would be a polynomial in the fundamental invariants. It is well known that the existence of such systems was proved by Hilbert in 1890, in a brilliant paper which made him famous overnight and which may be considered as the first paper in “modern algebra,” by its conceptual approach and methods. But Hilbert’s success also spelled the doom of XIXth Century invariant theory, which was left with no big problems to solve and soon faded into oblivion.\n(Dieudonné and Carrell 1970)\n\n\nHilbert’s paper did not immediately kill the subject, but rather acted as a progressive illness, beginning with an initial shock, and slowly consuming the computational body of the theory from within, so that by the early 1920’s the subject was clearly moribund. Abstraction ruled: the disciples of Emmy Noether, a student of Gordan, led the fight against the discredited computational empire, perhaps as a reaction to Noether’s original, onerous thesis topic that involved computing the invariants for a quartic form in three variables.\nClassical invariant theory, by Peter Olver 1999\n\n\n\nConsider all degree-2 homogeneous polynomials (over complex numbers). That is, consider functions like\n\\[\nf(z) = a_1 z_1^2 + a_2 z_1z_2 + a_3 z_2^2, \\quad a_1, a_2, a_3 \\in \\C.\n\\]\nEach such polynomial \\(f\\) is equivalent to a point in \\(\\C^3\\). As usual, we always try to hit the function with linear transforms if it simplifies the function. Let \\(A\\) be a linear transform, such that\n\\[\nA(z_1, z_2) = (A_{11}z_1 + A_{12}z_2, A_{21}z_1 + A_{22}z_2)\n\\]\nIt would not do if \\(A\\) collapses everything to zero, so we require \\(A\\) to be invertible. Further, we are only interested in solving \\(f=0\\), not the value of \\(f\\) itself. Therefore, scaling \\(A\\) by a constant does not matter, so we can remove this ambiguity by requiring \\(\\det A = 1\\). That is, we only consider the group \\(SL(2)\\).\nIn fact, we are not considering the whole space \\(\\C^3\\), but only the space of lines – the projective plane \\(\\P\\C^2\\). The idea can be visualized in real space \\(\\R^3\\) by first taking a homogeneous polynomial’s solutions can be found by first solving it on the unit sphere, then zoom it in and out to get all the whole solution.\nFor example, if we have a polynomial \\(f(x, y, z) = x^3 + y^2z + 2xyz\\), then its solution \\(f=0\\) is a surface in \\(\\R^3\\). We can solve the problem by first solving the surface’s intersection with the unit sphere, then for each point on the intersection, drawing a ray from the origin to the point. You can picture it thus: Take a steel ball, draw a curve on the surface with a marker pen, then drill in at each point on the curve, resulting in a cut-out cone.\n\n\n\nRings of a tree. You can solve a polynomial by finding the intersection of the surface with the bark of the tree, then draw a line from the center to each point.\n\n\nTheorem: Any invariant of \\(f\\) is divisible by the discriminant \\(\\Delta = a_2^2 - 4a_1a_3\\).\nHilbert’s basis theorem: For any form of polynomial, the space of invariants has a finite basis."
  },
  {
    "objectID": "blog/posts/decline-of-mathematics/index.html#invariant-theory",
    "href": "blog/posts/decline-of-mathematics/index.html#invariant-theory",
    "title": "The Decline of Mathematical Fields",
    "section": "",
    "text": "In the 1880’s and 90’s the Theory of Invariants was seen to have unified many areas of mathematics, but by 1940 mathematicians, if asked, would have said the theory was dead. … most contemporary mathematicians have difficulty in naming one practitioner of the theory.\n(Fisher 1966)\n\n\nInvariant theory has already been pronounced dead several times, and like the phoenix it has been again and again rising from its ashes. The first period in the history of the theory culminated with the discovery of the so-called “symbolic method” which in theory allowed the computation of all invariants by a quasi-mechanical process, But it was soon realized that, except in a very few simple cases, the actual computation would lead to enormous labor, disproportionate with the interest of the outcome, especially in a period when all calculations were done by hand (it might be worthwhile to push the XIXth Century computations of invariants a little further along, with the help of modern computers). Partly for that reason, the next problem in the theory was the search for “fundamental systems” of invariants, i.e., finite sets such that any invariant would be a polynomial in the fundamental invariants. It is well known that the existence of such systems was proved by Hilbert in 1890, in a brilliant paper which made him famous overnight and which may be considered as the first paper in “modern algebra,” by its conceptual approach and methods. But Hilbert’s success also spelled the doom of XIXth Century invariant theory, which was left with no big problems to solve and soon faded into oblivion.\n(Dieudonné and Carrell 1970)\n\n\nHilbert’s paper did not immediately kill the subject, but rather acted as a progressive illness, beginning with an initial shock, and slowly consuming the computational body of the theory from within, so that by the early 1920’s the subject was clearly moribund. Abstraction ruled: the disciples of Emmy Noether, a student of Gordan, led the fight against the discredited computational empire, perhaps as a reaction to Noether’s original, onerous thesis topic that involved computing the invariants for a quartic form in three variables.\nClassical invariant theory, by Peter Olver 1999\n\n\n\nConsider all degree-2 homogeneous polynomials (over complex numbers). That is, consider functions like\n\\[\nf(z) = a_1 z_1^2 + a_2 z_1z_2 + a_3 z_2^2, \\quad a_1, a_2, a_3 \\in \\C.\n\\]\nEach such polynomial \\(f\\) is equivalent to a point in \\(\\C^3\\). As usual, we always try to hit the function with linear transforms if it simplifies the function. Let \\(A\\) be a linear transform, such that\n\\[\nA(z_1, z_2) = (A_{11}z_1 + A_{12}z_2, A_{21}z_1 + A_{22}z_2)\n\\]\nIt would not do if \\(A\\) collapses everything to zero, so we require \\(A\\) to be invertible. Further, we are only interested in solving \\(f=0\\), not the value of \\(f\\) itself. Therefore, scaling \\(A\\) by a constant does not matter, so we can remove this ambiguity by requiring \\(\\det A = 1\\). That is, we only consider the group \\(SL(2)\\).\nIn fact, we are not considering the whole space \\(\\C^3\\), but only the space of lines – the projective plane \\(\\P\\C^2\\). The idea can be visualized in real space \\(\\R^3\\) by first taking a homogeneous polynomial’s solutions can be found by first solving it on the unit sphere, then zoom it in and out to get all the whole solution.\nFor example, if we have a polynomial \\(f(x, y, z) = x^3 + y^2z + 2xyz\\), then its solution \\(f=0\\) is a surface in \\(\\R^3\\). We can solve the problem by first solving the surface’s intersection with the unit sphere, then for each point on the intersection, drawing a ray from the origin to the point. You can picture it thus: Take a steel ball, draw a curve on the surface with a marker pen, then drill in at each point on the curve, resulting in a cut-out cone.\n\n\n\nRings of a tree. You can solve a polynomial by finding the intersection of the surface with the bark of the tree, then draw a line from the center to each point.\n\n\nTheorem: Any invariant of \\(f\\) is divisible by the discriminant \\(\\Delta = a_2^2 - 4a_1a_3\\).\nHilbert’s basis theorem: For any form of polynomial, the space of invariants has a finite basis."
  },
  {
    "objectID": "blog/posts/data-manifold/index.html",
    "href": "blog/posts/data-manifold/index.html",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "",
    "text": "This is a theory of neural scaling law, proposed by (Bahri et al. 2021; Sharma and Kaplan 2022)\nAccording to this theory, a neural network, when trained to convergence, allocates its \\(N\\) parameters in two parts: * A fixed number of parameters that map the data to an intrinsic data manifold of dim \\(d\\). * All other parameters that handle pieces of this manifold. Loss \\(\\propto\\) the volume of each manifold piece.\nThey argued that the loss function should scale as \\(L \\propto N^{-4/d}\\) for cross-entropy and mean-square losses."
  },
  {
    "objectID": "blog/posts/data-manifold/index.html#the-manifold-hypothesis",
    "href": "blog/posts/data-manifold/index.html#the-manifold-hypothesis",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "The manifold hypothesis",
    "text": "The manifold hypothesis\nConsider the space of all MNIST images. Each MNIST image is a 28x28 grayscale, so the total space is \\(\\mathbb R^{28\\times 28} = \\mathbb R^{784}\\).\nHowever, as you may have seen in experiments with the VAE, most of the MNIST dataset “collapses” onto a much smaller subset of \\(\\mathbb R^{784}\\). This is the “(intrinsic) data manifold”, with a dimension much smaller than \\(784\\). Indeed, the very fact that the intrinsic dimension is small allows us to have meaningful “2D slices” of the dataset:\n\n\n\nFigure from Convolutional Variational Autoencoder - Tensorflow Core Tutorials. More images can be found in Visualizing the Variational Autoencoder, and can be regenerated in about 10 minutes with a Google Colab GPU.\n\n\nReal data can be expensive, though, which is why we often use “toy” datasets with known dimensions, generated by a known random process. For example, the following is the “Swiss roll” dataset. It is generated by first populating a 2D square \\([0, 1]^2\\), then use a function \\(f: \\mathbb R^2 \\to \\mathbb R^3\\) to “roll up” the square into 3D space.\n\n\n\nThe Isomap algorithm (Tenenbaum, Silva, and Langford 2000), popular for constructing data manifolds. Figure from Wikipedia."
  },
  {
    "objectID": "blog/posts/data-manifold/index.html#synthetic-data-manifolds",
    "href": "blog/posts/data-manifold/index.html#synthetic-data-manifolds",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Synthetic data manifolds",
    "text": "Synthetic data manifolds\nConsider the simplest data manifold: \\(\\mathbb R^d\\), affinely transformed, then embedded in \\(\\mathbb R^n\\), with \\(n &gt; d\\).\nTo synthesize such a data manifold, we randomly initialize a teacher network, so-called because it implements the function that a student network will fit to by supervised training. Each teacher network is constructed thus:\n\nThe number of neurons in each layer are: \\([d, 9, 600, 600, 1]\\)\nIt has 0 bias.\nThe weights between layers are sampeled from \\(\\mathcal N(0, m^{-1/2})\\) , where \\(m\\) is the input size of the layer (a form of He initialization).\nThe activation function at the second (with 9 neurons) and last layers are identity. All other activation functions are ReLU.\n\nOnce we have constructed a teacher network, we use it to generate a dataset \\(\\{(x_i, y_i)\\}_i\\) thus:\n\nGenerate random gaussian vectors \\(\\{t_i\\}_i\\) in \\(\\mathbb R^d\\), with mean \\(0\\) and std \\(I_{d\\times d}\\).\nFor each \\(t \\in \\{t_i\\}_i\\), push \\(t\\) through the teacher network.\nLet \\(x \\in \\mathbb R^9\\) be the teacher network activation at the second layer, with 9 neurons.\nLet \\(y \\in \\mathbb R\\) be the teacher network output.\n\n\nSome proofs\nAssuming that we have Lipschitz continuity, we can make some proofs."
  },
  {
    "objectID": "blog/posts/data-manifold/index.html#learning-data-manifold-by-neural-networks",
    "href": "blog/posts/data-manifold/index.html#learning-data-manifold-by-neural-networks",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Learning data manifold by neural networks",
    "text": "Learning data manifold by neural networks\nAccording to the theory, if the data manifold has dimension \\(d\\), then as we scale up a neural network with \\(N\\) parameters, the MSE loss of a fully-trained network would scale like \\(L \\sim N^{-\\alpha}\\), where \\(\\alpha \\approx 4/d\\).\nLet’s test this.\nFirst, we define the “student” neural network architecture:\n\nThe number of neurons in each layer are: \\([9, n, n, 1]\\).\nThe biases are initialized to 0.\nThe weights between layers are sampeled from \\(\\mathcal N(0, m^{-1/2})\\), where \\(m\\) is the input size of the layer (a form of He initialization).\nAll activation functions are ReLU.\n\nA simple computation shows that the network has exactly \\(N = n^2+12n + 1\\) parameters11 \\(N = \\underbrace{(n+n+1)}_{\\text{first layer}} + \\underbrace{(9n + n^2 + n)}_{\\text{second layer}}\\)\n\n\n\nAn example teacher network architecture with \\([9, 5, 5, 1]\\) neurons."
  },
  {
    "objectID": "blog/posts/data-manifold/index.html#experiments",
    "href": "blog/posts/data-manifold/index.html#experiments",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Experiments",
    "text": "Experiments\nAccording to the theory, if the data manifold has dimension \\(d\\), then as we scale up a neural network with \\(N\\) parameters, the MSE loss of a fully-trained network would scale like \\(L \\sim N^{-\\alpha}\\), where \\(\\alpha \\approx 4/d\\). We test this in two ways, once with synthetic datasets, where we know that the data manifold has the desired number of dimensions, and once with the CIFAR10 dataset, where we do not have the .\n\nSynthetic data manifolds\nSince Consider the simplest data manifold: \\(\\mathbb R^d\\), affine-transformed, then embedded in \\(\\mathbb R^n\\), with \\(n &gt; d\\).\nTo synthesize such a data manifold, we randomly initialize a teacher network, so-called because it implements the function that a student network will fit to by supervised training. Each teacher network is constructed thus:\n\nThe number of neurons in each layer are: \\([d, 9, 600, 600, 1]\\)\nIt has 0 bias.\nThe weights between layers are sampled from \\(\\mathcal N(0, m^{-1/2})\\) , where \\(m\\) is the input size of the layer (a form of He initialization).\nThe activation function at the second (with 9 neurons) and last layers are identity. All other activation functions are ReLU.\n\nOnce we have constructed a teacher network, we use it to generate a dataset \\(\\{(x_i, y_i)\\}_i\\) in this way:\n\nGenerate random gaussian vectors \\(\\{t_i\\}_i\\) in \\(\\mathbb R^d\\), with mean \\(0\\) and std \\(I_{d\\times d}\\).\nFor each \\(t \\in \\{t_i\\}_i\\), push \\(t\\) through the teacher network.\nLet \\(x \\in \\mathbb R^9\\) be the teacher network activation at the second layer, with 9 neurons.\nLet \\(y \\in \\mathbb R\\) be the teacher network output.\n\nFirst, we define the “student” neural network architecture:\n\nThe number of neurons in each layer are: \\([9, n, n, 1]\\).\nThe biases are initialized to 0.\nThe weights between layers are sampled from \\(\\mathcal N(0, m^{-1/2})\\), where \\(m\\) is the input size of the layer (a form of He initialization).\nAll activation functions are ReLU.\n\n\n\n\nAn example teacher network architecture with \\([9, 5, 5, 1]\\) neurons.\n\n\nThe parameter count is\n\\[\nN = \\underbrace{(n+n+1)}_{\\text{first layer}} + \\underbrace{(9n + n^2 + n)}_{\\text{second layer}}\n\\]\n\n\nExperimental data\nI ran it for days on my laptop.\n\n\nAll code for generating the dataset, and for analyzing the dataset, are in a GitHub repo: yuxi-liu-wired/scaling-law-by-data-manifold."
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#brief-history",
    "href": "blog/posts/mixture-of-experts/index.html#brief-history",
    "title": "Mixture of Experts",
    "section": "Brief history",
    "text": "Brief history\nIn the beginning was the gaussian. The gaussian is a beautiful distribution, with linearity, the central limit theorem, fast inference, least squares regression, and so on. The problem is that it has just one peak.\nIf one wants to model a complicated distribution with several bumps, one can make one step up the staircase of complexity, and build distributions from a linear sum of several gaussians. This is the mixture of gaussians. More generally, simple statistical models like the Poisson distribution, the Bernoulli distribution, and so on, can be added together to create mixture models.\n\n\n\nA mixture of three gaussian bumps. Figure from Wikipedia.\n\n\nA mixture of experts is then a simple generalization, and training a mixture of experts, back in the old days, was mostly thought of as statistical inference. The main problem was simply modelling complex data with a larger family of statistical distribution. Their main worry was that the experts would overfit.\nThey had little data (enough to fit onto a floppy disk), and each expert was usually just a gaussian distribution or a logistic classifier (any more complex and they wouldn’t know how to calculate the integrals and derivatives). Consequently, what they ended up trying to solve was to fit a few thousand datapoints using tens of very simple experts.\nIt is a general fact of classical machine learning that they were very worried about overfitting, and it was a reasonable worry back then, since they had such small datasets (MNIST was in 1994). This, combined with their inability to hand-design learning algorithms for complex machine learning architectures and the slowness of pure gradient descent, meant that machine learning algorithms back then were simple ones fitted onto small datasets.\n\n\n\ncost of\n1980s\n2010s\n2020s\n\n\n\n\ndata\nhigh\nlow\nlow\n\n\nalgorithm\nhigh\nlow\nlow\n\n\ntraining\nlow\nmedium\nhigh\n\n\ninference\nlow\nmedium\nhigh\n\n\n\nThe overall effect is:\n\ngetting training data: expensive (you have to do it yourself);\ndesigning the algorithm: expensive (cheaper if you have graduate students);\ntraining compute: low (there was little funding for training);\ninference compute: very cheap (since you could not train large models).\n\nThis should be compared to the very different situation with deep learning since the 2010s:\n\ngetting training data: cheap (just download it online);\ndesigning the algorithm: cheap (make a standard network, add a few decorations, then use backprop with Adam optimizer);\ntraining compute: as expensive as you want;\ninference compute: as expensive as you want.\n\nWhile classical statistics and machine learning was mainly constrained by how many partial derivatives and integrals the statistician can calculate confidently on paper,1 deep learning is mainly constrained by memory and compute budget.1 If you want a taste of the old days, look at the formulas inside (Jordan and Jacobs 1994). They explicitly calculated the expectation-maximization algorithms for learning a hierarchy of linear experts.\nSo when the deep learning era came circa 2012, people immediately started looking into how to perform conditional computing, that is, to save compute by only calling a small portion of the model. The idea is that you would have different portions of the model be specialized for different forms of input, and for each input, the model would first cheaply find out which expert should handle it, then call upon only the few specialized experts to handle this particular input.\nThe first paper on applying MoE to deep learning was (Eigen, Ranzato, and Sutskever 2013), one year after AlexNet. However, the deep MoE (DMoE) proposed in the paper has no sparsity, and so it has no modern offsprings. Let \\(f_{1, 1}, f_{1, 2}, \\dots, f_{1, n}\\) be \\(n\\) layers with the same architecture. Now, each can bo treated as an expert, and be mixed by\n\\[\nf_1(x) = \\sum_i g_{1, i}(x) f_{1, i}(x)\n\\]\nwhere \\(g_{1, i}\\) is a tiny neural network, the gating network for the first layer. Now, stack multiple such layers, and we obtain the DMoE. As one can see, such a network still has to use all the parameters in each forward pass, and therefore saves no compute. It is simply a case of the dense MoE.\nModern2020s deep learning really arrived with the sparsely-gated MoE (Shazeer et al. 2017), which saves compute. Specifically, if each layer contains \\(8\\) experts, but only \\(2\\) are consulted, then the cost of compute is only about \\(1/4\\) for the full model."
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#load-balancing",
    "href": "blog/posts/mixture-of-experts/index.html#load-balancing",
    "title": "Mixture of Experts",
    "section": "Load balancing",
    "text": "Load balancing\nThe main problem with MoE is a kind of rich-get-richer effect. If at the start of training, some experts are consulted often by random fluctuation, they would be heavily trained by backpropagation, and become even better experts, a upward spiral resulting in a few good experts and many useless experts.\nFor example, in the very first paper on MoE, they trained up to 8 experts to recognize phonemes from 6 Japanese speakers. They found that:\n\nOnly experts 4, 5, and 6 are active in the final mixture. This solution is typical – in all simulations with mixtures of 4 or 8 experts all but 2 or 3 experts had mixing proportions that were effectively 0 for all cases. (Jacobs et al. 1991)\n\nThis might not have been a serious problem in the past, when neural networks were seen as merely a form of high-dimensional statistical model learnable by any one of the typical statistical algorithms (maximal likelihood, Bayesian inference, expectation maximization…), but nowadays, MoE are used because you need to throw more compute at the problem, but cannot afford a larger dense model. In this case, it would defeat the purpose of MoE if some experts end up neglected.\nIt is no coincidence, then, that the sparsely-gated MoE paper (Shazeer et al. 2017) specifically used two auxiliary loss functions to encourage the experts to have equal “weight” over time. It was simplified to just one in the Switch Transformers paper (Fedus, Zoph, and Shazeer 2022).\nSpecifically, consider the sparsely-gated MoE with \\(k=1\\) – where just the top-ranked expert is consulted every time. Let \\(n\\) be the number of experts, and consider a batch of queries \\(\\{x_1, x_2, ..., x_T\\}\\), then the auxiliary loss of the batch is\n\\[\nL := n \\sum_{i=1}^n f_i P_i\n\\]\nwhere \\(f_i=\\frac{1}{T} \\#(\\text{queries sent to expert $i$})\\) is the fraction of time where expert \\(i\\) is ranked highest, and \\(P_i=\\frac{1}{T} \\sum_{j=1}^T w_i\\left(x_j\\right)\\) is the fraction of weight on expert \\(i\\).\nIn the original paper, they claimed that we can obtain the minimal auxiliary loss \\(L\\) at the limit where every expert has equal weight \\(1 / n\\) on all samples, and every expert is ranked the highest equally often.\nPlugging in the equations, we find it is \\(1\\). Unfortunately, this is technically wrong. When there are many experts and large batch, a way to let \\(L\\) approach \\(1/2\\). It is not difficult to show that \\(1/2\\) is the true lower bound. Seeing that Google has been training those huge models since 2017, this definitely works in practice, despite being slightly incorrect.\nThere are plenty of other choices for load balancing, which are rather technical details. For example, the z-loss stabilizes mixed-precision training by discouraging logits that are too far from zero, avoiding large round-off errors (Zoph et al. 2022, secs. 3.3–3.4)."
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#sec-appendix-error",
    "href": "blog/posts/mixture-of-experts/index.html#sec-appendix-error",
    "title": "Mixture of Experts",
    "section": "Appendix: Error in load balancing",
    "text": "Appendix: Error in load balancing\n\nLet one expert get \\(1/2 - \\epsilon\\) on every question, but never consulted on anything, and let every other \\(n-1\\) expert evenly divide the rest of the questions. For example, this is how the weights should be distributed when there are 3 experts on 4 questions:\n\\[\n\\begin{bmatrix}\n\\frac 12 - \\epsilon & \\frac 12 + \\epsilon & 0 \\\\\n\\frac 12 - \\epsilon & \\frac 12 + \\epsilon & 0 \\\\\n\\frac 12 - \\epsilon & 0 & \\frac 12 + \\epsilon \\\\\n\\frac 12 - \\epsilon & 0 & \\frac 12 + \\epsilon \\\\\n\\end{bmatrix}\n\\]\ngiving \\(L = \\frac 34 (1+2\\epsilon)\\)."
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#sec-appendix-load-balancing-error",
    "href": "blog/posts/mixture-of-experts/index.html#sec-appendix-load-balancing-error",
    "title": "Mixture of Experts",
    "section": "Appendix: Error in load balancing",
    "text": "Appendix: Error in load balancing\n\nLet one expert get \\(1/2 - \\epsilon\\) on every question, but is never consulted on anything, and let every other \\(n-1\\) expert evenly divide the rest of the questions. For example, this is how the weights should be distributed when there are 3 experts on 4 questions:\n\\[\n\\begin{bmatrix}\n\\frac 12 - \\epsilon & \\frac 12 + \\epsilon & 0 \\\\\n\\frac 12 - \\epsilon & \\frac 12 + \\epsilon & 0 \\\\\n\\frac 12 - \\epsilon & 0 & \\frac 12 + \\epsilon \\\\\n\\frac 12 - \\epsilon & 0 & \\frac 12 + \\epsilon \\\\\n\\end{bmatrix}\n\\]\ngiving \\(L = \\frac 34 (1+2\\epsilon)\\). By generalizing this construction, when there are many experts and large batch, we have \\(L \\to 1/2\\). It is not difficult to show that \\(1/2\\) is the true lower bound.\nWith the global optimization method of dual annealing2, Python found something close to the true lower bound, as shown in the figure. The load balancing matrix has a bright strip of \\(1/2 - \\epsilon\\), and slightly brighter dots of \\(1/2+\\epsilon\\) jumping around the matrix, as expected.2 I tried using local optimization with SciPy’s minimize, but it always fails to converge to \\(\\sim 1/2\\). It even fails to converge to \\(\\sim 1\\). Indeed, often it just moves around the initial point a bit then immediately gives up and stops. My suspicion is that the loss landscape is too jagged.\n\n\n\nThe result of minimizing load-balancing loss, with 10 experts and 10 questions."
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#sparsifying-moe",
    "href": "blog/posts/mixture-of-experts/index.html#sparsifying-moe",
    "title": "Mixture of Experts",
    "section": "Sparsifying MoE",
    "text": "Sparsifying MoE\nGiven a MoE, there are two ways to use it. One can use it as-is, but then every expert must be consulted on every query, defeating the main purpose of MoE in the age of large models: conditional computing.\nWe will try out two ways to sparsify a trained MoE. In the first MoE paper (1991), they inspected the weights (the matrix \\(A\\) in our notation), and found that some experts would never be called on any input. Then they just removed those experts. This can be understood as sparsification “at compile time”.\nHere, we implement sparsification “at compile time” by ranking the rows of \\(A\\) according to their L2-norm, find the top-k rows of them, then mask out all the other experts.\nThis implementation is given to you as the method sparse_forward in class MoE.\nIn the sparsely-gated MoE, the sparsification is done “at runtime”. That is:\n\\[w(x) = \\mathrm{softmax}(\\mathrm{top}_k(Ax))\\]\nwhere \\(\\mathrm{top}_k(v)\\) preserves the top-k entries of \\(v\\), but set all other entries to \\(-\\infty\\)."
  },
  {
    "objectID": "blog/posts/data-manifold/index.html#deriving-the-scaling-law",
    "href": "blog/posts/data-manifold/index.html#deriving-the-scaling-law",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Deriving the scaling law",
    "text": "Deriving the scaling law\nWe can get a feel for where the number \\(4/d\\) came from by studying a simpler model.\n\nPrototype case\nConsider a problem of regression. We have to learn the true function on the \\(n\\)-dimensional cube: \\(f: [0, 1]^d \\to \\mathbb{R}\\). Assume it is Lipschitz continuous, that is, its first derivative is upper bounded by \\(\\lambda\\). In particular, this means \\(|f(x) - f(y)| \\leq \\lambda |x-y|\\) for all \\(x, y\\) in the domain.\nWe approximate the true function with a piecewise-constant function \\(\\hat f: [0, 1]^d \\to \\mathbb{R}\\), meaning that its graph looks like a staircase. We divide the cube into \\(N\\) equal smaller cubic pieces, and define \\(\\hat f\\) to be equal to the value of \\(f\\) at the center of each cubic piece.\n\nTheorem 1 When the loss is mean square error, it scales like \\(L = \\Theta(N^{-2/d})\\).\n\n\nProof. With \\(N\\) parameters, we can divide the \\([0, 1]^d\\) cube into \\(N\\) equal parts, therefore, each cube has side length \\(N^{-1/d}\\). Therefore, the distance between the center of each cube and the point farthest from the center is also \\(\\Theta(N^{-1/d})\\).\nNow, since the function has bounded first derivative, we know that the difference between the function value at the center of the cube and the function value at the farthest point is bounded by \\(\\lambda \\cdot \\Theta(N^{-1/d}) = \\Theta(N^{-1/d})\\). Therefore, the mean square loss on each individual little cube is bounded by \\(\\Theta(N^{-2/d})\\).\nAnd since the overall mean square loss is the average of the loss on each individual cube, the total loss is also bounded by \\(\\Theta(N^{-2/d})\\).\n\n\n\nGeneralization\nMore generally, if \\(f\\) has bounded second-derivative, and we use a piecewise-linear \\(\\hat f\\) function approximator, then the mean square loss scales like \\(\\Theta(N^{-4/d})\\). By piece-wise linear, we mean that the domain of \\(\\hat f\\) is divided into little cubes, and it is linear on each little cube.\nIndeed, this generalizes in the obvious way:\n\nTheorem 2 If the loss is mean \\(p\\)-th power loss, \\(f\\) has bounded \\(k+1\\)-th order derivatives, and \\(\\hat f\\) is composed of piece-wise \\(k\\)-degree polynomials, then the loss scales like \\(\\Theta(N^{-p(k+1)/d})\\).\n\n\nProof. We prove another case where the loss is still mean square error, but \\(f\\) has bounded \\(2\\)-th order derivatives.\nBy Taylor expansion, if we use the first-order Taylor expansion to approximate \\(f\\) at the center of each cube, then the error is bounded by \\(\\Theta(N^{-2/d})\\). And since the mean square loss is the average of the square of the error, the total mean squared loss is bounded by \\(\\Theta(N^{-4/d})\\) on each little cube.\nAnd since the overall mean square loss is the average of the loss on each individual cube, the total loss is also bounded by \\(\\Theta(N^{-4/d})\\).\nFor the general case, take the Taylor expansion to the \\(k\\)-th order at the center of each little cube.\n\n\n\nScaling of nearest neighbor rule\nWhat is the worst possible scaling? It would be when \\(k=0\\) and \\(p=1\\), giving us \\(L = \\Theta(N^{-1/d})\\). What does this mean? To have \\(k=0\\) means that we use piecewise-constant fitting function \\(\\hat f\\). To have \\(p=1\\) means that we are using the L1-loss. This is essentially piecewise constant, median regression.\nUnder mild assumptions, the nearest neighbor rule for classification has the same form of scaling, where the loss is not L1-loss, but 0-1 loss.\nPeople have found sharper scaling laws under assuming nicer datasets, using complicated functional analysis tools scaling laws. A dataset can be “nice” in several ways. One way is to have few outliers: it should have a thin tail, looking more like a box, rather than a gently rising hill. Another way is to have smoothly varying boundaries: its boundary should not look “bumpy”. See Two Phases of Scaling Laws for Nearest Neighbor Classifiers (TODO?) for a brief review and further citations to the literature."
  },
  {
    "objectID": "blog/posts/data-manifold/index.html#experiments-1",
    "href": "blog/posts/data-manifold/index.html#experiments-1",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Experiments",
    "text": "Experiments\nI ran it for days on my laptop."
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html#deriving-the-scaling-law",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html#deriving-the-scaling-law",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Deriving the scaling law",
    "text": "Deriving the scaling law\nWe can get a feel for where the number \\(4/d\\) came from by studying a simpler model.\n\nPrototype case\nConsider a problem of regression. We have to learn the true function on the \\(n\\)-dimensional cube: \\(f: [0, 1]^d \\to \\mathbb{R}\\). Assume it is Lipschitz continuous, that is, its first derivative is upper bounded by \\(\\lambda\\). In particular, this means \\(|f(x) - f(y)| \\leq \\lambda |x-y|\\) for all \\(x, y\\) in the domain.\nWe approximate the true function with a piecewise-constant function \\(\\hat f: [0, 1]^d \\to \\mathbb{R}\\), meaning that its graph looks like a staircase. We divide the cube into \\(N\\) equal smaller cubic pieces, and define \\(\\hat f\\) to be equal to the value of \\(f\\) at the center of each cubic piece.\n\nTheorem 1 When the loss is mean square error, it scales like \\(L = \\Theta(N^{-2/d})\\).\n\n\nProof. With \\(N\\) parameters, we can divide the \\([0, 1]^d\\) cube into \\(N\\) equal parts, therefore, each cube has side length \\(N^{-1/d}\\). Therefore, the distance between the center of each cube and the point farthest from the center is also \\(\\Theta(N^{-1/d})\\).\nNow, since the function has bounded first derivative, we know that the difference between the function value at the center of the cube and the function value at the farthest point is bounded by \\(\\lambda \\cdot \\Theta(N^{-1/d}) = \\Theta(N^{-1/d})\\). Therefore, the mean square loss on each individual little cube is bounded by \\(\\Theta(N^{-2/d})\\).\nAnd since the overall mean square loss is the average of the loss on each individual cube, the total loss is also bounded by \\(\\Theta(N^{-2/d})\\).\n\n\n\nGeneralization\nMore generally, if \\(f\\) has bounded second-derivative, and we use a piecewise-linear \\(\\hat f\\) function approximator, then the mean square loss scales like \\(\\Theta(N^{-4/d})\\). By piece-wise linear, we mean that the domain of \\(\\hat f\\) is divided into little cubes, and it is linear on each little cube.\nIndeed, this generalizes in the obvious way:\n\nTheorem 2 If the loss is mean \\(p\\)-th power loss, \\(f\\) has bounded \\(k+1\\)-th order derivatives, and \\(\\hat f\\) is composed of piece-wise \\(k\\)-degree polynomials, then the loss scales like \\(\\Theta(N^{-p(k+1)/d})\\).\nSince the KL-divergence is approximately MSE loss when the predictor is close to correct, the loss scales like \\(\\Theta(N^{-2(k+1)/d})\\) in this case.\n\n\nProof. We prove another case where the loss is still mean square error, but \\(f\\) has bounded \\(2\\)-th order derivatives.\nBy Taylor expansion, if we use the first-order Taylor expansion to approximate \\(f\\) at the center of each cube, then the error is bounded by \\(\\Theta(N^{-2/d})\\). And since the mean square loss is the average of the square of the error, the total mean squared loss is bounded by \\(\\Theta(N^{-4/d})\\) on each little cube.\nAnd since the overall mean square loss is the average of the loss on each individual cube, the total loss is also bounded by \\(\\Theta(N^{-4/d})\\).\nFor the general case, take the Taylor expansion to the \\(k\\)-th order at the center of each little cube.\n\n\n\nScaling of nearest neighbor rule\nWhat is the worst possible scaling? It would be when \\(k=0\\) and \\(p=1\\), giving us \\(L = \\Theta(N^{-1/d})\\). What does this mean? To have \\(k=0\\) means that we use piecewise-constant fitting function \\(\\hat f\\). To have \\(p=1\\) means that we are using the L1-loss. This is essentially piecewise constant, median regression.\nUnder mild assumptions, the nearest neighbor rule for classification has the same form of scaling, where the loss is not L1-loss, but 0-1 loss.\nPeople have found sharper scaling laws under assuming nicer datasets, using complicated functional analysis tools scaling laws. A dataset can be “nice” in several ways. One way is to have few outliers: it should have a thin tail, looking more like a box, rather than a gently rising hill. Another way is to have smoothly varying boundaries: its boundary should not look “bumpy”. See Two Phases of Scaling Laws for Nearest Neighbor Classifiers (TODO?) for a brief review and further citations to the literature."
  }
]