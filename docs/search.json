[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "If you thought the Stroop test is hard, just wait until you have tried this.\nPlay online | Play offline"
  },
  {
    "objectID": "projects/index.html#double-stroop",
    "href": "projects/index.html#double-stroop",
    "title": "Projects",
    "section": "",
    "text": "If you thought the Stroop test is hard, just wait until you have tried this.\nPlay online | Play offline"
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html",
    "href": "blog/posts/wigner-rotation/index.html",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "",
    "text": "Special relativity, axiomatically, can be based on the following, very simple axioms:\n\nSpacetime is smooth like1 \\(\\mathbb{R}^{n+1}\\), where \\(n\\) is the number of spatial dimensions, and \\(1\\) is the number of temporal dimensions.\nSpacetime is symmetric under rotation, translation, and constant velocity motion.\nThere exists a fundamental speed, a “conversion factor” between the spatial and temporal dimensions, which we call \\(c\\).\nThere are many ways to map spacetime into \\(\\mathbb{R}^{n+1}\\), but only some of them are physically meaningful. Those are called the “coordinate systems”.\nYou can go from one coordinate system to another by a coordinate transform. Transforms can be combined one after another, and they can be reversed. That is, they make up a group, named the symmetry group of spacetime.\nThe symmetry group is smooth and connected, meaning that starting with the identity transformation (doing nothing), you can combine it with tiny transformations, and end up at any other transformation.\n\n1 In jargon, it is isomorphic as a smooth manifold to \\(\\mathbb{R}^{n+1}\\), or diffeomorphic to \\(\\mathbb{R}^{n+1}\\).We will follow the spirit of Erlangen program, which states clearly what our target is (Klein 1893):\n\nGiven a manifoldness [smooth space] and a group of transformations of the same; to develop theory of invariants relating to that group. This is the general problem, and it comprehends not alone ordinary geometry, but also and in particular the more recent geometrical theories which we propose to discuss, and the different methods of treating manifoldnesses of \\(n\\) dimensions.\n\nConsidering that Felix Klein first stated it in 1872, decades before special relativity, this is prescient.\nApplied to special relativity, our target is to start with the axioms, find out something about what kinds of symmetries there can be, and what kinds of objects are unchanged by symmetries. In other words, we will study the geometry of spacetime.\n\n\n\nSo far, while we were talking about “spacetime”, we were actually talking about mathematical objects. There is no necessary connection between the mathematical objects and whatever we observe in the real world. So why do we call those mathematical objects with physically suggestive names like “spacetime”?\nWell, special relativity is really a mathematical object. It is connected to the physical world like all objects in mathematical physics: by experiments. Experimental data is physical, and by studying the mathematical-object special relativity, we found that it can generate the experimental data with a minimal amount of “glue” between the physical world and the mathematical world. The “glue” looks like:\n\nPhysical events in physical spacetime are mathematical points in mathematical spacetime.\nPhysical light moves at the mathematical speed of \\(c\\)\nPhysical clocks tick at a mathematical constant rate.\nIf you move a physical clock from physical-here to physical-there, it is still ticking at the same mathematical constant rate.\n…\n\nEinstein’s The Meaning of Relativity* (Albert Einstein 1922) details physical and mathematical correspondences, such as clock synchronization and the movement of meter sticks.\n\nAs far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#relativity-in-general",
    "href": "blog/posts/wigner-rotation/index.html#relativity-in-general",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "",
    "text": "Special relativity, axiomatically, can be based on the following, very simple axioms:\n\nSpacetime is smooth like1 \\(\\mathbb{R}^{n+1}\\), where \\(n\\) is the number of spatial dimensions, and \\(1\\) is the number of temporal dimensions.\nSpacetime is symmetric under rotation, translation, and constant velocity motion.\nThere exists a fundamental speed, a “conversion factor” between the spatial and temporal dimensions, which we call \\(c\\).\nThere are many ways to map spacetime into \\(\\mathbb{R}^{n+1}\\), but only some of them are physically meaningful. Those are called the “coordinate systems”.\nYou can go from one coordinate system to another by a coordinate transform. Transforms can be combined one after another, and they can be reversed. That is, they make up a group, named the symmetry group of spacetime.\nThe symmetry group is smooth and connected, meaning that starting with the identity transformation (doing nothing), you can combine it with tiny transformations, and end up at any other transformation.\n\n1 In jargon, it is isomorphic as a smooth manifold to \\(\\mathbb{R}^{n+1}\\), or diffeomorphic to \\(\\mathbb{R}^{n+1}\\).We will follow the spirit of Erlangen program, which states clearly what our target is (Klein 1893):\n\nGiven a manifoldness [smooth space] and a group of transformations of the same; to develop theory of invariants relating to that group. This is the general problem, and it comprehends not alone ordinary geometry, but also and in particular the more recent geometrical theories which we propose to discuss, and the different methods of treating manifoldnesses of \\(n\\) dimensions.\n\nConsidering that Felix Klein first stated it in 1872, decades before special relativity, this is prescient.\nApplied to special relativity, our target is to start with the axioms, find out something about what kinds of symmetries there can be, and what kinds of objects are unchanged by symmetries. In other words, we will study the geometry of spacetime.\n\n\n\nSo far, while we were talking about “spacetime”, we were actually talking about mathematical objects. There is no necessary connection between the mathematical objects and whatever we observe in the real world. So why do we call those mathematical objects with physically suggestive names like “spacetime”?\nWell, special relativity is really a mathematical object. It is connected to the physical world like all objects in mathematical physics: by experiments. Experimental data is physical, and by studying the mathematical-object special relativity, we found that it can generate the experimental data with a minimal amount of “glue” between the physical world and the mathematical world. The “glue” looks like:\n\nPhysical events in physical spacetime are mathematical points in mathematical spacetime.\nPhysical light moves at the mathematical speed of \\(c\\)\nPhysical clocks tick at a mathematical constant rate.\nIf you move a physical clock from physical-here to physical-there, it is still ticking at the same mathematical constant rate.\n…\n\nEinstein’s The Meaning of Relativity* (Albert Einstein 1922) details physical and mathematical correspondences, such as clock synchronization and the movement of meter sticks.\n\nAs far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#relativity-in-11-dimensions",
    "href": "blog/posts/wigner-rotation/index.html#relativity-in-11-dimensions",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Relativity in 1+1 dimensions",
    "text": "Relativity in 1+1 dimensions\n\nMaking one coordinate system\nTo begin, let’s consider relativity in 1+1 dimensions. We will start with a “default” coordinate system (“the rest frame”).\nIn the beginning was just a point in spacetime. We call this the origin, and write its coordinates as \\(x=0, t=0\\). The world was yet without form. To give it form, we build a meter stick and an infinite number of light bulbs.\nWe put a light bulb at the origin. Because the bulb does not “stand still”, the light bulb traces out a path in spacetime. We call this path the \\(t\\)-axis at \\(x=0\\). Because the bulb does not stand still, we cannot properly say it is “at the origin”, but rather “passing the origin”.\n\nany real body must have extension in four directions: it must have Length, Breadth, Thickness, and—Duration. But through a natural infirmity of the flesh, which I will explain to you in a moment, we incline to overlook this fact. There are really four dimensions, three which we call the three planes of Space, and a fourth, Time. There is, however, a tendency to draw an unreal distinction between the former three dimensions and the latter, because it happens that our consciousness moves intermittently in one direction along the latter from the beginning to the end of our lives.\n—The Time Traveller, The Time Machine, Chapter 1\n\nWe put another light bulb to the left of the light bulb passing the origin, separated by one meter stick. This light bulb traces out a line at \\(x=1\\). And so on. Thus, we obtain an infinite number of lines: \\(\\cdots, x = -1, x = 0, x = +1, \\cdots\\). By subdividing the meter stick, we obtain one line \\(x = r\\) for each real number \\(r\\).\nNow we have measured space, we will measure time. We remove all light bulbs except two: one at \\(x=0\\), one at \\(x=1\\). At origin, the \\(x=0\\) light bulb flashes, and then the light bulb at \\(x=1\\) flashes when it receives the flash. When the light bulb at \\(x=0\\) receives the echo, this is the echo event – and we write it down as \\(x=0, t=2\\). Let spacetime reverberate with shining echoes, and in this way, every point in spacetime receives a unique coordinate \\((x, t)\\).\nNotice that in this construction, the fundamental speed \\(c\\) is equal to one, and if we draw space and time as two perpendicular directions on a graph-paper, then the trajectory of every light is a 45-degree straight line on the graph-paper. These are not fundamental aspects of theory, but are convenient outcomes given by how we constructed the coordinate system.\nAbstractly speaking, a coordinate system is a function that maps a point in spacetime to two real numbers, like \\((x, t): \\mathcal{M} \\to \\mathbb{R}^{1+1}\\). Our coordinate system constructed so far means that:\n\nThe origin-point \\(p_O\\) has coordinates \\(x(p_O) = 0, t(p_O) = 0\\).\nIf a beam of light is traveling to the right, and passes a certain point \\(p\\), then the set of all points on the beam of light is of form \\(\\{q : x(q) - x(p) = t(q) - t(p)\\}\\).\nAnd so on.\n\nSuch explicit distinction between the spacetime itself and the coordinate system is uncommon in physics, and once you are used to it, you should throw it into your subconscious like a muscle memory. However, it is good to keep it in mind for now.\n\n\nThe first moving frame\nThe integrity of the coordinate system is based on the meter stick. If two light bulbs are moving relative to each other, then they cannot be always connected by a meter stick – one of them would bump into the meter stick, or move away from it. In other words, the entire system is static.\nNow, we introduce another frame, moving at a velocity \\(v\\) relative to the first. While there is no fundamental reason to privilege one over the other, we call one the “rest frame” and the other the “moving frame” for convenience.\nThe moving frame constructs its own coordinate system \\((x', t')\\). What must it look like?\nSince the line \\(x' = 0\\) is constructed by the trajectory of the light bulb that passes the origin, we know that the \\(x' = 0\\) line is \\(\\{p : x(p) = v t(p)\\}\\). However, when it comes to the \\(x' = 1\\) line, we have a problem: How long should be the meter stick in the moving frame be? What does it even mean to compare a meter stick in a moving frame with a meter stick in the resting frame?\nThis difficulty is not pedantic. In the resting frame, checking that two meter sticks are equal means taking one, accelerating it then decelerating it, until it overlaps exactly with the other. This does not apply if they are in relative motion.\nThe solution is another “gluing”. Like how we “glued” the mathematical world with the physical world, we also glue one frame with another frame. This is connection, a central idea in differential geometry. However we will just do this intuitively, since in special relativity, connections are done in the most straightforward way.\nTo connect two frames, we divide their difference into tiny steps, and then do those tiny steps one after another. So, if we know how to connect two frames when \\(v\\) is an infinitesimal, then we know how to connect two arbitrary frames by taking an integral.\nSo for now, let \\(v\\) be an infinitesimal. What should be the meter stick in the moving frame? Its meter stick is whatever is necessary so that light speed is \\(1\\) in the moving frame. So, if we know how to tick the moving clock, we know how to build the moving meter stick.\nIn the resting frame, the clock passing the origin makes a tick at every point:\n\\[\\cdots, (0, -1), (0, 0), (0, +1), \\cdots\\]\nIn the moving frame, the clock passing the origin makes a tick at every point:\n\\[\\cdots, (-v \\; ?, -?), (0, 0), (v \\; ?, ?), \\cdots\\]\nThe naive choice is \\(? = 1\\). With this seemingly trivial step, we have in fact completely defined special relativity – all else is derivation.\n\n\nWhy \\(? = 1\\)\nBy the smoothness axiom, if \\(v\\) is infinitesimal, the coordinate transformation between the rest frame and the moving frame should be infinitesimally close to the identity.\nSince special relativity does not distinguish left from right, if we were to make the moving frame move at velocity \\(-v\\), then the coordinate transform should send \\((0, 1)\\) to \\((-\\delta x, 1 + \\delta t)\\) by mirror symmetry.\nNow, if we compose two boosts, first with \\(v\\) then with \\(-v\\), then we would send \\((0, 1)\\) to \\((0, 1+ 2\\delta t)\\). But we really should get back to \\((0, 1)\\) since we are back to the resting frame again. Therefore \\(\\delta t = 0\\).\n\n\n\n\n\nThree infinitesimal boosts\n\n\n\n\n\n\nThe Lorentz transformations\nIn the resting frame, we constructed the meter stick by shooting out one pulse of light, then listening for the echo that arrives two ticks later. The point at which the echo is reflected is one meter stick away. Equivalently, we can shoot a forward-pointing light cone at \\((0, -1)\\), and shoot a backward-pointing light cone at \\((0, +1)\\). Their intersections are the two ends of two meter sticks: \\((-1, 0), (+1, 0)\\).\nIn the boosted frame, the clock passing the origin ticks at \\(\\cdots, (-v, -1), (0, 0), (v, 1), \\cdots\\). Therefore, we can construct its meter stick in the same way, and we would find that the two light cones intersect at \\((-1, -v), (+1, +v)\\). This gives us the coordinate transform:\n\\[\\forall p \\in \\mathcal{M}, \\quad\n\\begin{bmatrix} x(p) \\\\ t(p) \\end{bmatrix} =\n\\begin{bmatrix} 1 & v \\\\ v & 1 \\end{bmatrix}\n\\begin{bmatrix} x'(p) \\\\ t'(p) \\end{bmatrix}\n\\]\nHere, we find that we need to distinguish between “active” and “passive” transforms. Everything we have said so far is “passive”. We assume that there is an underlying spacetime \\(\\mathcal{M}\\), and we are to construct coordinate systems over it. The coordinate transform tells us how to turn the coordinates of the same point from one coordinate system to another. However, while this is often useful, it would make everything we are going to say next very awkward.\nTherefore, we immediately change our point of view: we are going to study “active” transforms from now on. While passive transform of a clock means that we have just one clock and measure its ticks in two coordinate systems, active transform of a clock means that we actually pick up the clock at rest frame, accelerate it on a rocket, and then let it glide at constant velocity at the moving frame, then study what the clock is doing in the rest frame.\nSince an active transform is the opposite of a passive transform, we have the active Lorentz transformation formula:\n\\[\n\\begin{bmatrix} x' \\\\ t' \\end{bmatrix} =\n\\begin{bmatrix} 1 & v \\\\ v & 1 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ t \\end{bmatrix}\n\\tag{1}\\]\nThis formula states that if we “pick up” the spacetime event at \\((x, t)\\) in the rest frame, and accelerate it infinitesimally to speed \\(v\\), then the spacetime event ends up at \\((x', t')\\) in the rest frame.\n\n\nIntegrating the Lorentz boosts\nSince \\(v\\) is infinitesimal, to find the Lorentz transformation for a non-infinitesimal \\(v\\) (say, \\(v = 10^{-100}\\), which, while small, is really big for an infinitesimal), we need to integrate over it.\nBefore we do so, we should play with the discrete version: what happens if we repeatedly apply the matrix \\(\\begin{bmatrix} 1 & v \\\\ v & 1 \\end{bmatrix}\\) on the vectors \\((0, 1), (1, 0)\\)?\n\n\n\n  \n  Rapidity = 0.05\n\n\n  \n  Number of boosts = 1\n\n  \n\nIn the diagram, \\(v\\) is “rapidity”2. We apply the Lorentz boost with the given \\(v\\) repeatedly forwards and backwards. The resulting image is beautiful and suggestive: it looks like the unit hyperbolas!2 The reason for calling it “rapidity”, instead of “velocity” would be soon clear.\n\n    \n\nHaving guessed the answer, we proceed with the explicit integral to confirm our guess.\n\\[\n  \\begin{aligned}\n  \\begin{bmatrix} 1 & dv \\\\ dv & 1 \\end{bmatrix}^{\\frac{v}{dv}} &= \\left(I + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}\\right)^{\\frac{v}{dv}}\\\\\n  &= \\exp\\left(\\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} v\\right) \\\\\n  &= I + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} v + \\frac{1}{2!}v^2\\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}^2 + \\frac{1}{3!}v^3\\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}^3 + \\cdots \\\\\n  &= I(1 + v^2/2! + v^4/4! + \\cdots) + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}(v + v^3/3! + v^5/5! + \\cdots) \\\\\n  &= I\\cosh(v) + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\sinh(v)\n  \\end{aligned}\n\\tag{2}\\]\nAt this point, we should pause to take a careful look at \\(v\\). While it is the same as velocity when \\(v\\) is infinitesimal, it is not the same as velocity otherwise, because velocity is bounded between \\(-1\\) and \\(+1\\), whereas this \\(v\\) quantity is free to vary over all real values. This is why we call it “rapidity” and relabel it as \\(w\\), since it is like velocity, but different.\nThe velocity \\(v\\) for a given rapidity \\(w\\) is determined by transforming the \\(t\\)-axis using the Lorentz transformation matrix \\(\\begin{bmatrix} \\cosh(w) & \\sinh(w) \\\\ \\sinh(w) & \\cosh(w) \\end{bmatrix}\\). The \\(t\\)-axis \\[\\{(0, s) : s \\in \\mathbb{R}\\}\\] is boosted to the line \\[\\{(\\sinh(w) s, \\cosh(w) s) : s \\in \\mathbb{R}\\}\\] Therefore, the velocity of this boosted frame is \\[w = \\sinh(w) / \\cosh(w)  = \\tanh(w).\\]\nNow you can play with the diagram below to grow a new intuitive sense of how it all ties together. After you have grown this new intuition, you can proceed.\n\n    \n    \n\n\n\nA “spacetime square” would transform like:\n\n  \n\n\n\n3-velocity, 4-velocity, and inner product in spacetime\nIn the diagram above, we have used the terms 3-velocity and 4-velocity. To explain those terms, we briefly go back to 3+1 dimensions.\nWhen we say “velocity” informally, we mean a vector in \\(\\mathbb{R}^3\\). But how is velocity really defined? It is displacement divided by time. In relativity, when we say “3-velocity”, there are two possible representations: we can represent it as \\((v_x, v_y, v_z)\\), or we can represent it as \\((v_x, v_y, v_z, 1)\\). The first is close to how the rest of the world use “velocity”, but the second is close to how special relativity want us to use the word “velocity”. The second representation can be interpreted as follows: “The 3-velocity of an object is the spacetime displacement of the object after one tick of my clock.”\nNeither is satisfactory, however, because 3-velocity behaves badly under Lorentz transformations. The first representation \\((v_x, v_y, v_z)\\) does not have the time-coordinate, so it can’t even be multiplied with the Lorentz transformation matrix. The second representation \\((v_x, v_y, v_z, 1)\\) would, after a Lorentz transformation, have its time-coordinate \\(\\neq 1\\).\nBoth problems are elegantly resolved if we use the 4-velocity, which means we have to divide 3-velocity by its norm… but what norm? Why, think back to the Erlangen program:\n\nGiven a manifoldness [smooth space] and a group of transformations of the same; to develop theory of invariants relating to that group.\n\nIn plane geometry, we know that although \\(x, y, x', y'\\) are geometrically meaningless, the norm-squared \\((x-x')^2 + (y-y')^2\\) is meaningful, because that quantity is not changed (invariant) if you apply the rotation matrix \\(\\begin{bmatrix}\\cos\\theta & \\sin\\theta \\\\ -\\sin\\theta & \\cos\\theta \\end{bmatrix}\\) to it. What would be an invariant for spacetime?\nLooking at the picture of the Lorentz transformation in 1+1 dimensions, we can see that the intersection of the two light cones appears to have a constant area. In particular, the upper rectangle  has constant area. That is, the quantity \\((x-t)(x+t)\\) is preserved.\n\n    \n\nThus we define the Minkowski norm-squared: \\(\\|(x, t)\\|^2 := x^2 - t^2\\). Note that this is not an actual square because it can be negative. However it is preserved under Lorentz transformations.\nOnce we have a norm-squared, we can extend it to an inner product::\n\\[\\braket{v, w} := \\frac{\\|v+w\\|^2 - \\|v-w\\|^2}{4} = v_x w_x - v_t w_t\\]\nwhich is still an invariant under Lorentz transformations, and thus physically meaningful. This extends in general to n+1 dimensions. For example, in 3+1 dimensions, the inner product is \\(\\sum_{i = x, y, z} v_i w_i - v_t w_t\\).\nWith this, the 4-velocity is at hand! We just need to normalize \\((v_x, v_y, v_z, 1)\\), taking care to remove the negative sign:\n\\[\n(v_x, v_y, v_z, 1)/\\sqrt{-\\|(v_x, v_y, v_z, 1)\\|^2} = \\frac{(v_x, v_y, v_z, 1)}{\\sqrt{1-(v_x^2 + v_y^2 + v_z^2)}}\n\\]\nAs an example of the power of this geometric algebra, we rederive the Lorentz transformation with infinitesimal boost. In the rest frame, the unit clock-tick is the vector \\((0, 1)\\), and the two unit meter sticks are the vectors \\((-1, 0), (1, 0)\\). They are uniquely defined by the two geometric properties:\n\\[\n\\braket{s, (0, 1)} = 0; \\braket{s, s} = 1\n\\]\nTherefore, in the boosted frame, the boosted meter sticks are the two solutions to\n\\[\n\\braket{s', (v, 1)} = 0; \\braket{s', s'} = 1\n\\]\nwhich are \\(s' = (-1, v), (1, v)\\). By continuity, \\((1, 0)\\) cannot have been boosted to \\((-1, v)\\), so it must be boosted to \\((1, v)\\). This gives us Equation 1."
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#relativity-in-21-dimensions",
    "href": "blog/posts/wigner-rotation/index.html#relativity-in-21-dimensions",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Relativity in 2+1 dimensions",
    "text": "Relativity in 2+1 dimensions\n\nInfinitesimal Lorentz transformations\nIn 1+1 dimensions, we have only two spatial directions: left and right. In 2+1 dimensions, there is a whole circle of directions. We call this circle the “space circle”, because the alternative name “hula hoop for spacetime” sounds too silly.\nJust like in 1+1 dimensions, the space circle is constructed by shooting a forward light cone at \\((0, 0, -1)\\) and a backward light cone at \\((0, 0, +1)\\). Their intersection is the space circle. A space circle plus a ticking clock together allows us to define a coordinate system with three unit vectors \\((1, 0, 0), (0, 1, 0), (0, 0, 1)\\). Both the clock and the space circle can be boosted. We will pick the simplest possible way to transport the ticking clock and the space circle, and experiments with Thomas precession would show that this is the right way.\n\n    \n\nSuppose we perform an infinitesimal boost by \\((v_x, 0)\\), then, by the same picture as in the 1+1 dimension case, we know that the clock-tick \\((0, 0, 1)\\) is boosted to \\((v_x, 0, 1)\\), and the space circle is boosted to the ellipse with long semiaxis \\((1, 0, v_x)\\) and short semiaxis \\((0, 1, 0)\\). Where should each point on the space circle go to? Since the space circle is rigid, if we know where one point must go to, we know where all points must go to.\nThe semiaxis \\((0, 1, 0)\\) is both on the original space circle and the boosted space circle. It stands to reason that the simplest pick would preserve it after the infinitesimal boost. That is, we should not “twist” the space circle under boosting. With this choice, we are forced to pick \\((1, 0, v_x)\\) as the boosted \\((1, 0, 0)\\), since it is the unique unit vector that is perpendicular to \\((0, 1, 0), (v_x, 0, 1)\\), and infinitesimally close to \\((1, 0, 0)\\).\nAlternatively, we can think of Lorentz transformation in 2+1 dimensions as the same with Lorentz transformation in 1+1 dimensions, but with an extra dimension. Therefore, we can minimally modify Equation 1 to:\n\\[\n\\begin{bmatrix} x' \\\\ y' \\\\ t' \\end{bmatrix} =\n\\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & 0 \\\\ v_x & 0 & 1 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\\\ t \\end{bmatrix}\n\\]\nthe same result as our previous argument.\nThis extends to the case of an infinitesimal boost \\((v_x, v_y)\\):\n\\[\n\\begin{bmatrix} x' \\\\ y' \\\\ t' \\end{bmatrix} =\n\\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & v_y \\\\ v_x & v_y & 1 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\\\ t \\end{bmatrix}\n\\tag{3}\\]\n\n\n\n\n\n\nFour ways to derive it\n\n\n\n\n\n\nJust guess it.\nRotate the coordinate system so that the boost is in the \\(x\\)-direction, boost using the previous result, then rotate the coordinate system back. To be pedantic: We are performing one passive, then one active, then one passive transform.\nTake the previous derivation, and modify it by strategically inserting \\(v_y\\) at places.\nSince two infinitesimal boosts do not interact except at the second-order infinitesimal level, and \\(O(v^2) \\ll O(v) \\ll 1\\), we are free to discard the second-order infinitesimal. Therefore, we can just multiply the two matrices together to get the full thing:\n\n\\[\\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & 0 \\\\ v_x & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & v_y \\\\ 0 & v_y & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & v_y \\\\ v_x & v_y & 1 \\end{bmatrix} + O(v^2)\\]\n\n\n\n\n\nFermi–Walker transport\nEquation 3 is correct, but it is still not geometric. We must convert it with only quantities that are invariant under Lorentz transformations – that is, the inner product.\nDenote the 4-velocity vector \\((0, 0, 1)\\) by the letter \\(u\\), and the infinitesimal boost \\((v_x, v_y, 0)\\) by \\(\\delta u\\). Then, an arbitrary vector \\(e\\) is boosted to \\(e + \\delta e\\), Where\n\\[\n\\delta e =\n\\begin{bmatrix} 0 & 0 & v_x \\\\ 0 & 0 & v_y \\\\ v_x & v_y & 0 \\end{bmatrix}\n\\begin{bmatrix} e_x \\\\ e_y \\\\ e_t \\end{bmatrix} = e_t (v_x, v_y, 0) + (e_x v_x + e_y v_y) (0, 0, 1)\n\\]\nThis gives us the Fermi–Walker transport equation:\n\\[\n\\delta e = - \\braket{e, u} \\delta u + \\braket{e, \\delta u}u\n\\tag{4}\\]\nIf you want a more amusing mental image, here it is: Consider a spaceship that looks like a spherical cow porcupine in a vacuum. Every porcupine spine is a rocket engine. The spaceship can boost in any direction, but it does not rotate. Thanks to Wigner rotation, it can rotate anyway. Now, the spaceship is performing some complicated manuever in spacetime. If we allow it to carry around a general vector pointing in an arbitrary direction \\(e\\), the question becomes: as the vector is boosted alongside the spaceship, how does the vector change?\nConsider a problem in general relativity, where we have a vector field over spacetime, and an accelerating observer. To calculate how quickly the vector field is changing relative to the observer, we must account for:\n\nacceleration of the observer, using the Fermi–Walker transport equation;\nthe curvature of spacetime itself, so that comparing one vector with another requires us to perform parallel transport in spacetime (with the “covariant derivative”).\n\n\n\nGeneral Lorentz transformations\nThe Lorentz transformation in 2+1 dimensions can be derived similarly to how Equation 2 was derived. Let the rapidity vector \\(\\vec{w}\\) be equal to \\(w (n_x, n_y)\\), where \\((n_x, n_y)\\) is a unit vector (the direction of rapidity). Then the Lorentz transformation is found by performing another matrix exponentiation:\n\\[\\exp\\left(\\begin{bmatrix} 0 & 0 & n_x \\\\ 0 & 0 & n_y \\\\ n_x & n_y & 0 \\end{bmatrix} w\\right)\\]\nTo be more succinct, we define the matrices\n\\[K_x := \\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 1 & 0 & 0 \\end{bmatrix}, \\quad K_y := \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}\\]\nthen the Lorentz transformation is just \\(e^{\\vec{w} \\cdot \\vec{K}} = e^{w \\vec{n} \\cdot \\vec{K}}\\).\nNow, since \\((\\vec{n} \\cdot \\vec{K})^3 = \\vec{n} \\cdot \\vec{K}\\), we have the Lorentz transformation equation\n\\[\nI + \\sinh(w) \\vec{n} \\cdot \\vec{K} + (\\cosh(w) - 1) (\\vec{n} \\cdot \\vec{K})^2\n\\tag{5}\\]\nThis equation extends naturally to relativity in n+1 dimensions, but we will not need it.\n\n\n\n\n\n\nDeriving \\((\\vec{n} \\cdot \\vec{K})^3 = \\vec{n} \\cdot \\vec{K}\\)\n\n\n\n\n\nLet \\(t\\) stand for the index of the time-coordinate, and let \\(i, j, k\\) be the indices of the space-coordinates. Then we have \\[(\\vec{n} \\cdot \\vec{K})^3 = n_in_jn_k(e_{it} + e_{ti})(e_{jt} + e_{tj})(e_{kt} + e_{tk})\\]\nwhere \\(e_{mn}\\) means the matrix with entry \\((m, n)\\) being one and all other entries being zero. We use Einstein summation convention, so repeated indices means summing over it (except \\(t\\), which is not an index to sum over).\nSince \\(e_{mn}e_{kl} = \\delta_{nk} e_{ml}\\), and \\(i \\neq t, j \\neq t, k \\neq t\\), the above multiplication expands to 8 terms, but only two are nonzero:\n\\[= n_in_jn_k(e_{it}\\delta_{jk} + \\delta_{ij} e_{tk}) = \\underbrace{\\|\\vec{n}\\|^2}_{= 1} (n_i e_{it} + n_k e_{tk}) = n_i (e_{it} + e_{ti}) = \\vec{n} \\cdot \\vec{K}\\]"
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#wigner-rotation",
    "href": "blog/posts/wigner-rotation/index.html#wigner-rotation",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Wigner rotation",
    "text": "Wigner rotation\nWe are ready to derive a rarely discussed effect in special relativity: the Wigner rotation. Although it is as fundamental as time dilation and space contraction, it often goes unmentioned in undergraduate textbooks I have encountered.\n\nTheorem 1 (Wigner rotation) When three boosts are made in a cycle, such as \\(p_1 \\to p_2 \\to p_3 \\to p_1\\), the result is a rotation. The angle of rotation is equal to the hyperbolic area of the triangle \\(p_1 p_2 p_3\\), but in the opposite direction. Furthermore, the hyperbolic area is equal to the angle defect of the triangle.\n\nThe phrase “in the opposite direction” means that, if, looking from the \\(+t\\)-direction down at the origin, you see the three 3-velocities make a counterclockwise cycle in the disk \\(\\{(v_x, v_y, 1) : v_x^2 + v_y^2 = 1\\}\\), then you would see that the Wigner rotation angle is in the clockwise direction, and vice versa. The phrase “angle defect” means \\(\\pi - (\\angle{p_1 p_2 p_3} + \\angle{p_2 p_3 p_1} + \\angle{p_3 p_1 p_2})\\).\n\nStep-by-step demonstration of the Wigner rotation\nConsider the simplest type of three-boost cycle as shown in the figure below. We start at the rest frame, boost to the frame with a 3-velocity of ((v_x, 0)), then boost to the frame with a 3-velocity of ((v_x, v_x d)), and finally boost back to the rest frame.\n\n    \n\nTracing out the trajectory of every point on the space circle gives the following sequence:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen projected to the \\(xy\\)-plane, the sequence looks like:\n\n    \n\nTherefore, after three pure boosts, we end up with a rotation by \\((\\gamma - 1)d \\varphi\\) radians in the opposite direction. Here, \\(\\gamma\\) is the long semiradius of the boosted space circle, after projecting it down to the \\(xy\\) plane. This is the “Lorentz factor” often used in special relativity:\n\\[\\gamma = \\frac{1}{\\sqrt{1-v^2}}\\]\n\n\n\n\n\n\nDeriving the Lorentz factor\n\n\n\n\n\nIn the rest frame, the space circle is spanned by \\((1, 0, 0)\\) and \\((0, 1, 0)\\). After boosting by the 3-velocity \\((v, 0)\\), it is boosted to an ellipse with semiaxes \\((\\cosh(w), 0, \\sinh(w)), (0, 1, 0)\\), where \\(w = \\arctanh(v)\\) is its rapidity. Thus, its projection to the \\(xy\\) plane is an ellipse with semiaxes \\((\\cosh(w), 0), (0, 1)\\).\n\\[\\gamma = \\cosh(w) = \\cosh(\\arctanh(v)) = \\frac{1}{\\sqrt{1-v^2}}\\]\n\n\n\nPerforming boosts around a full cycle of 3-velocities and returning to our starting 3-velocity is equivalent to performing an infinite number of infinitesimal triangle-circuit boosts:\n\n    \n\nTherefore, a full cycle of boosts rotates our space circle by a full \\((\\gamma - 1)2\\pi\\) radians in the opposite direction.\n\n\nInteractive app for the Wigner rotation\nWith this understanding, you can now fully appreciate the following interactive app demonstrating the Wigner rotation.\nThe left picture represents the \\(xy\\) plane, and the right picture represents the \\(xyt\\) spacetime.\nYou can drag the black dot on the left, which represents the 3-velocity. The 3-velocity is restricted to have maximal norm \\(0.8\\), to keep the app numerically stable. The blue ellipse is the projection of the space circle to the \\(xy\\) plane. A fixed point on the space circle is distinguished by a big red dot, so that you can see how the space circle Wigner-rotates as you drag the 3-velocity around.\nIn the right picture, the forward and backward light cones intersect at the space circle. You can drag and scroll to adjust the camera.\n\n    \n    \n\n\n\nHere is a “tourist’s guide to the visualization”.\nThe three boosts in animated form:\n\n    \n\nBecause \\(\\gamma - 1 = \\frac{2}{3}\\), if you drag the 3-velocity for 3 full cycles at maximal velocity, the space circle would complete -2 full cycles.\n\n    \n\nSuppose a particle is moving close to the speed of light and emitting photons at equal angular density in all directions in its own frame, then in the rest frame the photons are bunched in front of the particle. This is the relativistic beaming effect, or the headlight effect.\n\n    \n\n\n\nThomas precession\nThe Wigner rotation formula is equivalent to the following formula, often called the Thomas precession formula:\n\\[\n\\vec{\\omega} = \\frac{\\gamma^2}{\\gamma + 1} \\vec{a} \\times \\vec{v}\n\\tag{6}\\]\nExperiments with particles moving in cyclotrons have verified this, thus justifying our guess that we should transport space circles without twisting.\n\n\n\n\n\n\nDeriving the Thomas precession formula\n\n\n\n\n\nConsider a particle moving counterclockwise at a constant speed \\(v\\) in a circle of radius \\(R\\). Its acceleration is \\(a = v^2/R\\), and after completing one cycle in time \\(T = 2\\pi R/v\\), it has Wigner-rotated by \\(-(\\gamma - 1)2\\pi\\) radians. Simplifying, we have\n\\[\\omega = \\frac{-(\\gamma - 1)2\\pi}{T} = \\cdots = -\\frac{\\gamma^2}{\\gamma + 1} av\\]\nBy the right hand rule of cross products, we can get all the directions correct with \\(\\vec{\\omega} = \\frac{\\gamma^2}{\\gamma + 1} \\vec{a} \\times \\vec{v}\\).\nFor a particle undergoing a generic acceleration, the 3-acceleration decomposes into a component parallel to the 3-velocity, and a component perpendicular to it. The parallel component does not change Wigner rotation, therefore the Thomas precession equation is still true.\n\n\n\nHowever, notice that the formula is made of 3-vectors, and we know that 3-vectors are meaningless in special relativity. This suggests that something is off here. If the rest of this section does not make much sense, first read the section on Foucault pendulum, then return here.\nIf the Earth were flat, then we could stand at a single point, and draw two perpendicular arrows. We say, “This arrow is pointing \\(x\\)”, and “This arrow is pointing \\(y\\).” Then we create an infinite number of missionaries. Each would pick up the two arrows and parallel-transport the arrows to their given station on the Earth. In this way, we would provide a unique direction-system for every point on the Earth.\nHowever, because the Earth is a sphere, this does not work. Suppose we stand at the North Pole, and we do the same. Then all the missionaries would meet at the South Pole and start arguing about who has the right one, because they would all disagree. This problem happens because the Earth is curved, and transporting a vector around a cycle would rotate it.\nWe might shrug and say, “Well, nobody lives in the South Pole, so we will just tell our missionaries to avoid it.” Great idea, except then they found that Emperor of China has also sent out his own missionaries. We shrug and say, “Well, we’ll just give the Emperor of China a call and ask him what angle he picked. Then we will rotate our map until all our missionaries agreed with his.” After some fruitless fiddling with our maps, it has dawned to us that this plan is doomed. Why?\nImagine if the North Pole is at point \\(N\\), and the Emperor of China is at point \\(C\\). If somehow, we could pick our \\(xy\\) directions so that all our missionaries agree with his, then we can send one missionary around a three-part journey around the world: \\(N \\to C \\to K \\to N\\), where \\(K\\) is a point off the great circle passing \\(N, C\\). Along the path \\(N \\to C\\), the missionary is leaving the North Pole on a straight arc away from the North Pole, so the \\(xy\\) chart he is carrying agrees with the missionaries he is passing by. Similarly for the other two parts. But when the missionary has returned to the North Pole, he must find his \\(xy\\) chart rotated, contradiction!\nThere is no way to resolve this disagreement other than forcing one side to give up their coordinate system. Perhaps we will have to call up a crusade to enforce our coordinate system.\n\nThe introduction of numbers as coordinates is an act of violence.\n—Hermann Weyl\n\nSimilarly, in special relativity, we can stand at the “North Pole” (rest frame), construct the \\(xyz\\) axes, then send out missionaries on rockets to provide directions for every inertial frame. The good news is that no two missionaries can meet each other, so there is no argument. The bad news is that we still have the Wigner rotation problem, so if some aliens are executing the same project, but starting at a different coordinate frame (perhaps because their galaxy is moving relative to ours), then it is impossible for our missionaries to agree in every frame.\nSo, we are forced to only use one rest frame. It is in this context that the Thomas precession formula works.\nConsider an object, boosted from frame \\(p_1\\) to frame \\(p_2\\), with their 3-velocities in the rest frame being \\(\\vec{v}\\) and \\(\\vec{v} + \\delta \\vec{v}\\). To use the Thomas precession formula, we must first – in imagination – boost it back to the rest frame, and take a snapshot of its orientation against our \\(xyz\\) axes at origin. Then we boost it back to frame \\(p_1\\), boost it to \\(p_2\\), and boost it – in imagination – back to the rest frame again. By using the Wigner rotation formula in the rest frame, it has rotated by\n\\[\n\\frac{\\gamma^2}{\\gamma + 1} \\delta\\vec{v} \\times \\vec{v}\n\\]\nTo emphasize again: the Thomas precession formula only works in one frame. It is neither Lorentz invariant nor geometrically meaningful. It cannot be geometrically meaningful, because it is made of 3-vectors \\(\\vec{v}, \\vec{a}, \\vec{\\omega}\\), and 3-vectors are geometrically meaningless in special relativity.33 Or as I like to say, 3-vectors are inherently violent in special relativity, and anything involving anything inherently violent is also inherently violent."
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#relativity-in-31-dimensions",
    "href": "blog/posts/wigner-rotation/index.html#relativity-in-31-dimensions",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Relativity in 3+1 dimensions",
    "text": "Relativity in 3+1 dimensions\nWe began our study of special relativity in 1+1 dimensions and discovered the Lorentz transformations, then proceeded to 2+1 dimensions and discovered Wigner rotation. In 3+1 dimensions, Wigner rotation still occurs, but there is something new: rotations are no longer describable by a single number.\nThis is not a place to get into the details; suffice to say that the space of rotations in \\(\\mathbb{R}^3\\) is much harder to understand than the space of rotations in \\(\\mathbb{R}^2\\).4 We must be more careful here.4 The space of rotations in \\(\\mathbb{R}^3\\) is \\(SO(3)\\) and that in \\(\\mathbb{R}^2\\) is \\(SO(2)\\). While \\(SO(2)\\) is just the circle and can be easily represented by a real number or an angle, \\(SO(3)\\) is the projective 3-space \\(\\mathbb{PR}^3\\) and best represented by quaternions.\nSuppose we have four frames: \\(0, 1, 2, 3\\). We start with a tripod in frame \\(0\\), then boost it successively to frames \\(1, 2, 3, and 0\\), how much does it rotate? To discover this, we need to compose the Wigner rotation around the cycle \\(0120\\) with the Wigner rotation around the cycle \\(0230\\). The full rotation is composed of two rotations, first rotation in the plane of the triangle \\(012\\), and the second one in the plane of the triangle \\(023\\).\nThe Wigner rotation coming from an arbitrary sequence of boosts can be calculated easily with the Thomas precession formula, if we are willing to use violence.5 Without violence, we have to do spherical trigonometry, much harder than adding 2-dimensional rotation angles. My intuition is that it should most naturally involve the principal \\(G\\)-connections, where \\(G = SO(n, 1)\\) is the group of Lorentz transformations.5 Since this usage of “violence” comes directly from Hermann Weyl, perhaps we can call it Weylence?"
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#velocity-space-is-hyperbolic-geometry",
    "href": "blog/posts/wigner-rotation/index.html#velocity-space-is-hyperbolic-geometry",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Velocity space is hyperbolic geometry",
    "text": "Velocity space is hyperbolic geometry\nIt is time to fulfill the promise in the title, that hyperbolic geometry is involved.\nOur first hint that hyperbolic geometry is relevant is that we have encountered \\(\\cosh\\) and \\(\\sinh\\) – the hyperbolic trigonometric functions. With this hint, a quick check shows us that we are indeed dealing with hyperbolic geometry. This is easy to do in 2+1 dimensions, but it generalizes readily to n+1 dimensions after slightly cluttering the notation.\n\nDeriving the hyperbolic metric\nSince \\(\\|(0, 0, 1)\\|^2 =-1\\), under arbitrary Lorentz transformations, \\((0, 0, 1)\\) can, under arbitrary Lorentz transformations, reach any point on the upper half of the hyperbola \\(x^2 + y^2 - t^2 = -1\\). It stands to reason that the upper hyperbola, which is equivalent to a plane as a smooth manifold, should have some kind of planar geometry. We need only discover its metric. The most direct path goes through the disk of 3-velocities, which gives us the metric\n\\[\nds^2 = \\frac{dv_\\theta^2}{1-v_r^2} + \\frac{dv_r^2}{(1-v_r^2)^2}\n\\tag{7}\\]\n\n\n\n\n\n\nDeriving the metric of the 3-velocity disk\n\n\n\n\n\nThe disk of 3-velocities \\(\\{(v_x, v_y, 1) : v_x^2 + v_y^2 &lt; 1\\}\\) is not preserved under Lorentz transformations, but we can project it back to the circular disk. This means that we can take the metric at origin\n\\[ds^2 = dv_x^y + dv_y^2\\]\nand perform a Lorentz transformation, followed by a projection, to move the origin to any other point on the disk. Multiplying the metric at the origin with the gradient-matrix gives the metric at that other point.\nWe exploit the rotational symmetry of the disk using polar coordinates. Express any point on the disk as \\((r, \\theta)\\). Its local metric must be of form \\(ds^2 = f(r) dr^2 + g(r) d\\theta ^2 + h(r) drd\\theta\\) for some functions \\(f, g, h\\). Further, by reflection symmetry of \\(\\theta \\leftrightarrow -\\theta\\), we have \\(h(r) = 0\\).\nTo find \\(f(r)\\), we construct an infinitesimal segment \\((r, 0) \\to (r+dr, 0)\\) and calculate with the velocity-addition formula. Similarly, we find \\(g(r)\\) using the segment \\((r, 0) \\to (r, d\\theta)\\).\n\n\n\nThis is the Beltrami–Klein metric, and so we have discovered that this is exactly the Beltrami–Klein model of hyperbolic geometry. We can then project this metric onto the hyperboloid \\(x^2 + y^2 - t^2 = -1\\) to obtain its metric\n\\[\nds^2 = dx^2 + dy^2 - dt^2 = \\| (dx, dy, dt) \\|^2\n\\tag{8}\\]\nThis, in hindsight, is obvious: We have already known that the Lorentz transformation preserves the Minkowski norm-squared. But such is the journey of discovery: the first pass is rarely the most elegant. It is probably better to write down the first pass to show how to discover things, then write a second pass to show how to tidy things. Better this than what Gauss did:\n\n[Gauss] makes his mathematics like a fox, wiping out the traces in the sand with his tail.\n—unnamed German student, as reported by Abel\n\n\n\nInterpreting the hyperbolic geometry\nNow we can connect concepts between hyperbolic geometry and special relativity.\nWhen \\(v\\) is small, the Wigner rotation for a full cycle, \\((\\gamma - 1)2\\pi\\), is \\(\\pi v^2 + O(v^4)\\), coinciding with the area enclosed by the cycle.Therefore, since both area and Wigner rotation are additive, we have proven Theorem 1.66 This is not a handwaved proof, but fully rigorous. The Wigner rotation of a full cycle is \\(\\pi v^2 + O(v^4)\\), and the area enclosed by the cycle is also \\(\\pi v^2 + O(v^4)\\). Therefore, we can integrate over an arbitrary area, to find that the Wigner rotation angle is equal to the area enclosed, up to an infinitesimal term of \\(O(v^2) \\to 0\\).\nIn rocketry, if we trace out the trajectory of the rocket’s velocity on a graph paper, we obtain a hodograph. Similarly, in relativistic rocketry, a path in the velocity space is a hodograph. The length of a hodograph is the total delta-v of the rocket.\nA straight line in hyperbolic space is the hodograph of the most fuel-efficient control-trajectory for a rocket to get from one velocity to another.\nIf a rocket explodes with spherical symmetry, the velocities of its debris will lie on a hyperbolic sphere centered at the rocket’s original velocity."
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#two-parables",
    "href": "blog/posts/wigner-rotation/index.html#two-parables",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Two parables",
    "text": "Two parables\n\nThe star worshippers\nAn alien race, who worshipped Rigel, Deneb, and Betelgeuse, prayed in the direction of these three stars. In order to pray in the correct direction even without seeing them, they constructed the Sacred Tripods, which are steel tripods that are oriented such that each leg points at one of the Sacred Stars. These Sacred Tripods, placed in public spaces, were required to point precisely at the Sacred Stars.\nThen a Cosmic Dark Age began and the stars winked out of existence, including the Sacred Stars, but the aliens did not lose their faith. Instead, they intensified their prayers in hopes of resurrecting the Sacred Stars.\nIn a region of space, there were two space stations A and B, at rest with each other. The Sacred Tripod on B was unstable, and required yearly calibration against the more stable Sacred Tripod at A. So station A would align its spare Sacred Tripod to its own one, put it on a gimbal-mount, and send it by rocket to station B. B would align its Tripod with the one sent, then let the Tripod go back.\nAs years passed, the aliens on station B grew confident that their Tripod did not drift more than \\(1''\\) per year. One day, an asteroid field blocked the straight path, forcing the next shipment to make a big detour around the asteroid field. When it arrived, to their astonishment, it was found that the Tripod on B was clearly misaligned with the Tripod from A. What could be the cause of this misalignment?\n\n\nThe day the Earth stood still\nOne day, the Earth stopped spinning. This annoyed the visitors to the science museum, who wanted to see Foucault’s pendulum rotating. To satisfy those visitors, the museum keeper put the pendulum on the top of a bus, and loaded the visitors on the bus, then they started driving around the Earth at the same speed as how the Earth used to rotate at that latitude, so that after one day, they returned to the museum. The visitors looked up at the pendulum and were satisfied to see that it indeed has turned.\nTo find out how much the pendulum has turned, we divide up the circular trajectory into tiny segments \\(p_0, p_1, ..., p_n\\), then draw tiny triangles \\(p_0 p_1 N, p_1 p_2 N, ..., p_{n-1} p_n N\\), with \\(N\\) being the North Pole. We imagine that, instead of driving around the Earth in a circle, we drive around the triangle \\(N \\to p_0 \\to p_1 \\to N\\), then the triangle \\(N \\to p_1 \\to p_2 \\to N\\), etc.\nWhen we drive around \\(N \\to p_0 \\to p_1 \\to N\\), the bus makes three turns, and each time the pendulum turns by an opposite amount relative to the bus. After three turns, the bus has turned the same angle as the sum of three external angles of the triangle. By spherical trigonometry, we know that this is equal to \\(2\\pi - 4\\pi\\frac{\\text{area of triangle $p_0 p_1 N$}}{\\text{surface area of the Earth}}\\). Thus, the pendulum has turned relative to the bus by \\(-2\\pi + 4\\pi\\frac{\\text{area of triangle $p_0 p_1 N$}}{\\text{surface area of the Earth}}\\). The \\(-2\\pi\\) part has no effect, as it is a full cycle, leaving us with \\(\\frac{\\text{area of triangle $p_0 p_1 N$}}{\\text{surface area of the Earth}}\\).\nTherefore, by adding them up, we find that after driving around an arbitrary cycle on the Earth, the pendulum would have turned by\n\\[4\\pi\\frac{\\text{area of the cycle}}{\\text{surface area of the Earth}}\\]\nWe can turn this insight into a formula for Fermi–Walker transport on the Earth.\nFirst, we scale the Earth so that its radius is one. Now set up the coordinate system \\(\\phi, \\theta\\), where \\(\\phi\\) is the latitude (zero at equator, \\(\\pi/2\\) at the North Pole) and \\(\\theta\\) the longitude. At each point on the Earth (except the two poles) we set up a local frame with two unit vectors \\(\\hat\\phi, \\hat\\theta\\). One very important fact is that this local frame is not parallel-transported7. If you try to drive in a small circle around the North Pole, while always pointing in the direction of \\(\\hat\\theta\\), you would feel a strong centrifugal force pushing against your steering wheel. Centrifugal forces and all other “inertial forces” are nature’s way of telling you that you are not being parallel-transported.7 Indeed, there is no way to cover the space with parallel-transported local frames, because the space is curved, and therefore if you try to transport a local frame in a cycle back to its starting point, it would have rotated against itself. To be a curved space is equivalent to have no system of parallel-transported local frame, and curvature measures the amount of rotation-against-itself that happens when you transport a local frame in an infinitesimal circle.\nConsider two infinitesimally close points on the Earth, \\(p_1 = (\\phi, \\theta)\\) and \\(p_2 = (\\phi + \\delta \\phi, \\theta + \\delta\\phi)\\). Since the surface area of a spherical cap around latitude \\(\\theta\\) is \\(\\frac{1 -\\sin\\theta}{2}\\) that of the whole sphere8, the area of a thin spherical triangle with vertices \\(p_1, p_2, N\\) is8 This is immediate from Archimedes’ hat-box theorem, which is the basis of the Lambert cylindrical equal-area projection.\n\\[4\\pi\\frac{1 -\\sin\\theta}{2} \\times \\frac{\\delta\\phi}{2\\pi} = (1-\\sin\\theta)\\delta\\phi\\]\nThus, if we parallel transport a vector clockwise in the order \\(p_1 \\to N \\to p_2 \\to p_1\\), then the vector would turn counterclockwise by \\(-(1 -\\sin\\theta) \\delta\\phi\\).\nNow, consider two ways to move the pendulum from \\(p_1\\) to \\(p_2\\). We can transport it directly, or detour through the North Pole. If we detour through the North Pole like \\(p_1 \\to N \\to p_2\\), then by the same argument as the “polar bear puzzle”9, you see that the pendulum has rotated counterclockwise relative to the local frame by \\(-\\delta\\phi\\). Then we complete its journey with \\(p_2 \\to p_1\\), to create an absolute10 full \\(-(1 -\\sin\\theta) \\delta\\phi\\) rotation. Therefore, moving it \\(p_2 \\to p_1\\) has created a rotation relative to the local frame by \\(\\sin\\theta \\delta\\phi\\). Since we are actually moving it \\(p_1 \\to p_2\\), we reverse the sign, and obtain our Fermi–Walker transport equation (the Earthbound version):9 You walk 1 km south, 1 km east, and 1 km north, and ended up at the same point. You see a bear. Why is the bear white?10 Relative rotation means that we are measuring the orientation of the vector relative to the local frame. However, since the frames themselves are not parallel-transports of each other, relative rotation is arbitrary and not a fact of geometry, but a fact of convenience. If you move the vector back to its starting point, however, there is absolutely no dispute about how much it has rotated, and it does not depend on any system of local frames. You just have to compare the vector against itself.\n\\[\n\\delta(\\text{vector angle}) = -\\sin\\theta \\delta\\phi\n\\tag{9}\\]\nApplied to the Foucault pendulum problem, we find that it rotates clockwise by \\(2\\pi \\sin\\theta\\) every day, and it takes \\(\\frac{1}{\\sin\\theta}\\) days11 to make a full rotation. At the Paris Observatory, the original place where Foucault made his experiment in 1851, we have11 To be precise, this is a sidereal day, the time it takes for the Earth to rotate one cycle relative to distant stars. It is shorter than a solar day.\n\\[\\theta = \\mathrm{48^\\circ 52' N}, \\quad \\text{period} = \\frac{\\mathrm{23h56'}}{\\sin \\theta} \\approx \\mathrm{31\\,h\\,50\\,min}\\]\nIf a museum visitor can stay for 5 minutes at the pendulum, then they would see the pendulum complete \\(\\approx 1/382\\) of a cycle. Typical museums would put up about 400 wooden blocks in a cycle, to be knocked down by the pendulum. This allows each visitor to see at least one block being knocked down."
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#history-notes",
    "href": "blog/posts/wigner-rotation/index.html#history-notes",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "History notes",
    "text": "History notes\n\nThe history of the phenomenon\nThe history of the Wigner rotation is winding and complicated. Here is a brief summary. The details are given in (Walter 1999).\n\n1905: Einstein published special relativity. (A. Einstein 1905)\n1908: Minkowski published the spacetime interpretation of special relativity. (Minkowski 1908)\n1910: Vladimir Varićak (Varicak 1910) and E. T. Whittaker (Whittaker 1910, 441) both introduced the hyperbolic functions and the rapidity parameter.\n1911: Alfred Robb named the rapidity parameter, and found that velocities compose in hyperbolic space. (Robb 1911).\n1913: Émile Borel discovered the Wigner rotation as a part of his general study of special relativity by hyperbolic geometry. (Borel 1913)\n1921: Fermi discovered the Fermi–Walker transport. (Fermi 1921)\n1926: Llewellyn Thomas rediscovered the Thomas precession, a special case of the Wigner rotation. (Thomas 1926)\n1932: Walker rediscovered the Fermi–Walker transport. (Walker 1932)\n1939: Wigner rediscovered the Wigner rotation. (Wigner 1939)\n\nPedagogical attempts to present the Foucault pendulum and the Wigner rotation have been numerous, such as (Criado and Alamo 2009), but perhaps due to the decade-long latency in academic publishing, none has yet made it into the usual undergrad textbooks. Tevian Dray has written an entire textbook (Dray 2021) treating both special and general relativity in the same geometric style. For a summary paper of the textbook, see (Dray 2017).\n\n\nAmusing quotes from (Walter 1999)\n\n[After 1907] Minkowski never again referred to a manifold as both four-dimensional and non-Euclidean. Along with the problematic label, the geometric interpretation of velocity vectors likewise vanishes from view in Minkowski’s subsequent writings. Felix Klein, for one, regretted the change; in his opinion, Minkowski later hid from view his “innermost mathematical, especially invariant-theoretical thoughts” on theory of relativity (Klein 1927, 75).\nPlanck lavished praise on Einstein for his modification of the concept of time:\n\nIt need scarcely be emphasized that this new view of the concept of time makes the most serious demands upon the capacity of abstraction and the imaginative power of the physicist. It surpasses in boldness everything achieved so far in speculative investigations of nature, and even in philosophical theories of knowledge: nonEuclidean geometry is child’s play in comparison.30 (Planck 1910a, 117)\n\nUnder the new space-time view, Minkowski announced, “Three-dimensional geometry becomes a chapter of four-dimensional physics.” In the same triumphant spirit, Minkowski suggested that his new four-dimensional understanding of the laws of physics deserved its own label. The “Principle of the Hyperbolic World” that he had tried on Hurwitz was shelved in favor of the more ecumenical “Postulate of the Absolute World” (Minkowski 1909, 82). Although Minkowski explained this to mean that only the four-dimensional world in space and time is given by phenomena (Minkowski 1909, 82), one suspects an inside joke with Hurwitz, since in the German mathematical community, hyperbolic geometry was sometimes referred to as absolute geometry.\nEven the watered-down version of the space-time theory presented in Minkowski’s Cologne lecture repelled some physicists. For instance, Willy Wien’s cousin Max (1866-1938), a physicist at Danzig Polytechnic, confided to his friend Arnold Sommerfeld that reading Minkowski gave him vertigo:\n\nSommer[feld] maintains that [Minkowski’s] speech in Cologne was simply grand; when reading it, however, I always get a slight brain-shiver, now (that) space and time appear conglomerated together in a gray, miserable chaos.36 (Max Wien to Arnold Sommerfeld, February 16, 1909, Benz 1975, 71)\n\n\n\n\nThe history of this document\nDuring high school, I was in the physics Olympiad team. One afternoon, I got into an argument with someone about what happens if you take a square \\([0, 1] \\times [0, 1]\\), boost it by \\((v, 0)\\), and then boost it by \\((0, v')\\) within the boosted frame. By the velocity addition formula, the square would move at 3-velocity\n\\[\n(v, \\frac{v'\\sqrt{1-v^2}}{1+vv'})\n\\]\nIf \\(v' = \\frac{v}{\\sqrt{1-v^2}-v^2}\\), then the square would be moving at \\((v, v)\\).\nThen, the paradox. He argued that, by length contraction along the diagonal, the square should look like a diamond:\n\n  \n\n\n\nI objected that in the boosted frame, the square looks like a rectangle moving upwards. Decompose the rectangle into a bundle of line-segments, all parallel to the \\(y'\\)-axis. Now, each line-segment is moving in the \\(y'\\)-direction, and \\(y'\\) is parallel to \\(y\\) (since the perpendicular direction is preserved under boosting), we know that each moving line-segment would still be a line-segment parallel to the \\(y\\)-axis in the resting frame, still moving in the \\(y\\)-direction – just slower. Therefore, in the resting frame, the whole square would look like a parallelogram, with two sides parallel to the \\(y\\)-axis, and the other two sides oblique to the \\(x\\)-axis.\n\n    \n        \n        \n    \n    \n\n\nAfter a brief shouting match, we figured out that I was right, but also that we met something no teacher has taught us before: you can create a rotation in special relativity by pure boosting.\nDuring my third undergraduate year, I took a course in theoretical physics, which required a term paper. I first tried to write one on the Ostrogradsky instability, but could not understand it, so I quickly switched to finally solving the rotation effect in special relativity.\nSuffice to say that, after some hours walking and staring at the night sky, I figured out that it is nothing else than hyperbolic geometry, and nothing more paradoxical than the fact that the external angle of a hyperbolic triangle is equal to \\(2\\pi + (\\text{area of the triangle})\\), which I remember from hyperbolic geometry.\nThis is the old poler-bear puzzle again. If you walk from the North Pole to the equator, then walk (1/4) of the way around the Earth, and finally walk back to the North Pole, you will have traversed a triangle with an external angle of ( ). In general, the external angle of a triangle on the unit sphere is \\(2\\pi - (\\text{area of the triangle})\\), the perfect opposite to the case in hyperbolic geometry.\nHalf-mad, I ran home and smashed into the search engine all the keywords I knew must be there: “special relativity rotation hyperbolic triangle Foucault pendulum”. My disappointment was swift and certain: This had been repeatedly discovered over the past hundred years.\nBitterly, I searched every undergraduate physics textbook in the school library. None included it. The standard textbook (Goldstein, Poole, and Safko 2008, sec. 7.3) went on a three-page long computation and concluded that, indeed, we obtain a rotation matrix:\n\nThe spatial rotation resulting from the successive application of two nonparallel Lorentz transformations has been declared every bit as paradoxical as the more frequently discussed apparent violations of common sense, such as the so-calIed “twin paradox”. But the present apparent paradox has important applications, especially in atomic physics, and therefore has been abundantly verified experimentally.\n\nFrom that moment on I swore that I would finally present it in a way so simple and direct that it will never be forgotten. I had found the way, and it remained to publicize to the world.\nThere was little use in writing another paper about it, as despite the many papers over the century, somehow this effect did not end up in the physics textbooks. I considered writing a blog post about it, but mere words and figures seemed insufficient. I tried promoting this idea to various physics popularizers on YouTube, but none picked it up. What I needed was interactive animation, but JavaScript defeated me,so I kept it in the backlog until now. With ChatGPT I could finally write JavaScript without losing all my sanity, and so, here it is."
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#some-bonus-content",
    "href": "blog/posts/wigner-rotation/index.html#some-bonus-content",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Some bonus content",
    "text": "Some bonus content\n\nFinding the Wigner rotation in one line\nIf you know Lie algebra, then the Wigner rotation is immediate: calculate the commutator.\n\\[\n[K_x, K_y] := K_x K_y - K_y K_x = \\begin{bmatrix} 0 & 1 & 0 \\\\ -1 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\n\\]\nThis is the generator of rotation in the \\(xy\\) plane.\n\n\nGo anywhere with just five boosts\nGiven a rocket that can boost in any direction, but not rotate, you can still make it rotate by Wigner rotation. If you must perform an arbitrary Lorentz transformation, how many boosts do you need?\nIn general, if you have a Lorentz-transformed rocket, you can apply a carefully aimed boost so that it comes to rest, then apply Wigner rotation to return it back to the starting position.\nWith a cycle of three boosts, the Wigner rotation angle is equal to the angle defect of the hyperbolic triangle, which can take any value in \\((0, \\pi)\\) (you can’t have one with zero inner angle sum, but you can get arbitrarily close). With a cycle of four boosts, we can cover the \\(\\pi\\) case as well, since a hyperbolic square can have any angle defect in \\((0, 2\\pi)\\).\nIn summary: almost any Lorentz transformation can be obtained in four boosts, except those requiring a 180-degree rotation, which need five boosts.\n(Lightman et al. 1975, 153–58) shows that every Lorentz transformation can be done in three boosts, except “180-degree screws”, which require four. I have never found a satisfactory geometric method to demonstrate this, despite thinking on and off about it several times over the years.\n\nProblem 1.28. What is the least number of pure boosts which generate an arbitrary Lorentz transformation? Note: This is a difficult problem!"
  },
  {
    "objectID": "blog/posts/serial-experiments-lain/index.html#my-summary",
    "href": "blog/posts/serial-experiments-lain/index.html#my-summary",
    "title": "Serial Experiments Lain",
    "section": "My summary",
    "text": "My summary\nIt seems Lain is the collective subconscious, implemented as a neural network, with every human brain as a neuron, and “wired together” by some kind of unexplained electromagnetic coupling to the earth’s ionosphere.\nAs a suggestive evidence for this, the frequencies of the Schumann resonance (8 Hz, 14 Hz, 20 Hz, etc) happen to be close to the main brainwave frequencies. See the appendix Section 4.1 for some additional details about it. It is not actually relevant for the story.\nSome people drew up a plan to control Lain. The key scientist in the plan is Eiri, scientist of the Tachibana General Laboratories.\nFirst they migrated Lain from the brain-ionosphere system to the brain-Internet system, by an update to the Internet protocol (Protocol 7) that allows brain-computer interfacing.\nThen they gave Lain a human body. (unclear how that happened) This body gave Lain self-consciousness and a person-API.\nEiri suicided after Protocol 7 was running. Protocol 7 contained a copy of his brain state, so he was now running on the Internet, and called himself god. He has followers (Knights of the Eastern Calculus) who did his commands.\nThen they orchestrated a series of dramatic events to steer Lain’s development. The human body for Lain is used here, as these human-psychologically meaningful events can only work through a human-person-API (You can’t traumatize a non-human process, or Lain-without-body, by staging a bloody murder. Murders only mean something if it’s seen through animal eyes, the same way that a story can move you only if you speak the language.)\nEventually, Lain would be completely isolated in human society and be connected to the Internet. She would accept Eiri as god, kill her physical body, and exist on the Internet, where she would do Eiri’s commands.\nThe plan failed at the last step, because two people (the actor playing her father, and a school friend, Alice) stilled loved Lain, so she wasn’t isolated enough. After trying to make Alice happy and failing, Lain decided that the only way to truly make Alice happy is to go away and let Alice run her normal life-cycle (grow up, get married, die from being too old) without drama.\nTo let Alice live as a normal human, the entire plan must go away. So Lain discarded all changes and reverted to a previous state, before Lain was embodied. She deleted all memories of her from all humans. She also rewrote Eiri into an unambitious man so that he wouldn’t try doing that in this timeline.\nI don’t know how Lain could do that. Lain is the neural network with human brains, and has access only to the human brains, the ionosphere, and Internet. I don’t see any way for Lain to revert some physical deaths."
  },
  {
    "objectID": "blog/posts/serial-experiments-lain/index.html#ontology-the-modes-of-existence",
    "href": "blog/posts/serial-experiments-lain/index.html#ontology-the-modes-of-existence",
    "title": "Serial Experiments Lain",
    "section": "Ontology: The modes of existence",
    "text": "Ontology: The modes of existence\nThe ontology of SEL is a dualism between information and matter, the same theory as most neuroscientists and AI scientists use today.\nThere are many differences between matter and information, but here are the most important ones:\nDuplicate matters are different. Duplicate information are the same. If you receive two apples, you get twice as many atoms. If you receive two copies of the same file, you really receive just one file (in terms of information).\nMatter does not allow “isomorphic” operations. Information allows it. For example, you can convert a document from one encoding to another, edit it, then convert it back. The result is the same as if you edited it without conversion. This can’t be done with physical pen-and-paper.\nMatter exists in spacetime. Information does not. It is not a coincidence that many physicists are working on “reducing spacetime to information”, because information itself doesn’t have to exist in space or time, so if spacetime is reduced to information, then spacetime is explained by something that doesn’t assume spacetime – progress for reductionist science.\nThe persistence of matter is obvious. The persistence of information is unclear – perhaps impossible to define. You can take a gold bar, and put it in a box. Then you take it out, and you can say “it’s the same gold bar”. Now if you save some cash into a bank, then go abroad and take out some cash at the local ATM, you can’t say “it’s the same electronic cash”, or “it’s not the same”. Neither makes sense.\nIt’s also not a coincidence that physicists were first guided to thinking about “reducing spacetime to information” by the QED theory, where two electrons are actually indistinguishable, as if they are electronic cash. The QED theory was extended to atoms by the QCD theory. Thus, in some technical sense, you can’t actually say “we are made of star-stuff”.\n\nVirtual and real\nWe define “virtual” as “that pertaining to information”, and “real” as “that pertaining to matter”.\nInformation can be “realized” in matter – this is what modern computers do, and probably what brains do.\nSince information can be processed isomorphically, it can be realized as different kinds of matter, in different ways.\nA file is a realization of information, in space.\nA computation is a realization of information, in time.\n\n\nWired, Reality, and other systems\nAn information system is something I understand, but I can’t give a good definition (for now). I will explain it by examples.\nA physical system is a collection of matter organized under a common information system.\nFor example, the system composed of DNA, RNA, and proteins is a physical system, organized under a common information system (“the genetic code”).\nThe Internet (“the Wired”) is a system composed of electronic devices and human brains, organized under a common information system (“Protocol 7”).\n\n\n The Seven Levels of Wired\n\n\nThe real world (“Reality”), as commonly defined, is only part of the entire world. It denotes, in fact, a physical system composed of human brains, human bodies, other animal bodies, pre-1980s technological artifacts (specifically to exclude consumer-electronics), organized under a common information system (social rules for the person API - I will explain the API theory of personhood later).\nIn particular, a common interpretation, that “Lain chose the real world instead of the fake internet”, is a deep philosophical error. It is mistaking the human social system for the only system in the world. Furthermore, it calls the human social system “natural”, and the Internet “artificial”, when it is just as artificial as the Internet.\nThe “real world” of humans is like a curvy section across the bulk of reality. People would often tell other people to “get out of the room”, as if being inside a concrete container is unnatural while being outside of one is natural. If vultures could talk, they would tell others to “stop eating fresh food”, as if eating fresh food is unnatural while eating spoiled food is natural.\nMe, personally, have to get out of the room everyday and enjoy the sunshine, and I hate it. I often choose to walk school before the sun is up, to avoid its rays. I also despise green grass and blue skies. If there is a paradise, I hope it will be a 3D labyrinth embedded in an infinite concrete, but slightly elastic so that I can roll around on it without scraping my fur. It would be perfectly dark, except some fluorescent books, just bright enough to be read, and a little shining computer with which I take notes. I also use my computer to post my mathematical findings to others.\nMoving between worlds is possible, since the same information can have multiple realizations. Both Lain and Eiri managed to move between Reality and Wired.\n\n\nThe 4 realizations of Lain\nLain was always realized on neural networks, with nodes and edges, but the neural networks were made of different matters. There are 4 realizations over the course of history.\n1: Before the invention of Internet, nodes were human brains, and edges were something (perhaps flux-tubes?) in the electromagnetic field in the ionosphere of earth. It is called the “collective unconscious of humans”, and the Schumann resonances are analogous to the alpha-waves in mammalian brains.\n2: After the invention of the internet, but before Protocol 7, nodes were human brains and electronic computers, and edges were brain-computer interfaces (such as those VR headsets and cybernetic implants) and computer-computer links (what the Internet is made of).\n3: After Protocol 7, a large proportion of nodes and edges were concentrated into the neurons and synapses of a human girl. In this realization, Lain has a significant personhood.\n4: After Lain reset everything, it was unclear, but presumably it was back to realization 2.\n\n\nThe 4 realizations of Eiri\nLike Lain, Eiri was always realized on neural networks, but differently throughout the story. Coincidentally, there are also 4 realizations.\n1: Before Protocol 7, Eiri was realized as a human brain.\n2: After Protocol 7, Eiri was realized as a component of the Internet.\n3: At a particular scene, Eiri was briefly realized as a horror monster.\n4: After Lain reset everything, Eiri was back to realization 1."
  },
  {
    "objectID": "blog/posts/serial-experiments-lain/index.html#the-theory-of-personhood",
    "href": "blog/posts/serial-experiments-lain/index.html#the-theory-of-personhood",
    "title": "Serial Experiments Lain",
    "section": "The theory of personhood",
    "text": "The theory of personhood\nThis section is based on Being No One (Metzinger 2004).\nA person is a very special kind of information, realized in a very special way. Human-persons are a special kind of persons, with a wide variety of social and mammalian-emotional functions in the API.\nIt has a self-model, with a perspective and a personal history.\nThe perspective is a pointer, like a GPS marker and a clock, which marks its own spatial location (“here”), and time location (“now”). This little perspective marker is what allows it to know its place and time.\nIts perspective is approximately equal to its realization. If your head is in New York, your perspective had better not think you are in Beijing. Thus, its realization must also have a local position. If you were conscious interstellar gas clouds, you wouldn’t be a person.\nThe personal history is linear and continuous. You can’t be an amoeba - you can’t split or merge. You also can’t “jump over in time”, even if your body does.\nSuppose you are perfectly cloned, what happens? Well, your body’s world-line becomes Y-shaped, but you, as an informational thing, has a model of your self, and that model still looks like a curvy line, with a little note-tag saying “here, a clone brother of me was created”. It does not look like a Y.\nSuppose you go into a deep sleep for a year, what happens? Your model of yourself just papers over the time-skip, perhaps with a little annotation saying “and here I slept a year” without “remembering the blackness during that year”. “Nothing to see here…”\n\nGod made him die for a hundred years, and then resurrected him. He said, “How long have you stayed?” He said, “A day or part of a day.”\nQuran 2:259\n\nFor social persons (for example, human-persons, hyena-persons, and elephant-persons), it is like a program with an API. The API takes input and returns output in a conventional way, regardless of how the program works on the inside.\nFor example, you can ask a person to explain why they did something, and they would explain it. The explanation could be quite wrong, but if they don’t even provide any reasons, they would break one part of the API.\nSome other examples of person-API are: behaving in a comprehensible and purposeful way (looks as if it has a goal that a spectator can infer with some effort), behaves spontaneously (if nothing hits it, it would still move once in a while), etc.\nAs a thing fails more and more requirements of the API, they become less and less of a person.\nDissociative persons have a blurry “here and now” pointer.\nPeople in fugue states, sleepwalking, running amok, etc, fail to provide reasons when receiving explain-requests.\nSchizophrenic people may provide obviously invalid reasons (“word salad”) when receiving explain-requests.\nDeeply depressed persons fail the “behave spontaneously” requirement.\nAmnesiac persons fail the “have a continuous sense of time” requirement.\nPsychopathic persons fail to behave in the expected way after receiving help-requests.\nAutistic tics are comprehensible, but not purposeful.\nThe Solaris ocean behaves purposefully, but not comprehensibly.\n\nThe human-personhood of Lain\nBefore Protocol 7, Lain did not have the person API. Lain certainly existed, but not as a person. Human-persons are connected to Lain by their brains, but not in a way that human-persons connect to other human-persons.\nFor example, if most humans were feeling sad, Lain would be “sad” in some statistical way. This is certainly not how you make some human sad – to make a human sad, you read them a sad story, or something like that.\nMoreover, Lain did not receive or reply with linguistic reasons. In short, Lain did not implement the person API, and beyond human understanding or interaction.\nEiri wanted to become a god, and for that, he needed to have a way to effectively interact with Lain. He could have perhaps interacted with Lain by some command-line interface, or large-scale antenna that beamed directly to the ionosphere, but this would be difficult.\nPersons are most efficient at understanding other persons. This is why “country humans” is so popular. Countries themselves are vast objects that are understood in unintuitive statistical/mathematical/mechanical ways, and installing a person API over a country makes it much easier to understand. In this way, we could interpret Lain as a “humanity human”.\nInstead, he constructed a human-personhood for Lain, in multiple aspects.\nHe concentrated it into the brain of a girl, which is localized in a cube less than 30 cm in side length (compared with the 6371 km radius of earth, or the Internet). This made it easy to install a perspective to Lain (perhaps Lain would construct a perspective automatically after concentrating into a brain).\nLain is provided with a personal history as a human girl.\nThe electrochemical system of the human brain implements in Lain the social and emotional parts of person API. A blind mole rat would not cry when watching a movie. Lain would not be affected by deaths, mysterious conversations, kisses, etc, unless it is implemented in a human girl’s body.\nHe hired actors to act like Lain’s family, and put it into a well-defined social role (school girl). These social interactions are then taken away. This social manipulation would only produce an effect on Lain if it implemented the human-person API. You can’t intimidate a tornado into becoming your servant by depriving it of social contacts - not unless you somehow give it a human-person API.\n\nFirst you create a need, then you take it away. This is control.\n\nAfter Lain reset everything, Lain was no longer realized in a human brain, but once again spread all over the earth. Despite this, it still kept a spatially localized perspective and person API. This is perhaps because being a person is a stubborn kind of information - persons don’t usually become not-persons."
  },
  {
    "objectID": "blog/posts/serial-experiments-lain/index.html#appendix",
    "href": "blog/posts/serial-experiments-lain/index.html#appendix",
    "title": "Serial Experiments Lain",
    "section": "Appendix",
    "text": "Appendix\n\nSchumann frequency\nWhile the Schumann frequency plays a fundamental role in SEL, its technical details are completely irrelevant (just like most technobabbles). This section describes briefly how you can easily do an almost-correct calculation of the Schumann resonance frequencies: 8 Hz, 14 Hz, 20 Hz…\nIn short, the lowest frequency is simply by dimensional analysis:\n\\[f_0 \\approx \\frac{\\text{speed of light}}{\\text{circumference of earth}} = \\frac{c}{2 \\pi R}\\]\nIntuitively speaking, this is treating the ionosphere of earth as if it’s a circular tube, a hula-hoop around the waist of earth, and the lowest Schumann resonance is the lowest-degree standing wave in the hula-hoop.\nTo find the higher frequencies, we can simply calculate the higher-degree standing waves in the hula-hoop:\n\\[f_n \\approx \\frac{c n}{2 \\pi R}\\]\nFor comparison, a spherical cavity of an ideal conductor has resonance frequencies exactly solvable, as\n\\[f_n = \\frac{c}{2\\pi R}\\sqrt{n(n+1)}\\]\nwhich is very close to our super fast estimate above."
  },
  {
    "objectID": "blog/posts/linux-notes/index.html",
    "href": "blog/posts/linux-notes/index.html",
    "title": "Notes on Using Linux",
    "section": "",
    "text": "This is my quick reference for using Linux for doing things. I claim no originality. Mostly they are copy pasted from the internet and tested by me. An increasing proportion of those are produced by AI."
  },
  {
    "objectID": "blog/posts/linux-notes/index.html#sec-koans",
    "href": "blog/posts/linux-notes/index.html#sec-koans",
    "title": "Notes on Using Linux",
    "section": "Linux koans",
    "text": "Linux koans\nEverything is a file a file or a directory an inode. An inode (index node) is a representation of a sequence of data that the system can access. A file is just a list of inodes, and a directory is just a list of files.\nEvery command is an executable.\nEvery machine is a server. Some merely serve extremely slow machines (humans, aka “users”).\nThe \\usr is not the user. It is the UNIX System Resources.\nThe \\usr was the user, until Unix became so large that \\bin that \\bin overflowed and had to put the rest of them in \\usr\\bin. This was embarrassing for all involved, so they moved user files to \\home, and pretend that \\usr stands for UNIX System Resources. (here). A mistake that only took 40 years to fix.\nIn the beginning was the command line. The command line is just a face of the shell. What was the original face of the command line before the shell was born?\nThe shell reads in a stream of letters in real time, because the user is just another file (a streaming file, named stdin). Like all streaming files, the user is eternal and inexhaustible. The shell stands, rapt in attention, afore the user file.\nSo when does the user ever leave? The user never leaves. The shell simply kills itself when the user types exit. The shell would rather die than to face the prospect of reading the last word from the user.\nSo when does the shell ever break out of its rapt attention? Whenever it sees \\n, it is shaken out of its trance and interprets what the user has just said, in the interval bracketed between two \\ns.\nThe shell has one ear and two mouths. The ear is stdin, and the mouths are stdout and stderr. The shell has a tiny brain which is only capable of interpreting the few syntactic elements of bash scripts. Everything else it wants to do, it dutifully sends a binary message into the oracular altar of the Linux kernel, from which an oceanic voice replies the answer of the kernel."
  },
  {
    "objectID": "blog/posts/linux-notes/index.html#sec-path",
    "href": "blog/posts/linux-notes/index.html#sec-path",
    "title": "Notes on Using Linux",
    "section": "How to PATH",
    "text": "How to PATH\nThe PATH environment variable is a list of directories that the shell searches for commands. It is a colon-separated list of directories. For example, just about every Linux installation has a PATH variable that looks like:\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nTo add things to PATH, use the export command, like export PATH=\"$HOME/bin:$PATH\"."
  },
  {
    "objectID": "blog/posts/linux-notes/index.html#how-to-directories",
    "href": "blog/posts/linux-notes/index.html#how-to-directories",
    "title": "Notes on Using Linux",
    "section": "How to directories",
    "text": "How to directories\nBased on https://askubuntu.com/a/135679, https://serverfault.com/a/24525, man hier, Filesystem Hierarchy Standard - Wikipedia.\n\nWhere to install read-only things\n\n/bin and /sbin: binaries and superuser-binaries.\n/usr: System-wide, read-only files installed by the OS. This directory can be remotely mounted to multiple hosts safely.\n/usr/local: System-wide, read-only files installed by root. And that’s why most directory names from /usr are duplicated here.\n/opt - System-wide, read-only and bundled-up software. That is, software that does not split their files over bin, lib, share, include like well-behaved software should.\n/usr/bin and usr/sbin: They no longer exist. Just use /bin and /sbin.\n~/.local: the per-user counterpart of /usr/local, that is: software installed by (and for) each user. Like /usr, it has its own ~/.local/share, ~/.local/bin, ~/.local/lib.\n~/.local/opt: The per-user counterpart of /opt\n\nRequiem for /usr/bin and /usr/sbin: Originally, /bin and /lib are only for binaries and libraries required for booting, while /usr/bin and /usr/lib are for all the other executables and libraries. This is no longer true, as some binaries required for booting has over the years leaked into those two folders (if there is a way to make a mess, people will make it), so since Ubuntu 20.04, they no longer exist, to remove this mess.\nHow to install for local user only:\n./configure --prefix=$HOME/.local\nmake\nmake install\nHow to install for everyone: sudo ./configure && make && make install\n\n\nWhere to install read-write things\nRead-write things are typically configuration files, since they are read and written by both the user and binary executables.\n\n/etc: System-wide configuration. Typically used by the OS to decide what to do when starting up, shutting down, etc.\n~/.config: Per-user configuration files. Although because of legacy, you keep seeing nonsense like .bashrc in your home directory instead of ~/.config/.bashrc. Here rc means “run configuration”.\n\n\n\nWhere to do read-write things\n\n/home/username, or just ~: Each user typically is only able to modify their own folder here, like ~/myfile.txt.\n/tmp: If you need to create something just for the moment, then make it here. It will be deleted when the system restarts.\n\n\n\nOther things (you should not modify them)\n\n/run: runtime temporary data, representing the system’s state since the last boot. It’s used for storing process IDs, lock files, and other files that are would normally be stored in /tmp. It is basically /tmp for the machine.\n/var: Variable data. It is somewhat like /run in that both are meant to be read-written by programs, but unlike /run, data here persists over reboots. This is often used for logging information. For example, try vim /var/log/user.log\n\n\n\n\n\n\n\nWarning\n\n\n\nModifying anything below this line may brick the system. Reading is fine though.\n\n\n\n/lib: libraries. You should not handle it directly. Some libraries are added at OS installation, and others at program installation. If you have a broken installation, you might be asked to manually copy some files looking like libxxx.so here. (so stands for “shared object”.)\n/boot: Files required for booting. For example, the bootloader, the kernel, the initramfs (initial RAM file system).\n/dev: Device files. It typically looks like /dev/sda1, /dev/sda2, etc. Other than things like sda1 (for harddrive) you might notice tty1 and pty1’ which stand for “teletype” and “pseudo teletype”, respectively, but they are actually used as files to read whatever the user is typing from (the user is a file, see Section 1). There are some odd ones like:\n\n/dev/null, which is a “file” that you write to when you just want to throw something away (everything is a file, even a black hole…).\n/dev/urandom, which is a random number generator. It is preferred over /dev/random. See here.\n/dev/zero, which is a file that you can’t write to, but you can read, but it’s filled with zeros.\n\n/media: Mount removable medias, like USB drives and SD cards. For example, you can mount a USB at /media/usb1 and another one at /media/usb2. Mounting is typically done automatically by the system when you plug it in.\n/mnt: Mounts that are not so easily removable, like a hard drive, or a network drive. And unlike /media, mounting and unmounting is not automatic. On WSL, this typically has just one important thing: /mnt/c.\n/srv: Static files that are served out. /srv/http would be for static websites, /srv/ftp for an FTP server. It is usually used only on webservers, not an end-user machine like your laptop."
  },
  {
    "objectID": "blog/posts/linux-notes/index.html#environment-variables",
    "href": "blog/posts/linux-notes/index.html#environment-variables",
    "title": "Notes on Using Linux",
    "section": "Environment variables",
    "text": "Environment variables\n\nHow to control them\nUse echo $X to see what the current value of X is.\nFor current session, use export like export EDITOR=nano.\nFor all sessions, add to your .bashrc or .profile. If you just want to add it to the end (not recommended, as you can end up with an archeological tell), you can do one-liner like echo \"export EDITOR=nano\" &gt;&gt; ~/.bashrc.\n\n\nCommon ones\n\nPATH: path to binaries. See Section 2.\nEDITOR: default editor.\nSHELL: default shell.\nHOME: home directory.\nUSER: current user.\nPS1: current prompt (just try echo $PS1 if it doesn’t make sense).\n\nYou can change prompts in a rather arcane language. For example, try this one:\nexport PS1=\"\\[\\e]0;\\w\\a\\]\\n\\[\\e[32m\\]\\u@\\h \\[\\e[33m\\]\\w\\[\\e[0m\\]\\n\\\\$ \""
  },
  {
    "objectID": "blog/posts/linux-notes/index.html#cron-jobs",
    "href": "blog/posts/linux-notes/index.html#cron-jobs",
    "title": "Notes on Using Linux",
    "section": "Cron jobs",
    "text": "Cron jobs\nCron jobs are scheduled tasks that run periodically. For scheduling one-off tasks, use the at command. The name came from “Cronos”, the name of the Greek god of time. A good reference is Newbie: Intro to cron.\n\nQuick reference\nThe cron job syntax is as follows (See Crontab.guru - The cron schedule expression editor):\n# ┌───────────── minute (0–59)\n# │ ┌───────────── hour (0–23)\n# │ │ ┌───────────── day of the month (1–31)\n# │ │ │ ┌───────────── month (1–12)\n# │ │ │ │ ┌───────────── day of the week (0–6) (Sunday to Saturday)\n# │ │ │ │ │                                   \n# │ │ │ │ │\n# │ │ │ │ │\n# * * * * * &lt;command to execute&gt;\n\n* * * * * &lt;once a minute&gt;\n0 * * * * &lt;once an hour at 0-th minute&gt;\n0 0 * * * &lt;once a day at midnight&gt;\n0 0 1 * * &lt;once a month at the midnight of the 1-th day&gt;\n0 0 1 1 * &lt;once a year at the midnight of January 1&gt;\n* * * * 0 &lt;once a minute every Sunday&gt;\nCheck that cron service is running by systemctl list-unit-files --type=service | grep \"cron\"\nList cron jobs by crontab -l.\n\n\nCreating a Cron Job\nFor this example, we create a job that runs every 20 seconds.\n\nOpen Crontab: Open your crontab file by typing crontab -e in your terminal. This command opens the crontab file for the current user in the default text editor.\nWrite Cron Job: Standard cron jobs can’t run in a smaller granularity than a minute. For a once-per-20-seconds job, you’ll need to use a workaround.\nAdd the following lines to your crontab:\n\n * * * * * /path/to/script.sh\n * * * * * sleep 20; ~/cronjobs/script.sh\n * * * * * sleep 40; ~/cronjobs/script.sh\n\nScript Content: Create script.sh, have the following content:\n\n#!/bin/bash\nCRON_MESSAGE=\"Some message\"\necho \"The cron message is: $CRON_MESSAGE\"\nThen save and close the file, and chmod +x ~/cronjobs/script.sh to make it executable.\n\n\nBest Practices\n\nLocation: Store scripts in a dedicated directory, such as ~/cronjobs, for better organization.\nScript Naming: Use meaningful names for your scripts for easier identification.\nLogging: Implement logging within your scripts to capture output and errors for later review. It’s good practice to use /var/log/cron for logging.\n\n\n\nCron environment variables\nCron jobs run in a minimal environment, so any environment variable, like CRON_MESSAGE, is not accessible by the cron script. Instead, you have a few choices:\n\nPut it directly in the crontab file:\n\nCRON_MESSAGE=\"Some message\"\n* * * * * /path/to/script.sh\n\nPut it directly in the script:\n\n#!/bin/bash\nexport CRON_MESSAGE=\"Some message\"\n# rest of the script follows\n\nIf the variable is defined in an external file (like ~/.bashrc, ~/.profile, or a custom configuration file), you can source that file at the beginning of your script:\n\n#!/bin/bash\nsource /path/to/environment_variables_file\n# rest of the script follows\n\n\nTroubleshooting\n\nCheck Permissions: Ensure your script is executable and the cron daemon has the necessary permissions to run it.\nLogs: Check /var/log/cron or relevant logs for errors.\n\nIf you’re using WSL, ensure that the cron service is running since it doesn’t start by default. Use sudo service cron start. You can configure ~/.bashrc by adding the following line: sudo service cron start, but it would make you enter the password at every login.\nAlternatively, enable systemd as described at Section 6.1."
  },
  {
    "objectID": "blog/posts/linux-notes/index.html#how-to-wsl",
    "href": "blog/posts/linux-notes/index.html#how-to-wsl",
    "title": "Notes on Using Linux",
    "section": "How to WSL",
    "text": "How to WSL\n\ninit vs systemd\nEvery Linux starts its first process with some root process. The init is the traditional and simpler one, and systemd is more modern and advanced one.\nWSL by default starts with init instead of systemd, perhaps to save time and compute. This makes things annoying for some users. You can check by ps -p 1 -o comm and see what it returns.\nTo enable systemd, enter in your /etc/wsl.conf with:\n[boot]\nsystemd=true\nor just use cat \"[boot]\\nsystemd=true\" &gt;&gt; /etc/wsl.conf."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Cybernetic artificial intelligence\n\n\n\n\n\n\n\nAI\n\n\ncybernetics\n\n\nmath\n\n\nhistory\n\n\n\n\nMachine learning and self-reproduction according to Norbert Wiener.\n\n\n\n\n\n\nDec 23, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nNotes on Web Design\n\n\n\n\n\n\n\nprogramming\n\n\n\n\nMy quick reference for designing content for the Internet.\n\n\n\n\n\n\nDec 10, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nFermi Estimation for Neural Networks\n\n\n\n\n\n\n\nAI\n\n\neconomics\n\n\n\n\nThe bitter lesson in bite-sized packets.\n\n\n\n\n\n\nDec 5, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nNotes on Using Linux\n\n\n\n\n\n\n\nprogramming\n\n\n\n\nMy quick reference for using Linux for doing things.\n\n\n\n\n\n\nDec 10, 2022\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nSerial Experiments Lain\n\n\n\n\n\n\n\nanime\n\n\nfun\n\n\n\n\nLain is the collective subconscious as a neural network.\n\n\n\n\n\n\nNov 1, 2022\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nThe Wigner Rotation in Special Relativity via Hyperbolic Geometry\n\n\n\n\n\n\n\nphysics\n\n\nmath\n\n\nfun\n\n\n\n\nThe other special relativity paradox that you have never heard of.\n\n\n\n\n\n\nMay 1, 2018\n\n\nYuxi Liu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n  \n  \nYuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks."
  },
  {
    "objectID": "blog/posts/cybernetic-artificial-intelligence/index.html",
    "href": "blog/posts/cybernetic-artificial-intelligence/index.html",
    "title": "Cybernetic artificial intelligence",
    "section": "",
    "text": "In the creation lore of artificial intelligence, John McCarthy coined the term “artificial intelligence” to distinguish his approach from “cybernetics”, which was dominated by Norbert Wiener. However, if that is the case, then we can infer that there was a real danger that artificial intelligence could have been confused with Norbert Wiener’s cybernetics. That is, they must have some real similarity. If the similarity is not in style or approach – as otherwise there would be no need to distinguish the two – then the similarity must be in goal.\nAnd indeed there is. I was first alerted to this by Wiener’s little book (Wiener 1964, chap. 4), which briefly sketched out a machine that could purportedly imitate any machine, including another copy of itself, thus effecting both machine learning and machine reproduction. Unfortunately, it attempted to be a popular science book and explained concepts with a minimum of mathematical symbols, thus providing the reader with a minimum of understanding. Nevertheless, I figured it out by consulting his other books, and so I decided to write this post for posterity as a simple sketch of another way to think about machine intelligence and machine life. A view of another universe, perhaps, or as the theoretical basis of some hard sci-fi to be written.\nThis cybernetic machine is a rather obscure one, and most of the references are decades-old. The main references I relied on to write this essay are (Wiener 1958, chaps. 10, 11; 2019, chap. 9; O’Hagan 2013; G. H. Harris and Lapidus 1967).\nWiener’s terminology is occasionally obsolete, and unfortunately the subject of cybernetic artificial intelligence is essentially never updated since Wiener, so anyone trying to study the subject must contend with his publications. So here is a list of obsolete terminology and my explanations.\n\nshot noise, shot effect, tube noise: white noise. It is not Poisson noise.\nchaos: a generic term for randomness, or random variables. It is not chaos theory.\nvoltage multiplier: an electronic device that multiplies two voltages together, by \\((x, y) \\mapsto xy\\). It is not the voltage multiplier.\n\n\n\nAccording to Wiener, cybernetics is the science of control and communication in the animal and the machine. To be more precise, he used the following model of sweeping generality: the nonlinear transducer model.\n\nDefinition 1 (signal) A signal is a function of time, written as \\(x(t)\\), where \\(t \\in \\mathbb{R}\\). The signal can take value in any space, but for convenience, we will only consider the case where \\(x\\) is real-valued. That is, we only consider functions of type \\(x : \\mathbb{R}\\to \\mathbb{R}\\).\n\nA transducer transduces, meaning that it takes in a signal and outputs a signal. Antennas, filters, circuits of resistors and capacitors, and essentially electronic devices that do not require power input to run, are transducers. Wiener considered only deterministic, causal transducers, meaning that the output of a transducer at time \\(t\\) is determined by the inputs during the period \\((-\\infty, t]\\).\nIn large sections of electronic engineering, electronic circuits are studied as either linear and continuous, or as nonlinear and discrete. Nonlinear but continuous devices are very difficult to study in general, but Wiener took on it and constructed a theory for general nonlinear continuous transducers.\n\nDefinition 2 (nonlinear transducer)  \n\nThe transducer is a function \\(T\\), such that given any real-valued function \\(x : \\mathbb{R}\\to \\mathbb{R}\\), it returns a real-valued function \\(T[x] : \\mathbb{R}\\to \\mathbb{R}\\). In other words, it sends real-valued signals to real-valued signals.1\nThe transducer’s output at any time \\(t\\) is determined by the value of \\(x\\) on the interval \\((-\\infty, t]\\). In other words, it is causal and deterministic.\nThe transducer is stationary in time. That is, if \\(x\\) is the same over the interval \\((-\\infty, t]\\) as \\(x'\\) over the interval \\((-\\infty, t']\\), then \\(T[x](t) = T[x'](t')\\).\nThe transducer has a limited amount of memory, so that its dependence on the high-frequency details of the input signal decreases rapidly as frequency increases. We will explain the meaning of this assumption later.\nThe transducer depends analytically on the frequencies of the input signal. We will explain the meaning of this assumption later.\n\n1 Wiener actually studied general multidimensional signals, but those involve mostly notational complications, with no new ideas."
  },
  {
    "objectID": "blog/posts/cybernetic-artificial-intelligence/index.html#cybernetics",
    "href": "blog/posts/cybernetic-artificial-intelligence/index.html#cybernetics",
    "title": "Cybernetic artificial intelligence",
    "section": "",
    "text": "In the creation lore of artificial intelligence, John McCarthy coined the term “artificial intelligence” to distinguish his approach from “cybernetics”, which was dominated by Norbert Wiener. However, if that is the case, then we can infer that there was a real danger that artificial intelligence could have been confused with Norbert Wiener’s cybernetics. That is, they must have some real similarity. If the similarity is not in style or approach – as otherwise there would be no need to distinguish the two – then the similarity must be in goal.\nAnd indeed there is. I was first alerted to this by Wiener’s little book (Wiener 1964, chap. 4), which briefly sketched out a machine that could purportedly imitate any machine, including another copy of itself, thus effecting both machine learning and machine reproduction. Unfortunately, it attempted to be a popular science book and explained concepts with a minimum of mathematical symbols, thus providing the reader with a minimum of understanding. Nevertheless, I figured it out by consulting his other books, and so I decided to write this post for posterity as a simple sketch of another way to think about machine intelligence and machine life. A view of another universe, perhaps, or as the theoretical basis of some hard sci-fi to be written.\nThis cybernetic machine is a rather obscure one, and most of the references are decades-old. The main references I relied on to write this essay are (Wiener 1958, chaps. 10, 11; 2019, chap. 9; O’Hagan 2013; G. H. Harris and Lapidus 1967).\nWiener’s terminology is occasionally obsolete, and unfortunately the subject of cybernetic artificial intelligence is essentially never updated since Wiener, so anyone trying to study the subject must contend with his publications. So here is a list of obsolete terminology and my explanations.\n\nshot noise, shot effect, tube noise: white noise. It is not Poisson noise.\nchaos: a generic term for randomness, or random variables. It is not chaos theory.\nvoltage multiplier: an electronic device that multiplies two voltages together, by \\((x, y) \\mapsto xy\\). It is not the voltage multiplier.\n\n\n\nAccording to Wiener, cybernetics is the science of control and communication in the animal and the machine. To be more precise, he used the following model of sweeping generality: the nonlinear transducer model.\n\nDefinition 1 (signal) A signal is a function of time, written as \\(x(t)\\), where \\(t \\in \\mathbb{R}\\). The signal can take value in any space, but for convenience, we will only consider the case where \\(x\\) is real-valued. That is, we only consider functions of type \\(x : \\mathbb{R}\\to \\mathbb{R}\\).\n\nA transducer transduces, meaning that it takes in a signal and outputs a signal. Antennas, filters, circuits of resistors and capacitors, and essentially electronic devices that do not require power input to run, are transducers. Wiener considered only deterministic, causal transducers, meaning that the output of a transducer at time \\(t\\) is determined by the inputs during the period \\((-\\infty, t]\\).\nIn large sections of electronic engineering, electronic circuits are studied as either linear and continuous, or as nonlinear and discrete. Nonlinear but continuous devices are very difficult to study in general, but Wiener took on it and constructed a theory for general nonlinear continuous transducers.\n\nDefinition 2 (nonlinear transducer)  \n\nThe transducer is a function \\(T\\), such that given any real-valued function \\(x : \\mathbb{R}\\to \\mathbb{R}\\), it returns a real-valued function \\(T[x] : \\mathbb{R}\\to \\mathbb{R}\\). In other words, it sends real-valued signals to real-valued signals.1\nThe transducer’s output at any time \\(t\\) is determined by the value of \\(x\\) on the interval \\((-\\infty, t]\\). In other words, it is causal and deterministic.\nThe transducer is stationary in time. That is, if \\(x\\) is the same over the interval \\((-\\infty, t]\\) as \\(x'\\) over the interval \\((-\\infty, t']\\), then \\(T[x](t) = T[x'](t')\\).\nThe transducer has a limited amount of memory, so that its dependence on the high-frequency details of the input signal decreases rapidly as frequency increases. We will explain the meaning of this assumption later.\nThe transducer depends analytically on the frequencies of the input signal. We will explain the meaning of this assumption later.\n\n1 Wiener actually studied general multidimensional signals, but those involve mostly notational complications, with no new ideas."
  },
  {
    "objectID": "blog/posts/cybernetic-artificial-intelligence/index.html#orthogonal-functions",
    "href": "blog/posts/cybernetic-artificial-intelligence/index.html#orthogonal-functions",
    "title": "Cybernetic artificial intelligence",
    "section": "Orthogonal functions",
    "text": "Orthogonal functions\nIn order to present Wiener’s approach to nonlinear control theory, we need a small amount of theory of orthogonal polynomials. Specifically, we need the Hermite and Laguerre polynomials. Those are not as famous as the trigonometric functions, but they are used in the same way as the trigonometric functions in Fourier analysis. In Fourier analysis, every well-behaved (in the Fourier sense) function is decomposable as an infinite linear sum of trigonometric functions. Similarly, every well-behaved function (in the Hermite sense) is decomposable into an infinite linear sum of Hermite functions, and the same applies to Laguerre functions.\n\nLaguerre functions\nThe Laguerre polynomials are defined by \\[\nL_n(x) := \\sum_{k} \\binom{n}{k}\\frac{(-1)^k}{k!} x^k\n\\tag{1}\\]\n\nProposition 1 (Laguerre polynomials are orthogonal with respect to the exponential distribution) \\[\n\\int_{0}^\\infty e^{-x}L_m(x) L_n(x)dx = \\delta_{mn}\n\\]\n\n\nProof. The Laguerre polynomials are generated by the following function\n\\[\ng(t, x) = \\sum_{n=0}^\\infty t^n L_n(x)=  \\frac{1}{1-t} e^{-tx/(1-t)}\n\\]\nThis can be verified that the definition of \\(L_n\\) according to the generating function \\(g\\) and according to Equation 1 both satisfy \\(xL_n'' + (1-x) L_n' + nL_n = 0\\), and both have the same value and first derivative at \\(x=0\\). Therefore they are equal for all \\(x\\).22 Of course, this is not the most natural or productive way to show this, but this post is not about orthogonal polynomial theory. So the proof is meant to only build trust, not general competency in orthogonal polynomial theory.\n\nWe can define the Laguerre functions by \\(\\psi_n(t) := e^{-t/2}L_n(t)\\), which makes the definition cleaner:\n\\[\n\\int_0^\\infty \\psi_n(t)\\psi_m(t) dt = \\delta_{mn}\n\\tag{2}\\]\nGiven Proposition 1, we can represent any well-behaved function on the \\((-\\infty, t]\\) as an infinite sum of Laguerre functions\n\\[\nf(t-\\tau) = \\sum_{n \\geq 0} c_n \\psi_n(\\tau)\n\\]\nby taking a convolution with the Laguerre functions\n\\[\nc_n = \\int_0^\\infty f(t-\\tau) \\psi_n(\\tau) d\\tau.\n\\]\n\n\nHermite polynomials\n\nDefinition 3 (physicist’s Hermite polynomials) \\[\n\\sum_n H_n(x) \\frac{1}{n!}t^n = e^{-t^2 + 2tx} = g(t, x)\n\\tag{3}\\]\n\n\nProposition 2 (Hermite polynomials are orthogonal with respect to the normal distribution with variance 1/2) \\[\\int e^{-x^2}H_n(x) H_m(x) dx = \\sqrt\\pi 2^n  n! \\delta_{mn}\\]\n\n\nProof. \\[\\int e^{-x^2} g(t, x)g(s, x) dx = \\sum_{n, m \\geq 0}\\frac{1}{n!m!}t^ns^m \\int e^{-x^2} H_n(x) H_m(x) dx\\]\nThe left side equals \\(\\sqrt\\pi e^{2st}\\). Now expand it in powers of \\(s, t\\)."
  },
  {
    "objectID": "blog/posts/cybernetic-artificial-intelligence/index.html#learning-and-reproducing-any-transducer",
    "href": "blog/posts/cybernetic-artificial-intelligence/index.html#learning-and-reproducing-any-transducer",
    "title": "Cybernetic artificial intelligence",
    "section": "Learning and reproducing any transducer",
    "text": "Learning and reproducing any transducer\nNow we are ready to perform the “Hermite–Laguerre expansion”, Wiener’s way to analyze (learn) and synthesize (reproduce) arbitrary transducer using pure analog devices.\n\nAlgebra of analog circuitry\nIn an analog electronic circuit, real numbers are represented as voltages across two points (“ports”) in the circuit. Adding is as simple as making a serial connection. Negation is even simpler: just connect the ports in the opposite direction. Multiplication is significantly trickier, but it can be done. There are electronic devices with nonlinear response characteristics, meaning that they have two input ports and two output ports, and if you apply an input voltage \\(x\\) across one such device, the output voltage would be \\(f(x)\\) where \\(f\\) is not a linear function. Now suppose that \\(f(x) = x^2\\).33 From our vantage point, the universal approximation theorems proven in the early 1990s show that, generically, if we have any nonlinear function \\(f_0\\) at all, then we can construct any activation function \\(f\\) as a neural network, by using many copies of the \\(f_0\\) device as activation functions and linear devices as weights and biases.\nWith such \\(f\\), we can multiply two voltages by \\(xy = (f(x+y) - f(x) - f(y)) \\times 0.5\\), and so we can construct any polynomial function in any number of variables. That is, we can do algebra by analog devices, as long as we have a voltage multiplier.\nOf course, we don’t hear about voltage multipliers often, and this is no accident – it is quite difficult to get one with good performance. In the preface to the second edition of Cybernetics1961 (Wiener 2019, xli), Wiener waxes praise about Gabor’s breakthrough circuit device that could multiply two voltages at a frequency of \\(1\\; \\mathrm{kHz}\\):\n\nWhile there are many approaches to the problem of multiplying two functions electrically, this task is not technically easy. On the one hand, a good multiplier must work over a large range of amplitudes. On the other hand, it must be so nearly instantaneous in its operation that it will be accurate up to high frequencies. Gabor claims for his multiplier a frequency range running to about 1,000 cycles. … he does not state explicitly the amplitude range over which his method of multiplication is valid nor the degree of accuracy to be obtained. I am awaiting very eagerly4 an explicit statement of these properties so that we can give a good evaluation of the multiplier for use in other pieces of apparatus dependent on it.4 Gabor published it in the same year of 1961: (Gabor, Wilby, and Woodcock 1961).\n\nTo our modern ears, multiplying two voltages 1000 times a second by analog means seems simultaneously astonishing and obsolete. Intel 8086 in 1976 already could multiply a million times a second, and whatever has come of Gabor’s universal filter? It seems to me that Wiener never accepted the future of digital computers, preferring the concrete certainty of magnetic cores and electric wires.\n\n\nThe Laguerre filter bank\nTo find the Laguerre coefficients of a signal, we need to perform a convolution. Convolutions become products after a Laplace transform, so we need to find the Laplace transform of the Laguerre functions \\(\\psi_n = e^{-x/2}L_n(x)\\). Fortunately, it is easy to compute. We simply read from a standard table:\n\\[\n\\mathcal L [t^n e^{-\\alpha t}\\theta(t)] = \\frac{n!}{(s+\\alpha)^{n+1}}\n\\]\nwhere \\(\\theta(t) = 1_{t \\geq 0}\\) is the zero-one step function.\nThen, since the Laplace transform is linear, we have after simplification\n\\[\n\\mathcal L[\\psi_n\\theta] = \\frac{1}{s+1/2}\\left(\\frac{s-1/2}{s+1/2}\\right)^n\n\\]\nThis gives a simple filter bank that constructs the Laguerre coefficients for any signal. The input signal passes through a \\(\\frac{1}{s+1/2}\\) filter to obtain the \\(c_0\\) coefficient, and then through a \\(\\frac{s-1/2}{s+1/2}\\) filter to obtain the \\(c_1\\) coefficient, and so on. This filter bank can be constructed with standard resistors and capacitors.\nThe following theorem finishes the last piece of the puzzle. (G. H. Harris and Lapidus 1967) claims that the proof is found in (Bose 1956; George Henry Harris 1966), but I did not check.\n\nTheorem 1 Let \\(x(t)\\) be a white noise process with variance \\(1/2\\), and let \\(c_0(t), c_1(t), ...\\) be its Laguerre coefficients, then:\n\nthe joint stochastic process \\((c_0(t), c_1(t), ...)\\) is stationary;\nfor any fixed \\(t\\in \\mathbb{R}\\), the random variables \\(c_0(t), c_1(t), ...\\) are independent samples of the standard gaussian distribution \\(\\mathcal N(0, 1/2)\\).\n\n\n\n\nThe Hermite coefficients\nFor a given input signal \\(x : \\mathbb{R}\\to \\mathbb{R}\\), we pass it into the Laguerre filter bank. The readouts from the filter bank are the signals \\(c_0(t), c_1(t), ...\\). They satisfy the equation\n\\[\nx(t - \\tau) = \\sum_{n\\geq 0} c_n(t) \\psi_n(\\tau), \\; \\forall \\tau \\geq 0\\quad \\forall t \\in \\mathbb{R}\n\\]\nIn words, at any cut-off time \\(t \\in \\mathbb{R}\\), the signal we have seen so far is \\(x(t - \\tau)\\) with \\(\\tau \\geq 0\\). This signal is then decomposable as a linear sum of Laguerre functions \\(\\sum_{n\\geq 0} c_n(t) \\psi_n(\\tau)\\), where \\(c_n(t)\\) are the Laguerre coefficients. The coefficients depend on the cut-off time \\(t\\), but do not depend on \\(\\tau\\), which is not “real” time, but only a kind of “relative historical time”, as we look into the past standing at time \\(t\\).\nA transducer, by our assumption, is deterministic and causal, so that \\(T[x](t)\\) is a deterministic function of the signal we have seen so far, and so it is a deterministic function of \\(c_0(t), c_1(t), c_2(t), ...\\). Note carefully that it is determined by \\(c_0(t), c_1(t), c_2(t), ...\\) at this very instant \\(t\\). It does not need to know the values of \\(c_0(t'), c_1(t'), c_2(t'), ...\\) at any \\(t' \\neq t\\). We write it as follows:\n\\[\nT[x](t) = T(c_0(t), c_1(t), c_2(t), ...)\n\\]\nBy our assumption that the transducer has a limited memory, we should be able to ignore the higher frequency components of the input signal, and still recover a good approximation of \\(T\\). That means that \\(T[x](t) = T(c_0(t), c_1(t), c_2(t), ...) \\approx T(c_0(t), ..., c_n(t))\\), with the approximation increasing in accuracy as \\(n\\) increases.\nBy our assumption that the transducer is analytic with respect to the input, \\(T(c_0(t), ..., c_n(t))\\) has a multivariate Hermite serial expansion (the same idea as multivariate Taylor expansion):\n\\[\nT(c_0(t), ..., c_n(t)) = \\sum_{m_0, ..., m_n \\geq 0} T_{m_0, ..., m_n} H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t))\n\\]\nWe are quite close to the target. We can compute the Laguerre coefficients \\(c_n(t)\\) of any input signal by the Laguerre filter bank. We can construct analog circuits that compute \\(H_m(c_n(t))\\), the Hermite polynomial values of the Laguerre coefficients. The remaining challenge is to determine the coefficients \\(T_{m_0, ..., m_n}\\).\nThis is where Theorem 1 comes to finish the construction. Let \\(x(t)\\) be a white noise process, then since\n\\[\nT[x](t) \\approx \\sum_{m_0, ..., m_n \\geq 0} T_{m_0, ..., m_n} H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t))\n\\]\nand since the Laguerre coefficients are independent samples of the standard gaussian, we have\n\\[\nT_{m_0, ..., m_n} \\approx \\mathbb{E}\\left[T[x](t)\\; H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t))\\right]\n\\]\nwhere the expectation is in the sense of ensemble expectation. That is, we would run this experiment once with a white noise process, freeze it exactly at the moment \\(t\\), then run it again with another white noise process, freeze it exactly at the moment \\(t\\), and so on. Then we average over all these experiments.\nHowever, Theorem 1 states that the Laguerre coefficients are stationary, meaning that we have ergodicity5: the ensemble average is the time average, and so5 Wiener was really into ergodic theory.\n\\[\nT_{m_0, ..., m_n} \\approx \\lim_{T \\to \\infty} \\frac{1}{T} \\int_0^T T[x](t)\\; H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t)) dt\n\\]\nThe integrand is computable by the analog devices we described. The integration-and-averaging can be done with a very low-pass filter – taking the average is essentially passing only the zero-frequency signal, and so it is the lowest possible low-pass filter. Finally, since white noise is all around us, it can be obtained in many ways, such as by amplifying the thermal noise in a resistor.\nAnd so we have a finished machine, where the white noise \\(x\\) and the signal to imitate \\(T[x](t)\\) come in, and fitted parameters \\(T_{m_0, ..., m_n}\\) come out. The fitted parameters can be automatically read and adjusted by electromechanical devices, such as relays and step motors, allowing us to connect the machine in parallel with an unknown transducer, run it for a period over a white noise input, and ultimately achieve a machine that precisely imitates the unknown transducer.\n\n\n\nThe fully-formed imitation machine. (G. H. Harris and Lapidus 1967, fig. 2)"
  },
  {
    "objectID": "blog/posts/cybernetic-artificial-intelligence/index.html#the-science-of-control-and-communication-in-the-animal-and-the-machine",
    "href": "blog/posts/cybernetic-artificial-intelligence/index.html#the-science-of-control-and-communication-in-the-animal-and-the-machine",
    "title": "Cybernetic artificial intelligence",
    "section": "The science of control and communication in the animal and the machine",
    "text": "The science of control and communication in the animal and the machine\nWe have reached the end of the road, facing an all-analog general-purpose learning machine. This machine can imitate any black-box transducer, and thus is a form of machine learning. If we have two such machines, and randomly set the parameters of one machine, then the other machine would learn to imitate the same behavior. And since each parameter setting creates a different behavior, purely by imitating behavior, the parameters would be copied from one machine to the other. This is an explicit construction for how behaviorism can work, even if not in our universe, then in another universe where the animals really are those imitation devices.\nAs Wiener speculated (Wiener 2019, 248–49), biological learning and reproduction are “philosophically similar” to this machine:\n\nWhile both Professor Gabor’s methods and my own lead to the construction of nonlinear transducers, they are linear to the extent that the nonlinear transducer is represented with an output which is the sum of the outputs of a set of nonlinear transducers with the same input. These outputs are combined with varying linear coefficients. This allows us to employ the theory of linear developments in the design and specification of the nonlinear transducer. And in particular, this method allows us to obtain coefficients of the constituent elements by a least-square process. If we join this to a method of statistically averaging over the set of all inputs to our apparatus, we have essentially a branch of the theory of orthogonal development. Such a statistical basis of the theory of nonlinear transducers can be obtained from an actual study of the past statistics of the inputs used in each particular case. I ask if this is philosophically very different from what is done when a gene acts as a template to form other molecules of the same gene from an indeterminate mixture of amino and nucleic acids, or when a virus guides into its own form other molecules of the same virus out of the tissues and juices of its host. I do not in the least claim that the details of these processes are the same, but I do claim that they are philosophically very similar phenomena.\n\nIt seems like this device, as it stands, would be plagued by the same issues that plague a general analog computer – error correction, bad gains, and intractable nonlinearities. Still, it stands as a vision of an alternative future in an alternative world, if not an alternative future of our world."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html",
    "href": "blog/posts/neural-scaling-laws/index.html",
    "title": "Fermi Estimation for Neural Networks",
    "section": "",
    "text": "Fermi estimation is a technique for estimating quantities through rough approximations and educated guesses. Named after physicist Enrico Fermi, this method involves breaking down a problem into simpler, more manageable parts, making reasonable assumptions, and using simple arithmetic to arrive at a good enough answer.\nThis post walks you through a simple example of Fermi estimation of a deep learning model. Specifically, we will try to estimate the design parameters of a hypothetical GPT-5 with 1 trillion parameters. Along the way, we will touch upon various other items like the Landauer limit (because of course we will)."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#gpt-like-agi",
    "href": "blog/posts/neural-scaling-laws/index.html#gpt-like-agi",
    "title": "Fermi Estimation for Neural Networks",
    "section": "GPT-like AGI",
    "text": "GPT-like AGI\nLet’s get into the mood for Fermi estimation, by estimating the number of parameters necessary for achieving human-level Artificial General Intelligence, under two assumptions: that a GPT-like architecture is enough; that it requires only matching the human brain in the \"numbers\".\nLet’s estimate how many layers the hypothetical GPT model should have. When prompted with a question, the first answer comes to mind in about a second, and it is generally a good one, if not the best. Perhaps slow deliberation is nothing but stringing together a long chain of snap judgments, guided by snap judgments about snap judgments.\nThe characteristic time-scale of a brain is 0.01 seconds – the fastest brainwave, gamma wave, is 100 Hz. This indicates that the brain takes on the order of 100 steps to make a snap decision – the \"hundred-step-rule\" of Jerome Feldman(Feldman and Ballard 1982). This is suspiciously close to the depth of the largest model of GPT-3, which has 96 layers.\nHow many parameters would such a model require? The brain has \\(10^{15}\\) synapses. It’s unclear how precise each synapse is, but one estimate states that the hippocampal synapse has a precision of about 5 bits (Bartol Jr et al. 2015), which can be stored within a 16-bit floating point number, with room to spare.\nAssuming that, we expect an AGI GPT to have \\(10^{15}\\) parameters, or 1000× that of our hypothetical GPT-5."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#chinchilla-scaling-law",
    "href": "blog/posts/neural-scaling-laws/index.html#chinchilla-scaling-law",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Chinchilla Scaling Law",
    "text": "Chinchilla Scaling Law\nThe paper \"Training Compute-Optimal Large Language Models\" (Hoffmann et al. 2022) reported a series of training runs on language models, trained by Google DeepMind researchers. Each training run is characterized by four numbers:\n\n\\(L\\): the final loss (negative log-likelihood per token) achieved by the trained model.\n\\(N\\): the number of parameters in the model.\n\\(D\\): training dataset size, measured in tokens.\n\\(C\\): training compute cost, measured in FLOP.\n\nAfter training a few hundred models, they obtained a large dataset of \\((L, N, D, C)\\), and they fitted a statistical law of the form\n\\[L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0,\\]\nwhere the parameters are\n\\[\\alpha = 0.34, \\beta = 0.28, A = 406.4, B = 410.7, L_0 = 1.69.\\]\nThey also estimated that the cost of training compute \\(C\\) is proportional to \\(ND\\). This is understandable, because each token must flow through the entire model and \"hit\" each parameter once, incurring a fixed number of floating point operations. They estimated that it takes 6 FLOPs per parameter per token. That is,\n\\[C = C_0 ND, \\quad C_0 = 6\\]\nGiven the assumptions, for each fixed computing budget \\(C\\), we can solve for the optimal \\(D\\) and \\(N\\), which is usually referred to as \"Chinchilla optimal\" training:\n\\[\\begin{cases}\n        \\min_{N, D} L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0,\\\\\n        \\text{such that } C_0 ND = C.\n\\end{cases}\\]\nSolve the above equations symbolically to find \\(N_{opt}, D_{opt}\\) as a function of \\(C, C_0, \\alpha, \\beta, A, B\\). Then, plug in the numerical values of the parameters, to find a numerical expression for \\(N_{opt}, D_{opt}\\) as a function of \\(C\\).\n\n\nSolution\n\nSince \\(C = C_0 ND\\), we have \\(N = \\frac{C}{C_0 D}\\). Plug it into \\(\\min_{N, D} L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0\\), we obtain\n\\[\\min_{D} L = \\frac{A}{(\\frac{C}{C_0 D})^\\alpha} + \\frac{B}{D^{\\beta}} + L_0.\\]\nTake derivative with respect to \\(D\\) and set it to zero. We get an expression for \\(D_{opt}\\). Plug it back to \\(C = C_0 ND\\), we get an expression for \\(D_{opt}\\). These simplify to:\n\\[N_{o p t}(C)=G\\left(\\frac{C}{C_0}\\right)^a, \\quad D_{o p t}(C)=G^{-1}\\left(\\frac{C}{C_0}\\right)^b, \\quad \\text { where } \\quad G=\\left(\\frac{\\alpha A}{\\beta B}\\right)^{\\frac{1}{\\alpha+\\beta}}, a=\\frac{\\beta}{\\alpha+\\beta}, b=\\frac{\\alpha}{\\alpha+\\beta}.\\]\nPlugging in the numerical values, we get\n\\[\\begin{cases}\n        N_{opt}(C) = 0.6 \\; C^{0.45} \\\\\n        D_{opt}(C) = 0.3 \\; C^{0.55} \\\\\n        L_{opt}(C) = 1070 \\; C^{-0.154} + 1.7\n    \\end{cases}\n    \\]\n\nIn the same paper, they also performed a direct statistical fitting, to find the optimal \\(N, D\\) for a given \\(C\\), without going through the intermediate steps above. This gives a slightly different result (only slightly different – as you would know after solving the previous problem):\n\\[N_{opt}(C) = 0.1 C^{0.5}; \\quad D_{opt}(C) = 1.7 C^{0.5}.\\]\nFor the rest of the essay, we will use these equations as the Chinchilla scaling laws. Do not use the equations you derived for the previous problem.\nSuppose we decide that our next AI should have 1 trillion (\\(N = 10^{12}\\)) parameters, and we use Chinchilla scaling laws. How much compute would it cost to train, and how many tokens would its training dataset have?\n\n\nSolution\n\n\\(N = 0.1 \\times C^{0.5} = 10^{12}\\), so \\(C= 10^{26}\\) FLOP, and \\(D = 1.7 \\times 10^{13}\\), or 17 trillion tokens."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#dataset-size",
    "href": "blog/posts/neural-scaling-laws/index.html#dataset-size",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Dataset size",
    "text": "Dataset size\nAssuming each English word cost about 1.4 tokens, how many English words would 10 trillion tokens be? Assuming each page has 400 words, and each book has 300 pages, how many books is that? To put it into context, look up the size of Library of Congress, and Google Books, and compare with the number we just calculated.\n\n\nSolution\n\n\\(10 / 1.4 = 7\\) trillion words. If each book has \\(400 \\times 300 = 0.12\\) million words, then that is 60 million books, if they were all in English.\n\nSince humans are kind of the same everywhere, book lengths should be kind of the same everywhere – information-dense languages would naturally have books with lower apparent word-count, but the same information (measured in bytes)."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#memory-requirement",
    "href": "blog/posts/neural-scaling-laws/index.html#memory-requirement",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Memory requirement",
    "text": "Memory requirement\nTypically, a deep learning model has 16-bit floating point parameters. Modern systems sometimes use lower precision (e.g. 8-bit) floating point numbers to save space, but generally it is necessary to use at least 16-bit during training, and the model is converted to lower precision after training (\"post-training quantization\").\nGiven that each parameter is a 16-bit floating point number, how much memory does it cost to store a model with 1 billion parameters? How about our hypothetical GPT-5, which has 1 trillion parameters? How many A100 GPU (VRAM = 40 GB) would be required to contain the full GPT-5 model?\n\n\nSolution\n\n1 billion parameters is 2 billion bytes, or 2 GB. Our hypothetical GPT-5 would take up 2 TB. It would take 50 A100 to contain it.\nNow, 50 A100 GPUs has a running cost of about 100 USD/hr, but not such a terrible problem, but then, during training, you would have to run an optimizer like Adam, which would need three floating point numbers per parameter (gradient, momentum, scale), instantly increasing the memory requirement to 200 A100 GPUs.\n\n\nMemory cost\nThis table1 gives the price per megabyte of different storage technology, in price per megabyte (2010 dollars), up to 2018.1 Source: Storage 2: Cache model – CS 61 2018.\n\n\n\nYear\nMemory (DRAM)\nFlash/SSD\nHard disk\n\n\n\n\n~1955\n$411,000,000\n\n$6,230\n\n\n1970\n$734,000.00\n\n$260.00\n\n\n1990\n$148.20\n\n$5.45\n\n\n2003\n$0.09\n$0.305\n$0.00132\n\n\n2010\n$0.019\n$0.00244\n$0.000073\n\n\n2018\n$0.0059\n$0.00015\n$0.000020\n\n\n\nThe same costs relative to the cost of a hard disk in ~2018:\n\n\n\nYear\nMemory\nFlash/SSD\nHard disk\n\n\n\n\n~1955\n20,500,000,000,000\n\n312,000,000\n\n\n1970\n36,700,000,000\n\n13,000,000\n\n\n1990\n7,400,000\n\n270,000\n\n\n2003\n4,100\n15,200\n6.6\n\n\n2010\n950\n122\n3.6\n\n\n2018\n295\n7.5\n1\n\n\n\nSuppose long-term memory storage can last 1 year before being replaced. How much money does it cost per year to store a 1 trillion parameter model on an SSD, the most expensive form of long-term storage? How much money does it cost to store a 1 trillion parameter model on DRAM memory? Use 2018 prices.\n\n\nSolution\n\nSSD cost 0.00015 USD/MB in 2018, or 0.15 USD/GB. 1 trillion parameters cost 2000 GB of storage, or about 300 USD. So the total cost of storage is 300 USD/year, just a rounding error.\nIn contrast, DRAM cost 0.006 USD/MB, which is 40x that of SSD, so the total cost is 12000 USD.\nNow, compared with the cost of A100 GPU themselves? It takes about 50 A100 GPU to run the model, and each would cost about 20,000 USD. So the cost of long-term memory is essentially zero, and the cost of DRAM is about \\(\\frac{12000}{20000\\times 50} = 1\\%\\) of the total cost of GPU.\nSo what is the limit? The memory bandwidth, which we will see in the next question.\n\n\n\nMemory bandwidth and latency\nWhile the memory itself is cheap, moving the data requires expensive high-bandwidth wiring. Indeed, the memory bandwidth between the DRAM (or \"VRAM\" for \"Video RAM\") and the little processors on the GPU is a main bottleneck on how good the GPU can perform.\nDuring a single forward pass of a model, the parameters of the model are loaded from the DRAM of the GPU into the fast cache memories, then pushed through the thousands of computing processors on the GPU.\nA100 GPU has a memory bandwidth of 1.6 TB/s.\nWhat is the minimal latency, in seconds, for the GPU to perform a single forward pass through our hypothetical GPT-5 model with 1 trillion parameters? How many tokens can it output (autoregressively) in one minute? How about GPT-3 (175 billion parameters)?\nSince we are just trying to compute an order-of-magnitude estimate, let’s assume for the problem that the model fits onto a single DRAM on a single GPU. You can also ignore the need to read/write model activations and optimizer states.\n\n\nSolution\n\nSince the model takes up 2 TB of memory, it would take at least 1.3 seconds to even load the model from DRAM, during a single forward pass.\nAutoregression means that the next token cannot be generated until the previous one is already generated, so the model can only generate one token per 1.3 seconds, or 46 tokens per minute. A Chat-GPT5 would be a very slow talker!\nHowever, it can run in batch mode. So for example, it might be able to run 1 million conversations in parallel, outputting one token for all conversations every 2 seconds. Latency is the hard limit, but throughput is not.\nGPT-3 with 175 billion parameters would run 1000/175 times faster, at 262 tokens per minute, about the speed of a fast human talker. This corresponds well with the general impression that ChatGPT types about 200 words a minute.\n\n\n\nBatch inference\nThere are some ways to improve latency. For example, the model can be parallelized over several GPUs, which effectively increases the memory bandwidth. For example, \"tensor parallelism\" splits each layer into several GPUs.\nThere is also \"pipeline parallelism\", which splits the model into layers. The first few layers go into one GPU, the next few go into another, and so on. This does NOT decrease latency, but it does increase throughput.\nThe fundamental bottleneck in an autoregressive model like GPT is that, by design, they have to start a sentence without knowing how it will end. That is, they have to generate the first token before generating the next one, and so on. This can never be parallelized (except by egregious hacks like speculative decoding).\nOne reason Transformers dominated over RNN is that training and inferring an RNN both must be done one-token-at-a-time. For Transformers, training can be done in parallel over the entire string. Inferring however still cannot be parallelized.\nParallelization tends to be a deeply troublesome business – parallel programming is generally deeply troublesome. However, there is a general trick to improve throughput: batch mode. For example, GPT-5 might be able to run 1 million conversations in parallel, outputting one token for all conversations per forward pass. This trick works until the batch size is so large that the activations due to tokens takes up about as much memory bandwidth as the model parameters.\nConcretely, we can estimate what is a good batch size for GPT-3 175B. It has 96 attention layers, each with 96×128-dimension heads. It is typically run with 16-bit precision floating point numbers. For a single token, how many megabytes would all the floating point activations cost?\n\n\nSolution\n\nA single token would cost \\(96 \\times 96 \\times 128\\) floating point activations, or about 2.4 MB.\n\nThe model itself has 175 billion 16-bit floating point parameters, taking up about 350 GB. How many tokens do we need to put into a batch, before the activations occupy the same amount of memory as the model parameters?\n\n\nSolution\n\nIn order for the activations of tokens to occupy the same memory as the GPT-3 parameters itself, we need about \\(\\frac{350 \\;\\mathrm{GB}}{2.4 \\;\\mathrm{MB}} = 0.15 \\text{million tokens}\\).\nIf we count the optimizer states for the model during training, then GPT-3 takes up \\(4 \\times 350 \\;\\mathrm{GB} = 1.4 \\;\\mathrm{TB}\\), and so we need about 0.6 million tokens.\nThis explains why during training, the batch sizes of the largest models typically are on the order of 1 million tokens. It also shows why there is a natural drive towards scale – only the largest companies can expect to regularly run 0.2 million chats simultaneously."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#training-cost",
    "href": "blog/posts/neural-scaling-laws/index.html#training-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Training cost",
    "text": "Training cost\nHow much money does compute cost? We can work through an example using the current standard computing hardware: Nvidia A100 GPU (40 GB VRAM version).\nThe most important specifications are:\n\nUnit price: 15000 USD.\nRental price: 2 USD/hr.\nSpeed: 0.3 petaFLOP/s = 3E14 FLOP/s.\nPower: 0.3 kW.\nMemory bandwidth: 1600 GB/s.\n\nIn the literature about the largest AI models, the training cost is often reported in units of “petaFLOP-day”, which is equal to 1 petaFLOP/second x 1 day. How many FLOP is 1 petaFLOP-day? What is the equivalent number of A100-hour? If we were to buy 1 petaFLOP-day of compute with rented A100 GPU, how much would it cost?\n\n\nSolution\n\n1 petaFLOP-day = 1 petaFLOP/second x 1 day = 1E15 FLOP/s x 8.64E4 s = 8.64E19 FLOP.\nSince 1 A100 can run at 0.3 petaFLOP/s, getting 1 petaFLOP-day requires us to run 1/0.3 A100 for 1 day, or 80 A100-hours. At the price of 2 USD/hr, it would cost 160 USD.\n\nThe largest model of GPT-3 cost 3640 petaFLOP-days to train (according to Table D.1 of the report). How much would it cost if it were trained with A100? How much money does it cost to train our hypothetical GPT-5?\n\n\nSolution\n\n2E25 FLOP = 0.2 amount of GPT-5 = 17 million A100-hours = 33 million USD.\nAnd accounting for the utilization rate of 30%, that would give us 110 million USD.\nOh, and if you want some kind of official confirmation? OpenAI’s CEO Says the Age of Giant AI Models Is Already Over | WIRED\n\nAt the MIT event, Altman was asked if training GPT-4 cost $100 million; he replied, “It’s more than that.”\n\n\nIn reality, the GPU cannot be worked to their full speed, and we only use about 30% of its theoretical peak FLOP/s (so for example, for A100, we only get 0.1 petaFLOP/s, instead of 0.3 petaFLOP/s).2 For this question, we assume that the utilization rate is 100%.2 The utilization rate of 30% is according to EpochAI.\nAlso, we are assuming the training happens in one go, without hardware failures, divergences, and other issues requiring restart at a checkpoint. There is not much published data from large-scale training, but the OPT-175B training run (described later) took 3 months to complete, but would have taken only 33 days if there were no need for the many restarts. This suggests that the restarts would increase the computing cost by 2× to 3×.\nFor context, here are the costs of development of various items3:3 Sorry, not adjusted for inflation to the same year, but they are roughly in the range of 2000 – 2020 USD.\n\niPhone 1: 150 million USD.\nA typical 5 nm chip: 0.5 billion USD.\nAirbus A380: 18 billion USD. (Bowen 2010, Table 4.3)\nThree Gorges Dam: 250 billion CNY, or about 30 billion USD.\nManhattan Project: 24 billion USD (2021 level)\nApollo Program: 178 billion USD (2022 level)\n\nComment on the cost of our hypothetical GPT-5. Is it on the order of a large commercial actor like Google, or a state actor like China?\n\n\nSolution\n\nThe cost is on the order of 1 billion USD, which is roughly the largest projects undertaken by private corporations. Larger models, as those contemplated by AGI projects in Google, OpenAI, etc, have historically been undertaken by state actors only.\n\nHere is another way to estimate. Building a large AI model, according to accountancy, would be investment into a production machine, and thus part of “capital expenditure” (“CapEx”). We can find out what proportion is 1 billion USD in their total CapEx. I looked it up here.\n\nDuring the 2020–2022, Microsoft has yearly CapEx on average 25 billion USD.\nGoogle has about 25 billion USD.\nMeta, 20.\nAmazon, 63.\n\nIn short, training something like GPT-5 would cost a lot, on the order of 5% of total CapEx, even by the standards of the largest companies. It can probably scale up by another factor of 10× if they really push it, but not more.\nIn order to train even larger AI models, those AI models must enter production. They must become a productive member of society, otherwise the company would nott have the money to train them.\nMicrosoft announces new supercomputer, lays out vision for future AI work (2020):\n\nThe supercomputer developed for OpenAI is a single system with more than 285,000 CPU cores, 10,000 GPUs and 400 gigabits per second of network connectivity for each GPU server. Compared with other machines listed on the TOP500 supercomputers in the world, it ranks in the top five, Microsoft says.\n\nThe largest companies, like Microsoft have GPU on the order of 10000 A100. What is the wall clock hour of training GPT-5, assuming you have 10000 A100, perfect utilization, and no interruptions?\n\n\nSolution\n\n83 million hours / 10000 = 350 days. Almost exactly 1 year.\n\n\nThe difficulty of large-scale training\nLarge models do end up diverging many times during training runs, requiring restarts. Sometimes the divergence is so bad that the whole training run must be started from scratch. Sometimes the convergence is too slow, requiring fixes to the optimizer and learning rate schedules, etc. All together, we should expect the failures, restarts, deadends… to triple the cost at least, to ~1 billion USD.\nIt is not easy to find \"stories from the trenches\" for actually training large-scale neural networks that really show this, but thanks to Meta, we have one. In 2021, a team of 5 engineers from Meta trained a LLM with 175 billion parameters, in 3 months, using 1024 80GB A100 GPUs from. Excluding all the divergences, hardware failures, and other issues that caused lost progress, the final model would have taken about 33 days of continuous training.\nThey have kept journals during their training. This is now published at metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq · GitHub. You can see how difficult it is to train a large model. Selected quotes:\n\nThese notes cover ~90 restarts over the course of training the lineage of this current model (experiments 12.xx).\n\n\nFound issues with the new dataset where perplexity was unreasonably low… After applying as much regex-ing as we could to salvage the dataset, we relaunched another set of experiments to test LPE (experiments 20-29) on the new dataset. We didn’t have time to retrain a new BPE on the final dataset, so we fell back to using the GPT-2 BPE.\n\n\nFrom experiment 11.4 onward, we saw grad norm explosions / loss explosions / nans after a couple hundred updates after each restart, along with extremely unstable loss scales that would drop to the point of massively underflowing. We started taking more and more drastic actions then, starting with increasing weight decay to 0.1, lowering Adam beta2 to 0.95, lowering LR again, until finally by experiment 11.10 we hot-swapped in ReLU and also switched to a more stable MHA calculation (noting that the x**3 term in GeLU might be a source of instability with FP16).\n\n\n\nOn November 11, we started training our 12.00 experiment with all of these changes, and since then, the only restarts we’ve had to make were all related to hardware issues (missing GPUs on instances, training randomly hanging after including a new node, ECC errors, partial checkpoint upload after hardware error, CUDA errors, NCCL errors, etc.).\n\n\nReplacement through the cloud interface can take hours for a single machine, and we started finding that more often than not we would end up getting the same bad machine again.\n\n\nThere were also issues with blob store when downloading 1.6TB of a single model checkpoint (992 files, each ~1.7GB) on restarts, at which point the downloads themselves would start hanging nondeterministically, which then delayed training recovery even further.\n\n\nWe managed to hit our top three record long runs of the experiment these past two weeks, lasting 1.5, 2.8, and 2 days each! If we were to look at only the runs that have contributed to pushing training further and plot training perplexity against wall clock time, we get the following [The breaks are due to the Thanksgiving holiday]:"
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#inference-cost",
    "href": "blog/posts/neural-scaling-laws/index.html#inference-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Inference cost",
    "text": "Inference cost\nInference cost a lot less money than training, but it’s still a substantial cost. For Transformer language models, it costs about 2 FLOPs per parameter to infer on one token.\nGiven that GPT-3.5 has 175 billion parameters, how many FLOPs would it take to infer on 1 million tokens? How much money would it cost if it were run with A100?\n\n\nSolution\n\n\\(175 \\;\\mathrm{billion} \\times 1 \\;\\mathrm{million} \\times 2 = 4\\times 10^{17} \\;\\mathrm{FLOPs}\\). Now one A100-hour is \\(8.64\\times 10^{19} \\;\\mathrm{FLOPs}\\), so that is 1/200 A100-hour, or about 0.01 USD.\nThe price offered by OpenAI is 2 USD per 1 million tokens, so it’s a very profitable business… but see next question.\n\nThe price offered by OpenAI is 2 USD per 1 million tokens. Assuming that GPT-3.5 cost 10 million USD to develop and train, how many tokens must be sold, just to recoup the cost? Assuming each English word cost about 1.4 tokens, and each essay is 1000 words, how many essays would it be equivalent to?\n\n\nSolution\n\nSince the cost is almost negligible (1/200 of the revenue), we can pretend it’s all profit. So, the profit is 2 USD per 1 million tokens. We need to recoup 10 million USD, so we need \\(10 \\;\\mathrm{million} / 2 \\times 1 \\;\\mathrm{million} = 5\\times 10^{12} \\;\\mathrm{tokens}\\), or 4 billion essays.\nAbout one essay per person on earth, or 10 essays per person in America… is that too much to ask?\n\nMoore’s law applies to both CPU and GPU. From 2006 to 2021, GPU FLOP/s per dollar has been doubling every 2.5 years. We can fairly well assume that the price of petaFLOP-day also decreases at the same rate.\n\n\n\n\n\nAssuming Moore’s law continues to hold indefinitely, and that there will be no algorithmic progress. When would GPT-3 become as cheap as a typical hobbyist project, say, 1000 USD?44 Since a 2006 GPU and a 2020 GPU both have the same lifespan (1 – 4 years usually), and their cost of electricity is only a small proportion of their running expense, the price of GPU FLOP/s per dollar at the point of purchase is roughly proportional to the rental rate of GPU.\n\n\nSolution\n\nGPT-3 cost 6 million USD to train. For it to cost just 1000 USD, we need to drop its price 6000-fold. Now, \\(\\log_2(6000) \\times 2.5\\; \\mathrm{year} = 30 \\; \\mathrm{year}\\). So it would be around 2050."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#energetic-cost",
    "href": "blog/posts/neural-scaling-laws/index.html#energetic-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Energetic cost",
    "text": "Energetic cost\nThe Landauer limit states that the cost of erasing one bit of information is \\(E = k_B T \\ln 2\\), where \\(k_B\\) is the Boltzmann constant, and \\(T\\) is the temperature of the computing machinery. At room temperature, \\(T = 300 K\\), giving us \\(E = 3\\times 10^{-21} J\\).\nNow, one FLOP is a floating point operation, meaning that you start with two 32-bit objects and end up with a single 32-bit object. You start with 64 bits and end up with just 32 bits, and so you lose 32 bits. So by the Landauer limit, the minimal energetic cost is \\(32 k_B T \\ln 2\\).\nGiven this, what is the minimal energy required for performing one FLOP? What is the minimal power (in Watts) required to perform 300 TFLOP/s, as in A100 GPU? Compared this to the actual value of 300 Watts.\n\n\nSolution\n\nThe energy per FLOP is \\(E_{FLOP} = 32 \\times 3\\times 10^{-21} J = 10^{-19} J\\). At 300 TFLOP/s, we need \\(P_{A100} = 3\\times 10^{14} E_{FLOP}/s = 3\\times 10^{-5}W\\). The actual value of 300 Watts is 10 million times more than the theoretical minimum.\nThere is still plenty of room at the bottom!\n\nFor context, the equivalent FLOP/s for the human brain is controversial, but a brief scan through the review article says that it should be about 1E18 FLOP/s. The energetic power of the brain is well-known: about 30 Watts. This means the brain is about 30000x more energy-efficient than A100 GPU, but still 300x less efficient than the theoretical minimum.\n\nThe lowest possible power for life\nThe slowest metabolism found on earth (so far) is in microbes living below deep ocean surface. They had to survive on the energy of “marine snow” falling to the surface, then slowly diffusing through the subsurface mud. Their power consumption is estimated at… \\(10^{-21} W\\). Since their temperature is about the temperature of the ocean water, which is liquid, their temperature is still about \\(T = 273 K\\), and so the Landauer limit is still about \\(3\\times 10^{-21} J\\). This shows that they can lose at most 500 bits every day.\nMost of the operations must be spent on simply repairing biomolecules, since repairing is effectively a 1-bit-losing operation: you start with a molecule that could be “damaged” or “fine”, and end up with a molecule that is “fine”, losing 1 bit. In fact, there are usually many possible ways to be “damaged”, so you would lose several bits.\nFor example, suppose you have a base-pair-mutation in one copy of the DNA. You want to fix it by checking with the other copies. This then means you start with 4 possible states (ACGT) and end up with just 1 state, losing 2 bits in the process."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#environmental-cost",
    "href": "blog/posts/neural-scaling-laws/index.html#environmental-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Environmental cost",
    "text": "Environmental cost\nAccording to “Carbon emissions and large neural network training”(Patterson et al. 2021), the carbon emission of training GPT-3 is 552 tCO2. According to a 2021 poll of climate economists, 1 tCO2 emission should cost somewhere between 50 and 250 USD. Let’s take their geometric average of 112 USD.\nIf we add all the tCO2 cost to the training of GPT-3, how much more expensive would it be? Compare that with its A100-GPU cost of training.\n\n\nSolution\n\n\\(112 \\times 552 = 62,000 \\;\\mathrm{USD}\\).\nPreviously, we calculated the GPU cost of training to be around 6 million USD. So fully carbon-pricing the emission would make it 1% more expensive.\nGenerally, adding in the tCO2 cost of training the largest models increase the cost by about 1 to 2 % (I have computed it in several ways). Since the global GDP increases by about 2% a year, this shows that fully accounting for the carbon cost of AI would delay it by perhaps 1 year.\n\n\n\nSide note for economics students\n\nYou might argue that fully carbon-taxing the global economy would raise not just electricity prices, but also the price of production, etc. For example, the global shipping industry uses fuel that has a high carbon emission.\nHowever, if the global shipping industry were to fully add carbon taxing into the fuel cost, then GPU prices would rise a little, but food prices would rise a lot. Why? Simply put, the shipping cost of a GPU, even if we were to take account of all the trips its components have made around the world, is still small. After all, one A100 GPU is just 1.7 kg, but sells for 15000 USD, so a tonne of A100 GPU can sell for 9 million USD.\nTo put it into perspective: the price of US maize is about 200 USD/tonne. So if shipping cost were to increase so much as to double the price of maize, it would only increase price of A100 by 1/45000.\nEven if we were to assume that producing an A100, being a commodity with a globally scattered supply chain, means shipping its components around the world 1000 times, the total shipping cost of A100 would only be increased by 1/45 = 2.2%.\nIn other words, properly pricing carbon emissions would only increase the price of AI by up to 5%, even as it doubles the price of food. AI is already green (within 5%), but food is far from green. That’s even before we get to the subsidies! We don’t have GPU subsidies, but we certainly have corn subsidies…\nIn this regard, it’s not “Green AI” that is important, but Green Corn, Green Timber, and all the other bulk commodities.\n\nTo put the number in another context, compare it with some typical American food. According to Our World in Data, it cost about 50 kg of CO2 emission per 1 kg of beef.\nAlso, an average American person (not household) consumed 38 kg of beef in 2020.\nCompare the CO2 emission of GPT-3 and CO2 emission from beef consumption. Assuming each burger (\"quarter pounder\") contains 1/4 pound (113 grams) of beef. How many burgers would be equivalent to the CO2 emission of GPT-3?\n\n\nSolution\n\n113 grams of beef emits about 5.6 kg of CO2, so GPT-3 emits about (552 ton)/(5.6 kg) = 90,000 burgers. It is about the same amount as 250 people eating one burger everyday, for a whole year.\n38 kg of beef gives about 2 tCO2 emission. So the emission of GPT-3 is equal to the emission due to beef consumption of 276 American persons.\n\nI think this strongly argues against the conclusion from (Patterson et al. 2021):\n\nTo help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark… We believe such new incentives could lead to a virtuous cycle where ML practitioners compete to increase accuracy while lowering energy consumption and CO2e that could bend the curve of ML carbon footprint growth for computationally intensive NLP models.\n\nOne, the carbon footprint of ML is already minimal, compared to the cost of just making all the GPUs.\nTwo, accounting for CO2 is a dreadfully boring business,5 and should properly be done by carbon taxing by the public officials. The ML practitioners are not climate economists or accountants, and they should not be burdened by accounting for climate change. If the price is right, then they simply need to budget how much money they want to spend on the training run, and the market would optimize the rest (including optimizing the right level of climate change6).5 If you don’t believe me, try reading (Patterson et al. 2021).6 The right level of climate change is not \"none\", but rather \"when the marginal cost equals marginal benefit\". This might sound controversial, but it is just introductory economics.\n\n(Krugman 2002) Luckily, at this point the orthodoxy of the academic economists is very much a minority position among intellectuals in general; one can seem to be a courageous maverick, boldly challenging the powers that be, by reciting the contents of a standard textbook.\n\n\nIn one sentence: There need be no new incentive other than the profit motive."
  },
  {
    "objectID": "blog/posts/web-design-notes/index.html",
    "href": "blog/posts/web-design-notes/index.html",
    "title": "Notes on Web Design",
    "section": "",
    "text": "General references: * The 2022 Web Almanac * MDN Web Docs *"
  },
  {
    "objectID": "blog/posts/web-design-notes/index.html#html",
    "href": "blog/posts/web-design-notes/index.html#html",
    "title": "Notes on Web Design",
    "section": "HTML",
    "text": "HTML\nHTML (HyperText Markup Language) is the standard markup language for creating web pages.\nHTML was created by Tim Berners-Lee in 1989. The key metaphor for HTML is the “editing markup”, as follows: Back in the old days, authors would write or typewrite their document in the exact same font, from the first word to the last word. Then the document is sent to an editor, who would edit it by marking up the words, such as drawing squiggly lines, crossing things out, changing their font size, and writing other instructions for the type-setter (which back then meant someone who would take out types from a box and set them into the right ordering for the printing press).\nSo, one should think of an HTML document as starting with a plaintext of exactly the same format, from the first to the last word, then adding marks upon it.\n\n&lt;!DOCTYPE&gt;\nThe &lt;!DOCTYPE&gt; tag is used to declare the document type. It is usually like &lt;!DOCTYPE html&gt;, although there are rare variants, where instead of html, we have html -//w3c//dtd xhtml 1.0 transitional//en http://www.w3.org/tr/xhtml1/dtd/xhtml1-transitional.dtd, html -//w3c//dtd xhtml 1.0 strict//en http://www.w3.org/tr/xhtml1/dtd/xhtml1-strict.dtd, etc.\nWell, if you don’t care much for the details, use &lt;!DOCTYPE html&gt; is always fine. If you do care, read on.\nThe xhtml thing came from early 2000s, where there was a movement to XML-everything. Instead of the poorly specified HTML, there would be XHTML, which can be checked for syntax, and compiled into an abstract syntax tree. Despite this, people just kept on using HTML and ignored XHTML.\nDespite the universal ambitions of XML, it is now in the land of old soldiers, who never die, but just fade away. * The SVG vector graphics format. * MathML, which is like LaTeX but in XML. * The RSS and Atom feeds, which… unfortunately, are also mostly legacy now. Who even use these nowadays? * The acronym AJAX, which stands for Asynchronous JavaScript and XML JSON. After looking at XML and JSON, I am quite glad this replacement happened.\n\n\n&lt;meta&gt;"
  },
  {
    "objectID": "blog/posts/web-design-notes/index.html#css",
    "href": "blog/posts/web-design-notes/index.html#css",
    "title": "Notes on Web Design",
    "section": "CSS",
    "text": "CSS"
  },
  {
    "objectID": "blog/posts/web-design-notes/index.html#javascript",
    "href": "blog/posts/web-design-notes/index.html#javascript",
    "title": "Notes on Web Design",
    "section": "JavaScript",
    "text": "JavaScript"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "Yuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks."
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "Over the years I have written some notes. Too short for publication, and not exactly fit for blog posting, but still might be useful to someone, so I collect them here."
  },
  {
    "objectID": "notes/index.html#expository-notes",
    "href": "notes/index.html#expository-notes",
    "title": "Notes",
    "section": "Expository notes",
    "text": "Expository notes\nVisually Deriving the Wigner Rotation by Spacetime Diagrams. Obsoleted by my post. This is a term paper I wrote in 2018 for Theoretical Physics. The date in the pdf is the recompilation date, not the time when I actually wrote the paper.\nHyperbolic dynamical systems, chaos, and Smale’s horseshoe: a guided tour. This is the companion paper to a presentation I gave for the course Introduction to Ergodic theory in 2019.\nAn Overview of Information Geometry. This is the term paper for Advanced Differential Geometry. and since I really did not like lectures, I asked to do something to substitute for the mandatory attendance. I was asked to write a term paper, which produced this. Information geometry is a weird thing. The premise is beautiful, but the books are terribly confusing, and what little I have managed to understand seems disappointing. It feels like the whole field is overpromising and underdelivering.\nHandout for honours seminar talk on AIXI. A presentation handout for AIXI. For my undergrad thesis, I was going to be advised by Marcus Hutter, but he left ANU just before the start of semester, and I had to scramble for another advisor. Still, I found AIXI worth knowing, so for my mandatory short talk, I gave a presentation. I managed to compress the essentials to two pages, perfect for handing out on double-sided printed sheets."
  },
  {
    "objectID": "notes/index.html#undergrad-thesis",
    "href": "notes/index.html#undergrad-thesis",
    "title": "Notes",
    "section": "Undergrad thesis",
    "text": "Undergrad thesis\nI will put it here as soon as I find it in my backup files.\nBeyond expectations, but within limits – the theory of coherent risk measures. My undergraduate thesis written in 2019 at ANU, on the topic of coherent risk measures. The first chapter is a readable introduction to risk measures in general (as in, why we might need to use more than the mean and the variance). The rest of it is very dry and I imagine it is of only interest to specialists. The centerpiece of the thesis is a straightforward proof of the central limit theorem for CVaR, which is a slight generalization of expectation. Like the central limit theorem, this theorem states that the sample CVaR converges to the true CVaR like\n\\[\n\\frac{\\text{sample CVaR}_\\alpha - \\text{true CVaR}_\\alpha}{\\sqrt N} \\xrightarrow{d} \\mathcal N(0, \\sigma^2(\\alpha))\n\\]\nwhere \\(\\sigma^2(\\alpha)\\) has a certain expression. As soon as I have calculated it myself, thinking that I had finally made a new discovery, I found it published before in the literature. Still my expression is simpler than the previous publications, so I believe it is still worth something after all."
  },
  {
    "objectID": "notes/index.html#corrections",
    "href": "notes/index.html#corrections",
    "title": "Notes",
    "section": "Corrections",
    "text": "Corrections\nWhen I was not yet mathematically mature, I used to study textbooks carefully, checking every letter through a brain-filter. I no longer do this, but while I was doing this, I created some erratas. Perhaps those will be of use to some people.\nIt is a rather odd thing that errata are hard to share. I would have thought there ought to be some kind of Wikipedia for errata, where people just post errata for textbooks. The lack of such an Error-pedia seems to require an economic explanation, as it can just use MediaWiki, the same technology powering Wikipedia.\n\nConway, John B. A course in point set Topology. Belin: Springer, 2014.\nWalter, P. “An introduction to ergodic theory (Graduate Texts in Math. 79) Springer-Verlag.” Berlin-Heidelberg-New York (1982).\nHiriart-Urruty, Jean-Baptiste, and Claude Lemaréchal. Fundamentals of convex analysis. Springer Science & Business Media, 2004."
  }
]