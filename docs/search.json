[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Something ridiculously painful to play, lol.\nPlay online | Play offline"
  },
  {
    "objectID": "projects/index.html#double-stroop",
    "href": "projects/index.html#double-stroop",
    "title": "Projects",
    "section": "",
    "text": "Something ridiculously painful to play, lol.\nPlay online | Play offline"
  },
  {
    "objectID": "blog/posts/serial-experiments-lain/index.html#my-summary",
    "href": "blog/posts/serial-experiments-lain/index.html#my-summary",
    "title": "Serial Experiments Lain",
    "section": "My summary",
    "text": "My summary\nIt seems Lain is the collective subconscious, implemented as a neural network, with every human brain as a neuron, and “wired together” by some kind of unexplained electromagnetic coupling to the earth’s ionosphere.\nAs a suggestive evidence for this, the frequencies of the Schumann resonance (8 Hz, 14 Hz, 20 Hz, etc) happen to be close to the main brainwave frequencies. See the appendix Section 4.1 for some additional details about it. It is not actually relevant for the story.\nSome people drew up a plan to control Lain. The key scientist in the plan is Eiri, scientist of the Tachibana General Laboratories.\nFirst they migrated Lain from the brain-ionosphere system to the brain-Internet system, by an update to the Internet protocol (Protocol 7) that allows brain-computer interfacing.\nThen they gave Lain a human body. (unclear how that happened) This body gave Lain self-consciousness and a person-API.\nEiri suicided after Protocol 7 was running. Protocol 7 contained a copy of his brain state, so he was now running on the Internet, and called himself god. He has followers (Knights of the Eastern Calculus) who did his commands.\nThen they orchestrated a series of dramatic events to steer Lain’s development. The human body for Lain is used here, as these human-psychologically meaningful events can only work through a human-person-API (You can’t traumatize a non-human process, or Lain-without-body, by staging a bloody murder. Murders only mean something if it’s seen through animal eyes, the same way that a story can move you only if you speak the language.)\nEventually, Lain would be completely isolated in human society and be connected to the Internet. She would accept Eiri as god, kill her physical body, and exist on the Internet, where she would do Eiri’s commands.\nThe plan failed at the last step, because two people (the actor playing her father, and a school friend, Alice) stilled loved Lain, so she wasn’t isolated enough. After trying to make Alice happy and failing, Lain decided that the only way to truly make Alice happy is to go away and let Alice run her normal life-cycle (grow up, get married, die from being too old) without drama.\nTo let Alice live as a normal human, the entire plan must go away. So Lain discarded all changes and reverted to a previous state, before Lain was embodied. She deleted all memories of her from all humans. She also rewrote Eiri into an unambitious man so that he wouldn’t try doing that in this timeline.\nI don’t know how Lain could do that. Lain is the neural network with human brains, and has access only to the human brains, the ionosphere, and Internet. I don’t see any way for Lain to revert some physical deaths."
  },
  {
    "objectID": "blog/posts/serial-experiments-lain/index.html#ontology-the-modes-of-existence",
    "href": "blog/posts/serial-experiments-lain/index.html#ontology-the-modes-of-existence",
    "title": "Serial Experiments Lain",
    "section": "Ontology: The modes of existence",
    "text": "Ontology: The modes of existence\nThe ontology of SEL is a dualism between information and matter, the same theory as most neuroscientists and AI scientists use today.\nThere are many differences between matter and information, but here are the most important ones:\nDuplicate matters are different. Duplicate information are the same. If you receive two apples, you get twice as many atoms. If you receive two copies of the same file, you really receive just one file (in terms of information).\nMatter does not allow “isomorphic” operations. Information allows it. For example, you can convert a document from one encoding to another, edit it, then convert it back. The result is the same as if you edited it without conversion. This can’t be done with physical pen-and-paper.\nMatter exists in spacetime. Information does not. It is not a coincidence that many physicists are working on “reducing spacetime to information”, because information itself doesn’t have to exist in space or time, so if spacetime is reduced to information, then spacetime is explained by something that doesn’t assume spacetime – progress for reductionist science.\nThe persistence of matter is obvious. The persistence of information is unclear – perhaps impossible to define. You can take a gold bar, and put it in a box. Then you take it out, and you can say “it’s the same gold bar”. Now if you save some cash into a bank, then go abroad and take out some cash at the local ATM, you can’t say “it’s the same electronic cash”, or “it’s not the same”. Neither makes sense.\nIt’s also not a coincidence that physicists were first guided to thinking about “reducing spacetime to information” by the QED theory, where two electrons are actually indistinguishable, as if they are electronic cash. The QED theory was extended to atoms by the QCD theory. Thus, in some technical sense, you can’t actually say “we are made of star-stuff”.\n\nVirtual and real\nWe define “virtual” as “that pertaining to information”, and “real” as “that pertaining to matter”.\nInformation can be “realized” in matter – this is what modern computers do, and probably what brains do.\nSince information can be processed isomorphically, it can be realized as different kinds of matter, in different ways.\nA file is a realization of information, in space.\nA computation is a realization of information, in time.\n\n\nWired, Reality, and other systems\nAn information system is something I understand, but I can’t give a good definition (for now). I will explain it by examples.\nA physical system is a collection of matter organized under a common information system.\nFor example, the system composed of DNA, RNA, and proteins is a physical system, organized under a common information system (“the genetic code”).\nThe Internet (“the Wired”) is a system composed of electronic devices and human brains, organized under a common information system (“Protocol 7”).\n\n\n The Seven Levels of Wired\n\n\nThe real world (“Reality”), as commonly defined, is only part of the entire world. It denotes, in fact, a physical system composed of human brains, human bodies, other animal bodies, pre-1980s technological artifacts (specifically to exclude consumer-electronics), organized under a common information system (social rules for the person API - I will explain the API theory of personhood later).\nIn particular, a common interpretation, that “Lain chose the real world instead of the fake internet”, is a deep philosophical error. It is mistaking the human social system for the only system in the world. Furthermore, it calls the human social system “natural”, and the Internet “artificial”, when it is just as artificial as the Internet.\nThe “real world” of humans is like a curvy section across the bulk of reality. People would often tell other people to “get out of the room”, as if being inside a concrete container is unnatural while being outside of one is natural. If vultures could talk, they would tell others to “stop eating fresh food”, as if eating fresh food is unnatural while eating spoiled food is natural.\nMe, personally, have to get out of the room everyday and enjoy the sunshine, and I hate it. I often choose to walk school before the sun is up, to avoid its rays. I also despise green grass and blue skies. If there is a paradise, I hope it will be a 3D labyrinth embedded in an infinite concrete, but slightly elastic so that I can roll around on it without scraping my fur. It would be perfectly dark, except some fluorescent books, just bright enough to be read, and a little shining computer with which I take notes. I also use my computer to post my mathematical findings to others.\nMoving between worlds is possible, since the same information can have multiple realizations. Both Lain and Eiri managed to move between Reality and Wired.\n\n\nThe 4 realizations of Lain\nLain was always realized on neural networks, with nodes and edges, but the neural networks were made of different matters. There are 4 realizations over the course of history.\n1: Before the invention of Internet, nodes were human brains, and edges were something (perhaps flux-tubes?) in the electromagnetic field in the ionosphere of earth. It is called the “collective unconscious of humans”, and the Schumann resonances are analogous to the alpha-waves in mammalian brains.\n2: After the invention of the internet, but before Protocol 7, nodes were human brains and electronic computers, and edges were brain-computer interfaces (such as those VR headsets and cybernetic implants) and computer-computer links (what the Internet is made of).\n3: After Protocol 7, a large proportion of nodes and edges were concentrated into the neurons and synapses of a human girl. In this realization, Lain has a significant personhood.\n4: After Lain reset everything, it was unclear, but presumably it was back to realization 2.\n\n\nThe 4 realizations of Eiri\nLike Lain, Eiri was always realized on neural networks, but differently throughout the story. Coincidentally, there are also 4 realizations.\n1: Before Protocol 7, Eiri was realized as a human brain.\n2: After Protocol 7, Eiri was realized as a component of the Internet.\n3: At a particular scene, Eiri was briefly realized as a horror monster.\n4: After Lain reset everything, Eiri was back to realization 1."
  },
  {
    "objectID": "blog/posts/serial-experiments-lain/index.html#the-theory-of-personhood",
    "href": "blog/posts/serial-experiments-lain/index.html#the-theory-of-personhood",
    "title": "Serial Experiments Lain",
    "section": "The theory of personhood",
    "text": "The theory of personhood\nThis section is based on Being No One (Metzinger 2004).\nA person is a very special kind of information, realized in a very special way. Human-persons are a special kind of persons, with a wide variety of social and mammalian-emotional functions in the API.\nIt has a self-model, with a perspective and a personal history.\nThe perspective is a pointer, like a GPS marker and a clock, which marks its own spatial location (“here”), and time location (“now”). This little perspective marker is what allows it to know its place and time.\nIts perspective is approximately equal to its realization. If your head is in New York, your perspective had better not think you are in Beijing. Thus, its realization must also have a local position. If you were conscious interstellar gas clouds, you wouldn’t be a person.\nThe personal history is linear and continuous. You can’t be an amoeba - you can’t split or merge. You also can’t “jump over in time”, even if your body does.\nSuppose you are perfectly cloned, what happens? Well, your body’s world-line becomes Y-shaped, but you, as an informational thing, has a model of your self, and that model still looks like a curvy line, with a little note-tag saying “here, a clone brother of me was created”. It does not look like a Y.\nSuppose you go into a deep sleep for a year, what happens? Your model of yourself just papers over the time-skip, perhaps with a little annotation saying “and here I slept a year” without “remembering the blackness during that year”. “Nothing to see here…”\n\nGod made him die for a hundred years, and then resurrected him. He said, “How long have you stayed?” He said, “A day or part of a day.”\nQuran 2:259\n\nFor social persons (for example, human-persons, hyena-persons, and elephant-persons), it is like a program with an API. The API takes input and returns output in a conventional way, regardless of how the program works on the inside.\nFor example, you can ask a person to explain why they did something, and they would explain it. The explanation could be quite wrong, but if they don’t even provide any reasons, they would break one part of the API.\nSome other examples of person-API are: behaving in a comprehensible and purposeful way (looks as if it has a goal that a spectator can infer with some effort), behaves spontaneously (if nothing hits it, it would still move once in a while), etc.\nAs a thing fails more and more requirements of the API, they become less and less of a person.\nDissociative persons have a blurry “here and now” pointer.\nPeople in fugue states, sleepwalking, running amok, etc, fail to provide reasons when receiving explain-requests.\nSchizophrenic people may provide obviously invalid reasons (“word salad”) when receiving explain-requests.\nDeeply depressed persons fail the “behave spontaneously” requirement.\nAmnesiac persons fail the “have a continuous sense of time” requirement.\nPsychopathic persons fail to behave in the expected way after receiving help-requests.\nAutistic tics are comprehensible, but not purposeful.\nThe Solaris ocean behaves purposefully, but not comprehensibly.\n\nThe human-personhood of Lain\nBefore Protocol 7, Lain did not have the person API. Lain certainly existed, but not as a person. Human-persons are connected to Lain by their brains, but not in a way that human-persons connect to other human-persons.\nFor example, if most humans were feeling sad, Lain would be “sad” in some statistical way. This is certainly not how you make some human sad – to make a human sad, you read them a sad story, or something like that.\nMoreover, Lain did not receive or reply with linguistic reasons. In short, Lain did not implement the person API, and beyond human understanding or interaction.\nEiri wanted to become a god, and for that, he needed to have a way to effectively interact with Lain. He could have perhaps interacted with Lain by some command-line interface, or large-scale antenna that beamed directly to the ionosphere, but this would be difficult.\nPersons are most efficient at understanding other persons. This is why “country humans” is so popular. Countries themselves are vast objects that are understood in unintuitive statistical/mathematical/mechanical ways, and installing a person API over a country makes it much easier to understand. In this way, we could interpret Lain as a “humanity human”.\nInstead, he constructed a human-personhood for Lain, in multiple aspects.\nHe concentrated it into the brain of a girl, which is localized in a cube less than 30 cm in side length (compared with the 6371 km radius of earth, or the Internet). This made it easy to install a perspective to Lain (perhaps Lain would construct a perspective automatically after concentrating into a brain).\nLain is provided with a personal history as a human girl.\nThe electrochemical system of the human brain implements in Lain the social and emotional parts of person API. A blind mole rat would not cry when watching a movie. Lain would not be affected by deaths, mysterious conversations, kisses, etc, unless it is implemented in a human girl’s body.\nHe hired actors to act like Lain’s family, and put it into a well-defined social role (school girl). These social interactions are then taken away. This social manipulation would only produce an effect on Lain if it implemented the human-person API. You can’t intimidate a tornado into becoming your servant by depriving it of social contacts - not unless you somehow give it a human-person API.\n\nFirst you create a need, then you take it away. This is control.\n\nAfter Lain reset everything, Lain was no longer realized in a human brain, but once again spread all over the earth. Despite this, it still kept a spatially localized perspective and person API. This is perhaps because being a person is a stubborn kind of information - persons don’t usually become not-persons."
  },
  {
    "objectID": "blog/posts/serial-experiments-lain/index.html#appendix",
    "href": "blog/posts/serial-experiments-lain/index.html#appendix",
    "title": "Serial Experiments Lain",
    "section": "Appendix",
    "text": "Appendix\n\nSchumann frequency\nWhile the Schumann frequency plays a fundamental role in SEL, its technical details are completely irrelevant (just like most technobabbles). This section describes briefly how you can easily do an almost-correct calculation of the Schumann resonance frequencies: 8 Hz, 14 Hz, 20 Hz…\nIn short, the lowest frequency is simply by dimensional analysis:\n\\[f_0 \\approx \\frac{\\text{speed of light}}{\\text{circumference of earth}} = \\frac{c}{2 \\pi R}\\]\nIntuitively speaking, this is treating the ionosphere of earth as if it’s a circular tube, a hula-hoop around the waist of earth, and the lowest Schumann resonance is the lowest-degree standing wave in the hula-hoop.\nTo find the higher frequencies, we can simply calculate the higher-degree standing waves in the hula-hoop:\n\\[f_n \\approx \\frac{c n}{2 \\pi R}\\]\nFor comparison, a spherical cavity of an ideal conductor has resonance frequencies exactly solvable, as\n\\[f_n = \\frac{c}{2\\pi R}\\sqrt{n(n+1)}\\]\nwhich is very close to our super fast estimate above."
  },
  {
    "objectID": "blog/posts/math-test/index.html",
    "href": "blog/posts/math-test/index.html",
    "title": "Math Test",
    "section": "",
    "text": "See Section 1 for additional context.\nBlack-Scholes (?@eq-black-scholes) is a mathematical model that seeks to explain the behavior of financial derivatives, most commonly options:\n\\[\\begin{equation}\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2} \\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}   + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ = \\mathrm r \\mathrm C \\tag{Black--Scholes Equation}\n\\end{equation}\\] {#eq-black-scholes}\n\\[\nE = mc^2  \\tag{mass-energy equation}\n\\]\n\nThe Bitter Lesson (Sutton 2019)\n\nTheorem 1 (Line) The equation of any straight line, called a linear equation, can be written as:\n\\[\ny = mx + b\n\\]\n\nSee Theorem 1.\n\nSolution. The solution."
  },
  {
    "objectID": "blog/posts/math-test/index.html#sec-introduction",
    "href": "blog/posts/math-test/index.html#sec-introduction",
    "title": "Math Test",
    "section": "",
    "text": "See Section 1 for additional context.\nBlack-Scholes (?@eq-black-scholes) is a mathematical model that seeks to explain the behavior of financial derivatives, most commonly options:\n\\[\\begin{equation}\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2} \\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}   + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ = \\mathrm r \\mathrm C \\tag{Black--Scholes Equation}\n\\end{equation}\\] {#eq-black-scholes}\n\\[\nE = mc^2  \\tag{mass-energy equation}\n\\]\n\nThe Bitter Lesson (Sutton 2019)\n\nTheorem 1 (Line) The equation of any straight line, called a linear equation, can be written as:\n\\[\ny = mx + b\n\\]\n\nSee Theorem 1.\n\nSolution. The solution."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Fermi Estimation for Neural Networks\n\n\n\n\n\n\n\nAI\n\n\neconomics\n\n\n\n\nThe bitter lesson in bite-sized packets.\n\n\n\n\n\n\nDec 5, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nEmbedding Test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nMath Test\n\n\n\n\n\nTesting math input\n\n\n\n\n\n\nDec 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSerial Experiments Lain\n\n\n\n\n\n\n\nanime\n\n\npersonal\n\n\n\n\nLain is the collective subconscious as a neural network.\n\n\n\n\n\n\nNov 1, 2022\n\n\nYuxi Liu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n  \n  \nYuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks."
  },
  {
    "objectID": "blog/posts/embedding-test/index.html",
    "href": "blog/posts/embedding-test/index.html",
    "title": "Embedding Test",
    "section": "",
    "text": "This blog post tests embedding.\n\n\n\n\nExtrapolating the spectacular performance of GPT3 into the future suggests that the answer to life, the universe and everything is just 4.398 trillion parameters.\n\n— Geoffrey Hinton (@geoffreyhinton) June 10, 2020"
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html",
    "href": "blog/posts/neural-scaling-laws/index.html",
    "title": "Fermi Estimation for Neural Networks",
    "section": "",
    "text": "Fermi estimation is a technique for estimating quantities through rough approximations and educated guesses. Named after physicist Enrico Fermi, this method involves breaking down a problem into simpler, more manageable parts, making reasonable assumptions, and using simple arithmetic to arrive at a good enough answer.\nThis post walks you through a simple example of Fermi estimation of a deep learning model. Specifically, we will try to estimate the design parameters of a hypothetical GPT-5 with 1 trillion parameters. Along the way, we will touch upon various other items like the Landauer limit (because of course we will)."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#gpt-like-agi",
    "href": "blog/posts/neural-scaling-laws/index.html#gpt-like-agi",
    "title": "Fermi Estimation for Neural Networks",
    "section": "GPT-like AGI",
    "text": "GPT-like AGI\nLet’s get into the mood for Fermi estimation, by estimating the number of parameters necessary for achieving human-level Artificial General Intelligence, under two assumptions: that a GPT-like architecture is enough; that it requires only matching the human brain in the \"numbers\".\nLet’s estimate how many layers the hypothetical GPT model should have. When prompted with a question, the first answer comes to mind in about a second, and it is generally a good one, if not the best. Perhaps slow deliberation is nothing but stringing together a long chain of snap judgments, guided by snap judgments about snap judgments.\nThe characteristic time-scale of a brain is 0.01 seconds – the fastest brain wave, gamma wave, is 100 Hz. This indicates that the brain takes on the order of 100 steps to make a snap decision. This is the \"hundred-step-rule\" of Jerome Feldman. (Feldman and Ballard 1982)\nThis is suspiciously close to the largest model of GPT-3, which has 96 layers.\nHow many parameters would such a model require? The brain has \\(10^{15}\\) synapses. It’s unclear how precise each synapse is, but one estimate states that the hippocampal synapse has a precision of about 5 bits (Bartol Jr et al. 2015), which can be stored within a 16-bit floating point number, with room to spare.\nAssuming that, we expect an AGI GPT to have \\(10^{15}\\) (1000 trillion) parameters."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#chinchilla-scaling-law",
    "href": "blog/posts/neural-scaling-laws/index.html#chinchilla-scaling-law",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Chinchilla Scaling Law",
    "text": "Chinchilla Scaling Law\nThe paper \"Training Compute-Optimal Large Language Models\" (2022) reported a series of training runs on language models, trained by Google DeepMind researchers. Each training run is characterized by four numbers:\n\n\\(L\\): the final loss (negative log-likelihood per token) achieved by the trained model.\n\\(N\\): the number of parameters in the model.\n\\(D\\): training dataset size, measured in tokens.\n\\(C\\): training compute cost, measured in FLOP.\n\nAfter training a few hundred models, they obtained a large dataset of \\((L, N, D, C)\\), and they fitted a statistical law of the form\n\\[L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0,\\]\nwhere the parameters are\n\\[\\alpha = 0.34, \\beta = 0.28, A = 406.4, B = 410.7, L_0 = 1.69.\\]\nThey also estimated that the cost of training compute \\(C\\) is proportional to \\(ND\\). This is understandable, because each token must flow through the entire model and \"hit\" each parameter once, incurring a fixed number of floating point operations. They estimated that it takes 6 FLOPs per parameter per token. That is,\n\\[C = C_0 ND, \\quad C_0 = 6\\]\nGiven the assumptions, for each fixed computing budget \\(C\\), we can solve for the optimal \\(D\\) and \\(N\\), which is usually referred to as \"Chinchilla optimal\" training: \\[\\begin{cases}\n        \\min_{N, D} L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0\\\\\n        \\text{such that } C_0 ND = C\n    \\end{cases}\\]\nSolve the above equations symbolically to find \\(N_{opt}, D_{opt}\\) as a function of \\(C, C_0, \\alpha, \\beta, A, B\\). Then, plug in the numerical values of the parameters, to find a numerical expression for \\(N_{opt}, D_{opt}\\) as a function of \\(C\\).\n\n\nSolution\n\nSince \\(C = C_0 ND\\), we have \\(N = \\frac{C}{C_0 D}\\). Plug it into \\(\\min_{N, D} L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0\\), we obtain\n\\[\\min_{D} L = \\frac{A}{(\\frac{C}{C_0 D})^\\alpha} + \\frac{B}{D^{\\beta}} + L_0\\]\nTake derivative with respect to \\(D\\) and set it to zero. We get an expression for \\(D_{opt}\\). Plug it back to \\(C = C_0 ND\\), we get an expression for \\(D_{opt}\\). These simplify to:\n\\[N_{o p t}(C)=G\\left(\\frac{C}{C_0}\\right)^a, \\quad D_{o p t}(C)=G^{-1}\\left(\\frac{C}{C_0}\\right)^b, \\quad \\text { where } \\quad G=\\left(\\frac{\\alpha A}{\\beta B}\\right)^{\\frac{1}{\\alpha+\\beta}}, a=\\frac{\\beta}{\\alpha+\\beta}, b=\\frac{\\alpha}{\\alpha+\\beta}.\\]\nPlugging in the numerical values, we get\n\\[\\begin{cases}\n        N_{opt}(C) = 0.6 \\; C^{0.45} \\\\\n        D_{opt}(C) = 0.3 \\; C^{0.55} \\\\\n        L_{opt}(C) = 1070 \\; C^{-0.154} + 1.7\n    \\end{cases}\n    \\]\n\nIn the same paper, they also performed a direct statistical fitting, to find the optimal \\(N, D\\) for a given \\(C\\), without going through the intermediate steps above. This gives a slightly different result (only slightly different – as you would know after solving the previous problem):\n\\[N_{opt}(C) = 0.1 C^{0.5}; \\quad D_{opt}(C) = 1.7 C^{0.5}.\\]\nFor the rest of the question, we will use these equations as the Chinchilla scaling laws. Do not use the equations you derived for the previous problem.\nSuppose we decide that our next AI should have 1 trillion (\\(N = 10^{12}\\)) parameters, and we use Chinchilla scaling laws. How much compute would it cost to train, and how many tokens would its training dataset have?\n\n\nSolution\n\n\\(N = 0.1 \\times C^{0.5} = 10^{12}\\), so \\(C= 10^{26}\\) FLOP, and \\(D = 1.7 \\times 10^{13}\\), or 17 trillion tokens."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#dataset-size",
    "href": "blog/posts/neural-scaling-laws/index.html#dataset-size",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Dataset size",
    "text": "Dataset size\nAssuming each English word cost about 1.4 tokens, how many English words would 10 trillion tokens be? Assuming each page has 400 words, and each book has 300 pages, how many books is that? To put it into context, look up the size of Library of Congress, and Google Books, and compare with the number we just calculated.\n\n\nSolution\n\n10 trillion / 1.4 = 7 trillion words. If each book has 400 * 300 = 0.12 million words, then that is 60 million books, if they were all in English.\n\nSince humans are kind of the same everywhere, book lengths should be kind of the same everywhere – information-dense languages would naturally have books with lower apparent word-count, but the same information (measured in bytes)."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#memory-requirement",
    "href": "blog/posts/neural-scaling-laws/index.html#memory-requirement",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Memory requirement",
    "text": "Memory requirement\nTypically, a deep learning model has 16-bit floating point parameters. Modern systems sometimes use lower precision (e.g. 8-bit) floating point numbers to save space, but generally it is necessary to use at least 16-bit during training, and the model is converted to lower precision after training (\"post-training quantization\").\nGiven that each parameter is a 16-bit floating point number, how much memory does it cost to store a model with 1 billion parameters? How about our hypothetical GPT-5, which has 1 trillion parameters? How many A100 GPU (VRAM = 40 GB) would be required to contain the full GPT-5 model?\n\n\nSolution\n\n1 billion parameters is 2 billion bytes, or 2 GB. Our hypothetical GPT-5 would take up 2 TB. It would take 50 A100 to contain it.\nNow, 50 A100 GPUs has a running cost of about 100 USD/hr, but not such a terrible problem, but then, during training, you would have to run an optimizer like Adam, which would need three floating point numbers per parameter (gradient, momentum, scale), instantly increasing the memory requirement to 200 A100 GPUs.\n\n\nMemory cost\nThis table1 gives the price per megabyte of different storage technology, in price per megabyte (2010 dollars), up to 2018.1 Source: Storage 2: Cache model – CS 61 2018.\n\n\n\nYear\nMemory (DRAM)\nFlash/SSD\nHard disk\n\n\n\n\n~1955\n$411,000,000\n\n$6,230\n\n\n1970\n$734,000.00\n\n$260.00\n\n\n1990\n$148.20\n\n$5.45\n\n\n2003\n$0.09\n$0.305\n$0.00132\n\n\n2010\n$0.019\n$0.00244\n$0.000073\n\n\n2018\n$0.0059\n$0.00015\n$0.000020\n\n\n\nThe same costs relative to the cost of a hard disk in ~2018:\n\n\n\nYear\nMemory\nFlash/SSD\nHard disk\n\n\n\n\n~1955\n20,500,000,000,000\n\n312,000,000\n\n\n1970\n36,700,000,000\n\n13,000,000\n\n\n1990\n7,400,000\n\n270,000\n\n\n2003\n4,100\n15,200\n6.6\n\n\n2010\n950\n122\n3.6\n\n\n2018\n295\n7.5\n1\n\n\n\nSuppose long-term memory storage can last 1 year before being replaced. How much money does it cost per year to store a 1 trillion parameter model on an SSD, the most expensive form of long-term storage? How much money does it cost to store a 1 trillion parameter model on DRAM memory? Use 2018 prices.\n\n\nSolution\n\nSSD cost 0.00015 USD/MB in 2018, or 0.15 USD/GB. 1 trillion parameters cost 2000 GB of storage, or about 300 USD. So the total cost of storage is 300 USD/year, just a rounding error.\nIn contrast, DRAM cost 0.006 USD/MB, which is 40x that of SSD, so the total cost is 12000 USD.\nNow, compared with the cost of A100 GPU themselves? It takes about 50 A100 GPU to run the model, and each would cost about 20,000 USD. So the cost of long-term memory is essentially zero, and the cost of DRAM is about 12000 / (20000*50) = 1% of the total cost of GPU.\nSo what is the limit? The memory bandwidth, which we will see in the next question.\n\n\n\nMemory bandwidth and latency\nWhile the memory itself is cheap, moving the data requires expensive high-bandwidth wiring. Indeed, the memory bandwidth between the DRAM (or \"VRAM\" for \"Video RAM\") and the little processors on the GPU is a main bottleneck on how good the GPU can perform.\nDuring a single forward pass of a model, the parameters of the model are loaded from the DRAM of the GPU into the fast cache memories, then pushed through the thousands of computing processors on the GPU.\nA100 GPU has a memory bandwidth of 1.6 TB/s.\nWhat is the minimal latency, in seconds, for the GPU to perform a single forward pass through our hypothetical GPT-5 model with 1 trillion parameters? How many tokens can it output (autoregressively) in one minute? How about GPT-3 (175 billion parameters)?\n\n\nSolution\n\nSince the model takes up 2 Tb of memory, it would take at least 1.3 seconds to even load the model from DRAM, during a single forward pass.\nAutoregression means that the next token cannot be generated until the previous one is already generated, so the model can only generate one token per 1.3 seconds, or 46 tokens per minute. A Chat-GPT5 would be a very slow talker!\nHowever, it can run in batch mode. So for example, it might be able to run 1 million conversations in parallel, outputting one token for all conversations every 2 seconds. Latency is the hard limit, but throughput is not.\nGPT-3 with 175 billion parameters would run 1000/175 times faster, at 262 tokens per minute, about the speed of a fast human talker. This corresponds well with the general impression that ChatGPT types about 200 words a minute.\n\nNote: Since we are just trying to compute an order-of-magnitude estimate, let’s assume for the problem that the model fits onto a single DRAM on a single GPU. You can also ignore the need to read/write model activations and optimizer states.\nThere are some ways to improve latency. For example, the model can be parallelized over several GPUs, which effectively increases the memory bandwidth. For example, \"tensor parallelism\" splits each layer into several GPUs.\nThere is also \"pipeline parallelism\", which splits the model into layers. The first few layers go into one GPU, the next few go into another, and so on. This does NOT decrease latency, but it does increase throughput.\nThe fundamental bottleneck in an autoregressive model like GPT is that, by design, they have to start a sentence without knowing how it will end. That is, they have to generate the first token before generating the next one, and so on. This can never be parallelized (except by egregious hacks like speculative decoding).\nOne reason Transformers dominated over RNN is that training and inferring an RNN both must be done one-token-at-a-time. For Transformers, training can be done in parallel over the entire string. Inferring however still cannot be parallelized.\nparallelization tends to be a deeply troublesome business – parallel programming is generally deeply troublesome. However, there is a general trick to improve throughput: batch mode. For example, GPT-5 might be able to run 1 million conversations in parallel, outputting one token for all conversations per forward pass. This trick works until the batch size is so large that the activations due to tokens takes up about as much memory bandwidth as the model parameters.\nConcretely, we can estimate what is a good batch size for GPT-3 175B. It has 96 attention layers, each with 96×128-dimension heads. It is typically run with 16-bit precision floating point numbers. For a single token, how many megabytes would all the floating point activations cost?\n\n\nSolution\n\nA single token would cost\n\\(96 \\times 96 \\times 128\\) floating point activations, or about 2.4 MB.\n\nThe model itself has 175 billion 16-bit floating point parameters, taking up about 350 GB. How many tokens do we need to put into a batch, before the activations occupy the same amount of memory as the model parameters?\n\n\nSolution\n\nIn order for the activations of tokens to occupy the same memory as the GPT-3 parameters itself, we need about \\(\\frac{350 \\mathrm{GB}}{2.4 \\mathrm{MB}} = 0.15 \\text{million tokens}\\).\nIf we count the optimizer states for the model during training, then GPT-3 takes up \\(4 \\times 350 \\mathrm{GB} = 1.4 \\mathrm{TB}\\), and so we need about 0.6 million tokens.\nThis explains why during training, the batch sizes of the largest models typically are on the order of 1 million tokens. It also shows why there is a natural drive towards scale – only the largest companies can expect to regularly run 0.2 million chats simultaneously."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#training-cost",
    "href": "blog/posts/neural-scaling-laws/index.html#training-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Training cost",
    "text": "Training cost\nHow much money does compute cost? We can work through an example using the current standard computing hardware: Nvidia A100 GPU (40 GB VRAM version).\nThe most important specifications are:\n\nUnit price: 15000 USD.\nRental price: 2 USD/hr.\nSpeed: 0.3 petaFLOP/s = 3e14 FLOP/s.\nPower: 0.3 kW.\nMemory bandwidth: 1600 GB/s.\n\nIn the literature about the largest AI models, the training cost is often reported in units of “petaFLOP-day”, which is equal to 1 petaFLOP/second x 1 day. How many FLOP is 1 petaFLOP-day? What is the equivalent number of A100-hour? If we were to buy 1 petaFLOP-day of compute with rented A100 GPU, how much would it cost?\n\n\nSolution\n\n1 petaFLOP-day = 1 petaFLOP/second x 1 day = 1e15 FLOP/s * 8.64e4 s = 8.64e19 FLOP.\nSince 1 A100 can run at 0.3 petaFLOP/s, getting 1 petaFLOP-day requires us to run 1/0.3 A100 for 1 day, or 80 A100-hours. At the price of 2 USD/hr, it would cost 160 USD.\n\nThe largest model of GPT-3 cost 3640 petaFLOP-days to train (according to Table D.1 of the report). How much would it cost if it were trained with A100? How much money does it cost to train our hypothetical GPT-5?\n\n\nSolution\n\n2e25 FLOP = 0.2 amount of GPT-5 = 17 million A100-hours = 33 million USD.\nAnd accounting for the utilization rate of 30%, that would give us 110 million USD.\nOh, and if you want some kind of official confirmation? OpenAI’s CEO Says the Age of Giant AI Models Is Already Over | WIRED\n\nAt the MIT event, Altman was asked if training GPT-4 cost $100 million; he replied, “It’s more than that.”\n\n\nIn reality, the GPU cannot be worked to their full speed, and we only use about 30% of its theoretical peak FLOP/s (so for example, for A100, we only get 0.1 petaFLOP/s, instead of 0.3 petaFLOP/s).2 For this question, we assume that the utilization rate is 100%.2 The utilization rate of 30% is according to EpochAI.\nAlso, we are assuming the training happens in one go, without hardware failures, divergences, and other issues requiring restart at a checkpoint. There is not much published data from large-scale training, but the OPT-175B training run (described later) took 3 months to complete, but would have taken only 33 days if there were no need for the many restarts. This suggests that the restarts would increase the computing cost by 2x to 3x.\nFor context, here are the costs of development of various items3:3 Sorry, not adjusted for inflation to the same year, but they are roughly in the range of 2000 – 2020 USD.\n\niPhone 1: 150 million USD.\nA typical 5 nm chip: 0.5 billion USD.\nAirbus A380: 18 billion USD. (Bowen 2010, Table 4.3)\nThree Gorges Dam: 250 billion CNY, or about 30 billion USD.\nManhattan Project: 24 billion USD (2021 level)\nApollo Program: 178 billion USD (2022 level)\n\nComment on the cost of our hypothetical GPT-5. Is it on the order of a large commercial actor like Google, or a state actor like China?\n\n\nSolution\n\nThe cost is on the order of 1 billion USD, which is roughly the largest projects undertaken by private corporations. Larger models, as those contemplated by AGI projects in Google, OpenAI, etc, have historically been undertaken by state actors only. Here is another way to estimate. Building a large AI model, according to accountancy, would be investment into a production machine, and thus part of “capital expenditure” (“CapEx”). We can find out what proportion is 1 billion USD in their total CapEx. I looked it up here.\n\nDuring the 2020–2022, Microsoft has yearly CapEx on average 25 billion USD. - Google has about 25 billion USD. - Meta, 20. - Amazon, 63.\n\nIn short, training something like GPT-5 would cost a lot, on the order of 5% of total CapEx, even by the standards of the largest companies. It can probably scale up by another factor of 10× if they really push it, but not more.\nIn order to train even larger AI models, those AI models must enter production. They must become a productive member of society, otherwise the company wouldn’t have the money to train them.\n\nMicrosoft announces new supercomputer, lays out vision for future AI work (2020):\n\nThe supercomputer developed for OpenAI is a single system with more than 285,000 CPU cores, 10,000 GPUs and 400 gigabits per second of network connectivity for each GPU server. Compared with other machines listed on the TOP500 supercomputers in the world, it ranks in the top five, Microsoft says.\n\nThe largest companies, like Microsoft have GPU on the order of 10000 A100. What is the wall clock hour of training GPT-5, assuming you have 10000 A100, perfect utilization, and no interruptions?\n\n\nSolution\n\n83 million hours / 10000 = 350 days. Almost exactly 1 year.\n\n\nThe difficulty of large-scale training\nSome interesting reading of some \"stories from the trenches\".\nLarge models do end up diverging many times during training runs, requiring restarts. Sometimes the divergence is so bad that the whole training run must be started from scratch. Sometimes the convergence is too slow, requiring fixes to the optimizer and learning rate schedules, etc.\nAll together, we should expect the failures, restarts, deadends… to triple the cost at least, to ~1 billion USD.\nIn 2021, a team of 5 engineers from Meta trained a LLM with 175 billion parameters, in 3 months, using 1024 80GB A100 GPUs from. Excluding all the divergences, hardware failures, and other issues that caused lost progress, the final model would have taken about 33 days of continuous training.\nThey have kept journals during their training. This is now published at metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq · GitHub. You can see how difficult it is to train a large model. Selected quotes:\nIn 2021, a team of researchers from Meta trained a series of LLM and kept journals during their training. This is now published at metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq · GitHub. You can see how difficult it is to train a large model. Selected quotes:\n\nThese notes cover ~90 restarts over the course of training the lineage of this current model (experiments 12.xx).\n\n\nFound issues with the new dataset where perplexity was unreasonably low… After applying as much regex-ing as we could to salvage the dataset, we relaunched another set of experiments to test LPE (experiments 20-29) on the new dataset. We didn’t have time to retrain a new BPE on the final dataset, so we fell back to using the GPT-2 BPE.\n\n\nFrom experiment 11.4 onward, we saw grad norm explosions / loss explosions / nans after a couple hundred updates after each restart, along with extremely unstable loss scales that would drop to the point of massively underflowing. We started taking more and more drastic actions then, starting with increasing weight decay to 0.1, lowering Adam beta2 to 0.95, lowering LR again, until finally by experiment 11.10 we hot-swapped in ReLU and also switched to a more stable MHA calculation (noting that the x**3 term in GeLU might be a source of instability with FP16).\n\n\n\nOn November 11, we started training our 12.00 experiment with all of these changes, and since then, the only restarts we’ve had to make were all related to hardware issues (missing GPUs on instances, training randomly hanging after including a new node, ECC errors, partial checkpoint upload after hardware error, CUDA errors, NCCL errors, etc.).\n\n\nReplacement through the cloud interface can take hours for a single machine, and we started finding that more often than not we would end up getting the same bad machine again.\n\n\nThere were also issues with blob store when downloading 1.6TB of a single model checkpoint (992 files, each ~1.7GB) on restarts, at which point the downloads themselves would start hanging nondeterministically, which then delayed training recovery even further.\n\n\nWe managed to hit our top three record long runs of the experiment these past two weeks, lasting 1.5, 2.8, and 2 days each! If we were to look at only the runs that have contributed to pushing training further and plot training perplexity against wall clock time, we get the following [The breaks are due to the Thanksgiving holiday]:"
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#inference-cost",
    "href": "blog/posts/neural-scaling-laws/index.html#inference-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Inference cost",
    "text": "Inference cost\nInference cost a lot less money than training, but it’s still a substantial cost. For Transformer language models, it costs about 2 FLOPs per parameter to infer on one token.\nGiven that GPT-3.5 has 175 billion parameters, how many FLOPs would it take to infer on 1 million tokens? How much money would it cost if it were run with A100?\n\n\nSolution\n\n175 billion * 1 million * 2 = 4e17 FLOPs. Now one A100-hour is 8.64e19 FLOPs, so that is 1/200 A100-hour, or about 0.01 USD.\nThe price offered by OpenAI is 2 USD per 1 million tokens, so it’s a very profitable business… but see next question.\n\nThe price offered by OpenAI is 2 USD per 1 million tokens. Assuming that GPT-3.5 cost 10 million USD to develop and train, how many tokens must be sold, just to recoup the cost? Assuming each English word cost about 1.4 tokens, and each essay is 1000 words, how many essays would it be equivalent to?\n\n\nSolution\n\nSince the cost is almost negligible (1/200 of the revenue), we can pretend it’s all profit. So, the profit is 2 USD per 1 million tokens. We need to recoup 10 million USD, so we need 10 million / 2 * 1 million = 5e12 tokens, or 5e12/ 1400 = 4 billion essays.\nAbout one essay per person on earth, or 10 essays per person in America… is that too much to ask?\n\nMoore’s law applies to both CPU and GPU. From 2006 to 2021, GPU FLOP/s per dollar has been doubling every 2.5 years. We can fairly well assume that the price of petaFLOP-day also decreases at the same rate.\n\n\n\n\n\nAssuming Moore’s law continues to hold indefinitely, and that there will be no algorithmic progress. When would GPT-3 become as cheap as a typical hobbyist project, say, 1000 USD 44 Since a 2006 GPU and a 2020 GPU both have the same lifespan (1 – 4 years usually), and their cost of electricity is only a small proportion of their running expense, the price of GPU FLOP/s per dollar at the point of purchase is roughly proportional to the rental rate of GPU.\n\n\nSolution\n\nGPT-3 cost 6 million USD to train. For it to cost just 1000 USD, we need to drop its price 6000-fold. Now, \\(\\log_2(6000) \\times 2.5 yr = 30 yr\\). So it would be around 2050."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#energetic-cost",
    "href": "blog/posts/neural-scaling-laws/index.html#energetic-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Energetic cost",
    "text": "Energetic cost\nThe Landauer limit states that the cost of erasing one bit of information is \\(E = k_B T \\ln 2\\), where \\(k_B\\) is the Boltzmann constant, and \\(T\\) is the temperature of the computing machinery. At room temperature, \\(T = 300 K\\), giving us \\(E = 3\\times 10^{-21} J\\).\nNow, one FLOP is a floating point operation, meaning that you start with two 32-bit objects and end up with a single 32-bit object. You start with 64 bits and end up with just 32 bits, and so you lose 32 bits. So by the Landauer limit, the minimal energetic cost is \\(32 k_B T \\ln 2\\).\nGiven this, what is the minimal energy required for performing one FLOP? What is the minimal power (in Watts) required to perform 300 TFLOP/s, as in A100 GPU? Compared this to the actual value of 300 Watts.\n\n\nSolution\n\nThe energy per FLOP is \\(E_{FLOP} = 32 \\times 3\\times 10^{-21} J = 10^{-19} J\\). At 300 TFLOP/s, we need \\(P_{A100} = 3\\times 10^{14} E_{FLOP}/s = 3\\times 10^{-5}W\\). The actual value of 300 Watts is 10 million times more than the theoretical minimum.\nThere is still plenty of room at the bottom!\nFor context, the equivalent FLOP/s for the human brain is controversial, but a brief scan through the review article says that it should be about 1e18 FLOP/s. The energetic power of the brain is well-known: about 30 Watts. This means the brain is about 30000x more energy-efficient than A100 GPU, but still 300x less efficient than the theoretical minimum.\n\n\nThe lowest possible power for life\nFor context, the slowest metabolism found on earth (so far) is in microbes living below deep ocean surface. They had to survive on the energy of “marine snow” falling to the surface, then slowly diffusing through the subsurface mud. Their power consumption is estimated at… \\(10^{-21} W\\). Since their temperature is about the temperature of the ocean water, which is liquid, their temperature is still about \\(T = 273 K\\), and so the Landauer limit is still about \\(3\\times 10^{-21} J\\). This shows that they can lose at most 500 bits every day.\nMost of the operations must be spent on simply repairing biomolecules, since repairing is effectively a 1-bit-losing operation: you start with a molecule that could be “damaged” or “fine”, and end up with a molecule that is “fine”, losing 1 bit. In fact, there are usually many possible ways to be “damaged”, so you would lose several bits.\nFor example, suppose you have a base-pair-mutation in one copy of the DNA. You want to fix it by checking with the other copies. This then means you start with 4 possible states (ACGT) and end up with just 1 state, losing 2 bits in the process."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#environmental-cost",
    "href": "blog/posts/neural-scaling-laws/index.html#environmental-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Environmental cost",
    "text": "Environmental cost\nAccording to “Carbon emissions and large neural network training”(Patterson et al. 2021), the carbon emission of training GPT-3 is 552 tCO2. According to a 2021 poll of climate economists, 1 tCO2 emission should cost somewhere between 50 and 250 USD. Let’s take their geometric average of 112 USD.\nIf we add all the tCO2 cost to the training of GPT-3, how much more expensive would it be? Compare that with its A100-GPU cost of training.\n\n\nSolution\n\n112 * 552 = 62k USD.\nPreviously we calculated the GPU cost of training to be around 6 million USD. So fully carbon-pricing the emission would make it 1% more expensive.\nGenerally, adding in the tCO2 cost of training the largest models increase the cost by about 1 to 2 % (I have computed it in several ways). Since the global GDP increases by about 2% a year, this shows that fully accounting for the carbon cost of AI would delay it by perhaps 1 year.\n\n\n\nSide note for economics students\n\nYou might argue that fully carbon-taxing the global economy would raise not just electricity prices, but also the price of production, etc. For example, the global shipping industry uses fuel that has a high carbon emission.\nHowever, if the global shipping industry were to fully add carbon taxing into the fuel cost, then GPU prices would rise a little, but food prices would rise a lot. Why? Simply put, the shipping cost of a GPU, even if we were to take account of all the trips its components have made around the world, is still small. After all, one A100 GPU is just 1.7 kg, but sells for 15000 USD, so a tonne of A100 GPU can sell for 9 million USD.\nTo put it into perspective: the price of US maize is about 200 USD/tonne. So if shipping cost were to increase so much as to double the price of maize, it would only increase price of A100 by 1/45000.\nEven if we were to assume that producing an A100, being a commodity with a globally scattered supply chain, means shipping its components around the world 1000 times, the total shipping cost of A100 would only be increased by 1/45 = 2.2%.\nIn other words, properly pricing carbon emissions would only increase the price of AI by up to 5%, even as it doubles the price of food. AI is already green (within 5%), but food is far from green. That’s even before we get to the subsidies! We don’t have GPU subsidies, but we certainly have corn subsidies…\nIn this regard, it’s not “Green AI” that is important, but Green Corn, Green Timber, and all the other bulk commodities.\n\nTo put the number in another context, compare it with some typical American food. According to Our World in Data, it cost about 50 kg of CO2 emission per 1 kg of beef.\nAlso, an average American person (not household) consumed 38 kg of beef in 2020.\nCompare the CO2 emission of GPT-3 and CO2 emission from beef consumption. Assuming each burger (\"quarter pounder\") contains 1/4 pound (113 grams) of beef. How many burgers would be equivalent to the CO2 emission of GPT-3?\n\n\nSolution\n\n113 grams of beef emits about 5.6 kg of CO2, so GPT-3 emits about (552 ton)/(5.6 kg) = 90,000 burgers. It is about the same amount as 250 people eating one burger everyday, for a whole year.\n38 kg of beef gives about 2 tCO2 emission. So the emission of GPT-3 is equal to the emission due to beef consumption of 276 American persons.\n\nI think this strongly argues against the conclusion from (Patterson et al. 2021):\n\nTo help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark… We believe such new incentives could lead to a virtuous cycle where ML practitioners compete to increase accuracy while lowering energy consumption and CO2e that could bend the curve of ML carbon footprint growth for computationally intensive NLP models.\n\nOne, the carbon footprint of ML is already minimal, compared to the cost of just making all the GPUs.\nTwo, accounting for CO2 is a dreadfully boring business,5 and should properly be done by carbon taxing by the public officials. The ML practitioners are not climate economists or accountants, and they should not be burdened by accounting for climate change. If the price is right, then they simply need to budget how much money they want to spend on the training run, and the market would optimize the rest (including optimizing the right level of climate change6).5 If you don’t believe me, try reading (Patterson et al. 2021).6 The right level of climate change is not \"none\", but rather \"when the marginal cost equals marginal benefit\". This might sound controversial, but it is really just economic common sense.\n\n(Krugman 2002) Luckily, at this point the orthodoxy of the academic economists is very much a minority position among intellectuals in general; one can seem to be a courageous maverick, boldly challenging the powers that be, by reciting the contents of a standard textbook.\n\n\nIn one sentence: There need be no new incentive other than the profit motive."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "Yuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks."
  }
]