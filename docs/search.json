[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "If you thought the Stroop test is hard, just wait until you have tried this.\nPlay online | Play offline"
  },
  {
    "objectID": "projects/index.html#double-stroop",
    "href": "projects/index.html#double-stroop",
    "title": "Projects",
    "section": "",
    "text": "If you thought the Stroop test is hard, just wait until you have tried this.\nPlay online | Play offline"
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html",
    "href": "blog/posts/wigner-rotation/index.html",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "",
    "text": "In the eyes of a geometer, special relativity is not mysterious, but it is simply another geometry that resembles our familiar Euclidean geometry of the plane, and the spherical geometry of the earth. Just like how Euclid compressed the geometry of the plane into around 20 axioms, we can compress the geometry of spacetime into the following axioms:\n\nSpacetime is a smooth structure, shaped like1 the real vector space \\(\\mathbb{R}^{n+1}\\), where \\(n\\) is the number of spatial dimensions, and \\(1\\) is the number of temporal dimensions.\nSpacetime is symmetric under rotation, translation, and constant velocity motion.\nThere exists a fundamental speed, a “conversion factor” between the spatial and temporal dimensions, which we call \\(c\\).\nThere are many ways to map spacetime into \\(\\mathbb{R}^{n+1}\\), but only some of them are physically meaningful. Those are called the (inertial) coordinate systems.\nYou can go from any one coordinate system to any other by a coordinate transformation.\nTransformations can be combined one after another, and they can be reversed. That is, they make up a group, named the symmetry group of spacetime.\nThe symmetry group is smooth and connected, meaning that starting with the “do-nothing” identity transformation, you can combine it with tiny transformations, and end up at any other transformation.\n\n1 In jargon, it is isomorphic as a smooth manifold to \\(\\mathbb{R}^{n+1}\\), or diffeomorphic to \\(\\mathbb{R}^{n+1}\\).Do not worry if some terms were used without a precise definition. You can start with your best intuitive guess, and sculpt them as we go.\nWe will follow the spirit of Erlangen program, which states clearly what our target is (Klein 1893):\n\nGiven a manifoldness [smooth space] and a group of transformations of the same; to develop theory of invariants relating to that group. This is the general problem, and it comprehends not alone ordinary geometry, but also and in particular the more recent geometrical theories which we propose to discuss, and the different methods of treating manifoldnesses of \\(n\\) dimensions.\n\nConsidering that Felix Klein first stated it in 1872, decades before special relativity, this is prescient.\nApplied to special relativity, our target is to start with the axioms, find out something about what kinds of symmetries there can be, and what kinds of objects are unchanged by symmetries. In other words, we will study the geometry of spacetime.\n\n\n\nSo far, while we were talking about “spacetime”, we were actually talking about mathematical objects. There is no necessary connection between the mathematical objects and whatever we observe in the real world. So why do we call those mathematical objects with physically suggestive names like “spacetime”?\nWell, special relativity is really a mathematical object. It is connected to the physical world like all objects in mathematical physics: by experiments. Experimental data is physical, and by studying the mathematical-object special relativity, we found that it can generate the experimental data with a minimal amount of “glue” between the physical world and the mathematical world. The “glue” looks like:\n\nPhysical events in physical spacetime are mathematical points in mathematical spacetime.\nPhysical light moves at the mathematical speed of \\(c\\)\nPhysical clocks tick at a mathematical constant rate.\nIf you move a physical clock from physical-here to physical-there, it is still ticking at the same mathematical constant rate.\n…\n\nEinstein’s The Meaning of Relativity* (Albert Einstein 1922) details physical and mathematical correspondences, such as clock synchronization and the movement of meter sticks.\n\nAs far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#relativity-in-general",
    "href": "blog/posts/wigner-rotation/index.html#relativity-in-general",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "",
    "text": "In the eyes of a geometer, special relativity is not mysterious, but it is simply another geometry that resembles our familiar Euclidean geometry of the plane, and the spherical geometry of the earth. Just like how Euclid compressed the geometry of the plane into around 20 axioms, we can compress the geometry of spacetime into the following axioms:\n\nSpacetime is a smooth structure, shaped like1 the real vector space \\(\\mathbb{R}^{n+1}\\), where \\(n\\) is the number of spatial dimensions, and \\(1\\) is the number of temporal dimensions.\nSpacetime is symmetric under rotation, translation, and constant velocity motion.\nThere exists a fundamental speed, a “conversion factor” between the spatial and temporal dimensions, which we call \\(c\\).\nThere are many ways to map spacetime into \\(\\mathbb{R}^{n+1}\\), but only some of them are physically meaningful. Those are called the (inertial) coordinate systems.\nYou can go from any one coordinate system to any other by a coordinate transformation.\nTransformations can be combined one after another, and they can be reversed. That is, they make up a group, named the symmetry group of spacetime.\nThe symmetry group is smooth and connected, meaning that starting with the “do-nothing” identity transformation, you can combine it with tiny transformations, and end up at any other transformation.\n\n1 In jargon, it is isomorphic as a smooth manifold to \\(\\mathbb{R}^{n+1}\\), or diffeomorphic to \\(\\mathbb{R}^{n+1}\\).Do not worry if some terms were used without a precise definition. You can start with your best intuitive guess, and sculpt them as we go.\nWe will follow the spirit of Erlangen program, which states clearly what our target is (Klein 1893):\n\nGiven a manifoldness [smooth space] and a group of transformations of the same; to develop theory of invariants relating to that group. This is the general problem, and it comprehends not alone ordinary geometry, but also and in particular the more recent geometrical theories which we propose to discuss, and the different methods of treating manifoldnesses of \\(n\\) dimensions.\n\nConsidering that Felix Klein first stated it in 1872, decades before special relativity, this is prescient.\nApplied to special relativity, our target is to start with the axioms, find out something about what kinds of symmetries there can be, and what kinds of objects are unchanged by symmetries. In other words, we will study the geometry of spacetime.\n\n\n\nSo far, while we were talking about “spacetime”, we were actually talking about mathematical objects. There is no necessary connection between the mathematical objects and whatever we observe in the real world. So why do we call those mathematical objects with physically suggestive names like “spacetime”?\nWell, special relativity is really a mathematical object. It is connected to the physical world like all objects in mathematical physics: by experiments. Experimental data is physical, and by studying the mathematical-object special relativity, we found that it can generate the experimental data with a minimal amount of “glue” between the physical world and the mathematical world. The “glue” looks like:\n\nPhysical events in physical spacetime are mathematical points in mathematical spacetime.\nPhysical light moves at the mathematical speed of \\(c\\)\nPhysical clocks tick at a mathematical constant rate.\nIf you move a physical clock from physical-here to physical-there, it is still ticking at the same mathematical constant rate.\n…\n\nEinstein’s The Meaning of Relativity* (Albert Einstein 1922) details physical and mathematical correspondences, such as clock synchronization and the movement of meter sticks.\n\nAs far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#relativity-in-11-dimensions",
    "href": "blog/posts/wigner-rotation/index.html#relativity-in-11-dimensions",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Relativity in 1+1 dimensions",
    "text": "Relativity in 1+1 dimensions\n\nMaking one coordinate system\nTo begin, let’s consider relativity in 1+1 dimensions. We will start with a “default” coordinate system (“the rest frame”).\nIn the beginning was just a point in spacetime. We call this the origin, and write its coordinates as \\(x=0, t=0\\). The world was yet without form. To give it form, we build a meter stick and an infinite number of light bulbs.\nWe put a light bulb at the origin. Because the bulb does not “stand still”, the light bulb traces out a path in spacetime. We call this path the \\(t\\)-axis at \\(x=0\\). Because the bulb does not stand still, we cannot properly say it is “at the origin”, but rather “passing the origin”.\n\nany real body must have extension in four directions: it must have Length, Breadth, Thickness, and—Duration. But through a natural infirmity of the flesh, which I will explain to you in a moment, we incline to overlook this fact. There are really four dimensions, three which we call the three planes of Space, and a fourth, Time. There is, however, a tendency to draw an unreal distinction between the former three dimensions and the latter, because it happens that our consciousness moves intermittently in one direction along the latter from the beginning to the end of our lives.\n—The Time Traveller, The Time Machine, Chapter 1\n\nWe put another light bulb to the left of the light bulb passing the origin, separated by one meter stick. This light bulb traces out a line at \\(x=1\\). And so on. Thus, we obtain an infinite number of lines: \\(\\dots, x = -1, x = 0, x = +1, \\dots\\). By subdividing the meter stick, we obtain one line \\(x = r\\) for each real number \\(r\\).\nNow we have measured space, we will measure time. We remove all light bulbs except two: one at \\(x=0\\), one at \\(x=1\\). At origin, the \\(x=0\\) light bulb flashes, and then the light bulb at \\(x=1\\) flashes when it receives the flash. When the light bulb at \\(x=0\\) receives the echo, this is the echo event – and we write it down as \\(x=0, t=2\\). Let spacetime reverberate with shining echoes, and in this way, every point in spacetime receives a unique coordinate \\((x, t)\\).\nNotice that in this construction, the fundamental speed \\(c\\) is equal to one, and if we draw space and time as two perpendicular directions on a graph-paper, then the trajectory of every light is a 45-degree straight line on the graph-paper. These are not fundamental aspects of theory, but are convenient outcomes given by how we constructed the coordinate system.\nAbstractly speaking, a coordinate system is a function that maps a point in spacetime to two real numbers, like \\((x, t): \\mathcal{M} \\to \\mathbb{R}^{1+1}\\). Our coordinate system constructed so far means that:\n\nThe origin-point \\(p_O\\) has coordinates \\(x(p_O) = 0, t(p_O) = 0\\).\nIf a beam of light is traveling to the right, and passes a certain point \\(p\\), then the set of all points on the beam of light is of form \\(\\{q : x(q) - x(p) = t(q) - t(p)\\}\\).\nAnd so on.\n\nSuch explicit distinction between the spacetime itself and the coordinate system is uncommon in physics, and once you are used to it, you should throw it into your subconscious like a muscle memory. However, it is good to keep it in mind for now.\n\n\nThe first moving frame\nThe integrity of the coordinate system is based on the meter stick. If two light bulbs are moving relative to each other, then they cannot be always connected by a meter stick – one of them would bump into the meter stick, or move away from it. In other words, the entire system is static.\nNow, we introduce another frame, moving at a velocity \\(v\\) relative to the first. While there is no fundamental reason to privilege one over the other, we call one the “rest frame” and the other the “moving frame” for convenience.\nThe moving frame constructs its own coordinate system \\((x', t')\\). What must it look like?\nSince the line \\(x' = 0\\) is constructed by the trajectory of the light bulb that passes the origin, we know that the \\(x' = 0\\) line is \\(\\{p : x(p) = v t(p)\\}\\). However, when it comes to the \\(x' = 1\\) line, we have a problem: How long should be the meter stick in the moving frame be? What does it even mean to compare a meter stick in a moving frame with a meter stick in the resting frame?\nThis difficulty is not pedantic. In the resting frame, checking that two meter sticks are equal means taking one, accelerating it then decelerating it, until it overlaps exactly with the other. This does not apply if they are in relative motion.\nThe solution is another “gluing”. Like how we “glued” the mathematical world with the physical world, we also glue one frame with another frame. This is connection, a central idea in differential geometry. However we will just do this intuitively, since in special relativity, connections are done in the most straightforward way.\nTo connect two frames, we divide their difference into tiny steps, and then do those tiny steps one after another. So, if we know how to connect two frames when \\(v\\) is an infinitesimal, then we know how to connect two arbitrary frames by taking an integral.\nSo for now, let \\(v\\) be an infinitesimal. What should be the meter stick in the moving frame? Its meter stick is whatever is necessary so that light speed is \\(1\\) in the moving frame. So, if we know how to tick the moving clock, we know how to build the moving meter stick.\nIn the resting frame, the clock passing the origin makes a tick at every point:\n\\[\\dots, (0, -1), (0, 0), (0, +1), \\dots\\]\nIn the moving frame, the clock passing the origin makes a tick at every point:\n\\[\\dots, (-v \\; ?, -?), (0, 0), (v \\; ?, ?), \\dots\\]\nThe naive choice is \\(? = 1\\). With this seemingly trivial step, we have in fact completely defined special relativity – all else is derivation.\n\n\nWhy \\(? = 1\\)\nBy the smoothness axiom, if \\(v\\) is infinitesimal, the coordinate transformation between the rest frame and the moving frame should be infinitesimally close to the identity.\nSince special relativity does not distinguish left from right, if we were to make the moving frame move at velocity \\(-v\\), then the coordinate transformation should send \\((0, 1)\\) to \\((-\\delta x, 1 + \\delta t)\\) by mirror symmetry.\nNow, if we compose two boosts, first with \\(v\\) then with \\(-v\\), then we would send \\((0, 1)\\) to \\((0, 1+ 2\\delta t)\\). But we really should get back to \\((0, 1)\\) since we are back to the resting frame again. Therefore \\(\\delta t = 0\\).\n\n\n\nThree infinitesimal boosts\n\n\n\n\nThe Lorentz transformations\nIn the resting frame, we constructed the meter stick by shooting out one pulse of light, then listening for the echo that arrives two ticks later. The point at which the echo is reflected is one meter stick away. Equivalently, we can shoot a forward-pointing light cone at \\((0, -1)\\), and shoot a backward-pointing light cone at \\((0, +1)\\). Their intersections are the two ends of two meter sticks: \\((-1, 0), (+1, 0)\\).\nIn the boosted frame, the clock passing the origin ticks at \\(\\dots, (-v, -1), (0, 0), (v, 1), \\dots\\). Therefore, we can construct its meter stick in the same way, and we would find that the two light cones intersect at \\((-1, -v), (+1, +v)\\). This gives us the coordinate transformation:\n\\[\\forall p \\in \\mathcal{M}, \\quad\n\\begin{bmatrix} x(p) \\\\ t(p) \\end{bmatrix} =\n\\begin{bmatrix} 1 & v \\\\ v & 1 \\end{bmatrix}\n\\begin{bmatrix} x'(p) \\\\ t'(p) \\end{bmatrix}\n\\]\nHere, we find that we need to distinguish between “active” and “passive” transformations. Everything we have said so far is “passive”. We assume that there is an underlying spacetime \\(\\mathcal{M}\\), and we are to construct coordinate systems over it. The coordinate transformation tells us how to turn the coordinates of the same point from one coordinate system to another. However, while this is often useful, it would make everything we are going to say next very awkward.\nTherefore, we immediately change our point of view: we are going to study “active” transformations from now on. While passive transformation of a clock means that we have just one clock and measure its ticks in two coordinate systems, active transformation of a clock means that we actually pick up the clock at rest frame, accelerate it on a rocket, and then let it glide at constant velocity at the moving frame, then study what the clock is doing in the rest frame.\nSince an active transformation is the opposite of a passive transform, we have the active Lorentz transformation formula:\n\\[\n\\begin{bmatrix} x' \\\\ t' \\end{bmatrix} =\n\\begin{bmatrix} 1 & v \\\\ v & 1 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ t \\end{bmatrix}\n\\tag{1}\\]\nThis formula states that if we “pick up” the spacetime event at \\((x, t)\\) in the rest frame, and accelerate it infinitesimally to speed \\(v\\), then the spacetime event ends up at \\((x', t')\\) in the rest frame.\n\n\nIntegrating the Lorentz boosts\nSince \\(v\\) is infinitesimal, to find the Lorentz transformation for a non-infinitesimal \\(v\\) (say, \\(v = 10^{-100}\\), which, while small, is really big for an infinitesimal), we need to integrate over it.\nBefore we do so, we should play with the discrete version: what happens if we repeatedly apply the matrix \\(\\begin{bmatrix} 1 & v \\\\ v & 1 \\end{bmatrix}\\) on the vectors \\((0, 1), (1, 0)\\)?\n\n\n\n  \n  Rapidity = 0.05\n\n\n  \n  Number of boosts = 1\n\n  \n\nIn the diagram, \\(v\\) is “rapidity”2. We apply the Lorentz boost with the given \\(v\\) repeatedly forwards and backwards. The resulting image is beautiful and suggestive: it looks like the unit hyperbolas!2 The reason for calling it “rapidity”, instead of “velocity” would be soon clear.\n\n    \n\nHaving guessed the answer, we proceed with the explicit integral to confirm our guess.\n\\[\n  \\begin{aligned}\n  \\begin{bmatrix} 1 & dv \\\\ dv & 1 \\end{bmatrix}^{\\frac{v}{dv}} &= \\left(I + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}\\right)^{\\frac{v}{dv}}\\\\\n  &= \\exp\\left(\\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} v\\right) \\\\\n  &= I + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} v + \\frac{1}{2!}v^2\\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}^2 + \\frac{1}{3!}v^3\\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}^3 + \\cdots \\\\\n  &= I(1 + v^2/2! + v^4/4! + \\cdots) + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}(v + v^3/3! + v^5/5! + \\cdots) \\\\\n  &= I\\cosh(v) + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\sinh(v)\n  \\end{aligned}\n\\tag{2}\\]\nAt this point, we should pause to take a careful look at \\(v\\). While it is the same as velocity when \\(v\\) is infinitesimal, it is not the same as velocity otherwise, because velocity is bounded between \\(-1\\) and \\(+1\\), whereas this \\(v\\) quantity is free to vary over all real values. This is why we call it “rapidity” and relabel it as \\(w\\), since it is like velocity, but different.\nThe velocity \\(v\\) for a given rapidity \\(w\\) is determined by transforming the \\(t\\)-axis using the Lorentz transformation matrix \\(\\begin{bmatrix} \\cosh(w) & \\sinh(w) \\\\ \\sinh(w) & \\cosh(w) \\end{bmatrix}\\). The \\(t\\)-axis \\[\\{(0, s) : s \\in \\mathbb{R}\\}\\] is boosted to the line \\[\\{(\\sinh(w) s, \\cosh(w) s) : s \\in \\mathbb{R}\\}\\] Therefore, the velocity of this boosted frame is \\[w = \\sinh(w) / \\cosh(w)  = \\tanh(w).\\]\nNow you can play with the diagram below to grow a new intuitive sense of how it all ties together. After you have grown this new intuition, you can proceed.\n\n    \n    \n\n\n\nA “spacetime square” would transform like:\n\n  \n\n\n\n3-velocity, 4-velocity, and inner product in spacetime\nIn the diagram above, we have used the terms 3-velocity and 4-velocity. To explain those terms, we briefly go back to 3+1 dimensions.\nWhen we say “velocity” informally, we mean a vector in \\(\\mathbb{R}^3\\). But how is velocity really defined? It is displacement divided by time. In relativity, when we say “3-velocity”, there are two possible representations: we can represent it as \\((v_x, v_y, v_z)\\), or we can represent it as \\((v_x, v_y, v_z, 1)\\). The first is close to how the rest of the world use “velocity”, but the second is close to how special relativity want us to use the word “velocity”. The second representation can be interpreted as follows: “The 3-velocity of an object is the spacetime displacement of the object after one tick of my clock.”.\nNeither is satisfactory, however, because 3-velocity behaves badly under Lorentz transformations. The first representation \\((v_x, v_y, v_z)\\) does not have the time-coordinate, so it can’t even be multiplied with the Lorentz transformation matrix. The second representation \\((v_x, v_y, v_z, 1)\\) would, after a Lorentz transformation, have its time-coordinate \\(\\neq 1\\).\nBoth problems are elegantly resolved if we use the 4-velocity, which means we have to divide 3-velocity by its norm… but what norm? Why, think back to the Erlangen program:\n\nGiven a manifoldness [smooth space] and a group of transformations of the same; to develop theory of invariants relating to that group.\n\nIn plane geometry, we know that although \\(x, y, x', y'\\) are geometrically meaningless, the norm-squared \\((x-x')^2 + (y-y')^2\\) is meaningful, because that quantity is not changed (invariant) if you apply the rotation matrix \\(\\begin{bmatrix}\\cos\\theta & \\sin\\theta \\\\ -\\sin\\theta & \\cos\\theta \\end{bmatrix}\\) to it. What would be an invariant for spacetime?\nLooking at the picture of the Lorentz transformation in 1+1 dimensions, we can see that the intersection of the two light cones appears to have a constant area. In particular, the upper rectangle  has constant area. That is, the quantity \\((x-t)(x+t)\\) is preserved.\n\n    \n\nThus we define the Minkowski norm-squared: \\(\\|(x, t)\\|^2 := x^2 - t^2\\). Note that this is not an actual square because it can be negative. However it is preserved under Lorentz transformations.\nOnce we have a norm-squared, we can extend it to an inner product::\n\\[\\braket{v, w} := \\frac{\\|v+w\\|^2 - \\|v-w\\|^2}{4} = v_x w_x - v_t w_t\\]\nwhich is still an invariant under Lorentz transformations, and thus physically meaningful. This extends in general to n+1 dimensions. For example, in 3+1 dimensions, the inner product is \\(\\sum_{i = x, y, z} v_i w_i - v_t w_t\\).\nWith this, the 4-velocity is at hand! We just need to normalize \\((v_x, v_y, v_z, 1)\\), taking care to remove the negative sign:\n\\[\n(v_x, v_y, v_z, 1)/\\sqrt{-\\|(v_x, v_y, v_z, 1)\\|^2} = \\frac{(v_x, v_y, v_z, 1)}{\\sqrt{1-(v_x^2 + v_y^2 + v_z^2)}}\n\\]\nAs an example of the power of this geometric algebra, we rederive the Lorentz transformation with infinitesimal boost. In the rest frame, the unit clock-tick is the vector \\((0, 1)\\), and the two unit meter sticks are the vectors \\((-1, 0), (1, 0)\\). They are uniquely defined by the two geometric properties:\n\\[\n\\braket{s, (0, 1)} = 0; \\braket{s, s} = 1\n\\]\nTherefore, in the boosted frame, the boosted meter sticks are the two solutions to\n\\[\n\\braket{s', (v, 1)} = 0; \\braket{s', s'} = 1\n\\]\nwhich are \\(s' = (-1, v), (1, v)\\). By continuity, \\((1, 0)\\) cannot have been boosted to \\((-1, v)\\), so it must be boosted to \\((1, v)\\). This gives us Equation 1."
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#relativity-in-21-dimensions",
    "href": "blog/posts/wigner-rotation/index.html#relativity-in-21-dimensions",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Relativity in 2+1 dimensions",
    "text": "Relativity in 2+1 dimensions\n\nInfinitesimal Lorentz transformations\nIn 1+1 dimensions, we have only two spatial directions: left and right. In 2+1 dimensions, there is a whole circle of directions. We call this circle the “space circle”, because the alternative name “hula hoop for spacetime” sounds too silly.\nJust like in 1+1 dimensions, the space circle is constructed by shooting a forward light cone at \\((0, 0, -1)\\) and a backward light cone at \\((0, 0, +1)\\). Their intersection is the space circle. A space circle plus a ticking clock together allows us to define a coordinate system with three unit vectors \\((1, 0, 0), (0, 1, 0), (0, 0, 1)\\). Both the clock and the space circle can be boosted. We will pick the simplest possible way to transport the ticking clock and the space circle, and experiments with Thomas precession would show that this is the right way.\n\n    \n\nSuppose we perform an infinitesimal boost by \\((v_x, 0)\\), then, by the same picture as in the 1+1 dimension case, we know that the clock-tick \\((0, 0, 1)\\) is boosted to \\((v_x, 0, 1)\\), and the space circle is boosted to the ellipse with long semiaxis \\((1, 0, v_x)\\) and short semiaxis \\((0, 1, 0)\\). Where should each point on the space circle go to? Since the space circle is rigid, if we know where one point must go to, we know where all points must go to.\nThe semiaxis \\((0, 1, 0)\\) is both on the original space circle and the boosted space circle. It stands to reason that the simplest pick would preserve it after the infinitesimal boost. That is, we should not “twist” the space circle under boosting. With this choice, we are forced to pick \\((1, 0, v_x)\\) as the boosted \\((1, 0, 0)\\), since it is the unique unit vector that is perpendicular to \\((0, 1, 0), (v_x, 0, 1)\\), and infinitesimally close to \\((1, 0, 0)\\).\nAlternatively, we can think of Lorentz transformation in 2+1 dimensions as the same with Lorentz transformation in 1+1 dimensions, but with an extra dimension. Therefore, we can minimally modify Equation 1 to:\n\\[\n\\begin{bmatrix} x' \\\\ y' \\\\ t' \\end{bmatrix} =\n\\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & 0 \\\\ v_x & 0 & 1 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\\\ t \\end{bmatrix}\n\\]\nthe same result as our previous argument.\nThis extends to the case of an infinitesimal boost \\((v_x, v_y)\\):\n\\[\n\\begin{bmatrix} x' \\\\ y' \\\\ t' \\end{bmatrix} =\n\\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & v_y \\\\ v_x & v_y & 1 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\\\ t \\end{bmatrix}\n\\tag{3}\\]\n\n\n\n\n\n\nFour ways to derive it\n\n\n\n\n\n\nJust guess it.\nRotate the coordinate system so that the boost is in the \\(x\\)-direction, boost using the previous result, then rotate the coordinate system back. To be pedantic: We are performing one passive, then one active, then one passive transformation.\nTake the previous derivation, and modify it by strategically inserting \\(v_y\\) at places.\nSince two infinitesimal boosts do not interact except at the second-order infinitesimal level, and \\(O(v^2) \\ll O(v) \\ll 1\\), we are free to discard the second-order infinitesimal. Therefore, we can just multiply the two matrices together to get the full thing:\n\n\\[\\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & 0 \\\\ v_x & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & v_y \\\\ 0 & v_y & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & v_y \\\\ v_x & v_y & 1 \\end{bmatrix} + O(v^2)\\]\n\n\n\n\n\nFermi–Walker transport\nEquation 3 is correct, but it is still not geometric. We must convert it with only quantities that are invariant under Lorentz transformations – that is, the inner product.\nDenote the 4-velocity vector \\((0, 0, 1)\\) by the letter \\(u\\), and the infinitesimal boost \\((v_x, v_y, 0)\\) by \\(\\delta u\\). Then, an arbitrary vector \\(e\\) is boosted to \\(e + \\delta e\\), Where\n\\[\n\\delta e =\n\\begin{bmatrix} 0 & 0 & v_x \\\\ 0 & 0 & v_y \\\\ v_x & v_y & 0 \\end{bmatrix}\n\\begin{bmatrix} e_x \\\\ e_y \\\\ e_t \\end{bmatrix} = e_t (v_x, v_y, 0) + (e_x v_x + e_y v_y) (0, 0, 1)\n\\]\nThis gives us the Fermi–Walker transport equation:\n\\[\n\\delta e = - \\braket{e, u} \\delta u + \\braket{e, \\delta u}u\n\\tag{4}\\]\nIf you want a more amusing mental image, here it is: Consider a spaceship that looks like a spherical cow porcupine in a vacuum. Every porcupine spine is a rocket engine. The spaceship can boost in any direction, but it does not rotate. Thanks to Wigner rotation, it can rotate anyway. Now, the spaceship is performing some complicated manuever in spacetime. If we allow it to carry around a general vector pointing in an arbitrary direction \\(e\\), the question becomes: as the vector is boosted alongside the spaceship, how does the vector change?\nConsider a problem in general relativity, where we have a vector field over spacetime, and an accelerating observer. To calculate how quickly the vector field is changing relative to the observer, we must account for:\n\nacceleration of the observer, using the Fermi–Walker transport equation;\nthe curvature of spacetime itself, so that comparing one vector with another requires us to perform parallel transport in spacetime (with the “covariant derivative”).\n\n\n\nGeneral Lorentz transformations\nThe Lorentz transformation in 2+1 dimensions can be derived similarly to how Equation 2 was derived. Let the rapidity vector \\(\\vec{w}\\) be equal to \\(w (n_x, n_y)\\), where \\((n_x, n_y)\\) is a unit vector (the direction of rapidity). Then the Lorentz transformation is found by performing another matrix exponentiation:\n\\[\\exp\\left(\\begin{bmatrix} 0 & 0 & n_x \\\\ 0 & 0 & n_y \\\\ n_x & n_y & 0 \\end{bmatrix} w\\right)\\]\nTo be more succinct, we define the matrices\n\\[K_x := \\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 1 & 0 & 0 \\end{bmatrix}, \\quad K_y := \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}\\]\nthen the Lorentz transformation is just \\(e^{\\vec{w} \\cdot \\vec{K}} = e^{w \\vec{n} \\cdot \\vec{K}}\\).\nNow, since \\((\\vec{n} \\cdot \\vec{K})^3 = \\vec{n} \\cdot \\vec{K}\\), we have the Lorentz transformation equation\n\\[\nI + \\sinh(w) \\vec{n} \\cdot \\vec{K} + (\\cosh(w) - 1) (\\vec{n} \\cdot \\vec{K})^2\n\\tag{5}\\]\nThis equation extends naturally to relativity in n+1 dimensions, but we will not need it.\n\n\n\n\n\n\nDeriving \\((\\vec{n} \\cdot \\vec{K})^3 = \\vec{n} \\cdot \\vec{K}\\)\n\n\n\n\n\nLet \\(t\\) stand for the index of the time-coordinate, and let \\(i, j, k\\) be the indices of the space-coordinates. Then we have \\[(\\vec{n} \\cdot \\vec{K})^3 = n_in_jn_k(e_{it} + e_{ti})(e_{jt} + e_{tj})(e_{kt} + e_{tk})\\]\nwhere \\(e_{mn}\\) means the matrix with entry \\((m, n)\\) being one and all other entries being zero. We use Einstein summation convention, so repeated indices means summing over it (except \\(t\\), which is not an index to sum over).\nSince \\(e_{mn}e_{kl} = \\delta_{nk} e_{ml}\\), and \\(i \\neq t, j \\neq t, k \\neq t\\), the above multiplication expands to 8 terms, but only two are nonzero:\n\\[= n_in_jn_k(e_{it}\\delta_{jk} + \\delta_{ij} e_{tk}) = \\underbrace{\\|\\vec{n}\\|^2}_{= 1} (n_i e_{it} + n_k e_{tk}) = n_i (e_{it} + e_{ti}) = \\vec{n} \\cdot \\vec{K}\\]"
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#wigner-rotation",
    "href": "blog/posts/wigner-rotation/index.html#wigner-rotation",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Wigner rotation",
    "text": "Wigner rotation\nWe are ready to derive a rarely discussed effect in special relativity: the Wigner rotation. Although it is as fundamental as time dilation and space contraction, it often goes unmentioned in undergraduate textbooks I have encountered.\n\nTheorem 1 (Wigner rotation) When three boosts are made in a cycle, such as \\(p_1 \\to p_2 \\to p_3 \\to p_1\\), the result is a rotation. The angle of rotation is equal to the hyperbolic area of the triangle \\(p_1 p_2 p_3\\), but in the opposite direction. Furthermore, the hyperbolic area is equal to the angle defect of the triangle.\n\nThe phrase “in the opposite direction” means that, if, looking from the \\(+t\\)-direction down at the origin, you see the three 3-velocities make a counterclockwise cycle in the disk \\(\\{(v_x, v_y, 1) : v_x^2 + v_y^2 = 1\\}\\), then you would see that the Wigner rotation angle is in the clockwise direction, and vice versa. The phrase “angle defect” means \\(\\pi - (\\angle{p_1 p_2 p_3} + \\angle{p_2 p_3 p_1} + \\angle{p_3 p_1 p_2})\\).\n\nStep-by-step demonstration of the Wigner rotation\nConsider the simplest type of three-boost cycle as shown in the figure below. We start at the rest frame, boost to the frame with a 3-velocity of \\((v_x, 0)\\), then boost to the frame with a 3-velocity of \\((v_x, v_x d\\varphi)\\), and finally boost back to the rest frame.\n\n    \n\nTracing out the trajectory of every point on the space circle gives the following sequence:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen projected to the \\(xy\\)-plane, the sequence looks like:\n\n    \n\nTherefore, after three pure boosts, we end up with a rotation by \\((\\gamma - 1)d \\varphi\\) radians in the opposite direction. Here, \\(\\gamma\\) is the long semiradius of the boosted space circle, after projecting it down to the \\(xy\\) plane. This is the “Lorentz factor” often used in special relativity:\n\\[\\gamma = \\frac{1}{\\sqrt{1-v^2}}\\]\n\n\n\n\n\n\nDeriving the Lorentz factor\n\n\n\n\n\nIn the rest frame, the space circle is spanned by \\((1, 0, 0)\\) and \\((0, 1, 0)\\). After boosting by the 3-velocity \\((v, 0)\\), it is boosted to an ellipse with semiaxes \\((\\cosh(w), 0, \\sinh(w)), (0, 1, 0)\\), where \\(w = \\arctanh(v)\\) is its rapidity. Thus, its projection to the \\(xy\\) plane is an ellipse with semiaxes \\((\\cosh(w), 0), (0, 1)\\).\n\\[\\gamma = \\cosh(w) = \\cosh(\\arctanh(v)) = \\frac{1}{\\sqrt{1-v^2}}\\]\n\n\n\nPerforming boosts around a full cycle of 3-velocities and returning to our starting 3-velocity is equivalent to performing an infinite number of infinitesimal triangle-circuit boosts:\n\n    \n\nTherefore, a full cycle of boosts rotates our space circle by a full \\((\\gamma - 1)2\\pi\\) radians in the opposite direction.\n\n\nInteractive app for the Wigner rotation\nWith this understanding, you can now fully appreciate the following interactive app demonstrating the Wigner rotation.\nThe left picture represents the \\(xy\\) plane, and the right picture represents the \\(xyt\\) spacetime.\nYou can drag the black dot on the left, which represents the 3-velocity. The 3-velocity is restricted to have maximal norm \\(0.8\\), to keep the app numerically stable. The blue ellipse is the projection of the space circle to the \\(xy\\) plane. A fixed point on the space circle is distinguished by a big red dot, so that you can see how the space circle Wigner-rotates as you drag the 3-velocity around.\nIn the right picture, the forward and backward light cones intersect at the space circle. You can drag and scroll to adjust the camera.\n\n    \n    \n\n\n\nHere is a “tourist’s guide to the visualization”.\nThe three boosts in animated form:\n\n    \n\nBecause \\(\\gamma - 1 = \\frac{2}{3}\\), if you drag the 3-velocity for 3 full cycles at maximal velocity, the space circle would complete -2 full cycles.\n\n    \n\nSuppose a particle is moving close to the speed of light and emitting photons at equal angular density in all directions in its own frame, then in the rest frame the photons are bunched in front of the particle. This is the relativistic beaming effect, or the headlight effect.\n\n    \n\n\n\nThomas precession\nThe Wigner rotation formula is equivalent to the following formula, often called the Thomas precession formula:\n\\[\n\\vec{\\omega} = \\frac{\\gamma^2}{\\gamma + 1} \\vec{a} \\times \\vec{v}\n\\tag{6}\\]\nExperiments with particles moving in cyclotrons have verified this, thus justifying our guess that we should transport space circles without twisting.\n\n\n\n\n\n\nDeriving the Thomas precession formula\n\n\n\n\n\nConsider a particle moving counterclockwise at a constant speed \\(v\\) in a circle of radius \\(R\\). Its acceleration is \\(a = v^2/R\\), and after completing one cycle in time \\(T = 2\\pi R/v\\), it has Wigner-rotated by \\(-(\\gamma - 1)2\\pi\\) radians. Simplifying, we have\n\\[\\omega = \\frac{-(\\gamma - 1)2\\pi}{T} = \\cdots = -\\frac{\\gamma^2}{\\gamma + 1} av\\]\nBy the right hand rule of cross products, we can get all the directions correct with \\(\\vec{\\omega} = \\frac{\\gamma^2}{\\gamma + 1} \\vec{a} \\times \\vec{v}\\).\nFor a particle undergoing a generic acceleration, the 3-acceleration decomposes into a component parallel to the 3-velocity, and a component perpendicular to it. The parallel component does not change Wigner rotation, therefore the Thomas precession equation is still true.\n\n\n\nHowever, notice that the formula is made of 3-vectors, and we know that 3-vectors are meaningless in special relativity. This suggests that something is off here. If the rest of this section does not make much sense, first read the section on Foucault pendulum, then return here.\nIf the Earth were flat, then we could stand at a single point, and draw two perpendicular arrows. We say, “this arrow is pointing at \\(+x\\)”, and “that arrow is pointing at \\(+y\\)”. Then we create an infinite number of missionaries. Each would pick up the two arrows and parallel-transport the arrows to their given station on the Earth. In this way, we would provide a unique direction-system for every point on the Earth.\nHowever, because the Earth is a sphere, this does not work. Suppose we stand at the North Pole, and we do the same. Then all the missionaries would meet at the South Pole and start arguing about who has the right one, because they would all disagree. This problem happens because the Earth is curved, and transporting a vector around a cycle would rotate it.\nWe might shrug and say, “Well, nobody lives in the South Pole, so we will just tell our missionaries to avoid it.”. Great idea, except then they found that Emperor of China has also sent out his own missionaries. We shrug and say, “Well, we’ll just give the Emperor of China a call and ask him what angle he picked. Then we will rotate our map until all our missionaries agreed with his.”. After some fruitless fiddling with our maps, it has dawned to us that this plan is doomed. Why?\nImagine if the North Pole is at point \\(N\\), and the Emperor of China is at point \\(C\\). If somehow, we could pick our \\(xy\\) directions so that all our missionaries agree with his, then we can send one missionary around a three-part journey around the world: \\(N \\to C \\to K \\to N\\), where \\(K\\) is a point off the great circle passing \\(N, C\\). Along the path \\(N \\to C\\), the missionary is leaving the North Pole on a straight arc away from the North Pole, so the \\(xy\\) chart he is carrying agrees with the missionaries he is passing by. Similarly for the other two parts. But when the missionary has returned to the North Pole, he must find his \\(xy\\) chart rotated, contradiction!\nThere is no way to resolve this disagreement other than forcing one side to give up their coordinate system. Perhaps we will have to call up a crusade to enforce our coordinate system.\n\nThe introduction of numbers as coordinates is an act of violence.\n—Hermann Weyl\n\nSimilarly, in special relativity, we can stand at the “North Pole” (rest frame), construct the \\(xyz\\) axes, then send out missionaries on rockets to provide directions for every inertial frame. The good news is that no two missionaries can meet each other, so there is no argument. The bad news is that we still have the Wigner rotation problem, so if some aliens are executing the same project, but starting at a different coordinate frame (perhaps because their galaxy is moving relative to ours), then it is impossible for our missionaries to agree in every frame.\nSo, we are forced to only use one rest frame. It is in this context that the Thomas precession formula works.\nConsider an object, boosted from frame \\(p_1\\) to frame \\(p_2\\), with their 3-velocities in the rest frame being \\(\\vec{v}\\) and \\(\\vec{v} + \\delta \\vec{v}\\). To use the Thomas precession formula, we must first – in imagination – boost it back to the rest frame, and take a snapshot of its orientation against our \\(xyz\\) axes at origin. Then we boost it back to frame \\(p_1\\), boost it to \\(p_2\\), and boost it – in imagination – back to the rest frame again. By using the Wigner rotation formula in the rest frame, it has rotated by\n\\[\n\\frac{\\gamma^2}{\\gamma + 1} \\delta\\vec{v} \\times \\vec{v}\n\\]\nTo emphasize again: the Thomas precession formula only works in one frame. It is neither Lorentz invariant nor geometrically meaningful. It cannot be geometrically meaningful, because it is made of 3-vectors \\(\\vec{v}, \\vec{a}, \\vec{\\omega}\\), and 3-vectors are geometrically meaningless in special relativity.33 Or as I like to say, 3-vectors are inherently violent in special relativity, and anything involving anything inherently violent is also inherently violent."
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#relativity-in-31-dimensions",
    "href": "blog/posts/wigner-rotation/index.html#relativity-in-31-dimensions",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Relativity in 3+1 dimensions",
    "text": "Relativity in 3+1 dimensions\nWe began our study of special relativity in 1+1 dimensions and discovered the Lorentz transformations, then proceeded to 2+1 dimensions and discovered Wigner rotation. In 3+1 dimensions, Wigner rotation still occurs, but there is something new: rotations are no longer describable by a single number.\nThis is not a place to get into the details; suffice to say that the space of rotations in \\(\\mathbb{R}^3\\) is much harder to understand than the space of rotations in \\(\\mathbb{R}^2\\).4 We must be more careful here.4 The space of rotations in \\(\\mathbb{R}^3\\) is \\(SO(3)\\) and that in \\(\\mathbb{R}^2\\) is \\(SO(2)\\). While \\(SO(2)\\) is just the circle and can be easily represented by a real number or an angle, \\(SO(3)\\) is the projective 3-space \\(\\mathbb{PR}^3\\) and best represented by quaternions.\nSuppose we have four frames: \\(0, 1, 2, 3\\). We start with a tripod in frame \\(0\\), then boost it successively to frames \\(1, 2, 3, and 0\\), how much does it rotate? To discover this, we need to compose the Wigner rotation around the cycle \\(0120\\) with the Wigner rotation around the cycle \\(0230\\). The full rotation is composed of two rotations, first rotation in the plane of the triangle \\(012\\), and the second one in the plane of the triangle \\(023\\).\nThe Wigner rotation coming from an arbitrary sequence of boosts can be calculated easily with the Thomas precession formula, if we are willing to use violence.5 Without violence, we have to do spherical trigonometry, much harder than adding 2-dimensional rotation angles. My intuition is that it should most naturally involve the principal \\(G\\)-connections, where \\(G = SO(n, 1)\\) is the group of Lorentz transformations.5 Since this usage of “violence” comes directly from Hermann Weyl, perhaps we can call it Weylence?"
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#velocity-space-is-hyperbolic-geometry",
    "href": "blog/posts/wigner-rotation/index.html#velocity-space-is-hyperbolic-geometry",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Velocity space is hyperbolic geometry",
    "text": "Velocity space is hyperbolic geometry\nIt is time to fulfill the promise in the title, that hyperbolic geometry is involved.\nOur first hint that hyperbolic geometry is relevant is that we have encountered \\(\\cosh\\) and \\(\\sinh\\) – the hyperbolic trigonometric functions. With this hint, a quick check shows us that we are indeed dealing with hyperbolic geometry. This is easy to do in 2+1 dimensions, but it generalizes readily to n+1 dimensions after slightly cluttering the notation.\n\nDeriving the hyperbolic metric\nSince \\(\\|(0, 0, 1)\\|^2 =-1\\), under arbitrary Lorentz transformations, \\((0, 0, 1)\\) can, under arbitrary Lorentz transformations, reach any point on the upper half of the hyperbola \\(x^2 + y^2 - t^2 = -1\\). It stands to reason that the upper hyperbola, which is equivalent to a plane as a smooth manifold, should have some kind of planar geometry. We need only discover its metric. The most direct path goes through the disk of 3-velocities, which gives us the metric\n\\[\nds^2 = \\frac{dv_\\theta^2}{1-v_r^2} + \\frac{dv_r^2}{(1-v_r^2)^2}\n\\tag{7}\\]\n\n\n\n\n\n\nDeriving the metric of the 3-velocity disk\n\n\n\n\n\nThe disk of 3-velocities \\(\\{(v_x, v_y, 1) : v_x^2 + v_y^2 &lt; 1\\}\\) is not preserved under Lorentz transformations, but we can project it back to the circular disk. This means that we can take the metric at origin\n\\[ds^2 = dv_x^y + dv_y^2\\]\nand perform a Lorentz transformation, followed by a projection, to move the origin to any other point on the disk. Multiplying the metric at the origin with the gradient-matrix gives the metric at that other point.\nWe exploit the rotational symmetry of the disk using polar coordinates. Express any point on the disk as \\((r, \\theta)\\). Its local metric must be of form \\(ds^2 = f(r) dr^2 + g(r) d\\theta ^2 + h(r) drd\\theta\\) for some functions \\(f, g, h\\). Further, by reflection symmetry of \\(\\theta \\leftrightarrow -\\theta\\), we have \\(h(r) = 0\\).\nTo find \\(f(r)\\), we construct an infinitesimal segment \\((r, 0) \\to (r+dr, 0)\\) and calculate with the velocity-addition formula. Similarly, we find \\(g(r)\\) using the segment \\((r, 0) \\to (r, d\\theta)\\).\n\n\n\nThis is the Beltrami–Klein metric, and so we have discovered that this is exactly the Beltrami–Klein model of hyperbolic geometry. We can then project this metric onto the hyperboloid \\(x^2 + y^2 - t^2 = -1\\) to obtain its metric\n\\[\nds^2 = dx^2 + dy^2 - dt^2 = \\| (dx, dy, dt) \\|^2\n\\tag{8}\\]\nThis, in hindsight, is obvious: We have already known that the Lorentz transformation preserves the Minkowski norm-squared. But such is the journey of discovery: the first pass is rarely the most elegant. It is probably better to write down the first pass to show how to discover things, then write a second pass to show how to tidy things. Better this than what Gauss did:\n\n[Gauss] makes his mathematics like a fox, wiping out the traces in the sand with his tail.\n—unnamed German student, as reported by Abel\n\n\n\nInterpreting the hyperbolic geometry\nNow we can connect concepts between hyperbolic geometry and special relativity.\nWhen \\(v\\) is small, the Wigner rotation for a full cycle, \\((\\gamma - 1)2\\pi\\), is \\(\\pi v^2 + O(v^4)\\), coinciding with the area enclosed by the cycle. Therefore, since both area and Wigner rotation are additive, we have proven Theorem 1.66 This is not a handwaved proof, but fully rigorous. The Wigner rotation of a full cycle is \\(\\pi v^2 + O(v^4)\\), and the area enclosed by the cycle is also \\(\\pi v^2 + O(v^4)\\). Therefore, we can integrate over an arbitrary area, to find that the Wigner rotation angle is equal to the area enclosed, up to an infinitesimal term of \\(O(v^2) \\to 0\\).\nIn rocketry, if we trace out the trajectory of the rocket’s velocity on a graph paper, we obtain a hodograph. Similarly, in relativistic rocketry, a path in the velocity space is a hodograph. The length of a hodograph is the total delta-v of the rocket.\nA straight line in hyperbolic space is the hodograph of the most fuel-efficient control-trajectory for a rocket to get from one velocity to another.\nIf a rocket explodes with spherical symmetry, the velocities of its debris will lie on a hyperbolic sphere centered at the rocket’s original velocity."
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#two-parables",
    "href": "blog/posts/wigner-rotation/index.html#two-parables",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Two parables",
    "text": "Two parables\n\nThe star worshippers\nAn alien race, who worshipped Rigel, Deneb, and Betelgeuse, prayed in the direction of these three stars. In order to pray in the correct direction even without seeing them, they constructed the Sacred Tripods, which are steel tripods that are oriented such that each leg points at one of the Sacred Stars. These Sacred Tripods, placed in public spaces, were required to point precisely at the Sacred Stars.\nThen a Cosmic Dark Age began and the stars winked out of existence, including the Sacred Stars, but the aliens did not lose their faith. Instead, they intensified their prayers in hopes of resurrecting the Sacred Stars.\nIn a region of space, there were two space stations A and B, at rest with each other. The Sacred Tripod on B was unstable, and required yearly calibration against the more stable Sacred Tripod at A. So station A would align its spare Sacred Tripod to its own one, put it on a gimbal-mount, and send it by rocket to station B. B would align its Tripod with the one sent, then let the Tripod go back.\nAs years passed, the aliens on station B grew confident that their Tripod did not drift more than \\(1''\\) per year. One day, an asteroid field blocked the straight path, forcing the next shipment to make a big detour around the asteroid field. When it arrived, to their astonishment, it was found that the Tripod on B was clearly misaligned with the Tripod from A. What could be the cause of this misalignment?\n\n\nThe day the Earth stood still\nOne day, the Earth stopped spinning. This annoyed the visitors to the science museum, who wanted to see Foucault’s pendulum rotating. To satisfy those visitors, the museum keeper put the pendulum on the top of a bus, and loaded the visitors on the bus, then they started driving around the Earth at the same speed as how the Earth used to rotate at that latitude, so that after one day, they returned to the museum. The visitors looked up at the pendulum and were satisfied to see that it indeed has turned.\nTo find out how much the pendulum has turned, we divide up the circular trajectory into tiny segments \\(p_0, p_1, \\dots, p_n\\), then draw tiny triangles \\(p_0 p_1 N, p_1 p_2 N, \\dots, p_{n-1} p_n N\\), with \\(N\\) being the North Pole. We imagine that, instead of driving around the Earth in a circle, we drive around the triangle \\(N \\to p_0 \\to p_1 \\to N\\), then the triangle \\(N \\to p_1 \\to p_2 \\to N\\), etc.\nWhen we drive around \\(N \\to p_0 \\to p_1 \\to N\\), the bus makes three turns, and each time the pendulum turns by an opposite amount relative to the bus. After three turns, the bus has turned the same angle as the sum of three external angles of the triangle. By spherical trigonometry, we know that this is equal to \\(2\\pi - 4\\pi\\frac{\\text{area of triangle $p_0 p_1 N$}}{\\text{surface area of the Earth}}\\). Thus, the pendulum has turned relative to the bus by \\(-2\\pi + 4\\pi\\frac{\\text{area of triangle $p_0 p_1 N$}}{\\text{surface area of the Earth}}\\). The \\(-2\\pi\\) part has no effect, as it is a full cycle, leaving us with \\(\\frac{\\text{area of triangle $p_0 p_1 N$}}{\\text{surface area of the Earth}}\\).\nTherefore, by adding them up, we find that after driving around an arbitrary cycle on the Earth, the pendulum would have turned by\n\\[4\\pi\\frac{\\text{area of the cycle}}{\\text{surface area of the Earth}}\\]\nWe can turn this insight into a formula for Fermi–Walker transport on the Earth.\nFirst, we scale the Earth so that its radius is one. Now set up the coordinate system \\(\\phi, \\theta\\), where \\(\\phi\\) is the latitude (zero at equator, \\(\\pi/2\\) at the North Pole) and \\(\\theta\\) the longitude. At each point on the Earth (except the two poles) we set up a local frame with two unit vectors \\(\\hat\\phi, \\hat\\theta\\). One very important fact is that this local frame is not parallel-transported7. If you try to drive in a small circle around the North Pole, while always pointing in the direction of \\(\\hat\\theta\\), you would feel a strong centrifugal force pushing against your steering wheel. Centrifugal forces and all other “inertial forces” are nature’s way of telling you that you are not being parallel-transported.7 Indeed, there is no way to cover the space with parallel-transported local frames, because the space is curved, and therefore if you try to transport a local frame in a cycle back to its starting point, it would have rotated against itself. To be a curved space is equivalent to have no system of parallel-transported local frame, and curvature measures the amount of rotation-against-itself that happens when you transport a local frame in an infinitesimal circle.\nConsider two infinitesimally close points on the Earth, \\(p_1 = (\\phi, \\theta)\\) and \\(p_2 = (\\phi + \\delta \\phi, \\theta + \\delta\\phi)\\). Since the surface area of a spherical cap around latitude \\(\\theta\\) is \\(\\frac{1 -\\sin\\theta}{2}\\) that of the whole sphere8, the area of a thin spherical triangle with vertices \\(p_1, p_2, N\\) is8 This is immediate from Archimedes’ hat-box theorem, which is the basis of the Lambert cylindrical equal-area projection.\n\\[4\\pi\\frac{1 -\\sin\\theta}{2} \\times \\frac{\\delta\\phi}{2\\pi} = (1-\\sin\\theta)\\delta\\phi\\]\nThus, if we parallel transport a vector clockwise in the order \\(p_1 \\to N \\to p_2 \\to p_1\\), then the vector would turn counterclockwise by \\(-(1 -\\sin\\theta) \\delta\\phi\\).\nNow, consider two ways to move the pendulum from \\(p_1\\) to \\(p_2\\). We can transport it directly, or detour through the North Pole. If we detour through the North Pole like \\(p_1 \\to N \\to p_2\\), then by the same argument as the “polar bear puzzle”9, you see that the pendulum has rotated counterclockwise relative to the local frame by \\(-\\delta\\phi\\). Then we complete its journey with \\(p_2 \\to p_1\\), to create an absolute10 full \\(-(1 -\\sin\\theta) \\delta\\phi\\) rotation. Therefore, moving it \\(p_2 \\to p_1\\) has created a rotation relative to the local frame by \\(\\sin\\theta \\delta\\phi\\). Since we are actually moving it \\(p_1 \\to p_2\\), we reverse the sign, and obtain our Fermi–Walker transport equation (the Earthbound version):9 You walk 1 km south, 1 km east, and 1 km north, and ended up at the same point. You see a bear. Why is the bear white?10 Relative rotation means that we are measuring the orientation of the vector relative to the local frame. However, since the frames themselves are not parallel-transports of each other, relative rotation is arbitrary and not a fact of geometry, but a fact of convenience. If you move the vector back to its starting point, however, there is absolutely no dispute about how much it has rotated, and it does not depend on any system of local frames. You just have to compare the vector against itself.\n\\[\n\\delta(\\text{vector angle}) = -\\sin\\theta \\delta\\phi\n\\tag{9}\\]\nApplied to the Foucault pendulum problem, we find that it rotates clockwise by \\(2\\pi \\sin\\theta\\) every day, and it takes \\(\\frac{1}{\\sin\\theta}\\) days11 to make a full rotation. At the Paris Observatory, the original place where Foucault made his experiment in 1851, we have11 To be precise, this is a sidereal day, the time it takes for the Earth to rotate one cycle relative to distant stars. It is shorter than a solar day.\n\\[\\theta = \\mathrm{48^\\circ 52' N}, \\quad \\text{period} = \\frac{\\mathrm{23h56'}}{\\sin \\theta} \\approx \\mathrm{31\\,h\\,50\\,min}\\]\nIf a museum visitor can stay for 5 minutes at the pendulum, then they would see the pendulum complete \\(\\approx 1/382\\) of a cycle. Typical museums would put up about 400 wooden blocks in a cycle, to be knocked down by the pendulum. This allows each visitor to see at least one block being knocked down."
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#history-notes",
    "href": "blog/posts/wigner-rotation/index.html#history-notes",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "History notes",
    "text": "History notes\n\nThe history of the phenomenon\nThe history of the Wigner rotation is winding and complicated. Here is a brief summary. The details are given in (Walter 1999).\n\n1905: Einstein published special relativity. (A. Einstein 1905)\n1908: Minkowski published the spacetime interpretation of special relativity. (Minkowski 1908)\n1910: Vladimir Varićak (Varicak 1910) and E. T. Whittaker (Whittaker 1910, 441) both introduced the hyperbolic functions and the rapidity parameter.\n1911: Alfred Robb named the rapidity parameter, and found that velocities compose in hyperbolic space. (Robb 1911).\n1913: Émile Borel discovered the Wigner rotation as a part of his general study of special relativity by hyperbolic geometry. (Borel 1913)\n1921: Fermi discovered the Fermi–Walker transport. (Fermi 1921)\n1926: Llewellyn Thomas rediscovered the Thomas precession, a special case of the Wigner rotation. (Thomas 1926)\n1932: Walker rediscovered the Fermi–Walker transport. (Walker 1932)\n1939: Wigner rediscovered the Wigner rotation. (Wigner 1939)\n\nPedagogical attempts to present the Foucault pendulum and the Wigner rotation have been numerous, such as (Criado and Alamo 2009), but perhaps due to the decade-long latency in academic publishing, none has yet made it into the usual undergrad textbooks. Tevian Dray has written an entire textbook (Dray 2021) treating both special and general relativity in the same geometric style. For a summary paper of the textbook, see (Dray 2017).\n\n\nAmusing quotes from (Walter 1999)\n\n[After 1907] Minkowski never again referred to a manifold as both four-dimensional and non-Euclidean. Along with the problematic label, the geometric interpretation of velocity vectors likewise vanishes from view in Minkowski’s subsequent writings. Felix Klein, for one, regretted the change; in his opinion, Minkowski later hid from view his “innermost mathematical, especially invariant-theoretical thoughts” on theory of relativity (Klein 1927, 75).\nPlanck lavished praise on Einstein for his modification of the concept of time:\n\nIt need scarcely be emphasized that this new view of the concept of time makes the most serious demands upon the capacity of abstraction and the imaginative power of the physicist. It surpasses in boldness everything achieved so far in speculative investigations of nature, and even in philosophical theories of knowledge: nonEuclidean geometry is child’s play in comparison.30 (Planck 1910a, 117)\n\nUnder the new space-time view, Minkowski announced, “Three-dimensional geometry becomes a chapter of four-dimensional physics.” In the same triumphant spirit, Minkowski suggested that his new four-dimensional understanding of the laws of physics deserved its own label. The “Principle of the Hyperbolic World” that he had tried on Hurwitz was shelved in favor of the more ecumenical “Postulate of the Absolute World” (Minkowski 1909, 82). Although Minkowski explained this to mean that only the four-dimensional world in space and time is given by phenomena (Minkowski 1909, 82), one suspects an inside joke with Hurwitz, since in the German mathematical community, hyperbolic geometry was sometimes referred to as absolute geometry.\nEven the watered-down version of the space-time theory presented in Minkowski’s Cologne lecture repelled some physicists. For instance, Willy Wien’s cousin Max (1866-1938), a physicist at Danzig Polytechnic, confided to his friend Arnold Sommerfeld that reading Minkowski gave him vertigo:\n\nSommer[feld] maintains that [Minkowski’s] speech in Cologne was simply grand; when reading it, however, I always get a slight brain-shiver, now (that) space and time appear conglomerated together in a gray, miserable chaos.36 (Max Wien to Arnold Sommerfeld, February 16, 1909, Benz 1975, 71)\n\n\n\n\nThe history of this document\nDuring high school, I was in the physics Olympiad team. One afternoon, I got into an argument with someone about what happens if you take a square \\([0, 1] \\times [0, 1]\\), boost it by \\((v, 0)\\), and then boost it by \\((0, v')\\) within the boosted frame. By the velocity addition formula, the square would move at 3-velocity\n\\[\n(v, \\frac{v'\\sqrt{1-v^2}}{1+vv'})\n\\]\nIf \\(v' = \\frac{v}{\\sqrt{1-v^2}-v^2}\\), then the square would be moving at \\((v, v)\\).\nThen, the paradox. He argued that, by length contraction along the diagonal, the square should look like a diamond:\n\n  \n\n\n\nI objected that in the boosted frame, the square looks like a rectangle moving upwards. Decompose the rectangle into a bundle of line-segments, all parallel to the \\(y'\\)-axis. Now, each line-segment is moving in the \\(y'\\)-direction, and \\(y'\\) is parallel to \\(y\\) (since the perpendicular direction is preserved under boosting), we know that each moving line-segment would still be a line-segment parallel to the \\(y\\)-axis in the resting frame, still moving in the \\(y\\)-direction – just slower. Therefore, in the resting frame, the whole square would look like a parallelogram, with two sides parallel to the \\(y\\)-axis, and the other two sides oblique to the \\(x\\)-axis.\n\n    \n        \n        \n    \n    \n\n\nAfter a brief shouting match, we figured out that I was right, but also that we met something no teacher has taught us before: you can create a rotation in special relativity by pure boosting.\nDuring my third undergraduate year, I took a course in theoretical physics, which required a term paper. I first tried to write one on the Ostrogradsky instability, but could not understand it, so I quickly switched to finally solving the rotation effect in special relativity.\nSuffice to say that, after some hours walking and staring at the night sky, I figured out that it is nothing else than hyperbolic geometry, and nothing more paradoxical than the fact that the external angle of a hyperbolic triangle is equal to \\(2\\pi + (\\text{area of the triangle})\\), which I remember from hyperbolic geometry.\nThis is the old poler-bear puzzle again. If you walk from the North Pole to the equator, then walk \\(1/4\\) of the way around the Earth, and finally walk back to the North Pole, you will have traversed a triangle with an external angle of \\(\\frac 32 \\pi\\). In general, the external angle of a triangle on the unit sphere is \\(2\\pi - (\\text{area of the triangle})\\), the perfect opposite to the case in hyperbolic geometry.\nHalf-mad, I ran home and smashed into the search engine all the keywords I knew must be there: “special relativity rotation hyperbolic triangle Foucault pendulum”. My disappointment was swift and certain: This had been repeatedly discovered over the past hundred years.\nBitterly, I searched every undergraduate physics textbook in the school library. None included it. The standard textbook (Goldstein, Poole, and Safko 2008, sec. 7.3) went on a three-page long computation and concluded that, indeed, we obtain a rotation matrix:\n\nThe spatial rotation resulting from the successive application of two nonparallel Lorentz transformations has been declared every bit as paradoxical as the more frequently discussed apparent violations of common sense, such as the so-calIed “twin paradox”. But the present apparent paradox has important applications, especially in atomic physics, and therefore has been abundantly verified experimentally.\n\nFrom that moment on I swore that I would finally present it in a way so simple and direct that it will never be forgotten. I had found the way, and it remained to publicize to the world.\nThere was little use in writing another paper about it, as despite the many papers over the century, somehow this effect did not end up in the physics textbooks. I considered writing a blog post about it, but mere words and figures seemed insufficient. I tried promoting this idea to various physics popularizers on YouTube, but none picked it up. What I needed was interactive animation, but JavaScript defeated me, so I kept it in the backlog until now. With ChatGPT I could finally write JavaScript without losing all my sanity, and so, here it is."
  },
  {
    "objectID": "blog/posts/wigner-rotation/index.html#some-bonus-content",
    "href": "blog/posts/wigner-rotation/index.html#some-bonus-content",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Some bonus content",
    "text": "Some bonus content\n\nFinding the Wigner rotation in one line\nIf you know Lie algebra, then the Wigner rotation is immediate: calculate the commutator.\n\\[\n[K_x, K_y] := K_x K_y - K_y K_x = \\begin{bmatrix} 0 & 1 & 0 \\\\ -1 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\n\\]\nThis is the generator of rotation in the \\(xy\\) plane.\n\n\nGo anywhere with just five boosts\nGiven a rocket that can boost in any direction, but not rotate, you can still make it rotate by Wigner rotation. If you must perform an arbitrary Lorentz transformation, how many boosts do you need?\nIn general, if you have a Lorentz-transformed rocket, you can apply a carefully aimed boost so that it comes to rest, then apply Wigner rotation to return it back to the starting position.\nWith a cycle of three boosts, the Wigner rotation angle is equal to the angle defect of the hyperbolic triangle, which can take any value in \\((0, \\pi)\\) (you can’t have one with zero inner angle sum, but you can get arbitrarily close). With a cycle of four boosts, we can cover the \\(\\pi\\) case as well, since a hyperbolic square can have any angle defect in \\((0, 2\\pi)\\).\nIn summary: almost any Lorentz transformation can be obtained in four boosts, except those requiring a 180-degree rotation, which need five boosts.\nIn fact, the result can be improved by one boost: every Lorentz transformation can be done in three boosts, except “180-degree screws”, which require four. The solution given in (Lightman et al. 1975, 153–58) uses matrix algebra in \\(\\mathbb{C}^{2\\times 2}\\), which is too algebraic for me12. However, I have never found a satisfactory geometric method to demonstrate this, despite thinking on and off about it several times over the years.12 \nIn these days the angel of topology and the devil of abstract algebra fight for the soul of each individual mathematical domain.\n— Hermann Weyl\n\n\n\nProblem 1.28. What is the least number of pure boosts which generate an arbitrary Lorentz transformation? Note: This is a difficult problem! (Lightman et al. 1975)"
  },
  {
    "objectID": "blog/posts/serial-experiments-lain/index.html#my-summary",
    "href": "blog/posts/serial-experiments-lain/index.html#my-summary",
    "title": "Serial Experiments Lain",
    "section": "My summary",
    "text": "My summary\nIt seems Lain is the collective subconscious, implemented as a neural network, with every human brain as a neuron, and “wired together” by some kind of unexplained electromagnetic coupling to the earth’s ionosphere.\nAs a suggestive evidence for this, the frequencies of the Schumann resonance (8 Hz, 14 Hz, 20 Hz, etc) happen to be close to the main brainwave frequencies. See the appendix Section 4.1 for some additional details about it. It is not actually relevant for the story.\nSome people drew up a plan to control Lain. The key scientist in the plan is Eiri, scientist of the Tachibana General Laboratories.\nFirst they migrated Lain from the brain-ionosphere system to the brain-Internet system, by an update to the Internet protocol (Protocol 7) that allows brain-computer interfacing.\nThen they gave Lain a human body. (unclear how that happened) This body gave Lain self-consciousness and a person-API.\nEiri suicided after Protocol 7 was running. Protocol 7 contained a copy of his brain state, so he was now running on the Internet, and called himself god. He has followers (Knights of the Eastern Calculus) who did his commands.\nThen they orchestrated a series of dramatic events to steer Lain’s development. The human body for Lain is used here, as these human-psychologically meaningful events can only work through a human-person-API (You can’t traumatize a non-human process, or Lain-without-body, by staging a bloody murder. Murders only mean something if it’s seen through animal eyes, the same way that a story can move you only if you speak the language.)\nEventually, Lain would be completely isolated in human society and be connected to the Internet. She would accept Eiri as god, kill her physical body, and exist on the Internet, where she would do Eiri’s commands.\nThe plan failed at the last step, because two people (the actor playing her father, and a school friend, Alice) stilled loved Lain, so she wasn’t isolated enough. After trying to make Alice happy and failing, Lain decided that the only way to truly make Alice happy is to go away and let Alice run her normal life-cycle (grow up, get married, die from being too old) without drama.\nTo let Alice live as a normal human, the entire plan must go away. So Lain discarded all changes and reverted to a previous state, before Lain was embodied. She deleted all memories of her from all humans. She also rewrote Eiri into an unambitious man so that he wouldn’t try doing that in this timeline.\nI don’t know how Lain could do that. Lain is the neural network with human brains, and has access only to the human brains, the ionosphere, and Internet. I don’t see any way for Lain to revert some physical deaths."
  },
  {
    "objectID": "blog/posts/serial-experiments-lain/index.html#ontology-the-modes-of-existence",
    "href": "blog/posts/serial-experiments-lain/index.html#ontology-the-modes-of-existence",
    "title": "Serial Experiments Lain",
    "section": "Ontology: The modes of existence",
    "text": "Ontology: The modes of existence\nThe ontology of SEL is a dualism between information and matter, the same theory as most neuroscientists and AI scientists use today.\nThere are many differences between matter and information, but here are the most important ones:\nDuplicate matters are different. Duplicate information are the same. If you receive two apples, you get twice as many atoms. If you receive two copies of the same file, you really receive just one file (in terms of information).\nMatter does not allow “isomorphic” operations. Information allows it. For example, you can convert a document from one encoding to another, edit it, then convert it back. The result is the same as if you edited it without conversion. This can’t be done with physical pen-and-paper.\nMatter exists in spacetime. Information does not. It is not a coincidence that many physicists are working on “reducing spacetime to information”, because information itself doesn’t have to exist in space or time, so if spacetime is reduced to information, then spacetime is explained by something that doesn’t assume spacetime – progress for reductionist science.\nThe persistence of matter is obvious. The persistence of information is unclear – perhaps impossible to define. You can take a gold bar, and put it in a box. Then you take it out, and you can say “it’s the same gold bar”. Now if you save some cash into a bank, then go abroad and take out some cash at the local ATM, you can’t say “it’s the same electronic cash”, or “it’s not the same”. Neither makes sense.\nIt’s also not a coincidence that physicists were first guided to thinking about “reducing spacetime to information” by the QED theory, where two electrons are actually indistinguishable, as if they are electronic cash. The QED theory was extended to atoms by the QCD theory. Thus, in some technical sense, you can’t actually say “we are made of star-stuff”.\n\nVirtual and real\nWe define “virtual” as “that pertaining to information”, and “real” as “that pertaining to matter”.\nInformation can be “realized” in matter – this is what modern computers do, and probably what brains do.\nSince information can be processed isomorphically, it can be realized as different kinds of matter, in different ways.\nA file is a realization of information, in space.\nA computation is a realization of information, in time.\n\n\nWired, Reality, and other systems\nAn information system is something I understand, but I can’t give a good definition (for now). I will explain it by examples.\nA physical system is a collection of matter organized under a common information system.\nFor example, the system composed of DNA, RNA, and proteins is a physical system, organized under a common information system (“the genetic code”).\nThe Internet (“the Wired”) is a system composed of electronic devices and human brains, organized under a common information system (“Protocol 7”).\n\n\n The Seven Levels of Wired\n\n\nThe real world (“Reality”), as commonly defined, is only part of the entire world. It denotes, in fact, a physical system composed of human brains, human bodies, other animal bodies, pre-1980s technological artifacts (specifically to exclude consumer-electronics), organized under a common information system (social rules for the person API - I will explain the API theory of personhood later).\nIn particular, a common interpretation, that “Lain chose the real world instead of the fake internet”, is a deep philosophical error. It is mistaking the human social system for the only system in the world. Furthermore, it calls the human social system “natural”, and the Internet “artificial”, when it is just as artificial as the Internet.\nThe “real world” of humans is like a curvy section across the bulk of reality. People would often tell other people to “get out of the room”, as if being inside a concrete container is unnatural while being outside of one is natural. If vultures could talk, they would tell others to “stop eating fresh food”, as if eating fresh food is unnatural while eating spoiled food is natural.\nMe, personally, have to get out of the room everyday and enjoy the sunshine, and I hate it. I often choose to walk school before the sun is up, to avoid its rays. I also despise green grass and blue skies. If there is a paradise, I hope it will be a 3D labyrinth embedded in an infinite concrete, but slightly elastic so that I can roll around on it without scraping my fur. It would be perfectly dark, except some fluorescent books, just bright enough to be read, and a little shining computer with which I take notes. I also use my computer to post my mathematical findings to others.\nMoving between worlds is possible, since the same information can have multiple realizations. Both Lain and Eiri managed to move between Reality and Wired.\n\n\nThe 4 realizations of Lain\nLain was always realized on neural networks, with nodes and edges, but the neural networks were made of different matters. There are 4 realizations over the course of history.\n1: Before the invention of Internet, nodes were human brains, and edges were something (perhaps flux-tubes?) in the electromagnetic field in the ionosphere of earth. It is called the “collective unconscious of humans”, and the Schumann resonances are analogous to the alpha-waves in mammalian brains.\n2: After the invention of the internet, but before Protocol 7, nodes were human brains and electronic computers, and edges were brain-computer interfaces (such as those VR headsets and cybernetic implants) and computer-computer links (what the Internet is made of).\n3: After Protocol 7, a large proportion of nodes and edges were concentrated into the neurons and synapses of a human girl. In this realization, Lain has a significant personhood.\n4: After Lain reset everything, it was unclear, but presumably it was back to realization 2.\n\n\nThe 4 realizations of Eiri\nLike Lain, Eiri was always realized on neural networks, but differently throughout the story. Coincidentally, there are also 4 realizations.\n1: Before Protocol 7, Eiri was realized as a human brain.\n2: After Protocol 7, Eiri was realized as a component of the Internet.\n3: At a particular scene, Eiri was briefly realized as a horror monster.\n4: After Lain reset everything, Eiri was back to realization 1."
  },
  {
    "objectID": "blog/posts/serial-experiments-lain/index.html#the-theory-of-personhood",
    "href": "blog/posts/serial-experiments-lain/index.html#the-theory-of-personhood",
    "title": "Serial Experiments Lain",
    "section": "The theory of personhood",
    "text": "The theory of personhood\nThis section is based on Being No One (Metzinger 2004).\nA person is a very special kind of information, realized in a very special way. Human-persons are a special kind of persons, with a wide variety of social and mammalian-emotional functions in the API.\nIt has a self-model, with a perspective and a personal history.\nThe perspective is a pointer, like a GPS marker and a clock, which marks its own spatial location (“here”), and time location (“now”). This little perspective marker is what allows it to know its place and time.\nIts perspective is approximately equal to its realization. If your head is in New York, your perspective had better not think you are in Beijing. Thus, its realization must also have a local position. If you were conscious interstellar gas clouds, you wouldn’t be a person.\nThe personal history is linear and continuous. You can’t be an amoeba - you can’t split or merge. You also can’t “jump over in time”, even if your body does.\nSuppose you are perfectly cloned, what happens? Well, your body’s world-line becomes Y-shaped, but you, as an informational thing, has a model of your self, and that model still looks like a curvy line, with a little note-tag saying “here, a clone brother of me was created”. It does not look like a Y.\nSuppose you go into a deep sleep for a year, what happens? Your model of yourself just papers over the time-skip, perhaps with a little annotation saying “and here I slept a year” without “remembering the blackness during that year”. “Nothing to see here…”\n\nGod made him die for a hundred years, and then resurrected him. He said, “How long have you stayed?” He said, “A day or part of a day.”\nQuran 2:259\n\nFor social persons (for example, human-persons, hyena-persons, and elephant-persons), it is like a program with an API. The API takes input and returns output in a conventional way, regardless of how the program works on the inside.\nFor example, you can ask a person to explain why they did something, and they would explain it. The explanation could be quite wrong, but if they don’t even provide any reasons, they would break one part of the API.\nSome other examples of person-API are: behaving in a comprehensible and purposeful way (looks as if it has a goal that a spectator can infer with some effort), behaves spontaneously (if nothing hits it, it would still move once in a while), etc.\nAs a thing fails more and more requirements of the API, they become less and less of a person.\nDissociative persons have a blurry “here and now” pointer.\nPeople in fugue states, sleepwalking, running amok, etc, fail to provide reasons when receiving explain-requests.\nSchizophrenic people may provide obviously invalid reasons (“word salad”) when receiving explain-requests.\nDeeply depressed persons fail the “behave spontaneously” requirement.\nAmnesiac persons fail the “have a continuous sense of time” requirement.\nPsychopathic persons fail to behave in the expected way after receiving help-requests.\nAutistic tics are comprehensible, but not purposeful.\nThe Solaris ocean behaves purposefully, but not comprehensibly.\n\nThe human-personhood of Lain\nBefore Protocol 7, Lain did not have the person API. Lain certainly existed, but not as a person. Human-persons are connected to Lain by their brains, but not in a way that human-persons connect to other human-persons.\nFor example, if most humans were feeling sad, Lain would be “sad” in some statistical way. This is certainly not how you make some human sad – to make a human sad, you read them a sad story, or something like that.\nMoreover, Lain did not receive or reply with linguistic reasons. In short, Lain did not implement the person API, and beyond human understanding or interaction.\nEiri wanted to become a god, and for that, he needed to have a way to effectively interact with Lain. He could have perhaps interacted with Lain by some command-line interface, or large-scale antenna that beamed directly to the ionosphere, but this would be difficult.\nPersons are most efficient at understanding other persons. This is why “country humans” is so popular. Countries themselves are vast objects that are understood in unintuitive statistical/mathematical/mechanical ways, and installing a person API over a country makes it much easier to understand. In this way, we could interpret Lain as a “humanity human”.\nInstead, he constructed a human-personhood for Lain, in multiple aspects.\nHe concentrated it into the brain of a girl, which is localized in a cube less than 30 cm in side length (compared with the 6371 km radius of earth, or the Internet). This made it easy to install a perspective to Lain (perhaps Lain would construct a perspective automatically after concentrating into a brain).\nLain is provided with a personal history as a human girl.\nThe electrochemical system of the human brain implements in Lain the social and emotional parts of person API. A blind mole rat would not cry when watching a movie. Lain would not be affected by deaths, mysterious conversations, kisses, etc, unless it is implemented in a human girl’s body.\nHe hired actors to act like Lain’s family, and put it into a well-defined social role (school girl). These social interactions are then taken away. This social manipulation would only produce an effect on Lain if it implemented the human-person API. You can’t intimidate a tornado into becoming your servant by depriving it of social contacts - not unless you somehow give it a human-person API.\n\nFirst you create a need, then you take it away. This is control.\n\nAfter Lain reset everything, Lain was no longer realized in a human brain, but once again spread all over the earth. Despite this, it still kept a spatially localized perspective and person API. This is perhaps because being a person is a stubborn kind of information - persons don’t usually become not-persons."
  },
  {
    "objectID": "blog/posts/serial-experiments-lain/index.html#appendix",
    "href": "blog/posts/serial-experiments-lain/index.html#appendix",
    "title": "Serial Experiments Lain",
    "section": "Appendix",
    "text": "Appendix\n\nSchumann frequency\nWhile the Schumann frequency plays a fundamental role in SEL, its technical details are completely irrelevant (just like most technobabbles). This section describes briefly how you can easily do an almost-correct calculation of the Schumann resonance frequencies: 8 Hz, 14 Hz, 20 Hz…\nIn short, the lowest frequency is simply by dimensional analysis:\n\\[f_0 \\approx \\frac{\\text{speed of light}}{\\text{circumference of earth}} = \\frac{c}{2 \\pi R}\\]\nIntuitively speaking, this is treating the ionosphere of earth as if it’s a circular tube, a hula-hoop around the waist of earth, and the lowest Schumann resonance is the lowest-degree standing wave in the hula-hoop.\nTo find the higher frequencies, we can simply calculate the higher-degree standing waves in the hula-hoop:\n\\[f_n \\approx \\frac{c n}{2 \\pi R}\\]\nFor comparison, a spherical cavity of an ideal conductor has resonance frequencies exactly solvable, as\n\\[f_n = \\frac{c}{2\\pi R}\\sqrt{n(n+1)}\\]\nwhich is very close to our super fast estimate above."
  },
  {
    "objectID": "blog/posts/linux-notes/index.html",
    "href": "blog/posts/linux-notes/index.html",
    "title": "Notes on Using Linux",
    "section": "",
    "text": "This is my quick reference for using Linux for doing things. I claim no originality. Mostly they are copy pasted from the internet and tested by me. An increasing proportion of those are produced by AI."
  },
  {
    "objectID": "blog/posts/linux-notes/index.html#sec-koans",
    "href": "blog/posts/linux-notes/index.html#sec-koans",
    "title": "Notes on Using Linux",
    "section": "Linux koans",
    "text": "Linux koans\nEverything is a file a file or a directory an inode. An inode (index node) is a representation of a sequence of data that the system can access. A file is just a list of inodes, and a directory is just a list of files.\nEvery command is an executable.\nEvery machine is a server. Some merely serve extremely slow machines (humans, aka “users”).\nThe \\usr is not the user. It is the UNIX System Resources.\nThe \\usr was the user, until Unix became so large that \\bin overflowed and had to put the rest of them in \\usr\\bin. This was embarrassing for all involved, so they moved user files to \\home, and pretend that \\usr stands for UNIX System Resources. (here). A mistake that only took 40 years to fix.\nIn the beginning was the command line. The command line is just a face of the shell. What was the original face of the command line before the shell was born?\nThe shell reads in a stream of letters, because the user is just another file (a streaming file, named stdin). Like all streaming files, the user is eternal and inexhaustible. The shell stands, rapt in attention, afore the user file.\nSo when does the user ever leave? The user never leaves. The shell simply kills itself when the user types exit. The shell would rather die than to face the prospect of reading the last word from the user.\nSo when does the shell ever break out of its rapt attention? Whenever it sees \\n, it is shaken out of its trance and interprets what the user has just said, in the interval bracketed between two \\ns.\nThe shell has one ear and two mouths. The ear is stdin, and the mouths are stdout and stderr. The shell has a tiny brain which is only capable of interpreting the few syntactic elements of bash scripts. Everything else it wants to do, it dutifully sends a binary message into the oracular altar of the Linux kernel, from which an oceanic voice replies the answer of the kernel."
  },
  {
    "objectID": "blog/posts/linux-notes/index.html#sec-path",
    "href": "blog/posts/linux-notes/index.html#sec-path",
    "title": "Notes on Using Linux",
    "section": "How to PATH",
    "text": "How to PATH\nThe PATH environment variable is a list of directories that the shell searches for commands. It is a colon-separated list of directories. For example, just about every Linux installation has a PATH variable that looks like:\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nTo add things to PATH, use the export command, like export PATH=\"$HOME/bin:$PATH\".\nenv # same as printenv\nexport VAR_NAME=\"value\"\nunset VAR_NAME\n\nexport PATH=newpath:$PATH # There is no simple way to undo this one.\n# but you can try export OLD_PATH=$PATH; ...; export PATH=$OLD_PATH\nFor Windows, use Get-ChildItem Env:."
  },
  {
    "objectID": "blog/posts/linux-notes/index.html#how-to-directories",
    "href": "blog/posts/linux-notes/index.html#how-to-directories",
    "title": "Notes on Using Linux",
    "section": "How to directories",
    "text": "How to directories\nBased on https://askubuntu.com/a/135679, https://serverfault.com/a/24525, man hier, Filesystem Hierarchy Standard - Wikipedia.\n\nWhere to install read-only things\n\n/bin and /sbin: binaries and superuser-binaries.\n/usr: System-wide, read-only files installed by the OS. This directory can be remotely mounted to multiple hosts safely.\n/usr/local: System-wide, read-only files installed by root. And that’s why most directory names from /usr are duplicated here.\n/opt - System-wide, read-only and bundled-up software. That is, software that does not split their files over bin, lib, share, include like well-behaved software should.\n/usr/bin and usr/sbin: They no longer exist. Just use /bin and /sbin.\n~/.local: the per-user counterpart of /usr/local, that is: software installed by (and for) each user. Like /usr, it has its own ~/.local/share, ~/.local/bin, ~/.local/lib.\n~/.local/opt: The per-user counterpart of /opt\n\nRequiem for /usr/bin and /usr/sbin: Originally, /bin and /lib are only for binaries and libraries required for booting, while /usr/bin and /usr/lib are for all the other executables and libraries. This is no longer true, as some binaries required for booting has over the years leaked into those two folders (if there is a way to make a mess, people will make it), so since Ubuntu 20.04, they no longer exist, to remove this mess.\nHow to install for local user only:\n./configure --prefix=$HOME/.local\nmake\nmake install\nHow to install for everyone: sudo ./configure && make && make install\n\n\nWhere to install read-write things\nRead-write things are typically configuration files, since they are read and written by both the user and binary executables.\n\n/etc: System-wide configuration. Typically used by the OS to decide what to do when starting up, shutting down, etc.\n~/.config: Per-user configuration files. Although because of legacy, you keep seeing nonsense like .bashrc in your home directory instead of ~/.config/.bashrc. Here rc means “run configuration”.\n\n\n\nWhere to do read-write things\n\n/home/username, or just ~: Each user typically is only able to modify their own folder here, like ~/myfile.txt.\n/tmp: If you need to create something just for the moment, then make it here. It will be deleted when the system restarts.\n\n\n\nOther things (you should not modify them)\n\n/run: runtime temporary data, representing the system’s state since the last boot. It’s used for storing process IDs, lock files, and other files that are would normally be stored in /tmp. It is basically /tmp for the machine.\n/var: Variable data. It is somewhat like /run in that both are meant to be read-written by programs, but unlike /run, data here persists over reboots. This is often used for logging information. For example, try vim /var/log/user.log\n\n\n\n\n\n\n\nWarning\n\n\n\nModifying anything below this line may brick the system. Reading is fine though.\n\n\n\n/lib: libraries. You should not handle it directly. Some libraries are added at OS installation, and others at program installation. If you have a broken installation, you might be asked to manually copy some files looking like libxxx.so here. (so stands for “shared object”.)\n/boot: Files required for booting. For example, the bootloader, the kernel, the initramfs (initial RAM file system).\n/dev: Device files. It typically looks like /dev/sda1, /dev/sda2, etc. Other than things like sda1 (for harddrive) you might notice tty1 and pty1’ which stand for “teletype” and “pseudo teletype”, respectively, but they are actually used as files to read whatever the user is typing from (the user is a file, see Section 1). There are some odd ones like:\n\n/dev/null, which is a “file” that you write to when you just want to throw something away (everything is a file, even a black hole…).\n/dev/urandom, which is a random number generator. It is preferred over /dev/random. See here.\n/dev/zero, which is a file that you can’t write to, but you can read, but it’s filled with zeros.\n\n/media: Mount removable medias, like USB drives and SD cards. For example, you can mount a USB at /media/usb1 and another one at /media/usb2. Mounting is typically done automatically by the system when you plug it in.\n/mnt: Mounts that are not so easily removable, like a hard drive, or a network drive. And unlike /media, mounting and unmounting is not automatic. On WSL, this typically has just one important thing: /mnt/c.\n/srv: Static files that are served out. /srv/http would be for static websites, /srv/ftp for an FTP server. It is usually used only on webservers, not an end-user machine like your laptop."
  },
  {
    "objectID": "blog/posts/linux-notes/index.html#environment-variables",
    "href": "blog/posts/linux-notes/index.html#environment-variables",
    "title": "Notes on Using Linux",
    "section": "Environment variables",
    "text": "Environment variables\n\nHow to control them\nUse echo $X to see what the current value of X is.\nFor current session, use export like export EDITOR=nano.\nFor all sessions, add to your .bashrc or .profile. If you just want to add it to the end (not recommended, as you can end up with an archeological tell), you can do one-liner like echo \"export EDITOR=nano\" &gt;&gt; ~/.bashrc.\n\n\nCommon ones\n\nPATH: path to binaries. See Section 2.\nEDITOR: default editor.\nSHELL: default shell.\nHOME: home directory.\nUSER: current user.\nPS1: current prompt (just try echo $PS1 if it doesn’t make sense).\n\nYou can change prompts in a rather arcane language. For example, try this one:\nexport PS1=\"\\[\\e]0;\\w\\a\\]\\n\\[\\e[32m\\]\\u@\\h \\[\\e[33m\\]\\w\\[\\e[0m\\]\\n\\\\$ \""
  },
  {
    "objectID": "blog/posts/linux-notes/index.html#cron-jobs",
    "href": "blog/posts/linux-notes/index.html#cron-jobs",
    "title": "Notes on Using Linux",
    "section": "Cron jobs",
    "text": "Cron jobs\nCron jobs are scheduled tasks that run periodically. For scheduling one-off tasks, use the at command. The name came from “Cronos”, the name of the Greek god of time. A good reference is Newbie: Intro to cron.\n\nQuick reference\nThe cron job syntax is as follows (See Crontab.guru - The cron schedule expression editor):\n# ┌───────────── minute (0–59)\n# │ ┌───────────── hour (0–23)\n# │ │ ┌───────────── day of the month (1–31)\n# │ │ │ ┌───────────── month (1–12)\n# │ │ │ │ ┌───────────── day of the week (0–6) (Sunday to Saturday)\n# │ │ │ │ │                                   \n# │ │ │ │ │\n# │ │ │ │ │\n# * * * * * &lt;command to execute&gt;\n\n* * * * * &lt;once a minute&gt;\n0 * * * * &lt;once an hour at 0-th minute&gt;\n0 0 * * * &lt;once a day at midnight&gt;\n0 0 1 * * &lt;once a month at the midnight of the 1-th day&gt;\n0 0 1 1 * &lt;once a year at the midnight of January 1&gt;\n* * * * 0 &lt;once a minute every Sunday&gt;\nCheck that cron service is running by systemctl list-unit-files --type=service | grep \"cron\"\nList cron jobs by crontab -l.\n\n\nCreating a Cron Job\nFor this example, we create a job that runs every 20 seconds.\n\nOpen Crontab: Open your crontab file by typing crontab -e in your terminal. This command opens the crontab file for the current user in the default text editor.\nWrite Cron Job: Standard cron jobs can’t run in a smaller granularity than a minute. For a once-per-20-seconds job, you’ll need to use a workaround.\nAdd the following lines to your crontab:\n\n * * * * * /path/to/script.sh\n * * * * * sleep 20; ~/cronjobs/script.sh\n * * * * * sleep 40; ~/cronjobs/script.sh\n\nScript Content: Create script.sh, have the following content:\n\n#!/bin/bash\nCRON_MESSAGE=\"Some message\"\necho \"The cron message is: $CRON_MESSAGE\"\nThen save and close the file, and chmod +x ~/cronjobs/script.sh to make it executable.\n\n\nBest Practices\n\nLocation: Store scripts in a dedicated directory, such as ~/cronjobs, for better organization.\nScript Naming: Use meaningful names for your scripts for easier identification.\nLogging: Implement logging within your scripts to capture output and errors for later review. It’s good practice to use /var/log/cron for logging.\n\n\n\nCron environment variables\nCron jobs run in a minimal environment, so any environment variable, like CRON_MESSAGE, is not accessible by the cron script. Instead, you have a few choices:\n\nPut it directly in the crontab file:\n\nCRON_MESSAGE=\"Some message\"\n* * * * * /path/to/script.sh\n\nPut it directly in the script:\n\n#!/bin/bash\nexport CRON_MESSAGE=\"Some message\"\n# rest of the script follows\n\nIf the variable is defined in an external file (like ~/.bashrc, ~/.profile, or a custom configuration file), you can source that file at the beginning of your script:\n\n#!/bin/bash\nsource /path/to/environment_variables_file\n# rest of the script follows\n\n\nTroubleshooting\n\nCheck Permissions: Ensure your script is executable and the cron daemon has the necessary permissions to run it.\nLogs: Check /var/log/cron or relevant logs for errors.\n\nIf you’re using WSL, ensure that the cron service is running since it doesn’t start by default. Use sudo service cron start. You can configure ~/.bashrc by adding the following line: sudo service cron start, but it would make you enter the password at every login.\nAlternatively, enable systemd as described at Section 6.1."
  },
  {
    "objectID": "blog/posts/linux-notes/index.html#how-to-wsl",
    "href": "blog/posts/linux-notes/index.html#how-to-wsl",
    "title": "Notes on Using Linux",
    "section": "How to WSL",
    "text": "How to WSL\n\ninit vs systemd\nEvery Linux starts its first process with some root process. The init is the traditional and simpler one, and systemd is more modern and advanced one.\nWSL by default starts with init instead of systemd, perhaps to save time and compute. This makes things annoying for some users. You can check by ps -p 1 -o comm and see what it returns.\nTo enable systemd, enter in your /etc/wsl.conf with:\n[boot]\nsystemd=true\nor just use cat \"[boot]\\nsystemd=true\" &gt;&gt; /etc/wsl.conf."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "The Perceptron Controversy\n\n\n\n\n\n\n\nAI\n\n\nhistory\n\n\n\n\nConnectionism died in the 60s from technical limits to scaling, then resurrected in the 80s after backprop allowed scaling. The Minsky–Papert anti-scaling hypothesis explained, psychoanalyzed, and buried.\n\n\n\n\n\n\nJan 1, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nThe Backstory of Backpropagation\n\n\n\n\n\n\n\nAI\n\n\nmath\n\n\nphysics\n\n\nhistory\n\n\n\n\nWhy backprop was resisted for 20 years: assumption of discretely spiking neurons, goal of synthesizing Boolean logic, fear of local optima, and bad luck. Werbos has the best claim for invention.\n\n\n\n\n\n\nDec 26, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nCybernetic artificial intelligence\n\n\n\n\n\n\n\nAI\n\n\ncybernetics\n\n\nmath\n\n\nhistory\n\n\n\n\nMachine learning and self-reproduction according to Norbert Wiener.\n\n\n\n\n\n\nDec 23, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nReading Perceptrons\n\n\n\n\n\n\n\nAI\n\n\nmath\n\n\n\n\nA long, hard stare into the math of Perceptrons, the mythical neural network killer.\n\n\n\n\n\n\nDec 21, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nNotes on Web Design\n\n\n\n\n\n\n\nprogramming\n\n\n\n\nMy quick reference for designing content for the Internet.\n\n\n\n\n\n\nDec 10, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nFermi Estimation for Neural Networks\n\n\n\n\n\n\n\nAI\n\n\neconomics\n\n\n\n\nThe bitter lesson in bite-sized packets.\n\n\n\n\n\n\nDec 5, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nNotes on Using Linux\n\n\n\n\n\n\n\nprogramming\n\n\n\n\nMy quick reference for using Linux for doing things.\n\n\n\n\n\n\nDec 10, 2022\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nSerial Experiments Lain\n\n\n\n\n\n\n\nanime\n\n\nfun\n\n\n\n\nLain is the collective subconscious as a neural network.\n\n\n\n\n\n\nNov 1, 2022\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nThe Wigner Rotation in Special Relativity via Hyperbolic Geometry\n\n\n\n\n\n\n\nphysics\n\n\nmath\n\n\nfun\n\n\n\n\nThe other special relativity paradox that you have never heard of.\n\n\n\n\n\n\nMay 1, 2018\n\n\nYuxi Liu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n  \n  \nYuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks."
  },
  {
    "objectID": "blog/posts/cybernetic-artificial-intelligence/index.html",
    "href": "blog/posts/cybernetic-artificial-intelligence/index.html",
    "title": "Cybernetic artificial intelligence",
    "section": "",
    "text": "In the creation lore of artificial intelligence, John McCarthy coined the term “artificial intelligence” to distinguish his approach from “cybernetics”, which was dominated by Norbert Wiener. However, if that is the case, then we can infer that there was a real danger that artificial intelligence could have been confused with Norbert Wiener’s cybernetics. That is, they must have some real similarity. If the similarity is not in style or approach – as otherwise there would be no need to distinguish the two – then the similarity must be in goal.\nAnd indeed there is. I was first alerted to this by Wiener’s little book (Wiener 1964, chap. 4), which briefly sketched out a machine that could purportedly imitate any machine, including another copy of itself, thus effecting both machine learning and machine reproduction. Unfortunately, it attempted to be a popular science book and explained concepts with a minimum of mathematical symbols, thus providing the reader with a minimum of understanding. Nevertheless, I figured it out by consulting his other books, and so I decided to write this post for posterity, as a simple sketch of another way to think about machine intelligence and machine life. A view of another universe, perhaps, or the theoretical basis of some hard sci-fi to be written.\nThis cybernetic machine is a rather obscure one, and most of the references are decades-old. The main references I relied on to write this essay are (Wiener 1958, chaps. 10, 11; 2019, chap. 9; O’Hagan 2013; G. H. Harris and Lapidus 1967).\nWiener’s terminology is occasionally obsolete; unfortunately, the subject of cybernetic artificial intelligence has essentially never been updated since Wiener, so anyone trying to study the subject must contend with his publications. So here is a list of obsolete terminology and my explanations.\n\nshot noise, shot effect, tube noise: white noise. It is not Poisson noise.\nchaos: a generic term for randomness, or random variables. It is not chaos theory.\nvoltage multiplier: an electronic device that multiplies two voltages together, by \\((x, y) \\mapsto xy\\). It is not the voltage multiplier.\n\n\n\nAccording to Wiener, cybernetics is the science of control and communication in the animal and the machine. To be more precise, he used the following model of sweeping generality: the nonlinear transducer model.\n\nDefinition 1 (signal) A signal is a function of time, written as \\(x(t)\\), where \\(t \\in \\mathbb{R}\\). The signal can take values in any space, but for convenience, we will only consider the case where \\(x\\) is real-valued. That is, we only consider functions of type \\(x : \\mathbb{R}\\to \\mathbb{R}\\).\n\nA transducer transduces, meaning that it takes in a signal and outputs a signal. Antennas, filters, circuits of resistors and capacitors, and essentially electronic devices that do not require power input to run, are transducers. Wiener considered only deterministic, causal transducers, meaning that the output of a transducer at time \\(t\\) is determined by the inputs during the period \\((-\\infty, t]\\).\nIn large sections of electronic engineering, electronic circuits are studied as either linear and continuous, or as nonlinear and discrete. Nonlinear but continuous devices are very difficult to study in general, but Wiener took on it and constructed a theory for general nonlinear continuous transducers.\n\nDefinition 2 (nonlinear transducer)  \n\nThe transducer is a function \\(T\\) that, given any real-valued function \\(x : \\mathbb{R}\\to \\mathbb{R}\\), returns a real-valued function \\(T[x] : \\mathbb{R}\\to \\mathbb{R}\\). In other words, it sends real-valued signals to real-valued signals.1\nThe transducer’s output at any time \\(t\\) is determined by the value of \\(x\\) on the interval \\((-\\infty, t]\\). In other words, it is causal and deterministic.\nThe transducer is stationary in time. That is, if \\(x\\) is the same over the interval \\((-\\infty, t]\\) as \\(x'\\) over the interval \\((-\\infty, t']\\), then \\(T[x](t) = T[x'](t')\\).\nThe transducer has a limited amount of memory, so that its dependence on the high-frequency details of the input signal decreases rapidly as frequency increases. We will explain the meaning of this assumption later.\nThe transducer depends analytically on the frequencies of the input signal. We will explain the meaning of this assumption later.\n\n1 Wiener actually studied general multidimensional signals, but those mostly involve notational complications, with no new ideas."
  },
  {
    "objectID": "blog/posts/cybernetic-artificial-intelligence/index.html#cybernetics",
    "href": "blog/posts/cybernetic-artificial-intelligence/index.html#cybernetics",
    "title": "Cybernetic artificial intelligence",
    "section": "",
    "text": "In the creation lore of artificial intelligence, John McCarthy coined the term “artificial intelligence” to distinguish his approach from “cybernetics”, which was dominated by Norbert Wiener. However, if that is the case, then we can infer that there was a real danger that artificial intelligence could have been confused with Norbert Wiener’s cybernetics. That is, they must have some real similarity. If the similarity is not in style or approach – as otherwise there would be no need to distinguish the two – then the similarity must be in goal.\nAnd indeed there is. I was first alerted to this by Wiener’s little book (Wiener 1964, chap. 4), which briefly sketched out a machine that could purportedly imitate any machine, including another copy of itself, thus effecting both machine learning and machine reproduction. Unfortunately, it attempted to be a popular science book and explained concepts with a minimum of mathematical symbols, thus providing the reader with a minimum of understanding. Nevertheless, I figured it out by consulting his other books, and so I decided to write this post for posterity, as a simple sketch of another way to think about machine intelligence and machine life. A view of another universe, perhaps, or the theoretical basis of some hard sci-fi to be written.\nThis cybernetic machine is a rather obscure one, and most of the references are decades-old. The main references I relied on to write this essay are (Wiener 1958, chaps. 10, 11; 2019, chap. 9; O’Hagan 2013; G. H. Harris and Lapidus 1967).\nWiener’s terminology is occasionally obsolete; unfortunately, the subject of cybernetic artificial intelligence has essentially never been updated since Wiener, so anyone trying to study the subject must contend with his publications. So here is a list of obsolete terminology and my explanations.\n\nshot noise, shot effect, tube noise: white noise. It is not Poisson noise.\nchaos: a generic term for randomness, or random variables. It is not chaos theory.\nvoltage multiplier: an electronic device that multiplies two voltages together, by \\((x, y) \\mapsto xy\\). It is not the voltage multiplier.\n\n\n\nAccording to Wiener, cybernetics is the science of control and communication in the animal and the machine. To be more precise, he used the following model of sweeping generality: the nonlinear transducer model.\n\nDefinition 1 (signal) A signal is a function of time, written as \\(x(t)\\), where \\(t \\in \\mathbb{R}\\). The signal can take values in any space, but for convenience, we will only consider the case where \\(x\\) is real-valued. That is, we only consider functions of type \\(x : \\mathbb{R}\\to \\mathbb{R}\\).\n\nA transducer transduces, meaning that it takes in a signal and outputs a signal. Antennas, filters, circuits of resistors and capacitors, and essentially electronic devices that do not require power input to run, are transducers. Wiener considered only deterministic, causal transducers, meaning that the output of a transducer at time \\(t\\) is determined by the inputs during the period \\((-\\infty, t]\\).\nIn large sections of electronic engineering, electronic circuits are studied as either linear and continuous, or as nonlinear and discrete. Nonlinear but continuous devices are very difficult to study in general, but Wiener took on it and constructed a theory for general nonlinear continuous transducers.\n\nDefinition 2 (nonlinear transducer)  \n\nThe transducer is a function \\(T\\) that, given any real-valued function \\(x : \\mathbb{R}\\to \\mathbb{R}\\), returns a real-valued function \\(T[x] : \\mathbb{R}\\to \\mathbb{R}\\). In other words, it sends real-valued signals to real-valued signals.1\nThe transducer’s output at any time \\(t\\) is determined by the value of \\(x\\) on the interval \\((-\\infty, t]\\). In other words, it is causal and deterministic.\nThe transducer is stationary in time. That is, if \\(x\\) is the same over the interval \\((-\\infty, t]\\) as \\(x'\\) over the interval \\((-\\infty, t']\\), then \\(T[x](t) = T[x'](t')\\).\nThe transducer has a limited amount of memory, so that its dependence on the high-frequency details of the input signal decreases rapidly as frequency increases. We will explain the meaning of this assumption later.\nThe transducer depends analytically on the frequencies of the input signal. We will explain the meaning of this assumption later.\n\n1 Wiener actually studied general multidimensional signals, but those mostly involve notational complications, with no new ideas."
  },
  {
    "objectID": "blog/posts/cybernetic-artificial-intelligence/index.html#orthogonal-functions",
    "href": "blog/posts/cybernetic-artificial-intelligence/index.html#orthogonal-functions",
    "title": "Cybernetic artificial intelligence",
    "section": "Orthogonal functions",
    "text": "Orthogonal functions\nIn order to present Wiener’s approach to nonlinear control theory, we need a small amount of the theory of orthogonal polynomials. Specifically, we need the Hermite and Laguerre polynomials. They are not as famous as the trigonometric functions, but they are used in the same way as trigonometric functions in Fourier analysis. In Fourier analysis, every well-behaved function (in the Fourier sense) is decomposable as an infinite linear sum of trigonometric functions. Similarly, every well-behaved function (in the Hermite sense) is decomposable into an infinite linear sum of Hermite functions, and the same applies to Laguerre functions.\n\nLaguerre functions\nThe Laguerre polynomials have many equivalent definitions. We will use three of those.\n\nDefinition 3 (Laguerre polynomials) Direct definition:\n\\[\nL_n(x) := \\sum_{k} \\binom{n}{k}\\frac{(-1)^k}{k!} x^k\n\\tag{1}\\]\nDefinition by a generating function:\n\\[\ng(t, x) = \\sum_{n=0}^\\infty t^n L_n(x)=  \\frac{1}{1-t} e^{-tx/(1-t)}\n\\tag{2}\\]\nDefinition by ordinary differential equation:\n\\[\n\\begin{cases}\nxL_n'' + (1-x) L_n' + nL_n &= 0 \\\\\nL_n(0) &= 1 \\\\\nL_n'(0) &= -n\n\\end{cases}\n\\tag{3}\\]\n\n\nProposition 1 The three definitions are equivalent.\n\n\nProof. The direct definition of \\(L_n\\) satisfies \\(xL_n'' + (1-x) L_n' + nL_n = 0\\), and has the same value and first derivative at \\(x=0\\). By the uniqueness theorem of ordinary differential equations, the two definitions are equal.\nBy taking the first partial derivatives \\(\\partial_x g, \\partial_t g\\), and simplifying, we obtain some recurrence relations of \\(L_n\\), including \\(xL_n'' + (1-x) L_n' + nL_n = 0\\). Plugging in the case where \\(x=0\\), we obtain \\(L_n(0) = 1\\) and \\(L_n'(0) = -n\\).\n\n\nProposition 2 (Laguerre polynomials are orthogonal with respect to the exponential distribution) \\[\n\\int_{0}^\\infty e^{-x}L_m(x) L_n(x)dx = \\delta_{mn}\n\\]\n\n\nProof. Explicitly integrate\n\\[\\int_{0}^\\infty e^{-x} g(t, x)  g(s, x) dx\\]\nthen expand the Taylor series of both sides in powers of \\(s, t\\).\n\n\nDefinition 4 (Laguerre functions) \\[\n\\psi_n(t) := e^{-t/2}L_n(t)\n\\]\n\nThe Laguerre functions make the orthogonality cleaner:\n\\[\n\\int_0^\\infty \\psi_n(t)\\psi_m(t) dt = \\delta_{mn}\n\\tag{4}\\]\nJust like in Fourier analysis, by virtue of orthogonality, we can represent any well-behaved function on \\((-\\infty, t]\\) as an infinite sum of Laguerre functions\n\\[\nf(t-\\tau) = \\sum_{n \\geq 0} c_n \\psi_n(\\tau)\n\\]\nby taking a convolution with the Laguerre functions\n\\[\nc_n = \\int_0^\\infty f(t-\\tau) \\psi_n(\\tau) d\\tau.\n\\]\n\n\nHermite polynomials\n\nDefinition 5 (physicist’s Hermite polynomials) \\[\n\\sum_n H_n(x) \\frac{1}{n!}t^n = e^{-t^2 + 2tx} = g(t, x)\n\\tag{5}\\]\n\n\nProposition 3 (Hermite polynomials are orthogonal with respect to the normal distribution with variance 1/2) \\[\\int e^{-x^2}H_n(x) H_m(x) dx = \\sqrt\\pi 2^n  n! \\delta_{mn}\\]\n\n\nProof. \\[\\int e^{-x^2} g(t, x)g(s, x) dx = \\sum_{n, m \\geq 0}\\frac{1}{n! m!}t^ns^m \\int e^{-x^2} H_n(x) H_m(x) dx\\]\nDirectly compute the left-hand side and find that it equals \\(\\sqrt\\pi e^{2st}\\). Now expand it in powers of \\(s\\) and \\(t\\)."
  },
  {
    "objectID": "blog/posts/cybernetic-artificial-intelligence/index.html#learning-and-reproducing-any-transducer",
    "href": "blog/posts/cybernetic-artificial-intelligence/index.html#learning-and-reproducing-any-transducer",
    "title": "Cybernetic artificial intelligence",
    "section": "Learning and reproducing any transducer",
    "text": "Learning and reproducing any transducer\nNow we are ready to perform the “Hermite–Laguerre expansion”, Wiener’s way to analyze (learn) and synthesize (reproduce) arbitrary transducer using pure analog devices.\n\nAlgebra of analog circuitry\nIn an analog electronic circuit, real numbers are represented as voltages across two points (“ports”) in the circuit. Adding is as simple as making a serial connection. Negation is even simpler: just connect the ports in the opposite direction. Multiplication is significantly trickier, but it can be done. There are electronic devices with nonlinear response characteristics, meaning that they have two input ports and two output ports, and if you apply an input voltage \\(x\\) across one such device, the output voltage would be \\(f(x)\\) where \\(f\\) is not a linear function. Now suppose that \\(f(x) = x^2\\).22 From our vantage point, the universal approximation theorems proven in the early 1990s show that, generically, if we have any nonlinear function \\(f_0\\) at all, then we can construct any activation function \\(f\\) as a neural network, by using many copies of the \\(f_0\\) device as activation functions and linear devices as weights and biases.\nWith such an \\(f\\), we can multiply two voltages by \\(xy = (f(x+y) - f(x) - f(y)) \\times 0.5\\), and so we can construct any polynomial function in any number of variables. That is, we can do algebra by analog devices, as long as we have a voltage multiplier.\nOf course, we don’t hear about voltage multipliers often, and this is no accident – it is quite difficult to get one with good performance. In the preface to the second edition of Cybernetics1961 (Wiener 2019, xli), Wiener waxes praise about Gabor’s breakthrough circuit device that could multiply two voltages at a frequency of \\(1\\; \\mathrm{kHz}\\):\n\nWhile there are many approaches to the problem of multiplying two functions electrically, this task is not technically easy. On the one hand, a good multiplier must work over a large range of amplitudes. On the other hand, it must be so nearly instantaneous in its operation that it will be accurate up to high frequencies. Gabor claims for his multiplier a frequency range running to about 1,000 cycles. … he does not state explicitly the amplitude range over which his method of multiplication is valid nor the degree of accuracy to be obtained. I am awaiting very eagerly3 an explicit statement of these properties so that we can give a good evaluation of the multiplier for use in other pieces of apparatus dependent on it.3 Gabor published it in the same year of 1961: (Gabor, Wilby, and Woodcock 1961).\n\nTo our modern ears, multiplying two voltages 1000 times a second by analog means seems simultaneously astonishing and obsolete. Intel 8086 in 1976 already could multiply a million times a second, and whatever has come of Gabor’s universal filter? It seems to me that Wiener never accepted the future of digital computers, preferring the concrete certainty of magnetic cores and electric wires.\n\n\nThe Laguerre filter bank\nTo find the Laguerre coefficients of a signal, we need to perform a convolution. Convolutions become products after a Laplace transform, so we need to find the Laplace transform of the Laguerre functions \\(\\psi_n = e^{-x/2}L_n(x)\\). Fortunately, it is easy to compute. We simply read from a standard table:\n\\[\n\\mathcal L [t^n e^{-\\alpha t}\\theta(t)] = \\frac{n!}{(s+\\alpha)^{n+1}}\n\\]\nwhere \\(\\theta(t) = 1_{t \\geq 0}\\) is the zero-one step function.\nThen, since the Laplace transform is linear, we have after simplification\n\\[\n\\mathcal L[\\psi_n\\theta] = \\frac{1}{s+1/2}\\left(\\frac{s-1/2}{s+1/2}\\right)^n\n\\]\nThis gives a simple filter bank that constructs the Laguerre coefficients for any signal. The input signal passes through a \\(\\frac{1}{s+1/2}\\) filter to obtain the \\(c_0\\) coefficient, and then through a \\(\\frac{s-1/2}{s+1/2}\\) filter to obtain the \\(c_1\\) coefficient, and so on. This filter bank can be constructed with standard resistors and capacitors.\nThe following theorem finishes the last piece of the puzzle. (G. H. Harris and Lapidus 1967) claims that the proof is found in (Bose 1956; George Henry Harris 1966), but I did not check.\n\nTheorem 1 Let \\(x(t)\\) be a white noise process with variance \\(1/2\\), and let \\(c_0(t), c_1(t), \\dots\\) be its Laguerre coefficients, then:\n\nthe joint stochastic process \\((c_0(t), c_1(t), \\dots)\\) is stationary;\nfor any fixed \\(t\\in \\mathbb{R}\\), the random variables \\(c_0(t), c_1(t), \\dots\\) are independent samples of the standard gaussian distribution \\(\\mathcal N(0, 1/2)\\).\n\n\n\n\nThe Hermite coefficients\nFor a given input signal \\(x : \\mathbb{R}\\to \\mathbb{R}\\), we pass it into the Laguerre filter bank. The readouts from the filter bank are the signals \\(c_0(t), c_1(t), \\dots\\). They satisfy the equation\n\\[\nx(t - \\tau) = \\sum_{n\\geq 0} c_n(t) \\psi_n(\\tau), \\; \\forall \\tau \\geq 0\\quad \\forall t \\in \\mathbb{R}\n\\]\nIn words, at any cut-off time \\(t \\in \\mathbb{R}\\), the signal we have seen so far is \\(x(t - \\tau)\\) with \\(\\tau \\geq 0\\). This signal can then be decomposed as a linear sum of Laguerre functions \\(\\sum_{n\\geq 0} c_n(t) \\psi_n(\\tau)\\), with \\(c_n(t)\\) as the Laguerre coefficients. The coefficients depend on the cut-off time \\(t\\), but do not depend on \\(\\tau\\), which is not “real” time, but only a kind of “relative historical time”, as we look into the past standing at time \\(t\\).\nA transducer, by our assumption, is deterministic and causal, so that \\(T[x](t)\\) is a deterministic function of the signal we have seen so far, and so it is a deterministic function of \\(c_0(t), c_1(t), c_2(t), \\dots\\). Note carefully that it is determined by \\(c_0(t), c_1(t), c_2(t), \\dots\\) at this very instant \\(t\\). It does not need the values of \\(c_0(t'), c_1(t'), c_2(t'), \\dots\\) at any \\(t' \\neq t\\). We write it as follows:\n\\[\nT[x](t) = T(c_0(t), c_1(t), c_2(t), ...)\n\\]\nBy our assumption that the transducer has a limited memory, we should be able to ignore the higher frequency components of the input signal, and still recover a good approximation of \\(T\\). That means that \\(T[x](t) = T(c_0(t), c_1(t), c_2(t), \\dots) \\approx T(c_0(t), \\dots, c_n(t))\\), with the approximation increasing in accuracy as \\(n\\) increases.\nBy our assumption that the transducer is analytic with respect to the input, \\(T(c_0(t), \\dots, c_n(t))\\) has a multivariate Hermite serial expansion (the same idea as multivariate Taylor expansion):\n\\[\nT(c_0(t), \\dots , c_n(t)) = \\sum_{m_0, \\dots , m_n \\geq 0} T_{m_0, \\dots , m_n} H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t))\n\\]\nWe are quite close to the target. We can compute the Laguerre coefficients \\(c_n(t)\\) of any input signal by the Laguerre filter bank. We can construct analog circuits that compute \\(H_m(c_n(t))\\), the Hermite polynomial values of the Laguerre coefficients. The remaining challenge is to determine the coefficients \\(T_{m_0, \\dots, m_n}\\).\nThis is where Theorem 1 comes to finish the construction. Let \\(x(t)\\) be a white noise process, then since\n\\[\nT[x](t) \\approx \\sum_{m_0, \\dots , m_n \\geq 0} T_{m_0, \\dots , m_n} H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t))\n\\]\nand since the Laguerre coefficients are independent samples of the standard gaussian, we have\n\\[\nT_{m_0, \\dots , m_n} \\approx \\mathbb{E}\\left[T[x](t)\\; H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t))\\right]\n\\]\nwhere the expectation is in the sense of ensemble expectation. That is, we would run this experiment once with a white noise process, freeze it exactly at the moment \\(t\\), then run it again with another white noise process, freeze it exactly at the moment \\(t\\), and so on. Then we average over all these experiments.\nHowever, Theorem 1 states that the Laguerre coefficients are stationary, meaning that we have ergodicity4: the ensemble average is the time average, and so4 Wiener was really into ergodic theory.\n\\[\nT_{m_0, \\dots , m_n} \\approx \\lim_{T \\to \\infty} \\frac{1}{T} \\int_0^T T[x](t)\\; H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t)) dt\n\\]\nThe integrand is computable by the analog devices we described. The integration-and-averaging can be done with a very-low-pass filter – taking the average is essentially passing only the zero-frequency signal, thus it is the low-pass filter with the lowest possible passband. Finally, since white noise is all around us, it can be obtained in many ways, such as by amplifying the thermal noise in a resistor.\nAnd so we have a finished machine, where the white noise \\(x\\) and the signal to imitate \\(T[x](t)\\) come in, and the fitted parameters \\(T_{m_0, \\dots, m_n}\\) come out. The fitted parameters can be automatically read and adjusted by electromechanical devices, such as relays and step motors, allowing us to connect the machine in parallel with an unknown transducer, run it for a period over a white noise input, and ultimately achieve a machine that precisely imitates the unknown transducer.\n\n\n\nThe fully-formed imitation machine. (G. H. Harris and Lapidus 1967, fig. 2)"
  },
  {
    "objectID": "blog/posts/cybernetic-artificial-intelligence/index.html#the-science-of-control-and-communication-in-the-animal-and-the-machine",
    "href": "blog/posts/cybernetic-artificial-intelligence/index.html#the-science-of-control-and-communication-in-the-animal-and-the-machine",
    "title": "Cybernetic artificial intelligence",
    "section": "The science of control and communication in the animal and the machine",
    "text": "The science of control and communication in the animal and the machine\nWe have reached the end of the road, facing an all-analog general-purpose learning machine. This machine can imitate any black-box transducer, and thus is a form of machine learning. If we have two such machines, and randomly set the parameters of one machine, then the other machine would learn to imitate the same behavior. And since each parameter setting creates a different behavior, purely by imitating behavior, the parameters would be copied from one machine to the other. This is an explicit construction for how behaviorism can work, even if not in our universe, then in another universe where the animals really are those imitation devices.\nAs Wiener speculated (Wiener 2019, 248–49), biological learning and reproduction are “philosophically similar” to this machine:\n\nWhile both Professor Gabor’s methods and my own lead to the construction of nonlinear transducers, they are linear to the extent that the nonlinear transducer is represented with an output which is the sum of the outputs of a set of nonlinear transducers with the same input. These outputs are combined with varying linear coefficients. This allows us to employ the theory of linear developments in the design and specification of the nonlinear transducer. And in particular, this method allows us to obtain coefficients of the constituent elements by a least-square process. If we join this to a method of statistically averaging over the set of all inputs to our apparatus, we have essentially a branch of the theory of orthogonal development. Such a statistical basis of the theory of nonlinear transducers can be obtained from an actual study of the past statistics of the inputs used in each particular case. I ask if this is philosophically very different from what is done when a gene acts as a template to form other molecules of the same gene from an indeterminate mixture of amino and nucleic acids, or when a virus guides into its own form other molecules of the same virus out of the tissues and juices of its host. I do not in the least claim that the details of these processes are the same, but I do claim that they are philosophically very similar phenomena.\n\nIt seems like this device, as it stands, would be plagued by the same issues that plague a general analog computer – error correction, bad gains, and intractable nonlinearities. Still, it stands as a vision of an alternative future in an alternative world, if not an alternative future of our world."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html",
    "href": "blog/posts/neural-scaling-laws/index.html",
    "title": "Fermi Estimation for Neural Networks",
    "section": "",
    "text": "Fermi estimation is a technique for estimating quantities through rough approximations and educated guesses. Named after physicist Enrico Fermi, this method involves breaking down a problem into simpler, more manageable parts, making reasonable assumptions, and using simple arithmetic to arrive at a good enough answer.\nThis post walks you through a simple example of Fermi estimation of a deep learning model. Specifically, we will try to estimate the design parameters of a hypothetical GPT-5 with 1 trillion parameters. Along the way, we will touch upon various other items like the Landauer limit (because of course we will)."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#gpt-like-agi",
    "href": "blog/posts/neural-scaling-laws/index.html#gpt-like-agi",
    "title": "Fermi Estimation for Neural Networks",
    "section": "GPT-like AGI",
    "text": "GPT-like AGI\nLet’s get into the mood for Fermi estimation, by estimating the number of parameters necessary for achieving human-level Artificial General Intelligence, under two assumptions: that a GPT-like architecture is enough; that it requires only matching the human brain in the \"numbers\".\nLet’s estimate how many layers the hypothetical GPT model should have. When prompted with a question, the first answer comes to mind in about a second, and it is generally a good one, if not the best. Perhaps slow deliberation is nothing but stringing together a long chain of snap judgments, guided by snap judgments about snap judgments.\nThe characteristic time-scale of a brain is 0.01 seconds – the fastest brainwave, gamma wave, is 100 Hz. This indicates that the brain takes on the order of 100 steps to make a snap decision – the \"hundred-step-rule\" of Jerome Feldman(Feldman and Ballard 1982). This is suspiciously close to the depth of the largest model of GPT-3, which has 96 layers.\nHow many parameters would such a model require? The brain has \\(10^{15}\\) synapses. It’s unclear how precise each synapse is, but one estimate states that the hippocampal synapse has a precision of about 5 bits (Bartol Jr et al. 2015), which can be stored within a 16-bit floating point number, with room to spare.\nAssuming that, we expect an AGI GPT to have \\(10^{15}\\) parameters, or 1000× that of our hypothetical GPT-5."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#chinchilla-scaling-law",
    "href": "blog/posts/neural-scaling-laws/index.html#chinchilla-scaling-law",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Chinchilla Scaling Law",
    "text": "Chinchilla Scaling Law\nThe paper \"Training Compute-Optimal Large Language Models\" (Hoffmann et al. 2022) reported a series of training runs on language models, trained by Google DeepMind researchers. Each training run is characterized by four numbers:\n\n\\(L\\): the final loss (negative log-likelihood per token) achieved by the trained model.\n\\(N\\): the number of parameters in the model.\n\\(D\\): training dataset size, measured in tokens.\n\\(C\\): training compute cost, measured in FLOP.\n\nAfter training a few hundred models, they obtained a large dataset of \\((L, N, D, C)\\), and they fitted a statistical law of the form\n\\[L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0,\\]\nwhere the parameters are\n\\[\\alpha = 0.34, \\beta = 0.28, A = 406.4, B = 410.7, L_0 = 1.69.\\]\nThey also estimated that the cost of training compute \\(C\\) is proportional to \\(ND\\). This is understandable, because each token must flow through the entire model and \"hit\" each parameter once, incurring a fixed number of floating point operations. They estimated that it takes 6 FLOPs per parameter per token. That is,\n\\[C = C_0 ND, \\quad C_0 = 6\\]\nGiven the assumptions, for each fixed computing budget \\(C\\), we can solve for the optimal \\(D\\) and \\(N\\), which is usually referred to as \"Chinchilla optimal\" training:\n\\[\\begin{cases}\n        \\min_{N, D} L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0,\\\\\n        \\text{such that } C_0 ND = C.\n\\end{cases}\\]\nSolve the above equations symbolically to find \\(N_{opt}, D_{opt}\\) as a function of \\(C, C_0, \\alpha, \\beta, A, B\\). Then, plug in the numerical values of the parameters, to find a numerical expression for \\(N_{opt}, D_{opt}\\) as a function of \\(C\\).\n\n\nSolution\n\nSince \\(C = C_0 ND\\), we have \\(N = \\frac{C}{C_0 D}\\). Plug it into \\(\\min_{N, D} L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0\\), we obtain\n\\[\\min_{D} L = \\frac{A}{(\\frac{C}{C_0 D})^\\alpha} + \\frac{B}{D^{\\beta}} + L_0.\\]\nTake derivative with respect to \\(D\\) and set it to zero. We get an expression for \\(D_{opt}\\). Plug it back to \\(C = C_0 ND\\), we get an expression for \\(D_{opt}\\). These simplify to:\n\\[N_{o p t}(C)=G\\left(\\frac{C}{C_0}\\right)^a, \\quad D_{o p t}(C)=G^{-1}\\left(\\frac{C}{C_0}\\right)^b, \\quad \\text { where } \\quad G=\\left(\\frac{\\alpha A}{\\beta B}\\right)^{\\frac{1}{\\alpha+\\beta}}, a=\\frac{\\beta}{\\alpha+\\beta}, b=\\frac{\\alpha}{\\alpha+\\beta}.\\]\nPlugging in the numerical values, we get\n\\[\\begin{cases}\n        N_{opt}(C) = 0.6 \\; C^{0.45} \\\\\n        D_{opt}(C) = 0.3 \\; C^{0.55} \\\\\n        L_{opt}(C) = 1070 \\; C^{-0.154} + 1.7\n    \\end{cases}\n    \\]\n\nIn the same paper, they also performed a direct statistical fitting, to find the optimal \\(N, D\\) for a given \\(C\\), without going through the intermediate steps above. This gives a slightly different result (only slightly different – as you would know after solving the previous problem):\n\\[N_{opt}(C) = 0.1 C^{0.5}; \\quad D_{opt}(C) = 1.7 C^{0.5}.\\]\nFor the rest of the essay, we will use these equations as the Chinchilla scaling laws. Do not use the equations you derived for the previous problem.\nSuppose we decide that our next AI should have 1 trillion (\\(N = 10^{12}\\)) parameters, and we use Chinchilla scaling laws. How much compute would it cost to train, and how many tokens would its training dataset have?\n\n\nSolution\n\n\\(N = 0.1 \\times C^{0.5} = 10^{12}\\), so \\(C= 10^{26}\\) FLOP, and \\(D = 1.7 \\times 10^{13}\\), or 17 trillion tokens."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#dataset-size",
    "href": "blog/posts/neural-scaling-laws/index.html#dataset-size",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Dataset size",
    "text": "Dataset size\nAssuming each English word cost about 1.4 tokens, how many English words would 10 trillion tokens be? Assuming each page has 400 words, and each book has 300 pages, how many books is that? To put it into context, look up the size of Library of Congress, and Google Books, and compare with the number we just calculated.\n\n\nSolution\n\n\\(10 / 1.4 = 7\\) trillion words. If each book has \\(400 \\times 300 = 0.12\\) million words, then that is 60 million books, if they were all in English.\n\nSince humans are kind of the same everywhere, book lengths should be kind of the same everywhere – information-dense languages would naturally have books with lower apparent word-count, but the same information (measured in bytes)."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#memory-requirement",
    "href": "blog/posts/neural-scaling-laws/index.html#memory-requirement",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Memory requirement",
    "text": "Memory requirement\nTypically, a deep learning model has 16-bit floating point parameters. Modern systems sometimes use lower precision (e.g. 8-bit) floating point numbers to save space, but generally it is necessary to use at least 16-bit during training, and the model is converted to lower precision after training (\"post-training quantization\").\nGiven that each parameter is a 16-bit floating point number, how much memory does it cost to store a model with 1 billion parameters? How about our hypothetical GPT-5, which has 1 trillion parameters? How many A100 GPU (VRAM = 40 GB) would be required to contain the full GPT-5 model?\n\n\nSolution\n\n1 billion parameters is 2 billion bytes, or 2 GB. Our hypothetical GPT-5 would take up 2 TB. It would take 50 A100 to contain it.\nNow, 50 A100 GPUs has a running cost of about 100 USD/hr, but not such a terrible problem, but then, during training, you would have to run an optimizer like Adam, which would need three floating point numbers per parameter (gradient, momentum, scale), instantly increasing the memory requirement to 200 A100 GPUs.\n\n\nMemory cost\nThis table1 gives the price per megabyte of different storage technology, in price per megabyte (2010 dollars), up to 2018.1 Source: Storage 2: Cache model – CS 61 2018.\n\n\n\nYear\nMemory (DRAM)\nFlash/SSD\nHard disk\n\n\n\n\n~1955\n$411,000,000\n\n$6,230\n\n\n1970\n$734,000.00\n\n$260.00\n\n\n1990\n$148.20\n\n$5.45\n\n\n2003\n$0.09\n$0.305\n$0.00132\n\n\n2010\n$0.019\n$0.00244\n$0.000073\n\n\n2018\n$0.0059\n$0.00015\n$0.000020\n\n\n\nThe same costs relative to the cost of a hard disk in ~2018:\n\n\n\nYear\nMemory\nFlash/SSD\nHard disk\n\n\n\n\n~1955\n20,500,000,000,000\n\n312,000,000\n\n\n1970\n36,700,000,000\n\n13,000,000\n\n\n1990\n7,400,000\n\n270,000\n\n\n2003\n4,100\n15,200\n6.6\n\n\n2010\n950\n122\n3.6\n\n\n2018\n295\n7.5\n1\n\n\n\nSuppose long-term memory storage can last 1 year before being replaced. How much money does it cost per year to store a 1 trillion parameter model on an SSD, the most expensive form of long-term storage? How much money does it cost to store a 1 trillion parameter model on DRAM memory? Use 2018 prices.\n\n\nSolution\n\nSSD cost 0.00015 USD/MB in 2018, or 0.15 USD/GB. 1 trillion parameters cost 2000 GB of storage, or about 300 USD. So the total cost of storage is 300 USD/year, just a rounding error.\nIn contrast, DRAM cost 0.006 USD/MB, which is 40x that of SSD, so the total cost is 12000 USD.\nNow, compared with the cost of A100 GPU themselves? It takes about 50 A100 GPU to run the model, and each would cost about 20,000 USD. So the cost of long-term memory is essentially zero, and the cost of DRAM is about \\(\\frac{12000}{20000\\times 50} = 1\\%\\) of the total cost of GPU.\nSo what is the limit? The memory bandwidth, which we will see in the next question.\n\n\n\nMemory bandwidth and latency\nWhile the memory itself is cheap, moving the data requires expensive high-bandwidth wiring. Indeed, the memory bandwidth between the DRAM (or \"VRAM\" for \"Video RAM\") and the little processors on the GPU is a main bottleneck on how good the GPU can perform.\nDuring a single forward pass of a model, the parameters of the model are loaded from the DRAM of the GPU into the fast cache memories, then pushed through the thousands of computing processors on the GPU.\nA100 GPU has a memory bandwidth of 1.6 TB/s.\nWhat is the minimal latency, in seconds, for the GPU to perform a single forward pass through our hypothetical GPT-5 model with 1 trillion parameters? How many tokens can it output (autoregressively) in one minute? How about GPT-3 (175 billion parameters)?\nSince we are just trying to compute an order-of-magnitude estimate, let’s assume for the problem that the model fits onto a single DRAM on a single GPU. You can also ignore the need to read/write model activations and optimizer states.\n\n\nSolution\n\nSince the model takes up 2 TB of memory, it would take at least 1.3 seconds to even load the model from DRAM, during a single forward pass.\nAutoregression means that the next token cannot be generated until the previous one is already generated, so the model can only generate one token per 1.3 seconds, or 46 tokens per minute. A Chat-GPT5 would be a very slow talker!\nHowever, it can run in batch mode. So for example, it might be able to run 1 million conversations in parallel, outputting one token for all conversations every 2 seconds. Latency is the hard limit, but throughput is not.\nGPT-3 with 175 billion parameters would run 1000/175 times faster, at 262 tokens per minute, about the speed of a fast human talker. This corresponds well with the general impression that ChatGPT types about 200 words a minute.\n\n\n\nBatch inference\nThere are some ways to improve latency. For example, the model can be parallelized over several GPUs, which effectively increases the memory bandwidth. For example, \"tensor parallelism\" splits each layer into several GPUs.\nThere is also \"pipeline parallelism\", which splits the model into layers. The first few layers go into one GPU, the next few go into another, and so on. This does NOT decrease latency, but it does increase throughput.\nThe fundamental bottleneck in an autoregressive model like GPT is that, by design, they have to start a sentence without knowing how it will end. That is, they have to generate the first token before generating the next one, and so on. This can never be parallelized (except by egregious hacks like speculative decoding).\nOne reason Transformers dominated over RNN is that training and inferring an RNN both must be done one-token-at-a-time. For Transformers, training can be done in parallel over the entire string. Inferring however still cannot be parallelized.\nParallelization tends to be a deeply troublesome business – parallel programming is generally deeply troublesome. However, there is a general trick to improve throughput: batch mode. For example, GPT-5 might be able to run 1 million conversations in parallel, outputting one token for all conversations per forward pass. This trick works until the batch size is so large that the activations due to tokens takes up about as much memory bandwidth as the model parameters.\nConcretely, we can estimate what is a good batch size for GPT-3 175B. It has 96 attention layers, each with 96×128-dimension heads. It is typically run with 16-bit precision floating point numbers. For a single token, how many megabytes would all the floating point activations cost?\n\n\nSolution\n\nA single token would cost \\(96 \\times 96 \\times 128\\) floating point activations, or about 2.4 MB.\n\nThe model itself has 175 billion 16-bit floating point parameters, taking up about 350 GB. How many tokens do we need to put into a batch, before the activations occupy the same amount of memory as the model parameters?\n\n\nSolution\n\nIn order for the activations of tokens to occupy the same memory as the GPT-3 parameters itself, we need about \\(\\frac{350 \\;\\mathrm{GB}}{2.4 \\;\\mathrm{MB}} = 0.15 \\text{million tokens}\\).\nIf we count the optimizer states for the model during training, then GPT-3 takes up \\(4 \\times 350 \\;\\mathrm{GB} = 1.4 \\;\\mathrm{TB}\\), and so we need about 0.6 million tokens.\nThis explains why during training, the batch sizes of the largest models typically are on the order of 1 million tokens. It also shows why there is a natural drive towards scale – only the largest companies can expect to regularly run 0.2 million chats simultaneously."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#training-cost",
    "href": "blog/posts/neural-scaling-laws/index.html#training-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Training cost",
    "text": "Training cost\nHow much money does compute cost? We can work through an example using the current standard computing hardware: Nvidia A100 GPU (40 GB VRAM version).\nThe most important specifications are:\n\nUnit price: 15000 USD.\nRental price: 2 USD/hr.\nSpeed: 0.3 petaFLOP/s = 3E14 FLOP/s.\nPower: 0.3 kW.\nMemory bandwidth: 1600 GB/s.\n\nIn the literature about the largest AI models, the training cost is often reported in units of “petaFLOP-day”, which is equal to 1 petaFLOP/second x 1 day. How many FLOP is 1 petaFLOP-day? What is the equivalent number of A100-hour? If we were to buy 1 petaFLOP-day of compute with rented A100 GPU, how much would it cost?\n\n\nSolution\n\n1 petaFLOP-day = 1 petaFLOP/second x 1 day = 1E15 FLOP/s x 8.64E4 s = 8.64E19 FLOP.\nSince 1 A100 can run at 0.3 petaFLOP/s, getting 1 petaFLOP-day requires us to run 1/0.3 A100 for 1 day, or 80 A100-hours. At the price of 2 USD/hr, it would cost 160 USD.\n\nThe largest model of GPT-3 cost 3640 petaFLOP-days to train (according to Table D.1 of the report). How much would it cost if it were trained with A100? How much money does it cost to train our hypothetical GPT-5?\n\n\nSolution\n\n2E25 FLOP = 0.2 amount of GPT-5 = 17 million A100-hours = 33 million USD.\nAnd accounting for the utilization rate of 30%, that would give us 110 million USD.\nOh, and if you want some kind of official confirmation? OpenAI’s CEO Says the Age of Giant AI Models Is Already Over | WIRED\n\nAt the MIT event, Altman was asked if training GPT-4 cost $100 million; he replied, “It’s more than that.”\n\n\nIn reality, the GPU cannot be worked to their full speed, and we only use about 30% of its theoretical peak FLOP/s (so for example, for A100, we only get 0.1 petaFLOP/s, instead of 0.3 petaFLOP/s).2 For this question, we assume that the utilization rate is 100%.2 The utilization rate of 30% is according to EpochAI.\nAlso, we are assuming the training happens in one go, without hardware failures, divergences, and other issues requiring restart at a checkpoint. There is not much published data from large-scale training, but the OPT-175B training run (described later) took 3 months to complete, but would have taken only 33 days if there were no need for the many restarts. This suggests that the restarts would increase the computing cost by 2× to 3×.\nFor context, here are the costs of development of various items3:3 Sorry, not adjusted for inflation to the same year, but they are roughly in the range of 2000–2020 USD.\n\niPhone 1: 150 million USD.\nA typical 5 nm chip: 0.5 billion USD.\nAirbus A380: 18 billion USD. (Bowen 2010, Table 4.3)\nThree Gorges Dam: 250 billion CNY, or about 30 billion USD.\nManhattan Project: 24 billion USD (2021 level)\nApollo Program: 178 billion USD (2022 level)\n\nComment on the cost of our hypothetical GPT-5. Is it on the order of a large commercial actor like Google, or a state actor like China?\n\n\nSolution\n\nThe cost is on the order of 1 billion USD, which is roughly the largest projects undertaken by private corporations. Larger models, as those contemplated by AGI projects in Google, OpenAI, etc, have historically been undertaken by state actors only.\n\nHere is another way to estimate. Building a large AI model, according to accountancy, would be investment into a production machine, and thus part of “capital expenditure” (“CapEx”). We can find out what proportion is 1 billion USD in their total CapEx. I looked it up here.\n\nDuring the 2020–2022, Microsoft has yearly CapEx on average 25 billion USD.\nGoogle has about 25 billion USD.\nMeta, 20.\nAmazon, 63.\n\nIn short, training something like GPT-5 would cost a lot, on the order of 5% of total CapEx, even by the standards of the largest companies. It can probably scale up by another factor of 10× if they really push it, but not more.\nIn order to train even larger AI models, those AI models must enter production. They must become a productive member of society, otherwise the company would nott have the money to train them.\nMicrosoft announces new supercomputer, lays out vision for future AI work (2020):\n\nThe supercomputer developed for OpenAI is a single system with more than 285,000 CPU cores, 10,000 GPUs and 400 gigabits per second of network connectivity for each GPU server. Compared with other machines listed on the TOP500 supercomputers in the world, it ranks in the top five, Microsoft says.\n\nThe largest companies, like Microsoft have GPU on the order of 10000 A100. What is the wall clock hour of training GPT-5, assuming you have 10000 A100, perfect utilization, and no interruptions?\n\n\nSolution\n\n83 million hours / 10000 = 350 days. Almost exactly 1 year.\n\n\nThe difficulty of large-scale training\nLarge models do end up diverging many times during training runs, requiring restarts. Sometimes the divergence is so bad that the whole training run must be started from scratch. Sometimes the convergence is too slow, requiring fixes to the optimizer and learning rate schedules, etc. All together, we should expect the failures, restarts, deadends… to triple the cost at least, to ~1 billion USD.\nIt is not easy to find \"stories from the trenches\" for actually training large-scale neural networks that really show this, but thanks to Meta, we have one. In 2021, a team of 5 engineers from Meta trained a LLM with 175 billion parameters, in 3 months, using 1024 80GB A100 GPUs from. Excluding all the divergences, hardware failures, and other issues that caused lost progress, the final model would have taken about 33 days of continuous training.\nThey have kept journals during their training. This is now published at metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq · GitHub. You can see how difficult it is to train a large model. Selected quotes:\n\nThese notes cover ~90 restarts over the course of training the lineage of this current model (experiments 12.xx).\n\n\nFound issues with the new dataset where perplexity was unreasonably low… After applying as much regex-ing as we could to salvage the dataset, we relaunched another set of experiments to test LPE (experiments 20-29) on the new dataset. We didn’t have time to retrain a new BPE on the final dataset, so we fell back to using the GPT-2 BPE.\n\n\nFrom experiment 11.4 onward, we saw grad norm explosions / loss explosions / nans after a couple hundred updates after each restart, along with extremely unstable loss scales that would drop to the point of massively underflowing. We started taking more and more drastic actions then, starting with increasing weight decay to 0.1, lowering Adam beta2 to 0.95, lowering LR again, until finally by experiment 11.10 we hot-swapped in ReLU and also switched to a more stable MHA calculation (noting that the x**3 term in GeLU might be a source of instability with FP16).\n\n\n\nOn November 11, we started training our 12.00 experiment with all of these changes, and since then, the only restarts we’ve had to make were all related to hardware issues (missing GPUs on instances, training randomly hanging after including a new node, ECC errors, partial checkpoint upload after hardware error, CUDA errors, NCCL errors, etc.).\n\n\nReplacement through the cloud interface can take hours for a single machine, and we started finding that more often than not we would end up getting the same bad machine again.\n\n\nThere were also issues with blob store when downloading 1.6TB of a single model checkpoint (992 files, each ~1.7GB) on restarts, at which point the downloads themselves would start hanging nondeterministically, which then delayed training recovery even further.\n\n\nWe managed to hit our top three record long runs of the experiment these past two weeks, lasting 1.5, 2.8, and 2 days each! If we were to look at only the runs that have contributed to pushing training further and plot training perplexity against wall clock time, we get the following [The breaks are due to the Thanksgiving holiday]:"
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#inference-cost",
    "href": "blog/posts/neural-scaling-laws/index.html#inference-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Inference cost",
    "text": "Inference cost\nInference cost a lot less money than training, but it’s still a substantial cost. For Transformer language models, it costs about 2 FLOPs per parameter to infer on one token.\nGiven that GPT-3.5 has 175 billion parameters, how many FLOPs would it take to infer on 1 million tokens? How much money would it cost if it were run with A100?\n\n\nSolution\n\n\\(175 \\;\\mathrm{billion} \\times 1 \\;\\mathrm{million} \\times 2 = 4\\times 10^{17} \\;\\mathrm{FLOPs}\\). Now one A100-hour is \\(8.64\\times 10^{19} \\;\\mathrm{FLOPs}\\), so that is 1/200 A100-hour, or about 0.01 USD.\nThe price offered by OpenAI is 2 USD per 1 million tokens, so it’s a very profitable business… but see next question.\n\nThe price offered by OpenAI is 2 USD per 1 million tokens. Assuming that GPT-3.5 cost 10 million USD to develop and train, how many tokens must be sold, just to recoup the cost? Assuming each English word cost about 1.4 tokens, and each essay is 1000 words, how many essays would it be equivalent to?\n\n\nSolution\n\nSince the cost is almost negligible (1/200 of the revenue), we can pretend it’s all profit. So, the profit is 2 USD per 1 million tokens. We need to recoup 10 million USD, so we need \\(10 \\;\\mathrm{million} / 2 \\times 1 \\;\\mathrm{million} = 5\\times 10^{12} \\;\\mathrm{tokens}\\), or 4 billion essays.\nAbout one essay per person on earth, or 10 essays per person in America… is that too much to ask?\n\nMoore’s law applies to both CPU and GPU. From 2006 to 2021, GPU FLOP/s per dollar has been doubling every 2.5 years. We can fairly well assume that the price of petaFLOP-day also decreases at the same rate.\n\n\n\n\n\nAssuming Moore’s law continues to hold indefinitely, and that there will be no algorithmic progress. When would GPT-3 become as cheap as a typical hobbyist project, say, 1000 USD?44 Since a 2006 GPU and a 2020 GPU both have the same lifespan (1–4 years usually), and their cost of electricity is only a small proportion of their running expense, the price of GPU FLOP/s per dollar at the point of purchase is roughly proportional to the rental rate of GPU.\n\n\nSolution\n\nGPT-3 cost 6 million USD to train. For it to cost just 1000 USD, we need to drop its price 6000-fold. Now, \\(\\log_2(6000) \\times 2.5\\; \\mathrm{year} = 30 \\; \\mathrm{year}\\). So it would be around 2050."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#energetic-cost",
    "href": "blog/posts/neural-scaling-laws/index.html#energetic-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Energetic cost",
    "text": "Energetic cost\nThe Landauer limit states that the cost of erasing one bit of information is \\(E = k_B T \\ln 2\\), where \\(k_B\\) is the Boltzmann constant, and \\(T\\) is the temperature of the computing machinery. At room temperature, \\(T = 300 K\\), giving us \\(E = 3\\times 10^{-21} J\\).\nNow, one FLOP is a floating point operation, meaning that you start with two 32-bit objects and end up with a single 32-bit object. You start with 64 bits and end up with just 32 bits, and so you lose 32 bits. So by the Landauer limit, the minimal energetic cost is \\(32 k_B T \\ln 2\\).\nGiven this, what is the minimal energy required for performing one FLOP? What is the minimal power (in Watts) required to perform 300 TFLOP/s, as in A100 GPU? Compared this to the actual value of 300 Watts.\n\n\nSolution\n\nThe energy per FLOP is \\(E_{FLOP} = 32 \\times 3\\times 10^{-21} J = 10^{-19} J\\). At 300 TFLOP/s, we need \\(P_{A100} = 3\\times 10^{14} E_{FLOP}/s = 3\\times 10^{-5}W\\). The actual value of 300 Watts is 10 million times more than the theoretical minimum.\nThere is still plenty of room at the bottom!\n\nFor context, the equivalent FLOP/s for the human brain is controversial, but a brief scan through the review article says that it should be about 1E18 FLOP/s. The energetic power of the brain is well-known: about 30 Watts. This means the brain is about 30000x more energy-efficient than A100 GPU, but still 300x less efficient than the theoretical minimum.\n\nThe lowest possible power for life\nThe slowest metabolism found on earth (so far) is in microbes living below deep ocean surface. They had to survive on the energy of “marine snow” falling to the surface, then slowly diffusing through the subsurface mud. Their power consumption is estimated at… \\(10^{-21} W\\). Since their temperature is about the temperature of the ocean water, which is liquid, their temperature is still about \\(T = 273 K\\), and so the Landauer limit is still about \\(3\\times 10^{-21} J\\). This shows that they can lose at most 500 bits every day.\nMost of the operations must be spent on simply repairing biomolecules, since repairing is effectively a 1-bit-losing operation: you start with a molecule that could be “damaged” or “fine”, and end up with a molecule that is “fine”, losing 1 bit. In fact, there are usually many possible ways to be “damaged”, so you would lose several bits.\nFor example, suppose you have a base-pair-mutation in one copy of the DNA. You want to fix it by checking with the other copies. This then means you start with 4 possible states (ACGT) and end up with just 1 state, losing 2 bits in the process."
  },
  {
    "objectID": "blog/posts/neural-scaling-laws/index.html#environmental-cost",
    "href": "blog/posts/neural-scaling-laws/index.html#environmental-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Environmental cost",
    "text": "Environmental cost\nAccording to “Carbon emissions and large neural network training”(Patterson et al. 2021), the carbon emission of training GPT-3 is 552 tCO2. According to a 2021 poll of climate economists, 1 tCO2 emission should cost somewhere between 50 and 250 USD. Let’s take their geometric average of 112 USD.\nIf we add all the tCO2 cost to the training of GPT-3, how much more expensive would it be? Compare that with its A100-GPU cost of training.\n\n\nSolution\n\n\\(112 \\times 552 = 62,000 \\;\\mathrm{USD}\\).\nPreviously, we calculated the GPU cost of training to be around 6 million USD. So fully carbon-pricing the emission would make it 1% more expensive.\nGenerally, adding in the tCO2 cost of training the largest models increase the cost by about 1 to 2 % (I have computed it in several ways). Since the global GDP increases by about 2% a year, this shows that fully accounting for the carbon cost of AI would delay it by perhaps 1 year.\n\n\n\nSide note for economics students\n\nYou might argue that fully carbon-taxing the global economy would raise not just electricity prices, but also the price of production, etc. For example, the global shipping industry uses fuel that has a high carbon emission.\nHowever, if the global shipping industry were to fully add carbon taxing into the fuel cost, then GPU prices would rise a little, but food prices would rise a lot. Why? Simply put, the shipping cost of a GPU, even if we were to take account of all the trips its components have made around the world, is still small. After all, one A100 GPU is just 1.7 kg, but sells for 15000 USD, so a tonne of A100 GPU can sell for 9 million USD.\nTo put it into perspective: the price of US maize is about 200 USD/tonne. So if shipping cost were to increase so much as to double the price of maize, it would only increase price of A100 by 1/45000.\nEven if we were to assume that producing an A100, being a commodity with a globally scattered supply chain, means shipping its components around the world 1000 times, the total shipping cost of A100 would only be increased by 1/45 = 2.2%.\nIn other words, properly pricing carbon emissions would only increase the price of AI by up to 5%, even as it doubles the price of food. AI is already green (within 5%), but food is far from green. That’s even before we get to the subsidies! We don’t have GPU subsidies, but we certainly have corn subsidies…\nIn this regard, it’s not “Green AI” that is important, but Green Corn, Green Timber, and all the other bulk commodities.\n\nTo put the number in another context, compare it with some typical American food. According to Our World in Data, it cost about 50 kg of CO2 emission per 1 kg of beef.\nAlso, an average American person (not household) consumed 38 kg of beef in 2020.\nCompare the CO2 emission of GPT-3 and CO2 emission from beef consumption. Assuming each burger (\"quarter pounder\") contains 1/4 pound (113 grams) of beef. How many burgers would be equivalent to the CO2 emission of GPT-3?\n\n\nSolution\n\n113 grams of beef emits about 5.6 kg of CO2, so GPT-3 emits about (552 ton)/(5.6 kg) = 90,000 burgers. It is about the same amount as 250 people eating one burger everyday, for a whole year.\n38 kg of beef gives about 2 tCO2 emission. So the emission of GPT-3 is equal to the emission due to beef consumption of 276 American persons.\n\nI think this strongly argues against the conclusion from (Patterson et al. 2021):\n\nTo help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark… We believe such new incentives could lead to a virtuous cycle where ML practitioners compete to increase accuracy while lowering energy consumption and CO2e that could bend the curve of ML carbon footprint growth for computationally intensive NLP models.\n\nOne, the carbon footprint of ML is already minimal, compared to the cost of just making all the GPUs.\nTwo, accounting for CO2 is a dreadfully boring business,5 and should properly be done by carbon taxing by the public officials. The ML practitioners are not climate economists or accountants, and they should not be burdened by accounting for climate change. If the price is right, then they simply need to budget how much money they want to spend on the training run, and the market would optimize the rest (including optimizing the right level of climate change6).5 If you don’t believe me, try reading (Patterson et al. 2021).6 The right level of climate change is not \"none\", but rather \"when the marginal cost equals marginal benefit\". This might sound controversial, but it is just introductory economics.\n\n(Krugman 2002) Luckily, at this point the orthodoxy of the academic economists is very much a minority position among intellectuals in general; one can seem to be a courageous maverick, boldly challenging the powers that be, by reciting the contents of a standard textbook.\n\n\nIn one sentence: There need be no new incentive other than the profit motive."
  },
  {
    "objectID": "blog/posts/web-design-notes/index.html",
    "href": "blog/posts/web-design-notes/index.html",
    "title": "Notes on Web Design",
    "section": "",
    "text": "General references:"
  },
  {
    "objectID": "blog/posts/web-design-notes/index.html#html",
    "href": "blog/posts/web-design-notes/index.html#html",
    "title": "Notes on Web Design",
    "section": "HTML",
    "text": "HTML\nHTML (HyperText Markup Language) is the standard markup language for creating web pages.\nHTML was created by Tim Berners-Lee in 1989. The key metaphor for HTML is the “editing markup”, as follows: Back in the old days, authors would write or typewrite their document in the exact same font, from the first word to the last word. Then the document is sent to an editor, who would edit it by marking up the words, such as drawing squiggly lines, crossing things out, changing their font size, and writing other instructions for the type-setter (which back then meant someone who would take out types from a box and set them into the right ordering for the printing press).\nSo, one should think of an HTML document as starting with a plaintext of exactly the same format, from the first to the last word, then adding marks upon it.\n\n&lt;!DOCTYPE&gt;\nThe &lt;!DOCTYPE&gt; tag is used to declare the document type. It is usually like &lt;!DOCTYPE html&gt;, although there are rare variants, where instead of html, we have html -//w3c//dtd xhtml 1.0 transitional//en http://www.w3.org/tr/xhtml1/dtd/xhtml1-transitional.dtd, html -//w3c//dtd xhtml 1.0 strict//en http://www.w3.org/tr/xhtml1/dtd/xhtml1-strict.dtd, etc.\nWell, if you don’t care much for the details, use &lt;!DOCTYPE html&gt; is always fine. If you do care, read on.\nThe xhtml thing came from early 2000s, where there was a movement to XML-everything. Instead of the poorly specified HTML, there would be XHTML, which can be checked for syntax, and compiled into an abstract syntax tree. Despite this, people just kept on using HTML and ignored XHTML.\nDespite the universal ambitions of XML, it is now in the land of old soldiers, who never die, but just fade away. * The SVG vector graphics format. * MathML, which is like LaTeX but in XML. * The RSS and Atom feeds, which… unfortunately, are also mostly legacy now. Who even use these nowadays? * The acronym AJAX, which stands for Asynchronous JavaScript and XML JSON. After looking at XML and JSON, I am quite glad this replacement happened.\n\n\nSemantic markup\nUse &lt;article&gt; to surround a block of article. There can be any number of articles in a single HTML file.\nThe id of tags must be unique within each HTML file.\n\n\nTips for fast loading\nUse &lt;img decoding=\"async\" loading=\"lazy\" ...&gt; for images, to allow the rest of the page to load before the images are loaded. To avoid Cumulative Layout Shift (CLS), I think something like\nimg {\n  max-width: 100%;\n  height: auto;\n}"
  },
  {
    "objectID": "blog/posts/web-design-notes/index.html#css",
    "href": "blog/posts/web-design-notes/index.html#css",
    "title": "Notes on Web Design",
    "section": "CSS",
    "text": "CSS"
  },
  {
    "objectID": "blog/posts/web-design-notes/index.html#javascript",
    "href": "blog/posts/web-design-notes/index.html#javascript",
    "title": "Notes on Web Design",
    "section": "JavaScript",
    "text": "JavaScript"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "Yuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks."
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "Over the years I have written some notes. Too short for publication, and not exactly fit for blog posting, but still might be useful to someone, so I collect them here."
  },
  {
    "objectID": "notes/index.html#expository-notes",
    "href": "notes/index.html#expository-notes",
    "title": "Notes",
    "section": "Expository notes",
    "text": "Expository notes\nVisually Deriving the Wigner Rotation by Spacetime Diagrams. Obsoleted by my post. This is a term paper I wrote in 2018 for Theoretical Physics. The date in the pdf is the recompilation date, not the time when I actually wrote the paper.\nHyperbolic dynamical systems, chaos, and Smale’s horseshoe: a guided tour. This is the companion paper to a presentation I gave for the course Introduction to Ergodic theory in 2019.\nAn Overview of Information Geometry. This is the term paper for Advanced Differential Geometry. and since I really did not like lectures, I asked to do something to substitute for the mandatory attendance. I was asked to write a term paper, which produced this. Information geometry is a weird thing. The premise is beautiful, but the books are terribly confusing, and what little I have managed to understand seems disappointing. It feels like the whole field is overpromising and underdelivering.\nHandout for honours seminar talk on AIXI. A presentation handout for AIXI. For my undergrad thesis, I was going to be advised by Marcus Hutter, but he left ANU just before the start of semester, and I had to scramble for another advisor. Still, I found AIXI worth knowing, so for my mandatory short talk, I gave a presentation. I managed to compress the essentials to two pages, perfect for handing out on double-sided printed sheets."
  },
  {
    "objectID": "notes/index.html#undergrad-thesis",
    "href": "notes/index.html#undergrad-thesis",
    "title": "Notes",
    "section": "Undergrad thesis",
    "text": "Undergrad thesis\nBeyond expectations, but within limits – the theory of coherent risk measures.\nFor a quick summary, see my seminar presentation.\nMy undergraduate thesis written in 2019 at ANU, on the topic of coherent risk measures. The first chapter is a readable introduction to risk measures in general (as in, why we might need to use more than the mean and the variance). The rest of it is very dry and I imagine it is of only interest to specialists. The centerpiece of the thesis is a straightforward proof of the central limit theorem for CVaR, which is a slight generalization of expectation. Like the central limit theorem, this theorem states that the sample CVaR converges to the true CVaR like\n\\[\n\\frac{\\text{sample CVaR}_\\alpha - \\text{true CVaR}_\\alpha}{\\sqrt N} \\xrightarrow{d} \\mathcal N(0, \\sigma^2(\\alpha))\n\\]\nwhere \\(\\sigma^2(\\alpha)\\) has a certain expression. As soon as I have calculated it myself, thinking that I had finally made a new discovery, I found it published before in the literature. Still my expression is simpler than the previous publications, so I believe it is still worth something after all."
  },
  {
    "objectID": "notes/index.html#corrections",
    "href": "notes/index.html#corrections",
    "title": "Notes",
    "section": "Corrections",
    "text": "Corrections\nWhen I was not yet mathematically mature, I used to study textbooks carefully, checking every letter through a brain-filter. I no longer do this, but while I was doing this, I created some erratas. Perhaps those will be of use to some people.\nIt is a rather odd thing that errata are hard to share. I would have thought there ought to be some kind of Wikipedia for errata, where people just post errata for textbooks. The lack of such an Error-pedia seems to require an economic explanation, as it can just use MediaWiki, the same technology powering Wikipedia.\n\nConway, John B. A course in point set Topology. Belin: Springer, 2014.\nWalter, P. “An introduction to ergodic theory (Graduate Texts in Math. 79) Springer-Verlag.” Berlin-Heidelberg-New York (1982).\nHiriart-Urruty, Jean-Baptiste, and Claude Lemaréchal. Fundamentals of convex analysis. Springer Science & Business Media, 2004."
  },
  {
    "objectID": "blog/posts/backstory-of-backpropagation/index.html",
    "href": "blog/posts/backstory-of-backpropagation/index.html",
    "title": "The Backstory of Backpropagation",
    "section": "",
    "text": "The backpropagation algorithm could have been invented by a slight generalization of the adjoint method, or the costate method, which was widely known among control theorists in the 1950s. Hybrid control theory leads almost inexorably to the backpropagation algorithm.\nSeppo Linnainmaa’s claim of priority is his 1970 master thesis – in Finnish, and never published. An English paper in 1976 was published that may have been relevant to automatic differentiation research, but not to neural network research. He has priority but no paternity.\nPaul Werbos’ claim of priority is that he developed the backpropagation algorithm in 1971, but was frustrated at every turn whenever he tried publishing it, not managing until around 1982. After that, it was taken up by many neural network researchers. He has priority and paternity.\nAs he freely admits, David Rumelhart reinvented it in 1982, as he did not know the previous work. The 1986 paper he coauthored (Rumelhart, Hinton, and Williams 1986) became popular enough that nobody would need to reinvent it again. He has no priority, but paternity. As usual in science, it is not the first inventor who gets the credit, but the last re-inventor.\nI was puzzled by how none of the first wave of neural network researchers developed backpropagation. I still am. My guesses are, from most to least likely:\n\nBad luck.\nThey were misled by the contemporary understanding of real neurons, and the McCulloch–Pitts model. Back then everyone “knew” that real neurons are 0-1 spike generators.\nThey thought of machine learning as just another way to construct boolean functions, rather than smooth functions, so they kept using the 0-1 activation function.\nThey distrusted gradient descent, because it would get stuck at local optima. If only they had had cheap compute and data to experiment with, they would have discovered the “blessing of scale”: local optima are almost as good as the global optimum.\nThey attempted to achieve parity with digital computers, which were discrete.\nThey attempted to distance themselves from cybernetics, which included optimal control theory."
  },
  {
    "objectID": "blog/posts/backstory-of-backpropagation/index.html#abstract",
    "href": "blog/posts/backstory-of-backpropagation/index.html#abstract",
    "title": "The Backstory of Backpropagation",
    "section": "",
    "text": "The backpropagation algorithm could have been invented by a slight generalization of the adjoint method, or the costate method, which was widely known among control theorists in the 1950s. Hybrid control theory leads almost inexorably to the backpropagation algorithm.\nSeppo Linnainmaa’s claim of priority is his 1970 master thesis – in Finnish, and never published. An English paper in 1976 was published that may have been relevant to automatic differentiation research, but not to neural network research. He has priority but no paternity.\nPaul Werbos’ claim of priority is that he developed the backpropagation algorithm in 1971, but was frustrated at every turn whenever he tried publishing it, not managing until around 1982. After that, it was taken up by many neural network researchers. He has priority and paternity.\nAs he freely admits, David Rumelhart reinvented it in 1982, as he did not know the previous work. The 1986 paper he coauthored (Rumelhart, Hinton, and Williams 1986) became popular enough that nobody would need to reinvent it again. He has no priority, but paternity. As usual in science, it is not the first inventor who gets the credit, but the last re-inventor.\nI was puzzled by how none of the first wave of neural network researchers developed backpropagation. I still am. My guesses are, from most to least likely:\n\nBad luck.\nThey were misled by the contemporary understanding of real neurons, and the McCulloch–Pitts model. Back then everyone “knew” that real neurons are 0-1 spike generators.\nThey thought of machine learning as just another way to construct boolean functions, rather than smooth functions, so they kept using the 0-1 activation function.\nThey distrusted gradient descent, because it would get stuck at local optima. If only they had had cheap compute and data to experiment with, they would have discovered the “blessing of scale”: local optima are almost as good as the global optimum.\nThey attempted to achieve parity with digital computers, which were discrete.\nThey attempted to distance themselves from cybernetics, which included optimal control theory."
  },
  {
    "objectID": "blog/posts/backstory-of-backpropagation/index.html#the-backpropagation-algorithm",
    "href": "blog/posts/backstory-of-backpropagation/index.html#the-backpropagation-algorithm",
    "title": "The Backstory of Backpropagation",
    "section": "The backpropagation algorithm",
    "text": "The backpropagation algorithm\nTo set the stage, we should at least quickly derive the backpropagation algorithm. In one sentence, backpropagation is an algorithm that calculates the gradient of every parameter in an acyclic directed graph with respect to a single final parameter.\nThe graph can be finite or infinite, but in all cases, its acyclic directedness allows us to assign a “logical time” to each node.1 Some nodes are independent variables: they point to other nodes, but no nodes point to them. Other nodes are dependent. If we know the independent variables, we can propagate their values forward in logical time and determine the values of every node. This is the “forward pass”. Backpropagation goes backwards in logical time.1 An existence proof that anything we might care about must have a logical time: Anything that happens, happens in the real world. The real world has a single direction in time. Therefore, any computation that can happen, must have a logical time that is identical with physical time.\nNote that logical time does not have to coincide with physical time. For example, to model goal-directed behavior, it might be better to put the physical future into the logical past.\nWe use the convention of putting derivatives on the rows. So for example, for \\(f: \\mathbb{R}^2\\to\\mathbb{R}^2\\), we have\n\\[\n\\nabla_x f = \\frac{df}{dx} = \\begin{bmatrix}\n\\frac{df_1}{dx_1} & \\frac{df_1}{dx_2} \\\\\n\\frac{df_2}{dx_1} & \\frac{df_2}{dx_2}\n\\end{bmatrix}\n\\]\nThis convention simplifies a lot of equations, and completely avoids transposing any matrix.\n\nDiscrete logical time\nConsider an acyclic directed graph with a finite number of nodes. Each graph node is a finite-dimensional real vector, though they may have different dimensions. We order the acyclic graph on the number line, so that each node depends only on the earlier nodes. Now denote the graph nodes as \\(x_0, x_1, \\dots, x_T\\). By our ordering, \\(x_1\\) depends on only \\(x_0\\), and \\(x_2\\) depends on only \\(x_1, x_2\\), and so on:\n\\[\n\\begin{aligned}\nx_0 &= x_0 \\\\\nx_1 &= f_1(x_0) \\\\\n&\\cdots \\\\\nx_T &= f_T(x_0, x_1, \\dots , x_{T-1})\n\\end{aligned}\n\\]\nNow we perform an infinitesimal perturbation on every one of its independent variables. A forward propagation then causes an infinitesimal perturbation on every variable, independent or dependent. Denote these as \\(dx_0, dx_1, \\dots, dx_T\\). We can now compute the derivative of \\(dx_T\\) with respect to every other variable by backpropagating the perturbation. Suppose we can see only \\(dx_T\\), then the change in \\(x_T\\) due to \\(dx_T\\) is the identity. That is,\n\\[\n\\frac{dx_T}{dx_T} = I\n\\]\nNow suppose we can see only \\(dx_{T-1}\\), then the change in \\(x_T\\) due to \\(dx_{T-1}\\) can only come from the final step in the forward propagation. Therefore\n\\[\n\\frac{dx_T}{dx_{T-1}} = \\nabla_{x_{T-1}} f_T(x_0, x_1, \\dots , x_{T-1})\n\\]\nSimilarly, the change in \\(x_T\\) due to \\(dx_{T-2}\\) can come from either directly changing \\(x_T\\) or from changing \\(x_{T-1}\\) and thereby changing \\(x_T\\). Therefore,\n\\[\n\\frac{dx_T}{dx_{T-2}} =\n    \\nabla_{x_{T-1}} f_{T}(x_0, x_1, \\dots , x_{T-2}) +\n    \\underbrace{\\frac{dx_T}{dx_{T-1}}\\nabla_{x_{T-2}} f_{T-1}(x_0, x_1, \\dots , x_{T-2})}_{\\text{the chain rule}}\n\\]\nThis generalizes to the rest of the steps.\nThe above computation is fully general, but this makes it inefficient. In practical applications of backpropagation, the computation graph is usually very sparse, meaning that each \\(x_t\\) only directly influence a few more nodes down the line. In standard neural networks, typically \\(x_{t}\\) only directly influences \\(x_{t+1}, x_{t+2}\\). Thus, sparsity is vital for backpropagation to be relevant.\nAs a side note, we could in fact compute all derivatives, not just the first, in one single backward pass. Other than the second derivatives \\(\\nabla^2_{x_t}x_T\\), there is rarely any use for the other derivatives, such as \\(\\nabla_{x_t}\\nabla_{x_s}x_T, \\nabla^3_{x_t}x_T\\), etc. The second derivative was occasionally used for neural networks circa 1990, with the further simplification that they only keep the positive diagonal entries of \\(\\nabla^2_{x_t}x_T\\) and set all other entries to zero.(LeCun 1989)\n\n\nContinuous logical time\nConsider the problem of controlling a car along a highway. At a sufficient level of simplification, the car is just a single dot on a line, and the state of the car is determined by its speed and velocity. To avoid waiting forever, we require time \\(t\\in [0, T]\\). Write the state variable as follows:\n\\[\nx_t = (\\text{location at time }t, \\text{velocity at time }t)\n\\]\nIt might be confusing to use \\(x_t\\) for the state at time \\(t\\), instead of for location, but it makes the notation consistent.\nSuppose the only thing we can influence is how much we press the pedal. Write \\(u_t\\) to be the resulting acceleration we inflict on the car by pressing on the pedal. Further, assume that the car is slowed down by friction that is proportional to velocity. We then have:\n\\[\n\\dot x_t = f(x_t, u_t)\n\\]\nwhere \\(f(x_t, u_t) = (x_{t, 1}, -\\mu x_{t, 1} + u_t)\\) is the dynamics equation of the system, and \\(\\mu\\) is the friction coefficient.22 To allow for time-varying dynamics, simply replace \\(f(x_t, u_t)\\) with \\(f(t, x_t, u_t)\\). This clutters the notation without involving new ideas.\nNow, we can draw the computation graph as before. The computation graph has a continuum of nodes, but it has a very simple structure. The graph has two kinds of nodes: the \\(x_t\\) nodes and the \\(u_t\\) nodes. Its independent variables are \\(x_0\\) and all the \\(u_t\\) nodes. Each \\(x_{t+dt}\\) depends on only \\(x_{t}\\) and \\(u_t\\). This makes the two propagations particularly simple.\nThe forward propagation is obtained by integration, which we can unwrap into a form resembling the case of the discrete logical time:\n\\[\n\\begin{aligned}\nx_0 &= x_0 \\\\\nx_{dt} &= x_0 + f(x_0, u_0) dt \\\\\n&\\cdots \\\\\nx_{T} &= x_{T-dt} + f(x_{T-dt}, u_{T-dt}) dt \\\\\n\\end{aligned}\n\\]\nThe backpropagation is similarly obtained. By inspecting the computation graph, we can see that each \\(u_t\\) only directly influences \\(x_{t+dt}\\), giving\n\\[\n\\frac{dx_T}{du_t} = \\frac{dx_T}{dx_{t+dt}} \\nabla_{u_t}f(x_t, u_t) dt.\n\\]\nIt remains to compute \\(\\frac{dx_T}{dx_t}\\). This can be found by backpropagation too, since each \\(x_t\\) only directly influences \\(x_{t+dt}\\), we have\n\\[\n\\frac{dx_T}{dx_t} = \\frac{dx_T}{dx_{t+dt}} \\left[I + \\nabla_{x_t} f(x_t, u_t) dt\\right].\n\\]\nIf we denote the gradient as \\(g_t := \\frac{dx_T}{dx_t}\\), then we find an equation for \\(g_t\\):\n\\[\ng_t = \\left[I + (g_t + \\dot g_t dt) \\nabla_{x_t} f(x_t, u_t) dt\\right]\\implies \\dot g_t = -g_t\\nabla_{x_t} f(x_t, u_t)\n\\]\nThis equation bottoms out at the end time, \\(t=T\\), for which \\(g_T = \\frac{dx_T}{dx_T} = I\\). Thus we have the costate equations:\n\\[\n\\begin{cases}\ng_T &= I \\\\\n\\dot g_t &= - g_t \\nabla_{x_t} f(x_t, u_t)\n\\end{cases}\n\\]\nwhich must, as you can see, be integrated backwards in time – backpropagation again! Indeed, control theory practically compels us to find backpropagation.\n\n\nHybrid logical time\nWhen the computation graph has both nodes with discrete logical times and nodes with continuous logical times, we call such a system as having hybrid logical time.33 The name “hybrid” comes from “hybrid control theory”, which are systems with both discrete changes, or jumps, and continuous changes, or flow. They appear whenever a digital device is used to control a continuous system, such as a computer piloting an airplane.\nThe idea can be illustrated by the fundamental problem in control theory: how to optimize control? You cannot optimize without optimizing a real number, so we write down the number as \\(J\\), representing the “cost” of the trajectory. For example, in driving a car, we might want to optimize comfort, time-until-arrival, and fuel cost. We can then write \\(J\\) down as something like\n\\[\nJ = \\underbrace{(x_{T, 0} - x_{goal})^2}_{\\text{location should be at the goal location at the end time}}\n    + \\underbrace{(x_{T, 1} - 0)^2}_{\\text{speed should be zero at the end time}} + \\int_0^T u_t^2 dt\n\\]\nMore generally, the objective to be optimized is in the form\n\\[\nJ = A(x_T) + \\int_0^T L(x_t, u_t)dt\n\\]\nfor some real-valued functions \\(A, L\\).\nOf course, we can care about more than the state at the last time-step. We can care about multiple time-steps \\(t_0, t_1, \\dots, t_n\\) by writing down a cost function \\(J = \\sum_{i=0}^n A_i(x_{t_i}) + \\int_0^T L(x_T, u_T)dt\\), but this merely creates complications without introducing new ideas. Even more ornate computation graphs, weaving together multiple strands of continuous and discrete logical times, can be backpropagated in the same way without involving new ideas.\nDefine the costate \\(\\lambda_t := \\nabla_{x_t} J\\), then the costate backpropagates as:\n\\[\n\\lambda_t = L(x_t, u_t) dt + \\lambda_{t+dt}(I + \\nabla_{x_t}f(x_t, u_t)dt)\n\\]\nand by simplifying, we have the costate equation:\n\\[\n\\begin{cases}\n\\lambda_T &= \\nabla_{x_T} A(x_T) \\\\\n\\dot \\lambda_t &= - \\nabla_{x_t} L(x_t, u_t) - \\lambda_t \\nabla_{x_t} f(x_t, u_t)\n\\end{cases}\n\\]\nwhich can be solved by integrating backward in time.\nOnce we have obtained all the costates, we can compute \\(\\nabla_{u_t} J\\). Since \\(u_t\\) can only influence \\(J\\) either directly via \\(L(x_t, u_t)\\) or indirectly via \\(x_t\\), we have\n\\[\n\\nabla_{u_t} J = \\left[\\nabla_{u_t}f(x_t, u_t) \\lambda_t + \\nabla_{u_t}L(x_t, u_t)\\right]dt\n\\]\nNote that \\(\\nabla_{u_t} J\\) is an infinitesimal in \\(dt\\). This is qualitatively different from \\(\\nabla_{x_t}J = g_t\\), which is not an infinitesimal, but a mere matrix. Why is it so? The simplest example suffices to illustrate.\nConsider a mass point sliding on a frictionless plane. When we perturb \\(x_t\\), we push it to the side by \\(\\delta x_{t, 0}\\) and also change its velocity by \\(\\delta x_{t, 1}\\), and so at the end time \\(T\\), we would have changed \\(x_T\\) by \\((\\delta x_{t, 0} + (T-t)\\delta x_{t, 1}, \\delta x_{t, 1})\\), which is the same order of infinitesimal. Now, we can control the mass point by applying a force \\(u_t\\), which gives us the dynamics equation\n\\[\n\\dot x_t = (x_{t, 1}, u_t)\n\\]\nTo “perturb” \\(u_t\\) by \\(\\delta u_t\\) does not make sense on its own, as a “spike” of \\(\\delta u_t\\) that lasts for zero amount of time does not show up in the system trajectory. One can apply a durationless jump in state, but one cannot apply a durationless force. Therefore, it only makes sense to perturb \\(u_t\\) by \\(\\delta u_t\\) and persist the perturbation for \\(dt\\) time. This perturbs the state at the end time by \\(((T-t)\\delta u_t dt , \\delta u_t dt)\\), which means that \\(\\nabla_{u_t}x_T\\) is proportional to \\(dt\\).\n\n\nOptimal control theory\nAn optimal trajectory must have \\(\\nabla_{u_t} J = 0\\), since otherwise, we could shave off a little piece of cost by giving \\(u_t\\) a little boost in the opposite direction (infinitesimal gradient descent!). This gives us two equations that any optimal trajectory must satisfy\n\\[\n\\begin{cases}\n- \\dot \\lambda_t &= \\nabla_{x_t} L(x_t, u_t) + \\lambda_t \\nabla_{x_t} f(x_t, u_t) \\\\\n0                &= \\nabla_{u_t} L(x_t, u_t) + \\lambda_t \\nabla_{u_t} f(x_t, u_t) \\\\\n\\end{cases}\n\\]\nNow, what is the effect of perturbing \\(u_t\\) by \\(du_t\\)? It would perturb \\(x_{t+dt}\\) by \\(\\nabla_{u_t} f(x_t, u_t) du_t dt\\), a second-order infinitesimal. Consequently, it would perturb \\(x_T\\) by only a second-order infinitesimal, and thus \\(\\lambda\\) too. Therefore, we have\n\\[\n\\nabla_{u_t}\\lambda_t = 0\n\\]\ngiving us simplified equations for optimality:\n\\[\n\\begin{cases}\n- \\dot \\lambda_t &= \\nabla_{x_t} L(x_t, u_t) + \\lambda_t \\nabla_{x_t} f(x_t, u_t) \\\\\n0                &= \\nabla_{u_t} (L(x_t, u_t) + \\lambda_t f(x_t, u_t)) \\\\\n\\end{cases}\n\\]\nUnfortunately, we cannot simplify the first equation similarly because \\(\\nabla_{x_t}\\lambda_t \\neq 0\\). Still, it seems \\(L(x_t, u_t) + \\lambda_t f(x_t, u_t)\\) should be an important quantity:\n\\[\nH(x_t, u_t, \\lambda_t) := L(x_t, u_t) + \\lambda_t f(x_t, u_t)\n\\]\nThe letters are meaningful. \\(L\\) is the “Lagrangian”, and \\(H\\) is the “Hamiltonian”. Indeed, classical Hamiltonian mechanics is a special case of optimal4 control theory.4 To be completely precise, it is a necessary but insufficient condition for optimality. Just like how a function can have zero derivatives on the peaks, valleys and shoulders, a trajectory can have zero functional derivative, even if it is the best, the worst, and the … shouldered? In jargon, we say that those conditions are first order optimality conditions, since they use only the derivative, not the second-derivative.\nIf we interpret economically the quantities, then \\(J\\) is the cost of the entire trajectory, \\(\\lambda_t\\) is the marginal cost of the point \\(x_t\\) in the trajectory, and \\(L(x_t, u_t)\\) is the cost-rate at time \\(t\\). The second equation of optimality \\(\\nabla_{u_t} H(x_t, u_t, \\lambda_t) = 0\\) states that when the trajectory is optimal, the marginal cost of control is zero: there is no saving in perturbing the control in any direction.\nTherefore, define the “optimized” Hamiltonian and the optimized control relative to it:\n\\[\n\\begin{cases}\nH^*(x_t, \\lambda_t) &:= \\min_{u_t}H(x_t, u_t, \\lambda_t) = \\min_{u_t} \\left(L(x_t, u_t) + \\lambda_t f(x_t, u_t)\\right) \\\\\nu^*(x_t, \\lambda_t) &:= \\mathop{\\mathrm{argmin}}_{u_t}H(x_t, u_t, \\lambda_t)\n\\end{cases}\n\\]\nThen, by Hotelling’s lemma, we derive the Hamiltonian equations of motion:\n\\[\n\\begin{cases}\n- \\dot \\lambda_t &= \\nabla_{x_t} L(x_t, u_t^*) + \\lambda_t \\nabla_{x_t} f(x_t, u_t^*) &= \\nabla_{x_t} H^*(x_t, \\lambda_t) \\\\\n  \\dot x_t       &= f(x_t, u_t^*)                                                     &= \\nabla_{\\lambda_t} H^*(x_t, \\lambda_t) \\\\\n\\end{cases}\n\\]\nThis is often called the Pontryagin’s maximum principle, as Pontryagin’s school of optimal control theory is based on this principle and its extensions. Control theory was a national security issue during the Cold War, as it was used from the space race to the missile race.\nIn classical control theory, the equation is sometimes solved in closed form, as in the case of linear quadratic control. When it cannot be solved in closed form, it can still be numerically solved using a continuous form of the Bellman equation. Similar to how the Hamiltonian equations have the alternative form of the Hamilton–Jacobi equation, the Pontryagin equations have an alternative form in the Hamilton–Jacobi–Bellman equation. Possibly, the name “dynamic programming” appears later in Paul Werbos’ invention of backpropagation, which he named “dynamic feedback”.\nIn economics, one can modify this framework slightly to allow discounting cost over time, yielding theories about “optimal investment” such as the Ramsey optimal growth theory."
  },
  {
    "objectID": "blog/posts/backstory-of-backpropagation/index.html#leibniz",
    "href": "blog/posts/backstory-of-backpropagation/index.html#leibniz",
    "title": "The Backstory of Backpropagation",
    "section": "Leibniz",
    "text": "Leibniz\nThe chain rule dates back to Calculus Tangentium differentialis [Differential calculus of tangents], a manuscript by Leibniz dated 1676 November (Child 1917). It says\n\nit does not matter, whether or no the letters \\(x, y, z\\) have any known relation, for this can be substituted afterward.\n\nIn mathematical notation, he found that \\(dy = dx \\frac{dy}{dx}\\), or in his notation, \\(\\overline{dy} = \\overline{dx} \\frac{dy}{dx}\\), where the overbar denotes the thing to be differentiated. You can read it as a bracket: \\(d(y) = d(x) \\frac{dy}{dx}\\).\nHe then gave the following examples5:5 Yes, there is a sign error. No, I’m not going to fix it. He just has to live with his mistakes.\n\\[\n\\begin{aligned}\n& \\overline{d \\sqrt[2]{a+b z+c}} z^2 \\text {. Let } a+b z+c z^2=x \\text {; } \\\\\n\\text{Then} \\quad  & \\overline{d \\sqrt[2]{x}}=-\\frac{1}{2 \\sqrt{x}} \\text {, and } \\frac{d x}{d z}=b+2 c z \\text {; } \\\\\n\\text{Therefore} \\quad  & \\overline{d \\sqrt{a+b z+c z^2}}=-\\frac{b+2 c z}{2 \\overline{d z} \\sqrt{a+b z+c z^2}} \\\\\n&\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "blog/posts/backstory-of-backpropagation/index.html#mcculloch-and-pitts",
    "href": "blog/posts/backstory-of-backpropagation/index.html#mcculloch-and-pitts",
    "title": "The Backstory of Backpropagation",
    "section": "McCulloch and Pitts",
    "text": "McCulloch and Pitts\nIn the famous paper (McCulloch and Pitts 1943), McCulloch and Pitts proposed that\n\nBecause of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes.\n\nThe McCulloch and Pitts paper is difficult to read, and is rooted in early 20th century mathematical logic. In fact, it uses the same abstruse notations of the Principia Mathematica, which is cited in the paper.\n\n\n\n\n\n\nThe infamous proof of 1+1=2 in Principia Mathematica\n\n\n\n\n\n\n\nThe same notation is used by McCulloch and Pitts\n\n\n\n\n\nThe McCulloch and Pitts paper, like the Perceptrons book, is something often cited but rarely read. The best introduction to the McCulloch and Pitts neural network is still (Minsky 1967, chap. 3), which explicitly designs, neuron by neuron, threshold by threshold, how to construct neural networks that perform various tasks, such as simulating arbitrary Turing machines, process signals, perform additions, etc."
  },
  {
    "objectID": "blog/posts/backstory-of-backpropagation/index.html#frank-rosenblatt",
    "href": "blog/posts/backstory-of-backpropagation/index.html#frank-rosenblatt",
    "title": "The Backstory of Backpropagation",
    "section": "Frank Rosenblatt",
    "text": "Frank Rosenblatt\nFrank Rosenblatt is the originator of the term “backpropagation”, or more precisely, “back-propagating error-correction procedure” (Rosenblatt 1962, vol. 55, chap. 13), although it was quite far from our current use of backpropagation. He is the originator of many concepts in neural networks, and he was even popularly famous during his own time. During 1957–1962, he did a lot of the theory and experiments with perceptron networks. Some experiments ran on digital computer simulations, and others on the Mark I Perceptron, a bespoke machine that takes up a whole room.\nA perceptron is a function of the form \\(\\theta(w^T x + b)\\), where \\(\\theta\\) is the 0-1 step function, and \\(w \\in \\mathbb{R}^n, b \\in \\mathbb{R}\\) are its learnable parameters. A perceptron network is a network of perceptrons. Rosenblatt experimented with a large number of perceptron network architectures, but most often, he experimented with a certain 2-layered feedforward architecture, which is sometimes called “the perceptron machine”.\nThe perceptron machine is a machine that computes a function of type \\(\\{0, 1\\}^n \\to \\{0, 1\\}\\). Its input layer is composed of units named “Stimulus units” or “S units”. The S units do not perform any computation, but merely pass binary outputs to the hidden layer. In the Mark I perceptron, the S units were 20×20 cadmium sulfide photocells, making a 400-pixel image. The hidden layer is composed of perceptrons named “Association units” or “A units”. Their outputs pass on to the output layer, composed of perceptrons named “Response units” or “R units”.\nWe can describe the perceptron machine in one equation:\n\\[\nf(x) = \\theta\\left(b^{R} + \\sum_i w^{AR}_i \\theta\\left((w^{SA, i})^T x + b^{A}_i\\right)\\right)\n\\]\nRosenblatt proved some mathematical theorems, the most important of which is the perceptron convergence theorem, which shows that assuming the dataset is linearly separable, then the perceptron learning algorithm converges after making a bounded number of errors6. He also performed experiments on the Mark I Perceptron machine, IBM computers, and some other computers.6 There is another technical assumption on the dataset, but it is omitted for space. Suffice to say it is satisfied for all finite datasets.\nHis theorems and experiments were exhaustively documented in his book (Rosenblatt 1962). Its breadth is quite astonishing. It contains:\n\nperceptrons with continuous activation functions (section 10.2);\nperceptron convergence theorem for perceptrons with continuous activation functions that are also monotonic and sign-preserving (page 249);\nperceptron layers with random delay in transmission time (chapter 11);\nlayers with connections between units within the same layer, with possibly closed loops (chapter 17–19);\nlayers with connections from a later layer to a previous layer (chapter 20);\nresidual connections (Figure 42);\nmultimodal perceptron networks that learns to associate image and audio inputs (Figure 58);\nprogram-learning perceptrons (chapter 22);\nperceptron networks that analyze videos and audios (chapter 23).\n\nFrom our vantage point, we can fairly say that he invented randomization, residual connections, weight initiation schemes, neural Turing machines, recurrent neural networks, vision models, time delay neural networks, multimodal neural networks…\n\n\n\nFigure 58. The first multimodal neural network?\n\n\n\n\n\nFigure 42. The dashed lines denote variable weights, and the solid lines denote fixed weights. This is the residual connection.\n\n\nWhat is even more astonishing is that, as far as I can see, he did not make use of gradient descent learning at all, not even on a single layer like Widrow and Hoff. The perceptron learning rule converges, but only for a single-layered perceptron network on linearly separable data. Thus, when it came to two-layered perceptron networks, he randomly wired the first layer, then froze it, and only adapted the second layer. This would prove to be a turning point in the “perceptron controversy”.\nIn the chapter where he talked about backpropagation (Rosenblatt 1962, vol. 55, chap. 13), he tried to train both layers in the two-layered perceptron network by extending the perceptron learning algorithm. The algorithm works on each individual perceptron unit, but only if the desired output is known. The desired outputs for the output units are supplied by the experimenter, but the desired outputs for the hidden units could not be known, so Rosenblatt tried some hacks. Essentially, he took the error signals at the output units, and used heuristic rules to “backpropagate the error” to synthesize error signals at the hidden units. The precise details are hacky, and no longer of any relevance.\nOne last thing about his backpropagation rule: he also discovered the per-layer learning rate trick. Because his backpropagation algorithm was so hacky, he found that he had to stabilize it by making the first layer learn at a much lower rate than the second.\n\nIt is found that if the probabilities of changing the S-A connections are large, and the threshold is sufficiently small, the system becomes unstable, and the rate of learning is hindered rather than helped by the variable S-A network. Under such conditions, the S-A connections are apt to change into some new configuration while the system is still trying to adjust its values to a solution which might be perfectly possible with the old configuration. Better performance is obtained if the rate of change in the S-A network is sufficiently small to permit an attempt at solving the problem before drastic changes occur."
  },
  {
    "objectID": "blog/posts/backstory-of-backpropagation/index.html#bernard-widrow-and-marcian-hoff",
    "href": "blog/posts/backstory-of-backpropagation/index.html#bernard-widrow-and-marcian-hoff",
    "title": "The Backstory of Backpropagation",
    "section": "Bernard Widrow and Marcian Hoff",
    "text": "Bernard Widrow and Marcian Hoff\nWidrow and Hoff worked on neural networks in the early 1960s. They started with training a single perceptron with gradient descent on the squared loss, then proceeded to spend years trying to train a two-layered network without gradient descent. I know – I cannot make this sound any less puzzling.\nThe Widrow–Hoff machine, which they called the ADALINE (“ADAptive Linear NEuron”), is a function of type \\(\\mathbb{R}^n \\to \\{0,1\\}\\) defined by\n\\[\nf(x) = \\theta(w^T x + b)\n\\]\nand here it is again, that dreaded Heaviside step-function that was the bane of every neural network before backpropagation. What was odd about this one is that ADALINE was trained by gradient descent with the squared loss function \\(\\frac 12 (w^T x + b - y)^2\\), which is continuous, not discrete:\n\\[\nw \\leftarrow w - \\alpha (w^T x + b - y) w, \\quad b \\leftarrow b - \\alpha (w^T x + b - y) b\n\\]\nThe first ADALINE machine was a box that learned to classify binary patterns on a \\(4 \\times 4\\) grid. It was pretty amusing, as everything was done manually. The patterns were inputted by flipping 16 switches by hand. The error \\(w^T x + b - y\\) was read from a voltmeter, and the parameters \\(w, b\\) were individually adjusted by turning knobs controlling rheostats inside the box. They then automated the knob-turning process using electrochemical devices called memistors, though the pattern input \\(x\\) and the desired output \\(y\\) were still entered by manual switches.\n\n\n\n\n\n\nA memistor ADALINE with glass-sealed memistors. (Widrow 2023, fig. 26.12)\n\n\n\n\n\n\n\nTest patterns, and a learning curve, for the ADALINE machine. (Widrow 2023, fig. 26.4)\n\n\n\n\n\nWidrow recounts an amusing encounter with Rosenblatt:\n\nI just put the pattern in and the Adaline went “phut,” and the needle was reading to the right or to the left. So I just held the adapt button down so some of the cells are plating while others are deplating, depending on the direction of the error signal. Rosenblatt’s students put the pattern into the percept ron. You could see it in the lights on the percept ron. You could hear the potentiometers grinding away. We put another pattern into the Adaline, and it went “blip ,” and there it was, adapted. They put it in the perceptron, and it’s still grinding away. We put in a couple more patterns. Then we test the Adaline and test the percept ron to see whether the patterns are still in there.\nThey’re in the Adaline. In the perceptron, they’re all gone. I don’t know whether the machine was temperamental or what, but it was difficult to train. I argued with Rosenblatt about that first random layer. I said, “You’d be so much better off if you just took the signal from the pixels and ran them straight into the weights of the second layer.” He insisted that a perceptron had to be built this way because the human retina is built that way. That is, there’s a first layer that’s randomly connected to the retina. He said the reason why you can get something to interpret and make sense of random connections is because it’s adaptive. You can unravel all this random scrambling. What I was trying to do was to not model nature. I was trying to do some engineering.\n\nAfter the ADALINE showed good behavior, they went on and on trying to train a two-layered ADALINE (“MADALINE”, or “many ADALINE”), which of course could not be trained by gradient descent, because the hidden layer ADALINE units used the Heaviside step function! It is almost inconceivable nowadays, but they simply refused to use a continuous activation function and tried every other trick except that. They ended up with the MADALINE I rule. In short, it was a heuristic rule for synthesizing supervision signals for the hidden layer, much like Rosenblatt’s heuristic rule.77 The MADALINE I machine consists of multiple hidden ADALINE units, and a single output layer consisting of a single unit. The output unit merely takes the majority vote from the hidden units. If the desired output is \\(+1\\), but the actual output is \\(-1\\), then the machine picks those ADALINE units that are negative, but closest to being positive, and make them update their weights, according to the ADALINE learning rule. It was thought of as a form of “minimal disturbance principle”.\nFrustrated by the difficulty, they left neural network research. Hoff went to Intel to coinvent the microprocessor, and Widrow set about applying the ADALINE to small problems that it could solve well8, such as adaptive antennas, adaptive noise filtering in telephone lines, adaptive filtering in medical scanning, etc. Even with just a single ADALINE, he brought breakthroughs to those fields.8 Marvin Minsky would approve\n\n[The perceptron] is such a simple machine that it would be astonishing if nature did not make use of it somewhere. It may be that one of the best things a neuron can have is a tiny Perceptron, since you get so much from it for so little. You can’t get one big Perceptron to do very much, but for some things it remains one of the most elegant and simple learning devices I know of.” (Bernstein 1981)\n\n\n\nApple have told me that every iPhone, since the iPhone 5, uses the LMS algorithm all over the device. They could not tell me where because, if they did, they would have to shoot me. Apple keeps secrets. (Widrow 2022, preface, page xix)\n\nPerhaps Widrow and Hoff were limited to considering only learning rules they could implement electrochemically? But it does not seem so, in the words of Widrow himself, it was a blind spot for all researchers:\n\nThe Madaline had an adaptive first layer and a fixed-logic second layer. What Rosenblatt had was a fixed-logic mst layer and an adaptive-logic second layer or output layer. Now, it is easy to adapt on output layer. But it’s very difficult to adapt a hidden layer. … We could adapt an adaptive first layer with a fixed second layer as long as we knew what the second layer was. But we never succeeded in developing an algorithm for adapting both layers, so that the second layer is not fixed and both layers are adaptive. It wasn’t that we didn’t try. I mean we would have given our eye teeth to come up with something like backprop.\nBackprop would not work with the kind of neurons that we were using because the neurons we were using all had quantizers that were sharp. In order to make backprop work, you have to have sigmoids; you have to have a smooth nonlinearity … no one knew anything about it at that time. This was long before Paul Werbos. Backprop to me is almost miraculous. (Rosenfeld and Anderson 2000)\n\nWhen he heard about the “miraculous” backpropagation in the 1980s, he immediately started writing papers in neural networks again.\nIf this was an individual case, then I might have dismissed it as Widrow being misled by his background. Before entering a PhD program Widrow took an internship at the military (Widrow 1997):\n\n… the sky was divided up into a grid, with one mile by one mile regions, and all the computer sensed was that there was a target within that one mile by one mile square. … The radar pulses were transmitted periodically, and then periodically you got samples of the flight path. Say you take a piece of graph paper and draw and nice, smooth flight path for an airplane, and then at uniform points in time put a mark on the actual path based on the radar sample. If you don’t show anybody that nice, smooth line but only the sequence of grid squares that the airplane was in, one could still reconstruct the smooth track.\n\nThe problem of reconstructing a smooth flight path from discretized radar signals got him interested in quantization, and he did his PhD thesis (1954) in the statistical theory of quantization noise. In the interview 43 years later, he called it “the best piece of work I ever did in my whole life”. He is still the world expert on quantization noise, and wrote an entire textbook on quantization noise (Widrow and Kollár 2008).\nSo regarding this background, one might be tempted to say that Widrow was misled by his infatuation with quantization, and attempted to use quantized neural networks. However, this does not explain why all the other pioneers went down the same wrong path."
  },
  {
    "objectID": "blog/posts/backstory-of-backpropagation/index.html#seppo-linnainmaa",
    "href": "blog/posts/backstory-of-backpropagation/index.html#seppo-linnainmaa",
    "title": "The Backstory of Backpropagation",
    "section": "Seppo Linnainmaa",
    "text": "Seppo Linnainmaa\nIt’s said that Seppo Linnainmaa’s master’s thesis in 1970 contains the backpropagation algorithm, but it is in Finnish and not available online either. A bibliography search shows that his thesis were basically never cited before the 2010s. It is one of those papers that people cite for ritual completion only. I think we can safely say that his thesis has priority but no paternity. (Griewank 2012) describes in detail what is in the thesis, according to which he developed it to analyze the cumulative effect of round-off error in long chains of computer computations.\nI checked all his English papers during the 1970s, and it seems only (Linnainmaa 1976) has been cited non-negligibly before 2000, and may have had some impact on automatic differentiation, but I cannot tell by how much. It contains two algorithms, one for computing the first derivative, one for computing the first two. Both were special cases of the general backpropagation algorithm for computing arbitrary orders of derivatives."
  },
  {
    "objectID": "blog/posts/backstory-of-backpropagation/index.html#rumelhart-and-sejnowski",
    "href": "blog/posts/backstory-of-backpropagation/index.html#rumelhart-and-sejnowski",
    "title": "The Backstory of Backpropagation",
    "section": "Rumelhart and Sejnowski",
    "text": "Rumelhart and Sejnowski\nRumelhart rediscovered backpropagation around 1982 and immediately set about publishing and telling others about it. Some still resisted, such as Geoffrey Hinton, but others immediately grasped it and set about using it, such as Terrence Sejnowski. In the interview, he was rather unbothered by the priority dispute (Rosenfeld and Anderson 2000, chap. 12):\n\nI had no idea that Paul Werbos had done work on it. … There are other examples of work in the control literature in the ’60s [the adjoint method]. … it’s just no one had really done anything with them. My view is that we not only discovered them, but we realized what we could do with them and did a lot of different things. I consider them independent discoveries, at least three and maybe more. [Shun’ichi] Amari, I think, suggests that he had another discovery, and you know, I believe that. You can look at his paper. He had, but he didn’t do anything with it. I think that was in the late ’60s. I don’t feel any problem. You know, maybe we should have done better scholarship and searched out all of the precursors to it, but we didn’t know there were any.\n\nIn 1983, Rumelhart showed backpropagation to Sejnowski, who immediately tried it and found that it was much faster than the Boltzmann machine learning rule. What a refreshing change from all those others who stubbornly refused to try it.\n\n… I was still working on the Boltzmann machine and was beginning to do simulations using backprop. I discovered very rapidly that backprop was about an order of magnitude faster than anything you could do with the Boltzmann machine. And if you let it run longer, it was more accurate, so you could get better solutions. (Rosenfeld and Anderson 2000, chap. 14)\n\nThis was vitally important later, when Sejnowski used backpropagation to train NETtalk, a huge network with 18,629 parameters.9 The model was a popular hit and appeared on prime-time television.(Sejnowski 2018, 112–15)9 People told him that it had too many parameters and would hopelessly overfit, but the network just needed enough data and it generalized uncannily well. It really does seem like scaling skepticism is a researcher universal, if not a human universal.\n\nThere were 18,629 weights in the network, a large number by the standards of 1986, and impossibly large by the standards of mathematical statistics of the time. With that many parameters, we were told that we would overfit the training set, and the network would not be able to generalize. (Sejnowski 2018, 113)"
  },
  {
    "objectID": "blog/posts/backstory-of-backpropagation/index.html#geoffrey-hinton",
    "href": "blog/posts/backstory-of-backpropagation/index.html#geoffrey-hinton",
    "title": "The Backstory of Backpropagation",
    "section": "Geoffrey Hinton",
    "text": "Geoffrey Hinton\nThe interview with Geoffrey Hinton is hilarious, mostly about how he spent an entire year refusing to use backpropagation. This section is mostly made of quotations from (Rosenfeld and Anderson 2000, chap. 16).\nAfter learning about Hopfield networks and simulated annealing both in 1982, he tried combining them, and discovered the Boltzmann learning algorithm in 1983, which he published in 1985 (Ackley, Hinton, and Sejnowski 1985).\n\nI remember I had to give a talk, a sort of research seminar, at Carnegie Mellon in either February or March of ’83 … I wanted to talk about this simulated annealing in Hopfield nets, but I figured I had to have a learning algorithm. I was terrified that I didn’t have a learning algorithm.\nThen we got very excited because now there was this very simple local-learning rule. On paper it looked just great. I mean, you could take this great big network, and you could train up all the weights to do just the right thing, just with a simple local learning rule. It felt like we’d solved the problem. That must be how the brain works.\nI guess if it hadn’t been for computer simulations, I’d still believe that, but the problem was the noise. It was just a very, very slow learning rule. It got swamped by the noise because in the learning rule, you take the difference between two noisy variables, two sampled correlations, both of which have sampling noise. The noise in the difference is terrible.\nI still think that’s the nicest piece of theory I’ll ever do. It worked out like a question in an exam where you put it all together and a beautiful answer pops out.\n\nAnd now, the hilarity we have been waiting for. When Rumelhart told him in 1982 about backpropagation,\n\nI first of all explained to him why it wouldn’t work, based on an argument in Rosenblatt’s book, which showed that essentially it was an algorithm that couldn’t break symmetry. … the standard way of doing learning is to have random initial weights, or you could just put a little bit of noise in the learning rule …\nThe next argument I gave him was that it would get stuck in local minima. There was no guarantee you’d find a global optimum. Since you’re bound to get stuck in local minima, it wasn’t really worth investigating.\nThen I tried to use it to get a very obscure effect. I couldn’t get this very obscure effect with it, so I lost interest in backpropagation. I managed to get this very obscure effect later with a Boltzmann machine. I’d realized that if you’ve got a net to learn something, and then you add noise to the weights by making it learn something else, it should be much faster at relearning what it had previously learned. You should get very fast relearning of stuff that it previously knew, as compared to the initial learning. We programmed a backpropagation net, and we tried to get this fast relearning. It didn’t give fast relearning, so I made one of these crazy inferences that people make – which was, that backpropagation is not very interesting.\n\nAfter one year of failing to get Boltzmann machines to train, he discovered that Boltzmann machines also got stuck in local minima. Desperate times call for desperate measures, and he finally resorted to gradient descent.\n\nAfter initially getting them to work, I dedicated over a year to refining their performance. I experimented with various techniques, including weight decay, which helped in preventing large energy barriers. We attempted several other approaches, but none of them yielded satisfactory results. I distinctly recall a point where we observed that training a Boltzmann machine could model certain functions effectively, but prolonged training caused its performance to deteriorate, a phenomenon we referred to as “going sour.” We couldn’t initially comprehend why this was happening. I generated extensive reports, filling stacks of paper about three inches thick, as I couldn’t believe that these networks would degrade as you acquired more knowledge.\nIt took me several weeks to realize the root of the problem. The learning algorithm operated under the assumption that it could reach thermal equilibrium. However, as the weights grew larger, the annealing process failed to achieve thermal equilibrium, trapping the system in local minima. Consequently, the learning algorithm started behaving incorrectly. This realization led us to introduce weight decay as a solution to prevent this issue.\nAfter investing over a year in attempting to optimize these methods, I ultimately concluded that they were unlikely to deliver the desired results. In a moment of desperation, I considered revisiting [backpropagation].\n\nThen he asked the research group for volunteers. None of the ten students took up the offer (and thus giving up a place as the coauthor of the backpropagation paper??):\n\nThey’d all been thoroughly indoctrinated by then into Boltzmann machines. … They all said, “You know, why would you want to program that?” We had all the arguments: It’s assuming that neurons can send real numbers to each other; of course, they can only send bits to each other; you have to have stochastic binary neurons; these real-valued neurons are totally unrealistic. It’s ridiculous.” So they just refused to work on it, not even to write a program, so I had to do it myself.\nI went off and I spent a weekend. I wrote a LISP program to do it.\n\nHinton almost had one last chance at giving up on backpropagation.\n\nI almost blew it because the first thing I tried it on was an 8-3-8 encoder; that is, you have eight input units, three hidden units, and eight output units. You want to turn on one of the input units and get the same output unit to be on.\nIn a Boltzmann machine, since the units are binary, the states of the three hidden units have to be the eight different binary codes, so you know what the hidden units will look like when they’ve solved the problem. So I ran backpropagation on this problem and looked at the weights of the hidden units. They didn’t have the right structure of weights. … I thought, “Oh well, it turns out backpropagation’s not that good after all.” Then I looked at the error, and the error was zero. I was amazed.\n\nAnd so one year of excuses later, Hinton finally surrendered to backpropagation. They published the algorithm along with several examples, which became widely cited.\n\nThat was at the stage when we were just completing the PDP books, so we’d already agreed on what was going to be in the books. The final chapters were being edited. We decided we’d just slip in an extra chapter on backpropagation, so it was a late addition to the book. But I was always a bit disappointed. I mean, intellectually, backpropagation wasn’t nearly as satisfying as Boltzmann machines. … it didn’t have the nice probabilistic interpretation."
  },
  {
    "objectID": "blog/posts/backstory-of-backpropagation/index.html#werbos",
    "href": "blog/posts/backstory-of-backpropagation/index.html#werbos",
    "title": "The Backstory of Backpropagation",
    "section": "Werbos",
    "text": "Werbos\nAfter reviewing the story of multiple people, my personal opinion is that Paul Werbos most deserves the title of “originator of the backpropagation algorithm”, as he both has the priority and paternity of the algorithm. In 1993, Paul Werbos gave an interview, where he described the backstory of his invention of backpropagation (Rosenfeld and Anderson 2000, chap. 15). I will let him speak, only interjecting with brief comments.\nBefore entering Harvard, he tried implementing Hebbian learning, but noticed it wouldn’t work.\n\nIt was obvious to me from a statistical point of view that Hebbian learning was going to be measuring correlation coefficients, and for multivariate problems it would not work. I never gave the talk. I said, “Now I’m going to figure out something with the same flavor that does work.”\n[Understanding human learning] will help us understand the human mind; it will help human beings understand themselves. Therefore, they will make better political decisions, and better political decisions are vital for the future of humanity.\nMinsky was one of my major influences. Well, Minsky and Hebb and Asimov and Freud.\n\nSometime before 1968, he was inspired to do backpropagation from reading Freud.\n\nI talk in there about the concept of translating Freud into mathematics. This is what took me to backpropagation, so the basic ideas that took me to backpropagation were in this journal article in ’68 (P. Werbos 1968). I talked a lot about what was wrong with the existing [two state] McCulloch-Pitts neuron model, and how it was only “1” and “0.” I wanted to make that neuron probabilistic. I was going to apply Freudian notions to an upper-level, associative drive reinforcement system. When\n\nFor his masters (1969), he majored in mathematical physics, and minored in decision and control. During his PhD at Harvard, he had to take some compulsory courses, which he didn’t want to. To salvage the situation, he took computer courses which would provide him with some computer hours, a scarce resource at the time. I would guess the following event happened early 1971.\n\nInitially, I was going to do something on quantum physics. I learned something about partial differential equations, but not enough. I couldn’t produce a really useful product at the end of \\(x\\) number of months.\nSo I went back to the committee, and I said, “Gee, I can’t do that, but I have this little method for adapting multilayer perceptrons. It’s really pretty trivial. It’s just a by-product of this model of intelligence I developed. And I’d like to do it for my paper for this computer course.”\n[Larry] Ho’s position was, “I understand you had this idea, and we were kind of open-minded. But look, at this point, you’ve worked in this course for three months, admittedly on something else. I’m sorry, you’re just going to have to take an incomplete in the course.”\nAnd I said, “You mean I can’t do it?”\n“No, no, you’ll have to take an incomplete because, basically, the first thing didn’t work. We’re very skeptical this new thing is going to work.”\n“But look, the mathematics is straightforward.”\n“Yeah, yeah, but you know, we’re not convinced it’s so straightforward. You’ve got to prove some theorems first.”\nSo they wouldn’t let me do it. One of the reasons that is amusing to me is that there are now some people who are saying backprop was invented by Bryson and Ho. They don’t realize it was the same Larry Ho, who was on my committee and who said this wasn’t going to work.\n\nI am not sure if this is sarcastic or not. It reminds me of the “summer vision project” (Papert 1966) that expected some undergraduate students to construct “a significant part of a visual system” in a single summer.\n\nBy the time my orals came around, it was clear to me that the nature of reality is a hard problem, that I’d better work on that one later and finish my Ph.D. thesis on something small – something I can finish by the end of a few years, like a complete mathematical model of human intelligence.\n\nThe oral was amusing, and touched on the still-hot issue of recent human evolution.\n\n… I made a statement that there might be parameters affecting utility functions in the brain, parameters that vary from person to person. You could actually get a significant amount of adaptation in ten generations’ time. I was speculating that maybe the rise and fall of human civilizations, as per Toynbee and Spengler, might correlate with these kinds of things. The political scientist on the committee, Karl Deutsch, raised his hand. … His book, The Nerves of Government, which compares governments to neural networks, is one of the classic, accepted, authoritative books in political science.\nHe raised his hand and he said, “Wait a minute, you can’t get significant genetic change in ten generations. That cannot be a factor in the rise and fall of civilizations. That’s crazy.”\nNext to him was a mathematical biologist by the name of Bossert, who was one of the world’s authorities on population biology. He raised his hand and said, “What do you mean? In our experiments, we get it in seven generations. This guy is understating it. Let me show you the experiments.”\nAnd Deutsch said, “What do you mean, it’s common knowledge? All of our political theories are based on the assumption this cannot happen.” And Bossert said, “Well, it happens. Here’s the data.”\n… I passed the orals having said about two sentences and not having discussed models of intelligence.\n\nIt turns out Werbos interpreted backpropagation as the mathematical interpretation of Freudian psychic energy flow. The thesis committee was not amused.\n\nBut the backpropagation was not used to adapt a supervised learning system; it was to translate Freud’s ideas into mathematics, to implement a flow of what Freud called “psychic energy” through the system. I translated that into derivative equations, and I had an adaptive critic backpropagated to a critic, the whole thing, in ’71 or ’72. … The thesis committee said, “We were skeptical before, but this is just unacceptable … You have to find a patron. You must find a patron anyway to get a Ph.D. That’s the way Ph.D.s work.\n\nThe committee gave him three acceptable patrons. He first went to Stephen Grossberg.\n\n… he said, ’Well, you’re going to have to sit down. Academia is a tough business, and you have to develop a tough stomach to survive in it. I’m sure you can pull through in the end, but you’re going to have to do some adaptation. So I want you to sit down and hold your stomach, maybe have an antacid. The bottom line is, this stuff you’ve done, it’s already been done before. Or else it’s wrong. I’m not sure which of the two, but I know it’s one of the two.”\n\nThanks, Grossberg, for using the law of excluded middle to crush Werbos’ dream.\nHe then went to Marvin Minsky, who gave us some new clues about why backpropagation took so long to discover: “everybody knows a neuron is a 1-0 spike generator”!\n\n“I’ve got a way now to adapt multilayer perceptrons, and the key is that they’re not Heaviside functions; they are differentiable. And I know that action potentials, nerve spikes, are 1 or 0, as in McCulloch-Pitts neurons, but here in this book that I had for my first course in neurophysiology are some actual tracings. If you look at these tracings in Rosenblith, they show volleys of spikes, and volleys are the unit of analysis. This is an argument for treating this activity as differentiable, at least as piecewise linear. If you look at that, I can show you how to differentiate through it.”\nMinsky basically said, “Look, everybody knows a neuron is a 1-0 spike generator. That is the official model from the biologists. Now, you and I are not biologists. If you and I come out and say the biologists are wrong, and this thing is not producing 1s and 0s, nobody is going to believe us. It’s totally crazy. I can’t get involved in anything like this.”\nHe was probably right, I guess, but he was clearly very worried about his reputation and his credibility in his community.\n\nAlthough I believe Minsky would have disapproved of the idea of backpropagation even if he had thought that neurons are not strictly 1-0 spike generators. In the epilogue to (Minsky and Papert 1988), he claimed that gradient descent does not scale, and using differentiable activation functions is just a hack intended to make backpropagation work, a pointless hack, as backpropagation would not scale.\n\nHowever, we suspect that this smoothing trick may entail a large (and avoidable) cost when the predicate to be learned is actually a composition of linear threshold functions. There ought to be a more efficient alternative based on how much each weight must be changed, for each stimulus, to make the local input sum cross the threshold. … We have the impression that many people in the connectionist community do not understand that this is merely a particular way to compute a gradient and have assumed instead that back-propagation is a new learning scheme that somehow gets around the basic limitations of hill-climbing.\n\nFurther, in a 1991 interview, Minsky made the same kind of statement:\n\nI don’t know what they would have done with the money. The story of DARPA and all that is just a myth. The problem is that there were no good ideas. The modern idea of backpropagation could be an old idea. There was someone . . . [trying to remember].\nQuestion: Paul Werbos?\nAnswer: That’s it! [excited]. But, you see, it’s not a good discovery. It’s alright, but it takes typically 100,000 repetitions. It converges slowly, and it cannot learn anything difficult. Certainly, in 1970 the computers were perhaps too slow and expensive to do it. I know that Werbos thought of that idea. It’s certainly trivial. The idea is how you do gradient descent. I didn’t consider it practical.\nQuestion: Because of the computational costs?\nAnswer: Yes, but also, with artificial intelligence, we had the experience that when you make a process like that you usually get stuck at a local minimum. We still don’t have any theory of what range of problems they work well for.” (Minsky, interview) (Olazaran 1991, 249)\n\nOut of curiosity, I looked up the “Rosenblith” book (Rosenblith 2012) that Werbos mentioned, and indeed there were a few tracings that show continuously varying neural activation.\n\nFigure 1: \n\n\n\n\n\n\nPage 146 of the book. Mechanisms of gustatory and olfactory receptor stimulation, Figure 2.\n\n\n\n\n\n\nThen Minsky dunked on reinforcement learning as well, because he had an unpublished “jitters machine” that failed to optimize its reward. Presumably the name “jitters machine” refers to how it would jitter in place, not able to move towards the goal.\n\nMinsky also said, “You know, I used to believe in all this kind of stuff with reinforcement learning because I knew reinforcement learning would work. I knew how to implement it. I had a nice guy named Oliver Selfridge who came in and acted as my patron and gave me permission to do it. We coauthored a paper, but it was really my idea, and he was just acting as patron on the Jitters machine. I’ll hand you the tech report, which we have deliberately never published.”\nIt was his bad experience with the Jitters machine that turned him off on reinforcement learning and all the neural net ideas. It just didn’t work. I later looked at that paper … He had a system that was highly multivariate with a single reinforcement signal. The system can’t learn efficiently with that. At any rate, he was totally turned off.\n\nThe brief description of the unpublished jitters machine was not making sense to me, so I looked around the literature. It was so unpublished that I found only two other references in the entire literature (P. J. Werbos 1982, 1987), both by Werbos. (P. J. Werbos 1987) described it as:\n\nThere are also some technical reasons to expect better results with the new approach. Almost all of the old perceptron work was based on the McCulloch–Pitts model of the neuron, which was both discontinuous and non-differentiable. It was felt, in the 1950’s, that the output of a brain cell had to be 1 or 0, because the output consisted of sharp discrete “spikes.” More recent work in neurology has shown that higher brain cells output “bursts” or “volleys” of spikes, and that the strength of a volley may vary continuously in intensity. This suggests the CLU model to be discussed in the Appendix. In any event, to exploit basic numerical methods, one must use differentiable processing units. Minsky once experimented with a “jitters” machine, which estimated one derivative per time cycle by making brute-force changes in selected variables; however, the methods to be discussed in the Appendix yield derivatives of all variables and parameters in one major time cycle and thereby multiply efficiency in proportion to the number of parameters \\((N)\\), which may be huge.\n\nThis makes things perfectly clear. Minsky’s jitters machine was an RL agent with multiple continuously adjustable parameters running a very primitive form of policy gradient. During every episode, it would estimate \\(\\partial_{\\theta_i} R(\\theta)\\) using finite difference, but since it used finite difference, they could only estimate the partial derivative for only one of the parameters \\(\\theta_i\\)! No wonder it never managed to learn. It is almost comical how much they failed to just use gradient descent. It sometimes feels as if they did everything to avoid just taking the gradient. In the case of Minsky, he made it very clear, in the epilogue of Perceptrons (1988), that he did not believe in gradient descent, period. But what explains the gradient-phobia of the others…?\nAnyway, back to the interview. Werbos went to Jerome Lettvin, the neuroscientist famous for What the Frog’s Eye Tells the Frog’s Brain. Turns out he was a proto-eliminativist. While I am an eliminativist too, Werbos was a Freudian, which could only collide badly with eliminativism.\n\n“Oh yeah, well, you’re saying that there’s motive and purpose in the human brain.” He said, “That ‘s not a good way to look at brains. I’ve been telling people, ’You cannot take an anthropomorphic view of the human brain.’ In fact, people have screwed up the frog because they’re taking bachtriomorphic views of the frog. If you really want to understand the frog, you must learn to be objective and scientific.”\n\nWithout patrons, he faced the committee again.\n\nI tried to simplify it. I said, “Look, I’ll pull out the backprop part and the multilayer perceptron part.” I wrote a paper that was just that - that was, I felt, childishly obvious. I didn’t even use a sigmoid [non-linearity]. I used piecewise linear. I could really rationalize that to the point where it looked obvious. I handed that to my thesis committee. I had really worked hard to write it up. They said, “Look, this will work, but this is too trivial and simple to be worthy of a Harvard Ph.D. thesis.”\n\nOh, now it’s too trivial?? I swear the backstory of backpropagation gets more and more ridiculous by the day.\n\n… they had discontinued support because they were not interested, so I had no money. … Not even money to buy food. A generous guy, who was sort of a Seventh Day Adventist and a Harvard Ph.D. candidate in ethnobotany, had a slum apartment that rented for about $40 a month in Roxbury in the ghetto. He let me share a room in his suite, in his suite with the plaster falling off, and didn’t ask for rent in advance. I had no money at that time for food. There was a period of three months when I was living there in the slums. To conserve the little bit of money I had, I remember eating soybean soup and chicken neck soup. I remember getting the shakes from inadequate nutrition.\nFinally, they said, “Look, you know, we’re not going to allow this.” There was this meeting where we sat around the table. The chairman of the applied math department at that time was a numerical analyst D. G. M. Anderson. He said, “We can’t even allow you to stay as a student unless you do something. You’ve got to come up with a thesis, and it can’t be in this area.”\n\nKarl Deutsch, who believed in Werbos, sponsored his PhD thesis on a “respectable” problem: fitting an ARMA model to a time-series data of political regimes, and use it to forecast nationalism and political assimilation. Box–Jenkins method ran too slowly, so Werbos programmed in the backpropagation which worked, and he finally obtained his PhD in 1974. This is the original reference point for modern neural network backpropagation algorithm.\n\nDeutsch said, “You ’re saying we need an application to believe this stuff? I have an application that we could believe. I have apolitical forecasting problem. I have this model, this theory of nationalism and social communications? What causes war and peace between nations? I have used up ten graduate students who’ve tried to implement this model on real-world data I’ve collected, and they’ve never been able to make it work. Now, do you think your model of intelligence could solve this problem and help us predict war and peace?”\nThe first application of backpropagation in the world in a generalized sense was a command that was put into the TSP at MIT , available to the whole MIT community as part of their standard software. It was published as part of MIT ’s report to the DOD [the Department of Defense] and part of the DOD’s report to the world . It was part of the computer manual, so the first publication of backpropagation was in a computer manual from MIT for a working command for people to use in statistical analysis. I was a second author of that manual. It was Brode, Werbos, and Dunn.\n… Once I started doing software for them, they discovered I was pretty good at it. Once I had put backpropagation into a TSP command, they offered me a full-time job at the Harvard Cambridge Project, so I did the Ph.D. thesis while having a full-time job. While I was doing these things at MIT, I remember going to one party. … one of the people there said, ’We have heard through the grapevine that somebody has developed this thing called continuous feedback that allows you to calculate all the derivatives in a single pass.”\n\nBut the saga is not over. After the PhD, Werbos was hired by the DoD, and promptly got sucked into several more years of misadventure, which meant that he could not even talk about backpropagation in a public journal until 1978 – and the algorithm got removed anyway due to page limits. This annoyed the DoD and he had to move to the Department of Energy.\n\nI found out that DARPA had spent a lot of effort building a worldwide conflict forecasting model for the Joint Chiefs of Staff that was used in the long-range strategy division of the Joint Chiefs. … I wound up sending a couple of graduate students to create a really good database of Latin America. I said, “You want variance, high variance. Something hard to predict.” I thought conflict in Latin America would be the most beautiful case. I figured there were enough cultural homogeneities that it would be a single stochastic process, but with lots and lots of variance in that process. So we got truly annual data going back, I don’t know, twenty or thirty years for all the countries in Latin America and then reestimated the Joint Chief’s model on it. It had an r2 of about .0016 at forecasting conflict. By jiggling and jiggling we could raise it to about .05. It could predict GNP and economic things decently. Conflict prediction we couldn’t improve, though; it was hopeless.\nDARPA wasn’t happy when I published that result. They wanted me to do something real and practical and useful for the United States. This was useful, but it was an exposé. They didn’t like that. Therefore, the report, which included backpropagation in detail and other advanced methods, was not even entered into DOCSUB. Every time I tried to enter it into DOCSUB, somebody jiggled influence to say, “No, no, no, we can’t publish this. This is too hot.”\nIt was published in (P. J. Werbos and Titus 1978) anyway because they couldn’t block the journals, but it didn’t include the appendices. So that paper in 1978 said, “We’ve got this great thing called dynamic feedback, which lets you estimate these things. It calculates derivatives in a single swoop, and you can use it for lots of things, like AI.” … the appendix on how to do it was not there because of page limits … At that point, DARPA was no longer happy.\n\nSo he went to the Department of Energy, used backpropagation to create another model, and was silenced once again, unable to publish that report until 1988 (P. J. Werbos 1988).\n\nThey had a million-dollar contract at Oak Ridge National Laboratory to study that model. They wanted me for several things. One, they wanted me to be a translator between engineering and economics. Two, they wanted a critique. They wanted exposés. They wanted me to rip apart the models of the Department of Energy in a very scientific, objective way that didn’t look like I was trying to rip them apart but was anyway. That’s exactly what they wanted to hire me for, and I didn’t really know that was the motive. These particular people didn’t like modeling very much.\nSo at some point, they wanted sensitivity analysis. And I said, “You know, I know a little bit about calculating derivatives.” … I applied it to a natural gas model at the Department of Energy. In the course of applying it, I did indeed discover dirt. They didn’t want me to publish it because it was too politically sensitive. It was a real-world application, but the problem was that it was too real. At DOE, you know, you don’t have First Amendment rights. That’s one of the terrible things somebody’s got to fix in this country. The reality of the First Amendment has deteriorated. Nobody’s breaking the law, but the spirit of the First Amendment has decayed too far for science. At any rate, they finally gave me permission to publish it around ’86 and ’87. I sent it to the journal Neural Nets - that is, how you do simultaneous recurrent nets and time-lag recurrent nets together. Then the editorial process messed around with it, made the paper perhaps worse, and it finally got published in ’88, which makes me very sad because now I gotta worry about, ’Well, gee, didn’t Pineda do this in ’88?\n\nAs a side note, one might feel that Werbos’ “turning Freud into mathematics” seems rather strange. This feeling is completely justified. I found a recent paper by him (P. J. Werbos 2009) with this crackpot illustration:\n\n\n\nThe text at the end of the arrow says “quantum and collective intelligence (Jung, Dao, Atman…)?”."
  },
  {
    "objectID": "blog/posts/reading-perceptron-book/index.html",
    "href": "blog/posts/reading-perceptron-book/index.html",
    "title": "Reading Perceptrons",
    "section": "",
    "text": "It would seem that Perceptrons has much the same role as The Necronomicon – that is, often cited but never read.\nMarvin Minsky, 1994. Quoted in (Berkeley 1997)\nIn one sentence, the mathematical portion of Perceptrons is a theory of two-layered perceptrons, mostly by methods typical of discrete mathematics and computational complexity theory, and no empirical results. One should forget about the modern theory and practice of neural networks, read the book in the same frame of mind as one would read a textbook on Turing machines, stack machines, Post tag systems, and other various theoretical objects in computational complexity theory. Indeed, this book is written in the same spirit and style as (Minsky 1967) and (Sipser 2006).\nPerceptron representation occupies chapters 0–10, and learning is only studied in chapters 11 and 12. The book also contained a chapter 13, but it contains mostly interpretations and brief “lessons” that they wanted the readers to take away from the exact mathematical results. They added some handwritten corrections and updates for a 1972 printing run.\nDuring the 1980s rise of connectionism, Minsky and Papert came out with a new edition in 1988. This is the same as the 1972 printing, except that they added a prologue and an epilogue, where they expounded at length the intended lesson of the book, as they felt that people have failed to learn it, and history was repeating itself.\nFor those interpretations, see The Perceptron Controversy."
  },
  {
    "objectID": "blog/posts/reading-perceptron-book/index.html#setup",
    "href": "blog/posts/reading-perceptron-book/index.html#setup",
    "title": "Reading Perceptrons",
    "section": "Setup",
    "text": "Setup\nLet \\(R\\) be a finite set, where “R” can be read as “region” or “rectangle”.\n\nDefinition 1 (Perceptron) A perceptron is a binary function of type \\(\\{0, 1\\}^R \\to \\{0, 1\\}\\), defined by a weight vector \\(w\\) and a threshold number \\(b\\):\n\\[\n\\Phi(x) := \\theta(w^T x + b)\n\\]\nwhere \\(\\theta(t) := 1_{t \\geq 0}\\) is the 0-1 step function.\n\n\nDefinition 2 (perceptron machine) A perceptron machine with \\(k\\) hidden neurons is a function of type \\(\\{0, 1\\}^R \\to \\{0, 1\\}\\), defined by\n\\[\n\\Phi(x) := \\psi_{k+1}(\\psi_1(x), \\psi_2(x), \\dots , \\psi_k(x))\n\\]\nwhere \\(\\psi_1, \\dots, \\psi_k\\) are (hidden) perceptrons in the hidden layer, and \\(\\psi_{k+1}\\) is the single output perceptron.\n\n\nDefinition 3 (perceptron orders) The order of a hidden perceptron is the number of nonzero weights.\nThe order of a perceptron machine is the maximum order of its hidden perceptrons.\nThe order of a boolean function is the minimum order necessary for a perceptron machine that implements it.\n\nFor example, the constant-0 and constant-1 boolean functions are both of order 0.\nA key focus of the perceptron controversy is the concept of being “conjunctively local”.\n\nDefinition 4 (conjunctively local) A family of boolean functions is conjunctively local iff their orders are upper bounded."
  },
  {
    "objectID": "blog/posts/reading-perceptron-book/index.html#over-finite-sets",
    "href": "blog/posts/reading-perceptron-book/index.html#over-finite-sets",
    "title": "Reading Perceptrons",
    "section": "Over finite sets",
    "text": "Over finite sets\nWhile their definition allows perceptron machines of any real-valued weights and biases, they immediately performed a computational reduction to the special case of mask perceptron machines. Most of the book considers the problem of classifying subsets of families of planar square grids, such as \\(\\{1, 2, 3\\} \\times \\{1, 2, 3, 4\\}\\). A subset is inputted to the perceptron machine by setting inputs in the subset to \\(1\\), and the rest to \\(0\\). Consequently, it is natural to consider this special case of perceptron machines.\nWhile their definition allows perceptron machines of any real-valued weights and biases, they immediately performed a computational reduction to the special case of the mask perceptron machine. Most of the book considers the problem of classifying subsets of families of planar square grids, such as \\(\\{1, 2, 3\\} \\times \\{1, 2, 3, 4\\}\\). A subset is input to the perceptron machine by setting inputs in the subset to \\(1\\), and the rest to \\(0\\). Consequently, it is natural to consider this special case of perceptron machines.\n\nDefinition 5 (mask perceptron machine) A mask for \\(A\\subset R\\) is a function of type \\(\\{0, 1\\}^R \\to \\mathbb{R}\\), such that \\(\\psi(x) = 1\\) if \\(x_i = 1\\) for all \\(i \\in A\\), and otherwise \\(\\psi(x) = 0\\).\nA mask perceptron machine is a perceptron machine where each hidden perceptron is a mask, and the threshold of the output perceptron is zero. In other words, it is of form\n\\[\n\\Phi(x) = \\theta\\left(\\sum_{i=1}^k a_i \\psi_{A_i}(x)\\right)\n\\]\nwhere each \\(\\psi_{A_i}\\) is a mask, each \\(a_i\\in \\mathbb{R}\\), and \\(k\\) is the number of hidden perceptrons.\n\n\n\nFigure 0.2\n\n\n\n\nTheorem 1 (Theorem 1.5.1) Restricting to mask perceptron machines does not change perceptron orders. That is, any boolean function implemented by a perceptron machine of order \\(k\\) can be implemented by a mask perceptron machine of order at most \\(k\\).\n\n\nProof. Take one such perceptron machine. The threshold of the output perceptron can be removed by adding a hidden perceptron that always outputs \\(1\\) – in other words, \\(\\psi_\\emptyset\\), the mask of the empty set. Now it remains to decompose each hidden perceptron into masks with the same order, or less.\nLet \\(\\psi\\) be a hidden perceptron with nonzero weights on the input points \\(x_{i_1}, \\dots, x_{i_k}\\); then, its output is determined by the values of \\(x_{i_1}, \\dots, x_{i_k}\\). Therefore, we can partition the binary set \\(\\{0, 1\\}^{i_1, \\dots, i_k}\\) into two subsets \\(A_0, A_1\\), such that for any input \\(x\\in\\{0, 1\\}^R\\), we have \\(\\psi(x) = 1\\) iff \\((x_{i_1}, \\dots, x_{i_k}) \\in A_1\\).\nIn other words, we only need to look at the binary values \\(x_{i_1}, \\dots, x_{i_k}\\) to determine the binary output \\(\\psi(x)\\).\nTherefore, we can replace \\(\\psi\\) with a boolean formula on \\(x_{i_1}, \\dots, x_{i_k}\\), then expand it to obtain up to \\(2^k\\) masks, each of order at most \\(k\\).\nFor example, suppose \\(\\psi\\) has nonzero weights on \\(x_1, x_2\\), and is 1 on all odd-sized subsets, then we can write it as a boolean formula:\n\\[\n\\left(x_1 \\wedge \\neg x_2\\right) \\vee\\left(\\neg x_1 \\wedge x_2\\right) = x_1\\left(1-x_2\\right)+\\left(1-x_1\\right) x_2 = x_1 + x_2 - 2 x_1 x_2\n\\]\n\nThe next tool they used is symmetry, formulated in the language of finite group actions.\nLet \\(S_R\\) be the permutation group on the elements of \\(R\\), and \\(G\\) be a subgroup of \\(S_R\\). We say that a boolean function \\(\\psi\\) is \\(G\\)-invariant iff \\(\\psi \\circ g=\\psi\\) for any \\(g \\in G\\). That is, for any \\(X \\subset R\\), we have \\(\\psi(X) = \\psi(g(X))\\). For example, the parity function is \\(S_R\\)-invariant, since any permutation of the set preserves the size, and thus the parity, of any of its subsets.\n\nTheorem 2 (group invariance theorem) If a boolean function is \\(G\\)-invariant, where \\(G\\) is a finite group, then any perceptron machine computing it can be converted to a perceptron machine \\(\\theta(\\sum_i a_i \\psi_i)\\), such that if \\(\\psi_i = \\psi_j \\circ g\\) for some \\(g \\in G\\), then \\(a_i = a_j\\).\n\n\nProof. Take the group-action average: any mask \\(\\psi\\) is equal to \\(\\frac{1}{|G|} \\sum_{g\\in G} \\psi\\circ g\\).\n\nOnce the groundwork was laid, they proceeded to prove a wide variety of theorems on the order of particular boolean functions.\nConsider the parity function. It is \\(1\\) iff exactly an odd number of inputs are \\(1\\) and the rest are \\(0\\).\n\nTheorem 3 (Theorem 3.1) The parity function has order \\(|R|\\).\n\n\nProof. Since the parity function is \\(S_R\\)-invariant, if it is implemented by a perceptron machine of order \\(k\\), it is implemented by some mask perceptron machine \\(\\theta(\\sum_{A_i \\subset R} a_{A_i} \\psi_{A_i})\\), where each mask is of size \\(\\leq k\\), and each weight \\(a_{A_i}\\) depends only on the size of \\(A_i\\). Let \\(b_{|A_i|} = a_{A_i}\\) be those coefficients. It remains to show \\(b_{|R|} \\neq 0\\).\nFor each \\(X \\subset R\\), we have by explicit computation\n\\[\n\\theta\\left(\\sum_{A_i \\subset R} a_{A_i} \\psi_{A_i}\\right) = 1\\left[f(|X|) \\geq 0\\right]\n\\]\nwhere \\(f(t) := \\sum_{i=0}^{|R|}b_i \\binom{t}{i}\\) is a polynomial in \\(t\\). Since this perceptron machine implements the parity function, as \\(t\\) increases from \\(0\\) to \\(|R|\\), the function \\(f(t) + \\epsilon\\) must intersect the \\(x\\)-axis at least \\(|R|\\) times for some real value \\(\\epsilon\\). Since \\(f\\) is a polynomial, it must have at least order \\(|R|\\), thus \\(b_{|R|} \\neq 0\\).\n\n\nTheorem 4 (Theorem 3.2, one-in-a-box) Let \\(A_1, A_2, \\dots, A_m\\) be disjoint subsets of \\(R\\), each of size \\(4 m^2\\), and define the predicate \\(\\psi(X) = \\forall i, \\left|X \\cap A_i\\right|&gt;0\\); that is, there is at least one point of \\(X\\) in each \\(A_i\\). The order of \\(\\psi\\) is \\(\\geq m\\).\n\n\nProof. Let the order of \\(\\psi\\) be \\(k\\).\nThe predicate \\(\\psi\\) is invariant with respect to the group \\(S_{A_1} \\times \\cdots \\times S_{A_m}\\), so by the same construction as the proof of Theorem 2, there exists a polynomial \\(P(t_1, \\dots, t_m)\\), where \\(P\\) has order \\(k\\), and\n\\[\n\\forall t_1, \\dots , t_m \\in \\{0, 1, \\dots , 4m^2\\}, P(t_1, \\dots , t_m) &lt; 0 \\iff t_1 = 0 \\vee \\cdots \\vee t_m = 0\n\\]\nNow define \\(Q(t) := P((t-1)^2, (t-3)^2, \\dots, (t-2m+1)^2)\\). By the above equation, \\(Q &lt; 0\\) at \\(t=1, 3, \\dots, 2m - 1\\) and \\(Q \\geq 0\\) at \\(t = 0, 2, \\dots, 2m\\). Thus, \\(Q\\) has order \\(\\geq 2m\\). Thus, \\(2k \\geq 2m\\).\n\n\nTheorem 5 (Theorem 4.0) There exist predicates \\(\\psi_1\\) and \\(\\psi_2\\) of order 1 such that \\(\\psi_1 \\wedge \\psi_2\\) and \\(\\psi_1 \\vee \\psi_2\\) are not of finite order. Specifically, if we partition \\(R\\) into three equal subsets \\(A, B, C\\), then the boolean function does not have bounded order:\n\\[\n(|X \\cap A| &gt; |X \\cap C|) \\wedge (|X \\cap B| &gt; |X \\cap C|)\n\\]\neven though both \\(|X \\cap A| &gt; |X \\cap C|\\) and \\(|X \\cap B| &gt; |X \\cap C|\\) are of order \\(1\\).\n\n\nProof. \\(|X \\cap A| &gt; |X \\cap C|\\) is computed by the order-\\(1\\) perceptron machine \\(\\theta\\left(\\sum_{i\\in A} x_i - \\sum_{i \\in C}x_i\\right)\\), and similarly for the other one.\nTo show the composite boolean function is not of bounded order, suppose otherwise, then by the same argument as the previous proof, we can construct a sequence of polynomials \\(P_1(a, b, c), P_2(a, b, c), P_3(a, b, c), \\dots\\), such that each \\(P_n\\) is the polynomial corresponding to the perceptron machine for the case where \\(|A| = |B| = |C| = n\\), and each of them has order at most \\(M\\), for some fixed \\(M\\).\nBeing the polynomial corresponding to the perceptron machine for the case where \\(|A| = |B| = |C| = n\\) means precisely that\n\\[\na &gt; c \\wedge b &gt; c \\implies P_n(a, b, c) \\geq 0; \\neg (a &gt; c \\wedge b &gt; c) \\implies P_n(a, b, c) &lt; 0;\n\\]\nfor all \\(a, b, c \\in \\{0, 1, \\dots, n\\}\\). This implies that each \\(P_1, P_2, \\dots \\neq 0\\). Now, divide each polynomial by the root-sum-square of its coefficients, so that if we interpret the coefficients of each \\(P_n\\) as a point in a high-dimensional space, the point falls on the unit sphere in that space. Since the unit sphere is compact, there exists a limit point, which we write as \\(P(a, b, c)\\).\nBy the limit construction, we have\n\\[\n\\forall a, b, c \\in \\mathbb{N}, \\quad a &gt; c \\wedge b &gt; c \\implies P(a, b, c) \\geq 0; \\neg (a &gt; c \\wedge b &gt; c) \\implies P(a, b, c) \\leq 0;\n\\]\nIf we color the points \\(\\mathbb{N}^3\\) with black for \\(P &lt; 0\\) and white for \\(P \\geq 0\\), then we can see the dusty outlines of a pyramid. We can construct a solid pyramid by zooming.\nLet \\(M'\\) be the order of \\(P\\), then we can “zoom out” by taking the projective limit \\(Q(a, b, c) := \\lim_{t \\to \\infty} t^{-M'} P(ta, tb, tc)\\). This \\(Q\\) is a homogeneous polynomial, and by continuity,\n\\[\n\\forall a, b, c \\geq 0, \\quad a &gt; c \\wedge b &gt; c \\implies P(a, b, c) \\geq 0; a &lt; c \\vee b &lt; c \\implies P(a, b, c) \\leq 0;\n\\]\nThis implies that \\(P\\) is identically zero on the “creased curve” \\(\\{ a, b, c \\geq 0, a = c \\vee b = c\\}\\) in the projective plane, which is not an algebraic curve, therefore it is identically zero, contradiction.\n\n\nTheorem 6 (Theorem 5.1) The connectedness function has order \\(\\Omega(|R|^{1/3})\\).\n\n\nProof. If we have a mask perceptron machine that can solve the connectedness problem, then it can be repurposed2 to solve the one-in-a-box problem of the following kind:2 Repurposing one machine to solve another problem is a common trick in computational complexity, called “reduction”. For perceptron machines, they called it “Theorem 5.4.1: The Collapsing Theorem”.\n\n\n\nFigure 5.2\n\n\nIn the picture, the rectangle \\(R\\) has width \\(4m^2\\) and height \\(2m+1\\). We fill in all the odd-numbered rows, and use the machine to solve the one-in-a-box problem for the even-numbered rows. By Theorem 4, the machine has order \\(\\geq m = \\Omega(|R|^{1/3})\\).\n\nIn fact, it turns out that essentially the only locally conjunctive topological invariant is the Euler number.\n\nTheorem 7 (Theorem 5.8.1) The Euler number itself is \\(E(X) = \\sum_{i \\in R} x_i - \\sum_{i, j \\in R} x_ix_j + \\sum_{i, j, k, l \\in R} x_ix_jx_kx_l\\), where the \\(i, j\\in R\\) ranges only over adjacent points, and \\(i, j, k, l \\in R\\) ranges only over quadruples that form a square. Thus the Euler number itself has order \\(4\\).\n\n\nTheorem 8 (Theorem 5.9) If \\(\\psi\\) is a boolean function, and it depends only on the topology of the input figure, and its order is upper-bounded by some number \\(k\\) that does not grow even as \\(R\\) grows into a larger and larger rectangle, then \\(\\psi\\) is of form \\(f \\circ E\\), for some function \\(f: \\mathbb{N}\\to 2\\)."
  },
  {
    "objectID": "blog/posts/reading-perceptron-book/index.html#over-infinite-spaces",
    "href": "blog/posts/reading-perceptron-book/index.html#over-infinite-spaces",
    "title": "Reading Perceptrons",
    "section": "Over infinite spaces",
    "text": "Over infinite spaces\nChapters 6–9 continue in the same style, but move to the case where the input space is made of one or two copies of the infinite line \\(\\mathbb{Z}\\), or the infinite plane \\(\\mathbb{Z}^2\\), and the predicate to recognize is still translation-invariant. In order to avoid blowing up the perceptron machine with infinities, they restricted to the case where the input figure is of finite size, meaning that \\(\\sum_i x_i\\) is finite.\nChapter 6 develops the idea of “spectra” of images. For example, the following picture shows that if we were to design a perceptron machine using only masks of size up to 2, and translation-invariant weights, then it cannot possibly distinguish between the two tetris-like pieces, because both figures contain exactly 4 instances of single-square, 1 instance of two squares side by side, and so on.\n\n\n\npage 100\n\n\nSections 6.1–6.5 of the chapter studies the same idea for masks of size up to 4. For example, Section 6.4 shows that “\\(X\\) is the perimeter of a complete circle” is of order \\(4\\).\nSection 6.6 claims that “recognizing figures in context” is generally not locally conjunctive, although they gave only two examples. Specifically, they showed that the predicate “\\(X\\) is a horizontal line across the rectangle” is order 2, the predicate “\\(X\\) contains one horizontal line across the plane” is not locally conjunctive. The same is true for the case with “a hollow square” instead of “a horizontal line”.\nChapter 7 uses the “stratification” construction to explicitly demonstrate that several binocular predicates are of finite order, but requires exponentially growing weights. It is essentially the same idea as Gödel numbering. A single illustrative example suffices to demonstrate the general point.\n\nExample 1 Given a line \\(\\mathbb{Z}\\), how to construct a perceptron machine that detects that input figure is symmetric?\nSuppose we know for certain that the input figure has leftmost point \\(m\\) and rightmost point \\(n\\), then we can test for symmetry by computing the value of:\n\\[\nf_{m, n}(x) := \\sum_{i = 0}^{n-m} x_{m+i}(x_{n-i} - 1)\n\\]\nWe have that \\(f_{m, n}(x) = 0\\) if the figure is symmetric, and \\(f_{m, n}(x) \\leq -1\\) otherwise.\nNow we define the entire perceptron machine by \\(\\sum_{m \\leq n} M_{n - m} x_m x_n (f_{m, n}(x) - 1/2)\\). If the sequence of \\(M_0, M_1, \\dots\\) grows as \\((d!)^2\\) roughly 3, then the largest bracket \\((m, n)\\) would “veto” every smaller bracket contained within, and so the entire perceptron machine is effectively first testing for the smallest interval containing the figure, before testing the symmetry within that interval.3 Expanding term by term, we have \\(|f_{m, n}(x) - 1/2| \\leq 2(n-m) + \\frac{1}{2}\\). Therefore, in order for \\(M_d\\) to “veto” every other bracket within, we need\n\\[\nM_d \\frac 12 &gt; \\sum_{d' = 1}^{d-1} \\left(M_{d'}(\\frac 12 + 2d')(d-d' + 1)\\right)\n\\]\nHere \\(d\\) should be read as “distance between two ends of a bracket”.\nTo bound the growth rate, we bound the recurrence relation \\(M_d = \\sum_{d' = 1}^{d-1} \\left(M_{d'}(4d' + 1)(d-d' + 1)\\right)\\). The sum on the right is bounded by\n\\[\n\\sum_{d' = 1}^{d-1} \\left(M_{d'}(4d' + 1)(d-d' + 1)\\right) \\in \\Theta{\\left[\n    \\sum_{d' = 1}^{d-1} \\left(M_{d'}d'\\right),\n    d^2\\sum_{d' = 1}^{d-1} \\left(M_{d'}\\right)\\right]}\n\\]\nThe lower bound implies \\(M_d = \\Omega((d!)^2 \\times d^{-1})\\) and the upper bound implies \\(M_d = O((d!)^2 \\times (d+1)^2)\\).\n\nThey made similar constructions for perceptron machines that decide whether two infinite lines or planes are translates of each other, whether two planes are translation and approximate dilations of each other, and whether a line or a plane is the translation of a fixed figure.\nChapter 8 studies diameter-limited perceptron machines, meaning that not only are the hidden perceptrons assumed to be masks of bounded size, those masks are assumed to be contained in a circle of radius \\(M\\) for some finite \\(M\\). It is intended to be a formalization of the intuition that the perceptron machines should model a real pair of eyes scanning a plane, and each retina is a circular disk with finite radius. No results in chapter 8 were used further in the book, and they resemble the case of finite-order perceptron machines, so we do not discuss the results in detail.\nChapter 9 shows that the connectedness problem is easy to solve using small serial programs, a contrast to the case of perceptron machines. The overarching lesson is that connectedness is an “inherently serial” decision problem. The whole chapter is beautiful computational complexity theory, in the same style of solving fun mathematical puzzles, similar to (Minsky 1967).\nThey designed a robot (a finite state machine) that moves around the plane, and can solve the connectedness problem, with a memory size of just two. Namely, it only needs to store up to two locations \\((x, y), (x', y')\\) in its memory during its operation, and it eventually halts in one of three states “empty”, “connected”, and “disconnected”. Notice that a robot with no memory can still remember a finite number of states. It is simply that those memory slots are its finite states, which do not scale with the size of the problem. The little robot with a “memory size of two” really has a memory size of \\(2 \\log_2|R|\\) bits, because it can remember two coordinates from the square \\(R\\), no matter how large the square grows.\nThey then described a few other more exotic computational models, such as a “pebble machine”, meaning a robot that has no memory, but provided with a single pebble. It can drop the pebble anywhere, and it can pick it up again. One can think of this as an intermediate level between pure finite state machines, which cannot write at all, and a Turing machine, which can write as much as it wants. They left as an exercise for the reader the task of translating the previous robot with two memory slots to this robot with one pebble. They conjectured that a finite state machine (a robot with no pebbles) would be unable to solve the task, but they could not prove it. I did a brief literature search and it seemed to be still unsolved."
  },
  {
    "objectID": "blog/posts/reading-perceptron-book/index.html#learning-theory",
    "href": "blog/posts/reading-perceptron-book/index.html#learning-theory",
    "title": "Reading Perceptrons",
    "section": "Learning theory",
    "text": "Learning theory\nThey claimed that Chapter 10 is part of the learning theory. However, it does not actually involve learning. Whereas in the construction Example 1, we saw coefficients growing exponentially on an infinite plane, chapter 10 proves similar results on a finite plane.\n\nExample 2 (Theorem 10.1) Suppose we have a perceptron machine that tests for parity; then, by Theorem 3, it must have order \\(|R|\\). As in the construction given in the proof, we use the group invariance theorem to obtain a group-symmetric machine with the form \\(\\theta(\\sum_{i=0}^{|R|}\\binom{|X|}{i} b_i)\\), where \\(b_0, b_1, \\dots, b_{|R|}\\) are real numbers. Then, assuming the machine is “reliable”, we can prove that \\((-1)^{M} b_{M+1} \\geq 2^{M}\\) for any \\(M \\in \\{0, 1, \\dots, |R|-1\\}\\).\nSince the group-symmetric construction can only average out the most extreme values, this implies that, before the group-symmetric construction, our perceptron machine had even more extreme coefficients.\nA “reliable” machine is a support vector machine with margin \\(\\geq 1/2\\). That is, it is a machine such that \\[\n\\sum_{i=0}^{|R|}\\binom{|X|}{i} b_i \\begin{cases}\n\\geq 1 & \\text{ if $|X|$ is odd}\n\\leq 0 & \\text{ if $|X|$ is even}\n\\end{cases}\n\\]\n\n\nProof. Define \\(A_n = \\sum_{i=0}^n \\binom{n}{i}b_i\\). Since the machine is reliable, we have that \\((-1)^{n}(A_{n+1} - A_n) \\geq 1\\) for each \\(n = 0, 1, \\dots, |R|-1\\). Simplifying the binomial coefficients, we have \\(A_{n+1} - A_n = \\sum_i \\binom{n}{i} b_{i+1}\\). Note that we use the convenient convention that \\(\\binom{x}{y} = 0\\) if \\(x &lt; y\\).\nNow fix any \\(M \\in \\{0, 1, \\dots, |R|-1\\}\\), and evaluate the following inequality:\n\\[\n2^{M} = \\sum_n \\binom{M}{n} \\cdot 1 \\leq \\sum_n \\binom{M}{n} (-1)^{n}(A_{n+1} - A_n)\n\\]\nBy manipulating the binomial coefficients, the right side simplifies to \\((-1)^M b_{M+1}\\).\n\nSections 10.2 and 10.3 construct two pathological examples. By restricting the shapes of hidden perceptron masks, they proved that certain predicates required superexponential weights. Section 10.4 purports to extend the group invariance theorem to the infinite case.4 None of these are used elsewhere in the book. They are virtuoso performances. However, they interpreted this as a serious problem:4 They purported to show in Theorem 10.4.1 that, if the weights are bounded, then the group invariance theorem applies again, but the proof is false, with a counterexample being \\(\\sum_{n \\in \\mathbb{Z}}e^{-n^2}x_n - e^{-n^4}x_nx_{n+1}\\). The theorem might still be correct with another proof, but I cannot find one.\n\nA proof, in Chapter 10, that coefficients can grow much faster than exponentially with \\(|R|\\) has serious consequences both practically and conceptually: the use of more memory capacity to store the coefficients than to list all the figures strains the idea that the machine is making some kind of abstraction\n\nIn Chapter 11, they finally began discussing perceptron learning, which is of a very restrictive form.\n\nDefinition 6 (Perceptron learning) To train a perceptron machine is to fix its hidden perceptrons, and adjust the weights and threshold of only the single output perceptron by the perceptron learning rule. For this chapter, it is cleaner to change the convention so that each perceptron outputs \\(-1, +1\\) instead of \\(0, 1\\).\nSince only the output perceptron is adapted, it suffices to discuss the case where there are no hidden perceptrons, and we only need to adapt the weights of a single perceptron. That is, we have a dataset \\(D\\), and we sample some \\((x, y) \\in D\\), and verify that \\(y = \\theta(\\braket{w, x})\\).\nIf this is true for all \\((x, y) \\in D\\), then the perceptron learning has converged. Otherwise, we update \\(w\\) using \\(w \\leftarrow w + \\alpha y x\\), where \\(\\alpha &gt; 0\\) is the learning rate.\n\n\nDefinition 7 (Perceptron learning theorem) Let \\(D\\) be a dataset with radius \\(R = \\max_{(x, y) \\in D} \\|x\\|\\). If there exists some unit \\(w^*\\) such that \\(\\gamma = \\min_{(x, y) \\in D} y\\braket{w^*, x}\\), then the perceptron learning algorithm converges after making at most \\((R/\\gamma)^2\\) updates.\n\n\nProof. By linearity of the learning rule, we can deal only with the case where \\(\\alpha = 1\\).\nBy multiplying each \\(x\\) with its \\(y\\), we can deal only with the case where all \\(y = +1\\).\nBy rotating and scaling the space, we can deal only with the case where \\(w^* = (1, 0, \\dots, 0)\\), and \\(\\gamma = 1\\).\nNow, each weight update increases the first coordinate of \\(w\\) by at least \\(1\\), so after \\(n\\) updates, \\(\\|w\\| \\geq n\\). However, each weight update of \\(w \\leftarrow w + x\\) uses a vector \\(x\\) that is pointing in a direction perpendicular to \\(w\\), or worse, pointing against \\(w\\). Therefore, by Pythagorean theorem, \\(\\|w\\|^2\\) increases by at most \\(\\|x\\|^2 \\leq R^2\\). So after \\(n\\) updates, \\(\\|w\\|^2 \\leq nR^2\\).\nCombining the two results, we have \\(n \\leq R^2\\).\n\nModifying the proof slightly, and applying the conclusion of Example 2, we find that starting with the zero weight vector, it takes at least \\(2^{|R|}/|R|\\) steps to learn the parity function.\nThey then suggested that, since gradient descent is “just” a more efficient perceptron learning rule, it also cannot escape local optima. No “local learning rule” can escape local optima, unlike symbolic programs that are provably capable of finding global optima.\nIf the dataset is not linearly separable, then the perceptron weights will not converge. However, the perceptron cycling theorem shows that if the dataset is finite, then the perceptron weights will still be trapped within a large but finite disk, no matter how the dataset is sampled.\nChapter 12 is not very mathematical, and consists mainly of quick sketches5 of other algorithms for learning. Those included: lookup table, nearest neighbor, k-means, ISODATA, maximum likelihood, Bayes, naive Bayes, etc. Sections 12.6 and 12.7 study variations on a toy problem: given a subset of \\(\\{0, 1\\}^n\\), decide whether an \\(n\\)-bit word is in it or not. This had relevance to the time-space tradeoff, a perennial topic in computational complexity.5 \nIn this chapter we will study a few of these to indicate points of contact with the perceptron and to reveal deep differences. … The chapter is written more in the spirit of inciting students to research than of offering solutions to problems."
  },
  {
    "objectID": "blog/posts/reading-perceptron-book/index.html#chapter-13",
    "href": "blog/posts/reading-perceptron-book/index.html#chapter-13",
    "title": "Reading Perceptrons",
    "section": "Chapter 13",
    "text": "Chapter 13\nChapter 13 is a summary of the “take-home lessons” for the readers. As the intended lessons were expounded in great length in the epilogue added in 1988, I would not analyze the chapter in detail.\nThey discussed “Gamba perceptrons”, which is just another name for two-layered perceptrons where the hidden layer consists of perceptrons, instead of merely masks. They made the infamous prediction that not only Gamba perceptrons but also arbitrary multilayer perceptrons would be a “sterile extension”.\n\nWell, we have considered Gamba machines, which could be described as “two layers of perceptron.” We have not found (by thinking or by studying the literature) any other really interesting class of multilayered machine, at least none whose principles seem to have a significant relation to those of the perceptron. To see the force of this qualification it is worth pondering the fact, trivial in itself, that a universal computer could be built entirely out of linear threshold modules. This does not in any sense reduce the theory of computation and programming to the theory of perceptrons. Some philosophers might like to express the relevant general principle by saying that the computer is so much more than the sum of its parts that the computer scientist can afford to ignore the nature of the components and consider only their connectivity. More concretely, we would call the student’s attention to the following considerations:\n\nMultilayer machines with loops clearly open all the questions of the general theory of automata.\nA system with no loops but with an order restriction at each layer can compute only predicates of finite order.\nOn the other hand, if there is no restriction except for the absence of loops, the monster of vacuous generality once more raises its head.\n\nThe perceptron has shown itself worthy of study despite (and even because of!) its severe limitations. It has many features to attract attention: its linearity; its intriguing learning theorem; its clear paradigmatic simplicity as a kind of parallel computation. There is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgment that the extension is sterile.\n\nIn short, their objection to multilayer perceptrons is that they suffer from Turing completeness, making it hard to prove anything concrete about them, except the trivial claim that they are Turing complete.6 A single perceptron is just a linear classifier, so it is possible to study mathematically. Experimental evidence is no justification, because:6 \nBeware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n\n\n\n13.5 Why Prove Theorems?\nWhy did you prove all these complicated theorems? Couldn’t you just take a perceptron and see if it can recognize \\(\\psi_{\\text {CONNECTED}}\\)?\nNo.\n\n\n\n\npage 239\n\n\nSince a perceptron-only architecture is not the right way, they illustrated what they believe to be the “right” way to do computer vision by describing in detail the scene analysis algorithm. In short, it starts by discovering edges, then performs some computational geometry on them to recover outlines of basic shapes, such as cubes, then fills in the faces, then fills in the bodies between the faces. The algorithm had to do something clever to deal with occlusions. In the end, a complete 3D scene populated with 3D objects is recovered.\n\n\n\nScene analysis. Figure from (Guzmán 1968)\n\n\nThey ended the book with a brief discussion of how they discovered the various results, as well as a list of people they thanked."
  },
  {
    "objectID": "blog/posts/perceptron-controversy/index.html",
    "href": "blog/posts/perceptron-controversy/index.html",
    "title": "The Perceptron Controversy",
    "section": "",
    "text": "During the 1950s and 1960s, the connectionist and the symbolic schools of artificial intelligence competed for researcher allegiance and funding, with the symbolic school winning by 1970. This “first neural network winter” lasted until the rise of connectionist school in the 1980s.\nIt is often stated that neural networks were killed off by the 1969 publication of Perceptrons by Marvin Minsky and Seymour Papert. This story is wrong on multiple accounts:\n\nMinsky and Papert had been working towards killing off neural networks since around 1965 by speaking at conferences and circulating preprints.\nThe mathematical content studies only the behavior of a single perceptron. It does not study multilayer perceptrons.\nBy 1969, most researchers had already left connectionism, frustrated by the lack of progress, as they did not develop backpropagation or deep learning. The last holdout, Frank Rosenblatt, died in 1971.\n\nThe book achieved its mythical status as the “neural network killer” by its opportune timing, appearing just as the symbolic school achieved dominance. Since the connectionist school was almost empty, it faced little objection at its publication, creating the illusion that it caused the closure of the perceptron controversy.\nIn the 1980s, the perceptron controversy reopened with the rise of connectionism. This time the controversy was bypassed without closure. Connectionists demonstrated that backpropagation with MLP bypassed most of the objections from Perceptrons. Minsky and Papert objected that the lessons of Perceptrons still applied, but their objections and lessons had by then become irrelevant.\nMinsky and Papert were possibly the most consistent critics of the scaling hypothesis, arguing over decades that neural networks cannot scale beyond mere toy problems. Minsky was motivated by mathematical certainty, as backpropagation–MLP cannot provably find global optima or accomplish any task efficiently, unlike a single perceptron. He rejected all experimental data with large MLP as theory-less data that cannot be extrapolated. Papert was motivated by social justice and epistemological equality. He rejected all scalable uniform architectures like backpropagation–MLP, as threats to his vision of a society with different but equally valid ways of knowing.\nAs of 2019, essentially all predictions by Minsky and Papert, concerning the non-scalability of neural networks, had been disproven."
  },
  {
    "objectID": "blog/posts/perceptron-controversy/index.html#abstract",
    "href": "blog/posts/perceptron-controversy/index.html#abstract",
    "title": "The Perceptron Controversy",
    "section": "",
    "text": "During the 1950s and 1960s, the connectionist and the symbolic schools of artificial intelligence competed for researcher allegiance and funding, with the symbolic school winning by 1970. This “first neural network winter” lasted until the rise of connectionist school in the 1980s.\nIt is often stated that neural networks were killed off by the 1969 publication of Perceptrons by Marvin Minsky and Seymour Papert. This story is wrong on multiple accounts:\n\nMinsky and Papert had been working towards killing off neural networks since around 1965 by speaking at conferences and circulating preprints.\nThe mathematical content studies only the behavior of a single perceptron. It does not study multilayer perceptrons.\nBy 1969, most researchers had already left connectionism, frustrated by the lack of progress, as they did not develop backpropagation or deep learning. The last holdout, Frank Rosenblatt, died in 1971.\n\nThe book achieved its mythical status as the “neural network killer” by its opportune timing, appearing just as the symbolic school achieved dominance. Since the connectionist school was almost empty, it faced little objection at its publication, creating the illusion that it caused the closure of the perceptron controversy.\nIn the 1980s, the perceptron controversy reopened with the rise of connectionism. This time the controversy was bypassed without closure. Connectionists demonstrated that backpropagation with MLP bypassed most of the objections from Perceptrons. Minsky and Papert objected that the lessons of Perceptrons still applied, but their objections and lessons had by then become irrelevant.\nMinsky and Papert were possibly the most consistent critics of the scaling hypothesis, arguing over decades that neural networks cannot scale beyond mere toy problems. Minsky was motivated by mathematical certainty, as backpropagation–MLP cannot provably find global optima or accomplish any task efficiently, unlike a single perceptron. He rejected all experimental data with large MLP as theory-less data that cannot be extrapolated. Papert was motivated by social justice and epistemological equality. He rejected all scalable uniform architectures like backpropagation–MLP, as threats to his vision of a society with different but equally valid ways of knowing.\nAs of 2019, essentially all predictions by Minsky and Papert, concerning the non-scalability of neural networks, had been disproven."
  },
  {
    "objectID": "blog/posts/perceptron-controversy/index.html#the-enigma-of-marvin-minsky",
    "href": "blog/posts/perceptron-controversy/index.html#the-enigma-of-marvin-minsky",
    "title": "The Perceptron Controversy",
    "section": "The enigma of Marvin Minsky",
    "text": "The enigma of Marvin Minsky\nIn an 1993 interview, Robert Hecht-Nielsen recounted an encounter between Marvin Minsky and the neural network community in the late 1980s1:1 This was corroborated by a contemporary news report on the International Conference on Neural Networks of 1988:\n\nMinsky who has been criticized by many for the conclusions he and Papert make in ‘Perceptrons,’ opened his defense with the line ‘Everybody seems to think I’m the devil.’ Then he made the statement, ‘I was wrong about Dreyfus too, but I haven’t admitted it yet,’ which brought another round of applause. (quoted in (Olazaran 1991, 285))\n\n\n\nMinsky had gone to the same New York “science” high school as Frank Rosenblatt, a Cornell psychology Ph.D. whose “perceptron” neural network pattern recognition machine was receiving significant media attention. The wall-to-wall media coverage of Rosenblatt and his machine irked Minsky. One reason was that although Rosenblatt’s training was in “soft science,” his perceptron work was quite mathematical and quite sound—turf that Minsky, with his “hard science” Princeton mathematics Ph.D., didn’t feel Rosenblatt belonged on. Perhaps an even greater problem was the fact that the heart of the perceptron machine was a clever motor-driven potentiometer adaptive element that had been pioneered in the world’s first neurocomputer, the “SNARC”, which had been designed and built by Minsky several years earlier! In some ways, Minsky’s early career was like that of Darth Vader. He started out as one of the earliest pioneers in neural networks but was then turned to the dark side of the force (AI) and became the strongest and most effective foe of his original community. This view of his career history is not unknown to him. When he was invited to give the keynote address at a large neural network conference in the late 1980s to an absolutely rapt audience, he began with the words: “I am not the Devil!” (Rosenfeld and Anderson 2000, 303–5)\n\nHowever, it appears he had changed his mind later. As recounted by Terry Sejnowski:\n\nI was invited to attend the 2006 Dartmouth Artificial Intelligence Conference, “AI@50,” a look back at the seminal 1956 Summer Research Project on artificial intelligence held at Dartmouth and a look forward to the future of artificial intelligence. … These success stories had a common trajectory. In the past, computers were slow and only able to explore toy models with just a few parameters. But these toy models generalized poorly to real-world data. When abundant data were available and computers were much faster, it became possible to create more complex statistical models and to extract more features and relationships between the features.\nIn his summary talk at the end of the conference [The AI@50 conference (2006)], Marvin Minsky started out by saying how disappointed he was both by the talks and by where AI was going. He explained why: “You’re not working on the problem of general intelligence. You’re just working on applications.” …\nThere was a banquet on the last day of AI@50. At the end of the dinner, the five returning members of the 1956 Dartmouth Summer Research Project on Artificial Intelligence made brief remarks about the conference and the future of AI. In the question and answer period, I stood up and, turning to Minsky, said: “There is a belief in the neural network community that you are the devil who was responsible for the neural network winter in the 1970s. Are you the devil?” Minsky launched into a tirade about how we didn’t understand the mathematical limitations of our networks. I interrupted him—“Dr. Minsky, I asked you a yes or no question. Are you, or are you not, the devil?” He hesitated for a moment, then shouted out, “Yes, I am the devil!” (Sejnowski 2018, 256–58)\n\nWhat are we to make of the enigma of Minsky? Was he the devil or not?\n\nThe intellectual history of Minsky\nDuring his undergrad years, Minsky was deeply impressed by Andrew Gleason,2 and decided to work on pure mathematics, resulting in his 1951 undergraduate thesis A Generalization of Kakutani’s Fixed-Point Theorem, which extended an obscure fixed point theorem of Kakutani – not the famous version, as Kakutani proved more than one fixed point theorems.2 \nI asked Gleason how he was going to solve it. Gleason said he had a plan that consisted of three steps, each of which he thought would take him three years to work out. Our conversation must have taken place in 1947, when I was a sophomore. Well, the solution took him only about five more years … Gleason made me realize for the first time that mathematics was a landscape with discernible canyons and mountain passes, and things like that. In high school, I had seen mathematics simply as a bunch of skills that were fun to master – but I had never thought of it as a journey and a universe to explore. No one else I knew at that time had that vision, either. (Bernstein 1981)\n\n\n\nTheorem 1 (Kakutani’s fixed point theorem on the sphere) If \\(f\\) is a \\(\\mathbb{R}^2\\)-valued continuous function on the unit sphere in \\(\\mathbb{R}^3\\), then for any side length \\(r \\in (0, \\sqrt 3)\\), there exists \\(x_1, x_2, x_n\\) on the sphere forming an equilateral triangle with side length \\(r\\), such that \\(f(x_1) = f(x_2) = f(x_3)\\).\nEquivalently, if \\(x_1, x_2, x_3\\) form an equilateral triangle on the unit sphere, then there exists a rotation \\(T\\) such that \\(f(T(x_1)) = f(T(x_2)) = f(T(x_3))\\).\n\nUsing knot theory, Minsky proved an extension where \\(x_1, x_2, x_3\\) are three points of a square or a regular pentagon (M. Minsky 2011). The manuscript has “disappeared” (M. L. Minsky n.d.).\n\nI wrote it up and gave it to Gleason. He read it and said, ‘You are a mathematician.’ Later, I showed the proof to Freeman Dyson, at the Institute for Advanced Study, and he amazed me with a proof (Dyson 1951) that there must be at least one square that has the same temperature at all four vertices. He had found somewhere in my proof a final remnant of unused logic. (Bernstein 1981)\n\nHe then got interested in neural networks and reinforcement learning, and constructed a very simple electromechanical machine called SNARC.3 The SNARC machine is a recurrent neural network, performing reinforcement learning by Hebbian learning rule. It simulates a mouse running around a maze, and the operator watches an indicator light showing the mouse. The operator can press a button as a reward signal, and this would cause an electric motor to turn a chain. The chain is clutched to rheostats that connect the neurons, and the stretch of the clutch is proportional to the charge in a capacitor. During the operation of the neural network, the capacitor charges up if there is neural co-activation on the connection, and decays naturally, thus serving as a short-term memory. When the reward button is pressed, the clutches turn by an amount proportional to the co-activation of neural connections, thus completing the Hebbian learning.3 It was published as (M. Minsky 1952), but the document is not available online, and I could only piece together a possible reconstruction from the fragments of information.\nMinsky was impressed by how well it worked. The machine was designed to simulate one mouse, but by some kind of error it simulated multiple mice, and yet it still worked.\n\nThe rats actually interacted with one another. If one of them found a good path, the others would tend to follow it. We sort of quit science for a while to watch the machine. … In those days, even a radio set with twenty tubes tended to fail a lot. I don’t think we ever debugged our machine completely, but that didn’t matter. By having this crazy random design, it was almost sure to work, no matter how you built it. (Bernstein 1981)\n\nThis was the last we see Minsky’s work with random neural networks. He has crossed the Rubicon, away from the land of brute reason and into the land of genuine insight.\n\nI had the naïve idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. … Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. (Bernstein 1981)\n\nFor his PhD thesis, Minsky worked on the mathematical theory of McCulloch–Pitts neural networks. In style, it was a fine piece of classical mathematics (M. L. Minsky 1954). Minsky would go on to write (M. Minsky 1967, chap. 3), still the best introduction to McCulloch–Pitts neural networks.\n\nMinsky’s doctoral dissertation in mathematics from Princeton in 1954 was a theoretical and experimental study of computing with neural networks. He had even built small networks from electronic parts to see how they behaved. The story I heard when I was a graduate student at Princeton in physics was that there wasn’t anyone in the Mathematics Department who was qualified to assess his dissertation, so they sent it to the mathematicians at the Institute for Advanced Study in Princeton who, it was said, talked to God. The reply that came back was, “If this isn’t mathematics today, someday it will be,” which was good enough to earn Minsky his PhD. (Sejnowski 2018, 259)\n\nReading the story, I was reminded of “Sussman attains enlightenment”, a hacker koan about Minsky and his student Sussman 4:4 This is based on a true story.\n\n… Sussman told Minsky that he was using a certain randomizing technique in his program because he didn’t want the machine to have any preconceived notions. Minsky said, “Well, it has them, it’s just that you don’t know what they are.” It was the most profound thing Gerry Sussman had ever heard. And Minsky continued, telling him that the world is built a certain way, and the most important thing we can do with the world is avoid randomness, and figure out ways by which things can be planned. (Levy 2010, 110–11)\n\n\n\nIn the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?”, asked Minsky. “I am training a randomly wired neural net to play Tic-Tac-Toe” Sussman replied. “Why is the net wired randomly?”, asked Minsky. “I do not want it to have any preconceptions of how to play”, Sussman said. Minsky then shut his eyes. “Why do you close your eyes?”, Sussman asked his teacher. “So that the room will be empty.” At that moment, Sussman was enlightened.\n\nAs for Sussman, I knew him for two things: writing the SICP book, and being the coordinator of the infamous summer vision project that was to construct “a significant part of a visual system” in a single summer, using only undergraduate student researchers. A brief read of his “reading list” shows where his loyalties lie: firmly in the school of neats.\n(Sejnowski 2018, 28) recounts the background of the summer vision project:\n\nIn the 1960s, the MIT AI Lab received a large grant from a military research agency to build a robot that could play Ping-Pong. I once heard a story that the principal investigator forgot to ask for money in the grant proposal to build a vision system for the robot, so he assigned the problem to a graduate student as a summer project. I once asked Marvin Minsky whether the story was true. He snapped back that I had it wrong: “We assigned the problem to undergraduate students.”\n\nAfter rejecting neural networks, Minsky became a leading researcher in AI. His style of AI is typically described as “symbolic AI”, although a more accurate description would be The Society of Mind (SoM). Minsky developed the SoM in the 1960s and 1970s with his long-time collaborator, Seymour Papert, inspired by their difficulty with building robots, and published the definitive account in (M. Minsky 1988). The SoM thesis states that “any brain, machine, or other thing that has a mind must be composed of smaller things that cannot think at all”.\nStated in this way, it seems patently compatible with neural networks, but only on the surface. Minsky described concretely how he expects a Society of Mind should work, based on his attempts at making Builder, a robot that can play with blocks:\n\nBoth my collaborator, Seymour Papert, and I had long desired to combine a mechanical hand, a television eye, and a computer into a robot that could build with children’s building-blocks. It took several years for us and our students to develop Move, See, Grasp, and hundreds of other little programs we needed to make a working Builder-agency. I like to think that this project gave us glimpses of what happens inside certain parts of children’s minds when they learn to “play” with simple toys. The project left us wondering if even a thousand microskills would be enough to enable a child to fill a pail with sand. It was this body of experience, more than anything we’d learned about psychology, that led us to many ideas about societies of mind.\nTo do those first experiments, we had to build a mechanical Hand, equipped with sensors for pressure and touch at its fingertips. Then we had to interface a television camera with our computer and write programs with which that Eye could discern the edges of the building-blocks. It also had to recognize the Hand itself. When those programs didn’t work so well, we added more programs that used the fingers’ feeling-sense to verify that things were where they visually seemed to be. Yet other programs were needed to enable the computer to move the Hand from place to place while using the Eye to see that there was nothing in its way. We also had to write higher-level programs that the robot could use for planning what to do—and still more programs to make sure that those plans were actually carried out. To make this all work reliably, we needed programs to verify at every step (again by using Eye and Hand) that what had been planned inside the mind did actually take place outside—or else to correct the mistakes that occurred. … Thousands and, perhaps, millions of little processes must be involved in how we anticipate, imagine, plan, predict, and prevent—and yet all this proceeds so automatically that we regard it as “ordinary common sense.” (M. Minsky 1988, sec. 2.5)\n\n\n\n\nMarvin Minsky and Builder the robot. It’s hard to tell who is having more fun here.\n\n\nFrom the concrete description, as well as the many attractive illustrations in the book, it is clear that Minsky intended the “Society of Mind” to be a uniform computing substrate (silicon or carbon) upon which millions of little symbolic programs are running, each capable of running some specific task, each describable by a distinct and small piece of symbolic program. They cannot be mere perceptrons in a uniform block of neural network, or mere logic gates in a uniform block of CPU.\nIn his 1988 book, Minsky described dozens of these heterogeneous components that he thought might make up a Society of Mind. However, the precise details are not relevant,5 as he freely admits that those are conjectured details. He was only adamant about the overarching scheme: heterogeneous little separated components, not a homogeneous big connected piece.5 The conjectured components included “K-lines”, “nomes”, “nemes”, “frames”, “frame-arrays”, etc. Although Minsky meant for this SoM project to last a very long time, building up to general intelligence brick by brick, my literature search shows that there had been no new development since the 2000s, so the overview (Singh 2003) still represents the SOTA of SoM.\nPerhaps a modern reincarnation of such an idea would be the dream of Internet agents operating in a digital economy, populated by agents performing simple tasks like spam filtering, listening for the price of petroleum. Some agents would interface with reality, while others interface with agents. Some agents are organized on a higher level into DAOs, made by a small committee of simple “manager agents” as the interface and coordinator of other agents. DAOs can interface with other DAOs by little speaker-agents, who are themselves composed of a simple text filter for the torrent of information, which they then outsource to text-weaving agents to compose the actual messages that they would send out.\n\n\nSeymour Papert\nSeymour Papert, the long-time collaborator of Minsky, was the second author of Perceptrons. To unlock the enigma of Minsky, we must look into Papert’s past as well.\nIn 1958, after a doctorate in mathematics, he met Jean Piaget and became his pupil for four years. This had a formative effect on Papert. Piaget’s work was an important influence on the constructivism philosophy in education, and Papert would go on to make serious progress towards applying constructivism to real education. He was particularly hopeful that computers can realize the constructivist dream of unlocking the kaleidoscopic creativity that a child can construct.\nThe main theme of Jean Piaget’s work was developmental psychology, how children’s understanding of the world changes as they grow up. What goes on in their mind as they progressively understand that things fall down, what dogs are, and solid steel sinks but hollow steel might float? Piaget discovered that children did not simply start with a blank sheet of paper and gradually fill in sketchy details of the true model, instead they constructed naive models of small aspects the world that, as it encounters phenomena it cannot explain, would be modified or completely replaced. In this way, Piaget claimed that children are “little scientists”.\nA small example would illustrate the idea. For example, when children see that a leaf floats on water, but a stone sinks, they might add a rule “Soft things float, while hard things sink.”. Then they see that a hard plastic boat floats too, so they might add a rule “Except hard and light things also float.”. Then they see that a large boat also float, so they rewrite the entire model to “Flat-bottomed things float, while small-bottomed things sink.”. And so on.\nThere are conservative and radical ways of using Piaget’s research for pedagogy. The conservative way is to study how children tend to construct their scientific theories, and discover a sequence of evidences to present to the little scientists so that they reach the scientific orthodoxy as fast as possible. For example, we might present children with videos of curling and air hockey, and then let them play with an air hockey table, then guide them through exercises, so that they arrive as quickly as possible at the Newton’s laws of motion.\nThe radical way is to decenter the orthodoxy and let a thousand heterodoxies bloom. Why go for the orthodoxy, when the Duhem–Quine thesis tells us that evidence is never enough to constrain us to only one orthodoxy? And given that objectively no theory deserves the name of “orthodoxy”, how come the scientific “orthodoxy” became dominant? A critical analysis of the history shows that its dominance over aboriginal and woman ways of knowing is merely a historical accident due to an alliance with the hegemonic reason of the center over the periphery.\nPapert went with the radical way.\nAfter four years of study under Piaget, he arrived in MIT in 1963, and began working with Minsky on various topics, including the Logo Turtle robot, and the Perceptrons book. The computer revolution was starting, and Papert saw computers as a way to bring radical constructivism to children.\nIn the real world, phenomena are limited by nature, and aspiring little heterodoxy-builders are limited by their ability to construct theories and check their consequences. In the computer world, every children could program and construct “microworlds” from their own theories. Thus, computers would bring constructivism to the classroom. Furthermore, the constructed world inside computers could then flow out to the physical world via robots. This is why Papert worked on both Logo the programming language and Logo the turtle robots. In his words, he intended to fight “instructionism” with “constructionism” by bringing the power of the computer to every child, so that they would grow up to be “bricoleurs”, working with whatever little tool they have available doing whatever is necessary to accomplish little things. This is a vital piece in his overarching project of epistemological pluralism, to liberate heterodoxical ways of knowing (S. A. Papert 1994, chap. 7):\n\nTraditional education codifies what it thinks citizens need to know and sets out to feed children this “fish.” Constructionism is built on the assumption that children will do best by finding (“fishing”) for themselves the specific knowledge they need … it is as well to have good fishing lines, which is why we need computers, and to know the location of rich waters, which is why we need to develop a large range of mathetically rich activities or “microworlds.”\n… School math, like the ideology, though not necessarily the practice, of modern science, is based on the ideal of generality – the single, universally correct method that will work for all problems and for all people. Bricolage is a metaphor for the ways of che old-fashioned traveling tinker, the jack-of-all-trades who knocks on the door offering to fix whatever is broken. Faced with a job, the tinker rummages in his bag of assorted tools to find one that will fit the problem at hand and, if one tool does nor work for the job, simply tries another without ever being upset in the slightest by the lack of generality. The basic tenets of bricolage as a methodology for intellectual activity are: Use what you’ve got, improvise, make do. And for the true bricoleur, the tools in the bag will have been selected over a long time by a process determined by more than pragmatic utility. These mental tools will be as well worn and comfortable as the physical tools of the traveling tinker; they will give a sense of the familiar, of being at ease with oneself …\nKitchen math provides a clear demonstration of bricolage in its seamless connection with a surrounding ongoing activity that provides the tinker’s bag of tricks and tools. The opposite of bricolage would be to leave the “cooking microworld” for a “math world,” to work the fractions problem using a calculator or, more likely in this case, mental arithmetic. But the practitioner of kitchen math, as a good bricoleur, does not stop cooking and turn to math; on the contrary, the mathematical manipulations of ingredients would be indistinguishable to an outside observer from the culinary manipulations.\n… The traditional epistemology is based on the proposition, so closely linked to the medium of text-written and especially printed. Bricolage and concrete thinking always existed but were marginalized in scholarly contexts by the privileged position of text. As we move into the computer age and new and more dynamic media emerge, this will change.\n\nAccording to Papert, his project is epistemological pluralism, or promoting different ways of knowing (Turkle and Papert 1990):\n\nThe diversity of approaches to programming suggests that equal access to even the most basic elements of computation requires accepting the validity of multiple ways of knowing and thinking, an epistemological pluralism. Here we use the word epistemology in a sense closer to Piaget’s than to the philosopher’s. In the traditional usage, the goal of epistemology is to inquire into the nature of knowledge and the conditions of its validity; and only one form of knowledge, the propositional, is taken to be valid. The step taken by Piaget in his definition of epistemologie genetique was to eschew inquiry into the “true” nature of knowledge in favor of a comparative study of the diverse nature of different kinds of knowledge, in his case the kinds encountered in children of different ages. We differ from Piaget on an important point, however. Where he saw diverse forms of knowledge in terms of stages to a finite end point of formal reason, we see different approaches to knowledge as styles, each equally valid on its own terms.\n… The development of a new computer culture would require more than environments where there is permission to work with highly personal approaches. It would require a new social construction of the computer, with a new set of intellectual and emotional values more like those applied to harpsichords than hammers. Since, increasingly, computers are the tools people use to write, to design, to play with ideas and shapes and images, they should be addressed with a language that reflects the full range of human experiences and abilities. Changes in this direction would necessitate the reconstruction of our cultural assumptions about formal logic as the “law of thought.” This point brings us full circle to where we began, with the assertion that epistemological pluralism is a necessary condition for a more inclusive computer culture.\n\nThe project of epistemological pluralism erupted into public consciousness during the “Science Wars” of 1990s. After that, it had stayed rather quiet."
  },
  {
    "objectID": "blog/posts/perceptron-controversy/index.html#the-perceptron-controversy",
    "href": "blog/posts/perceptron-controversy/index.html#the-perceptron-controversy",
    "title": "The Perceptron Controversy",
    "section": "The perceptron controversy",
    "text": "The perceptron controversy\n\nConnectionism, 1945–1970\nIn the early days, there were several centers of connectionist research, clustered around Frank Rosenblatt, Bernard Widrow, and the Stanford Research Institute (SRI). Out of those centers of research, Minsky and Papert targeted mostly Rosenblatt’s research.\nFrank Rosenblatt’s research had three modes: mathematical theory, experiments on bespoke machines, such as the Mark I Perceptron and the Tobermory, and experiments on serial digital computers, usually IBM machines. He was strongly inclined to building two-layered perceptron machines where the first layer was fixed 0-1 weights, and only the second layer contained real-valued weights learned by the perceptron learning rule. This is precisely the abstract model of the perceptron machine used by Minsky and Papert.\nAfter 4 years of research, he published a summary of his work in (Rosenblatt 1962). In the book, he noted that there were many problems that the perceptron machines could not learn well. As summarized in (Olazaran 1991, 116–21),\n\n… two stimuli (presented one after another) had to occupy nearly the same area of the retina in order to be classified as similar. … The lack of an adequate preprocessing system meant that a set of association units had to be dedicated to the recognition of each possible object, and this created an excessively large layer of association units in the perceptron. … Other problems were excessive learning time, excessive dependence on external evaluation (supervision), and lack of ability to separate essential parts in a complex environment. Rosenblatt (1962, pp. 309-310) included the ‘figure-ground’ or ‘connectedness’ problem in this last point.\n\nA number of perceptrons analyzed in the preceding chapters have been analyzed in a purely formal way, yielding equations which are not readily translated into numbers. This is particularly true in the case of the four-layer and cross-coupled systems, where the generality of the equations is reflected in the obscurity of their implications. … The previous questions [from the first to the twelfth] are all in the nature of ‘mopping-up’ operations in areas where some degree of performance is known to be possible . . . [However,] the problems of figure-ground separation (or recognition of unity) and topological relation recognition represent new territory, against which few inroads have been made.” (Rosenblatt, 1962a, pp. 580-581)\n\n\nAlmost every one of these problems were specifically targeted by Perceptrons. For example, the difficulty of testing for “connectedness” was a centerpiece of the entire book, the difficulty of recognizing symmetry was studied by “stratification” and shown to have exponentially growing coefficients (chapter 7), the requirement for “had to occupy nearly the same area of the retina” was targeted by studies on the limitations of “diameter-limited perceptrons” (chapter 8), the “figure-ground problem” was targeted by showing “recognition-in-context” has infinite order (Section 6.6), the “generality of the equations is reflected in the obscurity of their implications” was targeted by comments such as “if there is no restriction except for the absence of loops, the monster of vacuous generality once more raises its head” (Section 13.2),\nBernard Widrow worked mostly in collaboration with Marcian Hoff. Their work is detailed in my essay The Backstory of Backpropagation. In short, they first developed a least-mean-square gradient descent method to train a single perceptron, then proceeded to two-layered perceptrons and predictably failed to develop backpropagation, as the activation function is not differentiable. Thereafter, Widrow gave up neural networks until learning of backpropagation in the 1980s.\n\nWidrow and his students developed uses for the Adaline and Madaline. Early applications included, among others, speech and pattern recognition, weather forecasting, and adaptive controls. Work then switched to adaptive filtering and adaptive signal processing after attempts to develop learning rules for networks with multiple adaptive layers were unsuccessful. … After 20 years of research in adaptive signal processing, the work in Widrow’s laboratory has once again returned to neural networks. (Widrow and Lehr 1990)\n\n\nAt the time that Hoff left, about 1965 or 1966, we had already had lots of troubles with neural nets. My enthusiasm had dropped. But we were beginning to have successful adaptive filters, in other words, finding good applications. … So we stopped, basically stopped on neural nets, and began on adaptive antennas very strongly.” (Widrow, interview) (Olazaran 1991, 129–30)\n\nSRI had a strong AI program, which contained luminaries like Nils Nilsson, Charles Rosen, Duda, and Hart. At first they worked on a system called MINOS, with several iterations. MINOS II in 1962 had 3 layers, but only one was trainable, presumably because they also had no better training method than the perceptron learning rule. Since they used 0-1 activation functions like everyone else, they were frustrated by the same problem of not doing backpropagation, so they switched to symbolic AI techniques around 1965.\nIn 1973, Duda and Hart published the famous “Duda and Hart” book on pattern classification (Duda and Hart 1973). The book contained two halves. The first half was statistical: Bayes, nearest neighbors, perceptron, clustering, etc. The second half was on “scene analysis”, of the kind that Minsky and Papert promoted.6 Shakey the robot, built between 1966 and 1972, was a tour de force of scene analysis, and it could traverse a room in as short as an hour, avoiding geometric obstacles along the way. Its program was written in LISP, the staple programming language for symbolic AI.6 It is instructive to compare the first edition with the second, published in 2001 (Duda, Hart, and Stork 2001). It had become almost completely statistical, with new chapters on neural networks, Boltzmann machines, decision trees, and so on, and almost everything about scene analysis was deleted.\nIt says something about the obsolescence of scene analysis even in 2001, when Duda and Hart deleted half of their most famous book just to avoid talking about it. In fact, the only mention of “scene analysis” is a condemnation:\n\nSome of the earliest work on three-dimensional object recognition relied on complex grammars which described the relationships of corners and edges, in block structures such arches and towers. It was found that such systems were very brittle; they failed whenever there were errors in feature extraction, due to occlusion and even minor misspecifications of the model. For the most part, then, grammatical methods have been abandoned for object recognition and scene analysis. (Duda, Hart, and Stork 2001, sec. 8.8)\n\n\n\nI got very interested for a while in the problem of training more than one layer of weights, and was not able to make very much progress on that problem. … When we stopped the neural net studies at SRI, research money was running out, and we began looking for new ideas. (Nilsson, interview)\nAbout 1965 or 1966 we decided that we were more interested in the other artificial intelligence techniques. … Our group never solved the problem of training more than one layer of weights in an automatic fashion. We never solved that problem. That was most critical. Everybody was aware of that problem. (Rosen, interview) (Olazaran 1991, 131–33)\n\n\n\nThe perceptron controversy, 1960s\n\nIn the middle nineteen-sixties, Papert and Minsky set out to kill the Perceptron, or, at least, to establish its limitations – a task that Minsky felt was a sort of social service they could perform for the artificial-intelligence community. (Bernstein 1981)\n\nWhile the book was published only in 1969, at almost the end of the perceptron controversy, the influence of Minsky and Papert were felt years earlier, as they attended conferences and disseminated their ideas via talks and preprints, sometimes fighting on stage. Both sides had their motivations and the conflict was real.\n\nIn order to show the extent of the perceptron controversy, it is interesting to repeat some of the rhetorical expressions that were used in it: ‘many remember as great spectator sport the quarrels Minsky and Rosenblatt had;’ ‘Rosenblatt irritated a lot of people;’ ‘Rosenblatt was given to steady and extravagant statements about the performance of his machine;’ ‘Rosenblatt was a press agent’s dream, a real medicine man;’ ‘to hear Rosenblatt tell it, his machine was capable of fantastic things;’ ‘they disparaged everything Rosenblatt did, and most of what ONR did in supporting him;’ ‘a pack of happy bloodhounds;’ ‘Minsky knocked the hell out of our perceptron business;’ ‘Minsky and his crew thought that Rosenblatt’s work was a waste of time, and Minsky certainly thought that our work at SRI was a waste of time;’ ‘Minsky and Papert set out to kill the perceptron, it was a sort of social service they could perform for the Al community;’ ‘there was some hostility;’ ‘we became involved with a somewhat therapeutic compulsion;’ ‘a misconception that would threaten to haunt artificial intelligence;’ ‘the mystique surrounding such machines.’ These rhetorical expressions show the extent (the heat) of the perceptron controversy beyond doubt. (Olazaran 1991, 112)\n\nCharles Rosen of SRI recalls:\n\nMinsky and his crew thought that Frank Rosenblatt’s work was a waste of time, and they certainly thought that our work at SRI was a waste of time. Minsky really didn’t believe in perceptrons, he didn’t think it was the way to go. I know he knocked the hell out of our perceptron business. (Olazaran 1993, 622)\n\nWhen Perceptrons was finally published in 1969, the connectionist camp had already become mostly empty. The SRI group had switched to symbolic AI projects; Widrow’s group had switched to adapting single perceptrons to adaptive filtering; Frank Rosenblatt was still doggedly continuing research, but funds had been dwindling throughout the 1960s, and he would soon die in 1971.77 The 1972 reprinting of Perceptrons had a handwritten note “In memory of Frank Rosenblatt”. This was not an ironic dedication, as Minsky and Rosenblatt were personally friendly, although their research paradigms had been fighting for dominance.\nDuring the last days of Rosenblatt, he worked on a massive expansion of the Mark I Perceptron, named Tobermory. It would have been a multimodal neural network, capable of seeing and hearing in real time. From our lack of information on Tobermory, we can infer that it was a failure.\n\nThe four-layer Tobermory perceptron, designed and built at Cornell University between 1961 and 1967, had 45 S units, 1600 A1 units, 1000 A2 units, and 12 R units. Intended for speech recognition, the input section consisted of 45 band-pass filters attached to 80 difference detectors, with the output of each detector sampled at 20 time intervals. Its 12000 weights consisted of toroidal cores capable of storing over 100 different amplitudes. Each A2 unit could be connected to any of 20 A1 units by means of a wall-sized plugboard. As has happened with so many other projects in the last three decades, by the time Tobermory was completed, the technology of commercial Von Neumann computers had advanced sufficiently to outperform the special-purpose parallel hardware. (Nagy 1991)\n\n(Olazaran 1991) collected evidence that the publication of Perceptrons was not the cause, but a “marker event”, for the closure of the perceptron controversy, and the dominance of the symbolic AI school. The book was not the neural network killer, but its epitaph.\n\n\nConnectionist retrospectives, 1990s\nAfter the rise of connectionism in the 1980s, Anderson and Rosenfeld conducted interviews with the leading connectionists over the years of 1990s, connected into (Rosenfeld and Anderson 2000). The perceptron controversy comes up several times. Reading the interviews gives one a distinct feeling of Rashomon. The same events become recounted in multiple perspectives. I will excerpt some of the most important ones for the essay.\nJack D. Cowan gave an “eyewitness account” of Minsky and Papert’s work in fighting the controversy, before the book was published in 1969.\n\nER: I’m curious about one thing. You said that Minsky and Papert first presented their notions about exclusive-OR in the Perceptron work [in a 1965 conference].\nJC: Well, they first presented their notions about the limitations of perceptrons and what they could and couldn’t do.\nER: They hadn’t gotten to exclusive-OR yet?\nJC: They had, but that wasn’t a central issue for them. The essential issue was, suppose you had diameter-limited receptive fields in a perceptron, what could it compute?\nER: How was that received at that first conference?\nJC: Both of them were quite persuasive speakers, and it was well received. What came across was the fact that you had to put some structure into the perceptron to get it to do anything, but there weren’t a lot of things it could do. The reason was that it didn’t have hidden units. It was clear that without hidden units, nothing important could be done, and they claimed that the problem of programming the hidden units was not solvable. They discouraged a lot of research and that was wrong. … Everywhere there were people working on perceptrons, but they weren’t working hard on them. Then along came Minsky and Papert’s preprints that they sent out long before they published their book. There were preprints circulating in which they demolished Rosenblatt’s claims for the early perceptrons. In those days, things really did damp down. There’s no question that after ’62 there was a quiet period in the field.\nER: Robert Hecht-Nielsen has told me stories that long before Minsky and Papert ever committed anything to a paper that they delivered at a conference or published anywhere, they were going down to ARPA and saying, “You know, this is the wrong way to go. It shouldn’t be a biological model; it should be a logical model.”\nJC: I think that’s probably right. In those days they were really quite hostile to neural networks. I can remember having a discussion with Seymour … in the ’60s. We were talking about visual illusions. He felt that they were all higher-level effects that had nothing to do with neural networks as such. They needed a different, a top-down approach to understand. By then he had become a real, a true opponent of neural networks. I think Marvin had the same feelings as well. To some extent, David Marr had those feelings too. After he got to the AI lab, I think he got converted to that way of thinking. Then Tommy Poggio essentially persuaded him otherwise.\n\nTeuvo Kohonen seemed also angry at the Chomskyan school, for reasons that I might write about later. \n\nI was one of the people suffering from Minsky and Papert’s book [Perceptrons] because it went roughly this way: you start telling somebody about your work, and this visitor or whoever you talk to says, “Don’t you know that this area is dead?” It is something like what we experienced in the pattern recognition society when everything started to be structural and grammatical and semantic and so on. If somebody said, “I’m doing research on the statistical pattern recognition,” then came this remark, “Hey, don’t you know that is a dead idea already?”\n\nMichael A. Arbib thought the book did not cause the neural network winter, but rather caused funding change it.\n\nMinsky and Papert basically said that if you limit your networks to one layer in depth, then, unless you have very complicated individual neurons, you can’t do very much. This is not too surprising. … Many people see the book as what killed neural nets, but I really don’t think that’s true. I think that the funding priorities, the fashions in computer science departments, had shifted the emphasis away from neural nets to the more symbolic methods of AI by the time the book came out. I think it was more that a younger generation of computer scientists who didn’t know the earlier work may have used the book as justification for sticking with “straight AI” and ignoring neural nets.\n\nBernard Widrow concurred.\n\nI looked at that book, and I saw that they’d done some serious work here, and there was some good mathematics in this book, but I said, “My God, what a hatchet job.” I was so relieved that they called this thing the percept ron rather than the Adaline because actually what they were mostly talking about was the Adaline, not the percept ron. I felt that they had sufficiently narrowly defined what the percept ron was, that they were able to prove that it could do practically nothing. Long, long, long before that book, I was already success fully adapting Madaline [Madaline = many Adalines], which is a whole bunch of neural elements. All this worry and agony over the limitations of linear separability, which is the main theme of the book, was long overcome.\nWe had already stopped working on neural nets. As far as I knew, there wasn’t anybody working on neural nets when that book came out. I couldn’t understand what the point of it was, why the hell they did it. But I know how long it takes to write a book. I figured that they must have gotten inspired to write that book really early on to squelch the field, to do what they could to stick pins in the balloon. But by the time the book came out, the field was already gone. There was just about nobody doing it.\n\nJames A. Anderson pointed out that during the “winter”, neural networks survived outside of AI.\n\nThis was during the period sometimes called the neural network dark ages, after the Minsky and Papert book on perceptrons had dried up most of the funding for neural networks in engineering and computer science. Neural networks continued to be developed by psychologists, however, because they turned out to be effective models in psychology … What happened during the dark ages was that the ideas had moved away from the highly visible areas of big science and technology into areas of science that did not appear in the newspapers.\n\nDavid Rumelhart had nice things to say about Minsky, with no trace of bitterness. It is understandable as he only started working in neural networks years after the controversy died down.\n\nI always had one course that was like a free course in which I would choose a book of the year and teach out of that. In 1969, I think it was, or maybe ’70, I chose Perceptrons by Minsky and Papert as the book of the year. We then carefully went through it and read it in a group. … This was my most in-depth experience with things related to neural networks, or what were later called neural networks. I was quite interested in Minsky in those days because he also had another book which was called, I think, Semantic Information Processing. That book was a collection, including an article by Ross Quillian. It was a collection of dissertations from his graduate students. In a way, it was Minsky who led me to read about the perceptron more than anybody else.\n\nAs for Robert Hecht-Nielsen, we already saw how he believed Minsky to be “Darth Vader” and perhaps “the Devil”. Unsurprisingly, he was the most bitter, and placed the blame for the neural network winter squarely on Perceptrons.\n\nBy the mid-1970s, Minsky and his colleagues (notably Seymour Papert) began to take actions designed to root out neural networks and ensure large and, in their view, richly deserved funding for AI research by getting the money currently being “wasted” on neural networks, and more to boot, redirected. They did two things. First, Minsky and Papert began work on a manuscript designed to discredit neural network research. Second, they attended neural network and “bionics” conferences and presented their ever-growing body of mathematical results being compiled in their manuscript to what they later referred to as “the doleful responses” of members of their audiences.\nAt the heart of this effort was Minsky and Papert’s growing manuscript, which they privately circulated for comments. The technical approach they took in the manuscript was based on a mathematical theorem discovered and proven some years earlier—ironically, by a strong supporter of Rosenblatt—that the perceptron was incapable of ever implementing the “exclusive-OR” [X-OR] logic function. What Minsky and Papert and their colleagues did was elaborate and bulk up this idea to book length by devising many variants of this theorem. Some, such as a theorem showing that single-layer perceptrons, of many varied types, cannot compute topological connectedness, are quite clever. To this technical fabric, they wove in what amounted to a personal attack on Rosenblatt. This was the early form of their crusade manifesto.\nLater, on the strong and wise advice of colleagues, they expunged the vitriol. They didn’t quite get it all, as a careful reading will show. They did a complete flip-flop, dedicating the book to Rosenblatt! As their colleagues sensed it would, this apparently “objective” evaluation of perceptrons had a much more powerful impact than the original manuscript with its unseemly personal attack would have. Of course, in reality, the whole thing was intended, from the outset, as a book-length damnation of Rosenblatt’s work and many of its variants in particular, and, by implication, all other neural network research in general.\nMinsky and Papert’s book, Perceptrons, worked. The field of neural networks was discredited and destroyed. The book and the associated conference presentations created a new conventional wisdom at DARPA and almost all other research sponsorship organizations that some MIT professors have proven mathematically that neural networks cannot ever do anything interesting. The chilling effect of this episode on neural network research lasted almost twenty years."
  },
  {
    "objectID": "blog/posts/perceptron-controversy/index.html#the-message-of-perceptrons",
    "href": "blog/posts/perceptron-controversy/index.html#the-message-of-perceptrons",
    "title": "The Perceptron Controversy",
    "section": "The message of Perceptrons",
    "text": "The message of Perceptrons\nMinsky described how he and Papert came to be motivated to write the book:\n\nBoth of the present authors (first independently and later together) became involved with a somewhat therapeutic compulsion: to dispel what we feared to be the first shadows of a “holistic” or “Gestalt” misconception that would threaten to haunt the fields of engineering and artificial intelligence as it had earlier haunted biology and psychology. For this, and for a variety of more practical and theoretical goals, we set out to find something about the range and limitations of perceptrons. (M. Minsky and Papert 1988, 20).\n\nThe book is one of those things that have passed into the stuff of legend: often quoted, rarely read. However, for its historical importance, I have actually read the book.\nThe content of the book can be cleanly separated into two parts. Most of the book is a mathematical monograph on what functions can be implemented by a single perceptron with fixed featurizers, and the rest is a commentary on the wider implications of the mathematical monograph. The impact of the work is exactly reversed: most of the impact is done by the commentary derived from the results, and essentially no impact is done by the mathematical results themselves.\nDespite this lopsidedness, the mathematical work is solid, and the perceptron controversy turns critically on the flexible interpretation of the solid mathematical work. Therefore, I have described the mathematical content in a separate essay, Reading Perceptrons, which I refer to occasionally to gloss their interpretation.\n\nMinsky and Papert struck back\nIn the 1980s, neural networks rose again to prominence under the name of “connectionism”, prompting an eventual response from Minsky and Papert. The Perceptrons book was reissued in 1988, with new chapters dedicated to rejecting connectionism. They took the 1986 two-volume work of Parallel Distributed Processing (PDP), especially (Rumelhart, Hinton, and Williams 1985) 8, as the representative of connectionism, and made specific objections to them.8 This paper was reprinted in (Rumelhart and McClelland 1986, vol. 1, chapter 8), in which Minsky and Papert read it. This paper is often cited for the backpropagation algorithm, which I have discussed in The Backstory of Backpropagation.\nIn the prologue, they staked their central point as follows: Connectionism is a mistake caused by a new generation of researchers ignorant of history; while the theorems of the Perceptrons book apply to only a single perceptron, the lessons apply to all neural networks. To back up the claim, they made specific technical, historical, and philosophical objections, all with the central goal of showing that homogeneous neural networks cannot scale.\n\n… when we found that little of significance had changed since 1969, when the book was first published, we concluded that it would be more useful to keep the original text (with its corrections of 1972) and add an epilogue, so that the book could still be read in its original form. One reason why progress has been so slow in this field is that researchers unfamiliar with its history have continued to make many of the same mistakes that others have made before them.\n… there has been little clear-cut change in the conceptual basis of the field. The issues that give rise to excitement today seem much the same as those that were responsible for previous rounds of excitement. … many contemporary experimenters assume that, because the perceptron networks discussed in this book are not exactly the same as those in use today, these theorems no longer apply. Yet, as we will show in our epilogue, most of the lessons of the theorems still apply.\n\nIn an earlier interview, Minsky reiterated his belief that the proper place of perceptrons is solving tiny problems with tiny perceptron networks.\n\n… for certain purposes the Perceptron was actually very good. I realized that to make one all you needed in principle was a couple of molecules and a membrane. So after being irritated with Rosenblatt for overclaiming, and diverting all those people along a false path, I started to realize that for what you get out of it – the kind of recognition it can do – it is such a simple machine that it would be astonishing if nature did not make use of it somewhere. It may be that one of the best things a neuron can have is a tiny Perceptron, since you get so much from it for so little. You can’t get one big Perceptron to do very much, but for some things it remains one of the most elegant and simple learning devices I know of. (Bernstein 1981)\n\nThey also requested all AI researchers to convert to the Society of Mind hypothesis, or else face the charge of being un-thoughtful or drawing lines where none exists. It seems to me that Minsky wrote most of the prologue and epilogue, because in Papert’s solo paper, he went considerably further with sociological interpretation.\n\nThis broad division makes no sense to us, because these attributes are largely independent of one another; for example, the very same system could combine symbolic, analogical, serial, continuous, and localized aspects. Nor do many of those pairs imply clear opposites; at best they merely indicate some possible extremes among some wider range of possibilities. And although many good theories begin by making distinctions, we feel that in subjects as broad as these there is less to be gained from sharpening bound­ aries than from seeking useful intermediates.\n… Are there inherent incompatibilities between those connectionist and symbolist views? The answer to that depends on the extent to which one regards each separate connectionist scheme as a self-standing system. If one were to ask whether any particular, homogeneous network could serve as a model for a brain, the answer (we claim) would be, clearly. No. But if we consider each such network as a possible model for a part of a brain, then those two overviews are complementary. This is why we see no reason to choose sides.\n… Most researchers tried to bypass [the technical objections], either by ignoring them or by using brute force or by trying to discover powerful and generally applicable methods. Few researchers tried to use them as guides to thoughtful research. We do not believe that any completely general solution to them can exist …\n\nWe now proceed to the epilogue and its arguments.\n\n1980s connectionism is not that different\nThey speculated on the reason for the revival of neural networks. Was it because of the development of backpropagation, multilayer networks, and faster computers? Emphatically not. In fact, 1980s connectionists were not different from the 1960s connectionists. It is only the ignorance of history that made them think otherwise. In both periods, connectionism was focused on making small-scale experiments and then extrapolating to the largest scale, without mathematical theorems to justify the extrapolation. In both periods, connectionism failed (or would fail) to scale beyond toy problems.\n\nmost of the theorems in this book are explicitly about machines with a single layer of adjustable connection weights. But this does not imply (as many modern connectionists assume) that our conclusions don’t apply to multilayered machines. To be sure, those proofs no longer apply unchanged, because their antecedent conditions have changed. But the phenomena they describe will often still persist. One must examine them, case by case.\n\n\n… the situation in the early 1960s: Many people were impressed by the fact that initially unstructured networks composed of very simple devices could be made to perform many interesting tasks – by processes that could be seen as remarkably like some forms of learning. A different fact seemed to have impressed only a few people: While those networks did well on certain tasks and failed on certain other tasks, there was no theory to explain what made the difference – particularly when they seemed to work well on small (“toy“) problems but broke down with larger problems of the same kind. Our goal was to develop analytic tools to give us better ideas about what made the difference.\n\n\n\nThere is no silver bullet in machine learning\nThere are no general algorithms and no general problems. There are only particular algorithm-problem pairs. An algorithm-problem pair can be a good fit, or a bad fit. The parity problem is a bad fit with a neural network trained by backpropagation, but it is a good fit with a Turing machine.\nThere is no general and effective algorithm. Either the algorithm is so general that it is as useless as “just try every algorithm” akin to Ross Ashby’s homeostat, or it is useful but not general. This general lesson is similar to Gödel’s speedup theorem, Blum’s speedup theorem, the no free lunch theorem, etc.\n\nClearly, the procedure can make but a finite number of errors before it hits upon a solution. It would be hard to justify the term “learning” for a machine that so relentlessly ignores its experience. The content of the perceptron convergence theorem must be that it yields a better learning procedure than this simple homeostat. Yet the problem of relative speeds of learning of perceptrons and other devices has been almost entirely neglected. (M. Minsky and Papert 1988, sec. 11.7)\n\nArthur Samuel’s checker learning algorithm encountered two fundamental problems: credit assignment and inventing novel features. Those two problems are not just for the checker AI, but for all AI. There are no general and effective solutions to credit assignment, and there are no general and effective solutions to inventing novel features. There could be universal but impractical solutions, such as backpropagation on homogeneous neural networks, Solomonoff induction, trying every Turing machine, etc. There could be practical but not universal solutions, which is precisely what populates the Society of Mind in human brains.\n\nRosenblatt’s credit-assignment method turned out to be as effective as any such method could be. When the answer is obtained, in effect, by adding up the contributions of many processes that have no significant interactions among themselves, then the best one can do is reward them in proportion to how much each of them contributed.\n\n\nSeveral kinds of evidence impel us toward this view. One is the great variety of different and specific functions embodied in the brain’s biology. Another is the similarly great variety of phenom­ ena in the psychology of intelligence. And from a much more abstract viewpoint, we cannot help but be impressed with the practical limitations of each “general” scheme that has been proposed – and with the theoretical opacity of questions about how they behave when we try to scale their applications past the toy problems for which they were first conceived.\n\n\n\nThere is no efficient way to train homogeneous, high-order networks\nThey ask the reader to think back to the lesson of the parity predicate from Chapter 10: Even though it is learnable by a two-layered perceptron network, it would involve weights exponential in the input pixel count, and therefore take a very long time to learn. They expect this to generalize, so that any problem that require some perceptron in the network to have receptive field of size \\(\\Omega(|R|^\\alpha)\\), necessarily require that perceptron to have coefficients growing like \\(2^{\\Omega(|R|^\\alpha)}\\), and therefore taking \\(2^{\\Omega(|R|^\\alpha)}\\) steps to train.\n\nWe could extend them either by scaling up small connectionist models or by combining small-scale networks into some larger organization. In the first case, we would expect to encounter theoretical obstacles to maintaining GD’s effectiveness on larger, deeper nets. And despite the reputed efficacy of other alleged remedies for the deficiencies of hill-climbing, such as “annealing,” we stay with our research conjecture that no such procedures will work very well on large-scale nets, except in the case of problems that turn out to be of low order in some appropriate sense.\nThe second alternative is to employ a variety of smaller networks rather than try to scale up a single one. And if we choose (as we do) to move in that direction, then our focus of concern as theoretical psychologists must turn toward the organizing of small nets into effective large systems.\n\n\n\nThere is no effective use for homogeneous, high-order networks\nFully connected networks, or indeed any neural network without a strong constraint on “order” or “receptive field”, would hopelessly confuse itself with its own echoes as soon as it scales up, unless it has sufficient “insulation”, meaning almost-zero connection weights, such that it effectively splits into a large number of small subnets. That is, a large fully connected network is useless anyway, except if it already decomposes into many tiny networks arranged in a Society of Mind.\n\nCertain parallel computations are by their nature synergistic and cooperative: each part makes the others easier. But the And/Or of theorem 4.0 shows that under other circumstances, attempting to make the same network perform two simple tasks at the same time leads to a task that has a far greater order of difficulty. In those sorts of circumstances, there will be a clear advantage to having mechanisms, not to connect things together, but to keep such tasks apart. How can this be done in a connectionist net?\n\n\n… a brain is not a single, uniformly structured network. Instead, each brain contains hundreds of different types of machines, interconnected in specific ways which predestine that brain to become a large, diverse society of partially specialized agencies.\n\n\n\nGradient descent cannot escape local minima\nGradient descent, backpropagation, and all other hill-climbing algorithms are all vulnerable to getting trapped in local optima, and therefore they cannot work – except in problem-architecture pairs where the loss landscape of this particular problem, for this particular architecture, using this particular loss function, is a single bump whose width is shorter than this particular learning rate.\nGradient descent is just a form of hill-climbing, when the hill is differentiable. The perceptron learning algorithm can be interpreted as a hill-climbing algorithm too, as it makes localized decision to make one step in this direction or that, one error-signal at a time (Section 11.7). Therefore, the generic ineffectiveness of perceptron learning suggests that gradient descent is also generically ineffective and cannot scale. It does not even have a convergence theorem, so in that sense it’s worse than perceptron learning algorithm.99 This claim is astonishing, now that we see how powerful backpropagation works, and how the perceptron learning rule had crippled neural network research for 30 years. We can understand their sentiments by remembering that they, like most of the academic community in computer science, favored the certainty of mathematical theorems over mere empirical success. Leo Breiman observed that academic statistics had been hamstrung by the same grasp over mathematical certainty, and thus over 95% of its publications were useless. (Breiman 1995)\n\nWe were very pleased to discover (see section 11.6) that PC could be represented as hill-climbing; however, that very fact led us to wonder whether such procedures could dependably be generalized, even to the limited class of multilayer machines that we named Gamba perceptrons. The situation seems not to have changed much – we have seen no contemporary connectionist publication that casts much new theoretical light on the situation. Then why has GD become so popular in recent years? … we fear that its reputation also stems from unfamiliarity with the manner in which hill-climbing methods deteriorate when confronted with larger-scale problems. … Indeed, GD can fail to find a solution when one exists, so in that narrow sense it could be considered less powerful than PC.\n\n\n\nStochastic gradient descent cannot see through the noise\n\nSo far as we could tell, every experiment described in (Rumelhart, Hinton, and Williams 1985) involved making a complete cycle through all possible input situations before making any change in weights. Whenever this is feasible, it completely eliminates sampling noise—and then even the most minute correlations can become reliably detectable, be­ cause the variance is zero. But no person or animal ever faces situations that are so simple and arranged in so orderly a manner as to provide such cycles of teaching examples. Moving from small to large problems will often demand this transition from exhaustive to statistical sampling, and we suspect that in many realistic situations the resulting sampling noise would mask the signal completely. We suspect that many who read the connectionist literature are not aware of this phenomenon, which dims some of the prospects of successfully applying certain learning procedures to large-scale problems.\n\n\n\nDifferentiable activation is just a hack\nUsing differentiable activations for neural networks is an artificial trick of questionable future. It makes the learned boolean functions imprecise, and only appears to redeem itself by allowing backpropagation. However, backpropagation is a dead-end because it will not scale. It is better to look for a method that can directly train multilayer perceptron networks with discrete activation functions.\n\nThe trick is to replace the threshold function for each unit with a monotonic and differentiable function … However, we suspect that this smoothing trick may entail a large (and avoidable) cost when the predicate to be learned is actually a composition of linear threshold functions. There ought to be a more efficient alternative based on how much each weight must be changed, for each stimulus, to make the local input sum cross the threshold.\n\n\nWe conjecture that learning XOR for larger numbers of variables will become increasingly intractable as we increase the numbers of input variables, because by its nature the underlying parity function is absolutely uncorrelated with any function of fewer variables. Therefore, there can exist no useful correlations among the outputs of the lower-order units involved in computing it, and that leads us to suspect that there is little to gain from following whatever paths are indicated by the artificial introduction of smoothing functions that cause partial derivatives to exist.\n\n\n\nConnectionists have no theory, so they should not extrapolate from experiments\n\nIn the past few years, many experiments have demonstrated that various new types of learning machines, composed of multiple layers of perceptron-like elements, can be made to solve many kinds of small-scale prob­ lems. Some of those experimenters believe that these performances can be economically extended to larger problems without encountering the limitations we have shown to apply to single­ layer perceptrons. Shortly, we shall take a closer look at some of those results and see that much of what we learned about simple perceptrons will still remain quite pertinent.\n\nWithout a mathematical theory, experimental data cannot be extrapolated. If neural networks happen to work well on a problem, it merely shows that the problem is a good fit for this particular architecture trained in this particular way at this particular scale, not anything more general than that.\n\nAs the field of connectionism becomes more mature, the quest for a general solution to all learning problems will evolve into an understanding of which types of learning processes are likely to work on which classes of problems. And this means that, past a certain point, we won’t be able to get by with vacuous generalities about hill-climbing. We will really need to know a great deal more about the nature of those surfaces for each specific realm of problems that we want to solve.\n\n\n… the learning procedure required 1,208 cycles through each of the 64 possible examples – a total of 77,312 trials (enough to make us wonder if the time for this procedure to determine suitable coefficients increases exponentially with the size of the retina). PDP does not address this question. What happens when the retina has 100 elements? If such a network required on the order of \\(2^{200}\\) trials to learn. most observers would lose interest.\n\n\n\nConnectionist experiments can be extrapolated to show that they do not scale\nThough lacking a theory of their own on the operation of multilayer perceptrons, Minsky and Papert then proceeded to interpret the connectionist experiment data as showing that neural networks will fail to scale.1010 Without a mathematical theory of what neural networks can do, extrapolating from their behavior at small scales to the large scale is impossible and only reflect the bias behind those who make the extrapolation.\nConnectionists demonstrated that two-layered perceptrons, where both layers were trainable, bypassed the limits described in Perceptrons. For example, (Rumelhart, Hinton, and Williams 1985) showed that several problems unsolvable by a single perceptron – XOR, parity, symmetry, etc – were solved by a two-layered neural network.\n\n\n\nPage 253, Figure 2. Redrawn from (Rumelhart, Hinton, and Williams 1985)\n\n\nWhile the connectionist authors interpreted the result as encouraging large-scale experiments, Minsky and Papert interpreted it as showing that the experiments will not scale, because the coefficients appear to grow exponentially, in the same way they proved in Chapter 7.\n\nIn PDP it is recognized that the lower-level coefficients appear to be growing exponentially, yet no alarm is expressed about this. In fact, anyone who reads section 7.3 should recognize such a network as employing precisely the type of computational structure that we called stratification.\nalthough certain problems can easily by solved by perceptrons on small scales, the computational costs become prohibitive when the problem is scaled up. The authors of PDP seem not to recognize that the coefficients of this symmetry machine confirm that thesis, and celebrate this performance on a toy problem as a success rather than asking whether it could become a profoundly “bad” form of behavior when scaled up to problems of larger size.\n\n\n\n\nPapert struck back\nWhile it appears Minsky was the main author for the new prologue and epilogue, Papert solo-authored (S. Papert 1988), an essay giving the controversy a uniquely Papert-styled spin. It is an extensive reframing of the perceptron controversy into a social and philosophical issue, with the prediction of ultimate victory for epistemological pluralism:\n\nThe field of artificial intelligence is currently divided into what seem to be several competing paradigms … for mechanisms with a universal application. I do not foresee the future in terms of an ultimate victory for any of the present contenders. What I do foresee is a change of frame, away from the search for universal mechanisms. I believe that we have much more to learn from studying the differences, rather than the sameness, of kinds of knowing.\n\nHe diagnosed the source of the philosophical error as a “category error”.\n\nThere is the same mistake on both sides: the category error of supposing that the existence of a common mechanism provides both an explanation and a unification of all systems, however complex, in which this mechanism might play a central role.\nArtificial intelligence, like any other scientific enterprise, had built a scientific culture… more than half of our book is devoted to “pro-perceptron” findings about some very surprising and hitherto unknown things that perceptrons can do. But in a culture set up for global judgment of mechanisms, being understood can be a fate as bad as death. A real understanding of what a mechanism can do carries too much implication about what it cannot do… The same trait of universalism leads the new generation of connectionists to assess their own microlevel experiments, such as Exor, as a projective screen for looking at the largest macroissues in the philosophy of mind. The category error analogous to seeking explanations of the tiger’s stripes in the structure of DNA is not an isolated error. It is solidly rooted in AI’s culture.\n\nHe then discussed the compute-first interpretation, “The Bitter Lesson” for the 1980s, before rejecting it.\n\nIn the olden days of Minsky and Papert, neural networking models were hopelessly limited by the puniness of the computers available at the time and by the lack of ideas about how to make any but the simplest networks learn. Now things have changed. Powerful, massively parallel computers can implement very large nets, and new learning algorithms can make them learn. …\nI don’t believe it. The influential recent demonstrations of new networks all run on small computers and could have been done in 1970 with ease. Exor is a “toy problem” run for study and demonstration, but the examples discussed in the literature are still very small. Indeed, Minsky and I, in a more technical discussion of this history (added as a new prologue and epilogue to a reissue of Perceptrons), suggest that the entire structure of recent connectionist theories might be built on quicksand: it is all based on toy-sized problems with no theoretical analysis to show that performance will be maintained when the models are scaled up to realistic size. The connectionist authors fail to read our work as a warning that networks, like “brute force” programs based on search procedures, scale very badly.\n\nConsider Exor, a certain neural network he picked out of the pages of PDP, which learned to perform the infamous XOR task, but only after 2232 examples. Was it slow, or fast? A proper judgment requires a mathematical understanding of the algorithm-problem fit. By extension, to properly judge whether neural networks were good for any specific problem, one must first mathematically understand the fit. He insinuated that the connectionists who were confident that their neural networks were more than a sterile extension of the perceptron did not do their math, unlike he and Minsky.\n\ninstead of asking whether nets are good, we asked what they are good for. The focus of enquiry shifted from generalities about kinds of machines to specifics about kinds of tasks. From this point of view, Exor raises such questions as: Which tasks would be learned faster and which would be learned even more slowly by this machine? Can we make a theory of tasks that will explain why 2,232 repetitions were needed in this particular act of learning?\n… Minsky and I both knew perceptrons extremely well. We had worked on them for many years before our joint project of under standing their limits was conceived… I was left with a deep respect for the extraordinary difficulty of being sure of what a computational system can or cannot do. I wonder at people who seem so secure in their intuitive convictions, or their less-than-rigorous rhetorical arguments, about computers, neural nets, or human minds.\n\n\n\nInterjection\n\nI wonder at people who seem so secure in their intuitive convictions, or their less-than-rigorous rhetorical arguments, about computers, neural nets, or human minds:\n\nThere is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgment that the extension is sterile. Per­haps some powerful convergence theorem will be discovered, or some profound reason for the failure to produce an interesting “learning theorem” for the multilayered machine will be found. (M. Minsky and Papert 1988, 232)\n\n\nWhat, then, explains the rise of connectionism? Since Papert reframed the fall of perceptron socially, it only stands to reason he would reframe the rise of connectionism as the rise of a social myth caused by other social myths, not by the increase in computing power or new algorithms like backpropagation, convolutional networks, and such. For one, the computing powers used by the breakthrough connectionist models like NETtalk were already within reach even in the 1960s. For two, he and Minsky were firm in their convictions that any uniform architecture must scale very badly, and no amount of compute or algorithmic advance could be more than a sterile extension.\n\nMassively parallel supercomputers do play an important role in the connectionist revival. But I see it as a cultural rather than a technical role, another example of a sustaining myth. Connectionism does not use the new computers as physical machines; it derives strength from the “computer in the mind,” from its public’s largely nontechnical awareness of supercomputers. I see connectionism’s relationship to biology in similar terms. Although its models use biological metaphors, they do not depend on technical findings in biology any more than they do on modern supercomputers. … I also see a more subtle, but not less relevant, cultural resonance. This is a generalized turn away from the hard-edged rationalism of the time connectionism last went into eclipse and a resurgent attraction to more holistic ways of thinking."
  },
  {
    "objectID": "blog/posts/perceptron-controversy/index.html#rebuttal-to-minsky-and-papert",
    "href": "blog/posts/perceptron-controversy/index.html#rebuttal-to-minsky-and-papert",
    "title": "The Perceptron Controversy",
    "section": "Rebuttal to Minsky and Papert",
    "text": "Rebuttal to Minsky and Papert\n\nInterpreting the XOR problem\nWhen I first heard about the first neural network winter and the Perceptrons book, I was deeply confused by the story. The story went that “Perceptrons proved that the XOR problem is unsolvable by a single perceptron, a result that caused researchers to abandon neural networks”. How could it possibly cause researchers to abandon the field, unless it was news to them? But anyone could see that a single perceptron could only separate linearly separable points, and therefore the XOR problem is unsolvable by a single perceptron. When I first heard the story, I immediately saw why XOR was unsolvable by one perceptron, then took a few minutes to design a two-layered perceptron network that solved the XOR problem. I then noted that the NAND problem is solvable by a single perceptron, after which I immediately knew that perceptron networks are universal since the NAND gate is.\nIf a high school student could bypass the XOR problem in a few minutes, how could it have possibly been news to the researchers in 1969?\nWhen I started researching neural networks properly, the standard story about the XOR problem became more nonsensical the more I learned. The 1943 paper by McCulloch and Pitts (McCulloch and Pitts 1943) already said that their neural networks were equivalent in power to Turing machines. Marvin Minsky’s 1954 PhD thesis (M. L. Minsky 1954) develops an entire computer theory out of McCulloch–Pitts neural networks.\nOn the electric engineering side, perceptron networks were studied under another name of “linear threshold logic” by electric engineers since 1950s, who clearly would not have bothered if they could not even make an XOR gate out of it. In fact, in a standard reference from 1965, there are chapters on “Single-Threshold-Element Synthesis by Iteration” – learning a single perceptron by the perceptron learning algorithm – and “Network Synthesis” – which does not mean machine learning, but rather hand-designing perceptron networks.(Dertouzos 1965)\nWhat is going on?\nI believe the story got completely garbled during the teaching process. I am all for changing history for the sake of understanding – history is made for the winners, not the winners made for history – but the standard story about the XOR problem is nonsensical, as I have shown. So how did the story come about?\nI believe it is because the Perceptrons book contained a host of problems that their restricted form of perceptron machines could not do. The simplest one is the XOR problem. Teachers, who just wanted to spend two minutes on the first neural network winter, and move on, grabbed this XOR problem and pretended it is supposed to be the actual cause of it.1111 There is a Chinese legend about a mantis crawling on the wheel of a slowly moving train. It proudly said, “I am the prime mover of the train!”. When the caterpillar asked it to prove so, it jumped down and waved its arms in front of the train, which promptly crushed it.\nThis is my retelling of the Taoist story of 螳臂當車.\nThere is one thing left to explain: what is the significance of the XOR problem to the neural network researchers back in the days? It was clearly significant for something, as when the connectionists rose in the 1980s, one of the first things they did was to check that they could solve the XOR problem. Rumelhart read the Perceptrons book very carefully in 1970, and it inspired him to go into neural network research, entirely missing its intended message. After he developed backpropagation around 1982, he immediately tried training an MLP on the XOR problem:\n\nWhen I first did the XOR problem, it took a thousand iterations to solve it. If we thought that was the way it was going to go and that we were going to scale up to a hundred thousand input patterns, my God, we wouldn’t live long enough to see the results. But that’s not the way it’s gone. That problem turned out to be an anomaly. The scaling is about linear. We haven’t hit any exponential curves yet. (Rosenfeld and Anderson 2000)\n\nWhat is the significance of the XOR problem? In the context of the neural network research in the 1960s, the significance becomes clear. Nobody knew how to simultaneously adapt two or more layers well.\nBefore 1962, Rosenblatt had studied both theoretically and experimentally “four-layer perceptron with adaptive preterminal network”, meaning a perceptron network with three layers, the first layer being random and fixed, and the second and third layers being learned (Rosenblatt 1962, vol. 55, chap. 16). However, it had not a single derivative in it. The second layer was learned by the Hebb rule, and the third layer was by the perceptron learning rule.\nMeanwhile during the early 1960s, Widrow and Hoff trained a single perceptron with gradient descent, then proceeded to try every trick except gradient descent to train a two-layered perceptron network. They gave up and parted ways. Hoff went on to coinvent the microprocessor at Intel, while Widrow applied a single perceptron to adaptive filter design, revolutionizing electric engineering in the process. These and more of the ridiculous backstory can be read in The Backstory of Backpropagation.\nIn short, due to a variety of unfortunate developments, people spent about twenty years (1950–1970) failing to find an effective algorithm for training the pre-final layers of neural networks. They can train the final layer either by the perceptron learning rule of Rosenblatt, or the Widrow–Hoff rule of gradient descent on the squared error, but that is the extent of learning they could get the neural networks to do.\nConsider a two-layered neural network. The second layer is easy to learn. What should happen to the first layer? Rosenblatt’s solution was mainly just randomization, because he mistakenly believed that the retina was randomly wired to the visual cortex, and he believed in emulating nature. Rosenblatt was working with the standard knowledge of neuroscience in his time, as the first of the Hubel and Wiesel series of cat-vision paper only appeared in 1959. However, it seems Rosenblatt simply had a strong attachment to randomization, as (Rosenblatt 1962) cites (Hubel and Wiesel 1959) a few times, yet he kept randomizing the first layer. Rosenblatt had also experimented with Hebbian learning (Rosenblatt 1962, vol. 55, sec. 16.1), but since he did not use those extensively, I infer that it did not work well.\nWidrow’s solution was the MADALINE I rule, a complicated hack and a deadend. Without an effective method to train the first layer, those who worked on two-layered neural networks had only two choices, either randomize the first layer, or design it by hand. Both choices played right into the hands of Minsky and Papert.\nSeen from the viewpoint of the second layer, the first layer is the featurizer for the raw input. It is intuitively clear that if the raw input is featurized, then unless the features are adapted to the problem, the second layer would not be able to solve the problem.\nThe XOR problem requires two layers. Furthermore, if the first layer is not wired right, then the second layer would not be able to solve it either.\nPut yourself in the place of a 1960s connectionist. How do you solve the XOR problem by a perceptron network? Well, not a single perceptron, as it’s impossible. Not with three layers, because two layers are sufficient, and you have enough problem with two layers already. So, two layers.\nHow to train it? You know only how to fix the first layer and train the second. How do you fix the first layer? Do you randomize it? Unless you use many hidden perceptrons, this will fail with high probability. Do you design it by hand? But then, Minsky and Papert would interject, “You see, you cannot substitute thinking by tabula-rasa learning! You need some intelligent design to get it to work! You have to design it by hand, which means you must understand the problem itself, and the efficient method for representing the problem, because you cannot expect it to learn the problem unless it has built-in capacity for representing the problem, and you cannot expect it to learn the representation from a vacuous generality like the fully connected multilayer perceptron, unless you did not get the lesson from our book.”.\nNot to give up, you try one of the hacks like the MADALINE I learning rule, or the Hebbian learning rule, but they are extremely fiddly and unable to learn most of the time unless you tune them just right, and it seems to require a different turning for problems even slightly more complex than the XOR problem. Minsky and Papert interjects again, “You see, there is no universal learning algorithm! You need a bespoke learning algorithm for each problem!”.\nAnd so we stand at the impasse of 1960s. If only we had tried an activation function, any activation function, other than the dreaded 0-1 activation function…\n\n\nWhere did they go wrong?\nBrains are neural networks in hardware – in this regard, there is no controversy since early 1900s. Intelligence is what happens in the brain. This is the occasion for small controversies by the “embodiment cognition” or “externalism” school like James Gibson and Rodney Brooks, but nothing that has come to anything yet. Therefore, most people agree that intelligence is something that neural networks do, including those people who are otherwise dismissive of neural networks like Minsky and Papert.\nThe abstract of a key anti-connectionist paper (Fodor and Pylyshyn 1988) makes the point that the brain is symbolic at the “cognitive level”, and only beneath that level it is connectionist. Interpreted with sufficient charity, this hypothesis is unfalsifiable. Nobody respectable claim that the brain is not connectionist, and the operation of any hardware is symbolic if you throw in enough symbols to discretely represent the real numbers. However, at this level of charity, the hypothesis is also useless, therefore we must interpret less charitably.\nWhat did they really mean? They concretely rejected “Parallel Distributed Processing”, and claimed that trained neural networks work if and only if they implement approximations to symbolic programs, where each symbolic variable is represented locally by a small group of neurons (thus not “distributed”), and the variables are processed serially layer by layer through the network (thus not “parallel”). Further, the symbolic programs they approximate are not any kind of symbolic programs (otherwise we fall back to the trivial claim), but symbolic programs that people tend to write, things that on the small scale resemble subroutines and command line scripts, and on the large scale resemble operating systems and the Cyc project.\nAt this level, it is quantifiable and thus scientifically testable. However, scientific hypotheses become political disputes when large amounts of money or social justice is on the line. We can consider an alternative history with an alternative Minsky and Papert. In this history, they put this in the epilogue:\n\nOur mathematical results indicate that we need multilayer perceptrons as well as efficient methods for training them. Furthermore, simple estimates show that brain-level intelligence likely require computing power up to 10 orders of magnitude larger than currently available, suggesting the need for special hardware boards.\nWe also need to explore alternative architectures capable of correlating global information without using all-to-all connections. Perhaps they should have a two-level structure, with a meta-network generating weights for the network, or perhaps more generic mechanisms for multiplicative interactions. Certain inherently serial operations, such as the connectivity predicate, suggest that there must be ‘serial mode interfaces’ allowing neural networks to call external subroutines. It is a live scientific question whether the number of external subroutines can be kept small. Perhaps a hundred or so would suffice, or perhaps it would turn out that even large neural networks are incapable of most commonsense tasks, in which case the Society of Mind hypothesis would be more viable. However, we consider this an empirical question that can only be answered by attempting to scale up neural networks and seeing what they might do, as a priori estimates of computational difficulty is close to impossible.\n\nWhat distinguishes the two possible Minsky–Paperts? Not the facts present, but their prescientific commitments. Minsky’s commitment to elegant mathematics and simple programming structures led him to insist on things that he could prove theorems for – and denounce empirical methods, especially if large sums of money might be “misdirected” to large scale neural network machines. Papert’s commitment to epistemological pluralism led him to insist on computers that resemble his ideal society – and denounce a uniform computational structure like neural networks as flattening, enframing, and reproducing the hegemonic ideology of universalism.\nFor Papert and Minsky specifically, their claim to be “pro-perceptron” is a sophistry intended to shift the narrative on the perceptron controversy, as they only approved perceptrons with a single layer of learnable parameters. In other words, they were only pro-useless-perceptron. They were trying to kill the project of general large-scale perceptrons that Frank Rosenblatt and the new connectionists in the 1980s worked towards.\n\nThere is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgment that the extension is sterile. Per­haps some powerful convergence theorem will be discovered, or some profound reason for the failure to produce an interesting “learning theorem” for the multilayered machine will be found. (M. Minsky and Papert 1988, 232)\n\nThe irony is that decades later, despite the general neglect of neural networks, they quickly overtook symbolic or statistical AI12 as soon as compute and data price fell so low that they had to appear. And so in 2012, Alex Krizhevsky cobbled together 8 GPUs and train a neural network that outperformed every symbolic or statistical AI.13 We have large and homogeneous neural networks operating, and there are hints that some of them do have small groups of neurons representing symbolic concepts, and some of the symbolic concepts are engaged in serial computation across the layers. However, in order to find these hints of symbolic programs, we had to take through a large detour through the brute reason of uniform neural network architecture, uniform GPU architecture, and uniform training objectives.12 More precisely, classical-statistical AI, with fixed parameters, handcrafted features, and solvable models. A classical-statistical model is constructed as some form of \\(p_\\theta(y|x)\\), where \\(\\theta\\) are the parameters, \\(x\\) are the inputs, and \\(y\\) are the outputs.\nThe difference from neural networks is that for classical-statistical models, \\(p_\\theta\\) allows solvable inference from a dataset, such as by taking the average, derivative, variance, and such. Many of them were straight up linear regressions on handcrafted features (and thus subject to exactly the criticism of Minsky and Papert).\nA good example is the IBM alignment model 1, which can be trained by expectation-maximization with closed form solution (!). To see the difference, compare it with (Bahdanau, Cho, and Bengio 2014), which also learns to align from a corpus, but does not have any closed form solution.13 And if not Krizhevsky and Hinton, how much long would it have taken? In 2009, Andrew Ng’s research cluster trained a 100M model on GPUs (Raina, Madhavan, and Ng 2009), which suggests that the idea was ripe for taking due to the advance in compute and data, and would have happened around 2010 regardless. The rain might not follow the plow, but the AI does follow the compute and data.\nWhy must we take such a large detour? My guess is twofold. One, the resemblance to neat symbolic programs is partial. Large amounts of computing done by neural networks is only symbolic in the trivial, messy way. Only a small amount is symbolic in the neat way. Two, because symbolic programs suffer from diseconomies of scale. Peering into any large enough software project, be it the Cyc project, or the Linux source code, one feels that it is easier to start anew than to add to it. Perhaps with thousands of years of very patient work and many evolutionary deadends, purely symbolic AI research can succeed in constructing a general intelligence in the elegant style sketched by Minsky. The irony is that symbolic programs do not scale while neural networks scale, the exact opposite of the lesson that Minsky and Papert wished to impart by their book.\nAs an example, the history of computer vision demonstrates the problem with the symbolic AI approach. It is true that some problems, such as the parity problem or the connectedness problem, cannot be efficiently solved by neural networks. However, do they really matter? To make connection with the final goal of general computer vision, capable of understanding real scenes, turns out to be far less about provably detecting edges and cubes and cones in a picture, and far more about having a large dataset. In this sense, it’s Minsky and Papert who were misled by their experiments with building block-playing robots in a block world. It’s their work that could not scale.\n\n\nWhat is left of Perceptrons?\nI have never seen a piece of work so systematically opposed to the scaling hypothesis. Reading their theory, I have the feeling that at every turn, I could hear them say, “Neural networks work – if they have less than 100 neurons.”. To their credit, they made falsifiable hypotheses. To their blame, they were almost all proven wrong. Neural networks does scale, 100 billion and counting. Several standard architectures constitute almost the entirety of neural networks nowadays – MLP, CNN, GNN, LSTM, VAE, and Transformers. Six is quite far from the thousands of architectures they explicitly predicted.\nAmong all the objections to neural networks in the Perceptrons book, almost all were either disproven (the anti-scaling hypothesis), or made irrelevant (the perceptron learning rule).\nRecognizing connectivity is hard and requires a serial program, but that’s fine, because it’s hard for humans too. Learning to solve logical problems is difficult and requires a thousand iterations. Well, it looks inefficient, except that neural networks is still the best we have even 30 years later, so perhaps the XOR problem is just something neural networks have to work hard for. That’s fine – worst case, we’ll just let the neural network offload those logical operations to a symbolic program, much like how humans use calculators.\nThe only legitimate remaining problem is the recognition of symmetry. It is hard for all modern neural networks, including convolutional and fully connected versions.14 In any case, if human brains are neural networks, and they can instantly recognize symmetries, then it shows that there is some remaining architectural trick we don’t yet know.14 It might be solved efficiently with a Transformer, but I need to check this.\nTherefore, out of all the clever mathematics and wise lessons of Perceptrons, we ended up with… just one problem remaining? Minsky and Papert hoped to show that there will be thousands of different problems, each requiring a bespoke algorithm implemented by a bespoke neural network. In this regard, their project has been fully debunked."
  },
  {
    "objectID": "blog/posts/perceptron-controversy/index.html#appendix-the-three-camps-of-ai",
    "href": "blog/posts/perceptron-controversy/index.html#appendix-the-three-camps-of-ai",
    "title": "The Perceptron Controversy",
    "section": "Appendix: The three camps of AI",
    "text": "Appendix: The three camps of AI\nIn the early days of AI, there were mainly three camps: cybernetics, symbolic systems, and neural networks. In our current age, it seems like the other two camps have fallen largely into oblivion. This section gives a brief history and orienting perspective of their key ideas.\n\nCybernetic AI\nThe founding metaphor of the cybernetic camp was that intelligence is adaptive analog signal processing: homeostasis, Fourier transforms, Kalman filtering, PID control, linear predictive coding, and such.\nThe origin of cybernetics was tangled with the fast control of machinery in WWII. Norbert Wiener, the mastermind of cybernetics, worked on anti-aircraft gun (AA guns) controllers. As planes flew faster and higher than ever before, AA guns needed to “lead the target” to a greater and greater extent. This put severe strain on the operator of the AA guns. Wiener constructed electromechanical devices that performed linear prediction of future trajectory of an aircraft based on its past trajectory. As the aircraft is a human-machine system, Wiener modelled both together as a synthetic whole. After the war, Wiener continued to work on cybernetics, using the analogies he accumulated during the war.\n\nIf humans do not differ from machines from the “scientific standpoint,” it is because the scientific standpoint of the 1940s was one of men machines at war. The man-airplane-radar-predictor-artillery system is closed one in which it appeared possible to replace men by machines and machines by men. To an antiaircraft operator, the enemy really does act like an autocorrelated servomechanism. … In every piece of his writing on cybernetics, from the first technical exposition of the AA predictor before Pearl Harbor up through his essays of the 1960s on science and society, Wiener put feedback in the foreground, returning again and again to the torpedo, the guided missile, and his antiaircraft director. (Galison 1994)\n\nCybernetics entered the realm of popular consciousness with Wiener’s 1948 bestseller Cybernetics. In it, we find a curious description of artificial intelligence and self-reproduction, but from the analog signal processing point of view, detailed in Cybernetic artificial intelligence. The short version of it is that it was an analog-circuit quine:\n\nThese operations of analysis, synthesis, and automatic self-adjustment of white boxes into the likeness of black boxes … [by] learning, by choosing appropriate inputs for the black and white boxes and comparing them; and in many of these processes, including the method of Professor Gabor, multiplication devices play an important role. While there are many approaches to the problem of multiplying two functions electrically, this task is not technically easy. On the one hand, a good multiplier must work over a large range of amplitudes. On the other hand, it must be so nearly instantaneous in its operation that it will be accurate up to high frequencies. Gabor claims for his multiplier a frequency range running to about 1,000 cycles. (Wiener 2019, xli)\n\nThe cybernetic approach to AI is a strange chapter in the history of AI, filled with machines too sui generis to have followups. It seems to me that there is no issue with building our way to AI one nonlinear filter at a time, except technical issues, but the technical issues are insurmountable. One day I might write an essay that give justice to the cybernetic approach, but as this essay is not on cybernetics, we will only give a whirlwind tour of highlights from the cybernetic approach to AI.\nIn 1948, Ross Ashby built a “homeostat machine”, which consisted of four interacting electromechanical controllers. If one perturbs it .15 The other thing that Ashby is famous for is the “law of requisite variety”, which is equivalent to the theorem that to solve \\(f(x) = y\\), generically, the \\(x\\) must have at least as many dimensions as \\(y\\).15 Perhaps Marvin Minsky’s useless machine was a gentle parody of the homeostat machine. As the homeostat was built in 1948 and the useless machine in 1952, the timing checks out.\nStafford Beer started his professional life as a business consultant, and devised methods to increase steel plant production efficiency. He understood production management as a problem analogous to that of biological homeostasis, a problem solved by the nervous system, and so management should emulate the nervous system16. He also investigated a wide variety of strange machines, including using an entire pond ecosystem as a computer for black-box homeostatic control (Beer 1962):16 According to (Pickering 2004), Stafford Beer perhaps meant this literally:\n\n… it is clear from subsequent developments that the homeostatic system Beer really had in mind was something like the human spinal cord and brain. He never mentioned this in his work on biological computers, but the image that sticks in my mind is that the brain of the cybernetic factory should really have been an unconscious human body, floating in a vat of nutrients and with electronic readouts tapping its higher and lower reflexes …\n\n\n\nWhy not use an entire ecological system, such as a pond? The theory on which all this thinking is based does not require a knowledge of the black box employed. Accordingly, over the past year, I have been conducting experiments with a large tank or pond. The contents of the tank were randomly sampled from ponds in Derbyshire and Surrey. Currently there are a few of the usual creatures visible to the naked eye (Hydra, Cyclops, Daphnia, and a leech); microscopically there is the expected multitude of micro-organisms. In this tank are suspended four lights, the intensities of which can be varied to fine limits. At other points are suspended photocells with amplifying circuits which give them very high sensitivity. … There is an ecological guarantee that this system stabilizes itself, and is indeed ultra-stable. But, of course, the problem of making it act as a control system is very difficult indeed. I regard the machine as a tending-to-be-stable-but-not-quite homeostat …\n\nIn 1971, he was asked to be the principal architect of Project Cybersyn, a nervous system for the Chilean socialist economy by “algedonic control” (“algedonic” means “pain-pleasure”). The project came to an abrupt end with a military coup in 1973 that instated free market economy for Chile. After this, he lived as a hermit, though he still published a few books and did occasional business consulting.(Morozov 2014)\nGordon Pask in the 1950s made electrochemical “sensory organs”. He prepared an acidic solution of metal salts (such as \\(\\text{FeSO}_4\\)) in a dish, then immersed electrodes into the solution. Metal tends to dissolve in the acidic solution, but applying a voltage through the electrodes pulls metal out of the solution by electrolysis. This allows him to “reward” whatever metallic structure in the dish by applying a voltage. He reported success at growing some simple sensory organs in the dish (Gordon 1959; Cariani 1993):\n\nWe have made an ear and we have made a magnetic receptor. The ear can discriminate two frequencies, one of the order of fifty cycles per second and the other on the order of one hundred cycles per second. The ‘training’ procedure takes approximately half a day and once having got the ability to recognize sound at all, the ability to recognize and discriminate two sounds comes more rapidly. … The ear, incidentally, looks rather like an ear. It is a gap in the thread structure in which you have fibrils which resonate at the excitation frequency. (Gordon 1959)\n\nThe details of the electrochemical ear is lost, and this line of research had no followups.\nA faint echo of Pask’s electrochemical ear was heard in late 1990s, when Adrian Thompson used evolutionary algorithm to evolve circuits on field-programmable gate arrays to perform binary distinction between input signals \\(1 \\mathrm{~kHz}\\) and \\(10 \\mathrm{~kHz}\\). Curiously, some circuits succeeded at the task despite using no master clock signal. He traced this down to the precise details of electronic properties that digital circuit design was precisely meant to abstract away from. The circuits’ performance degraded when outside the temperature range in which they evolved in (Thompson and Layzell 1999; Thompson, Layzell, and Zebulum 1999).\n\n… at \\(43.0^{\\circ} \\mathrm{C}\\) the output is not steady at \\(+5 \\mathrm{~V}\\) for \\(\\mathrm{F} 1\\), but is pulsing to \\(0 \\mathrm{~V}\\) for a small fraction of the time. Conversely, at \\(23.5^{\\circ} \\mathrm{C}\\) the output is not a steady \\(0 \\mathrm{~V}\\) for \\(\\mathrm{F} 2\\), but is pulsing to \\(+5 \\mathrm{~V}\\) for a small fraction of the time. This is not surprising: the only time reference that the system has is the natural dynamical behaviour of the components, and properties such as resistance, capacitance and propagation delays are temperature dependent. The circuit operates perfectly over the \\(10^{\\circ} \\mathrm{C}\\) range of temperatures that the population was exposed to during evolution, and no more could reasonably be expected of it. (Thompson 1996)\n\nContinuing the tradition of one-hit wonders, there was no followup work to this.1717 I have tried tracking down what Adrian Thompson has been doing since his late 1990s work. It turned out that the hardware evolution was his PhD work (Thompson 1998). He has almost completely dropped off the face of academia. His website at University of Sussex did not see another update since 2002 and is currently dead. His minimalistic Google Site was created around 2014, and currently only survives on the Internet Archive. There was also a single gif of the circuit in operation, which I decided to download and save for posterity.\n\n\nSymbolic AI\nThe founding metaphor of the symbolic system camp was that intelligence is symbolic manipulation using preprogrammed symbolic rules: logical inference, heuristic tree search, list processing, syntactic trees, and such. The symbolic camp was not strongly centered, though it had key players like Alan Turing, John McCarthy, Herbert Simon, and Marvin Minsky.\nThe project of symbolic AI is a curious episode in AI history. Despite its early successes such as SHRDLU, LISP, and ELIZA, despite the support of most of AI researchers during 1960–2000, and despite the support of reigning cognitivists in the adjacent field of psychology, it never achieved something as successful as neural networks.\nA brief sketch of the greatest project in symbolic AI might give you an impression for the difficulty involved. Have you ever heard of the Cyc doing anything useful in its 39 years of life? Compare that with something even as small as BERT.\nIn 1984, Douglas Lenat started the Cyc project, an ambitious attempt to scale symbolic AI up to the real world. The Cyc project used a LISP-like symbolic logic language to encode common sense. Even from the vantage point of 1985, it was clear to all that there was a lot of common sense it had to incorporate 18, although few could have predicted that Lenat doggedly kept pushing the project for over 30 years. In 2016, Lenat finally declared Cyc project “done”, and set about commercializing it.18 They themselves probably underestimated the difficulty. In 1990, they confidently titled a paper “Cyc: A midterm report” (D. Lenat and Guha 1990), suggesting that they expected to be done around 1995.\n\nHaving spent the past 31 years memorizing an astonishing collection of general knowledge, the artificial-intelligence engine created by Doug Lenat is finally ready to go to work. Lenat’s creation is Cyc, a knowledge base of semantic information designed to give computers some understanding of how things work in the real world. … “Part of the reason is the doneness of Cyc,” explains Lenat, who left his post as a professor at Stanford to start the project in late 1984. “Not that there’s nothing else to do,” he says. But he notes that most of what is left to be added is relevant to a specific area of expertise, such as finance or oncology. Among other projects, the company is developing a personal assistant equipped with Cyc’s general knowledge. This could perhaps lead to something similar to Siri but less predisposed to foolish misunderstandings. Michael Stewart, a longtime collaborator of Lenat’s and the CEO of Lucid, says the new company is in talks with various others interested in using the Cyc knowledge base. Lucid has been working with the Cleveland Clinic, for example, to help automate the process of finding patients for clinical studies. (Knight 2016)\n\nThat was essentially the last we have heard from Cyc.\nWhy has the symbolic AI project failed to scale? It is hard to say, and a definitive answer would not be forthcoming until we have a human level AI as an existence proof of the necessary requirements for scalable AI. However, I can speculate. When looking at this figure from 1985, one is simultaneously filled with respect and sadness, for they were facing impossible odds, and yet they charged right into it.\n\n\n\n(D. B. Lenat, Prakash, and Shepherd 1985, fig. 1)\n\n\nTheir “midterm report” only accentuates this sense of tragedy, of seeing them fighting impossible odds, and losing. They saw with clarity that there is no shortcut to intelligence, no “Maxwell’s equations of thought”.\n\nThe majority of work in knowledge representation has dealt with the technicalities of relating predicate calculus to other formalisms and with the details of various schemes for default reasoning. There has almost been an aversion to addressing the problems that arise in actually representing large bodies of knowledge with content. However, deep, important issues must be addressed if we are to ever have a large intelligent knowledge-based program: What ontological categories would make up an adequate set for carving up the universe? How are they related? What are the important facts and heuristics most humans today know about solid objects? And so on. In short, we must bite the bullet.\nWe don’t believe there is any shortcut to being intelligent, any yet-to-be-discovered Maxwell’s equations of thought, any AI Risc architecture that will yield vast amounts of problem-solving power. Although issues such as architecture are important, no powerful formalism can obviate the need for a lot of knowledge.\nBy knowledge, we don’t just mean dry, almanac-like or highly domain-specific facts. Rather, most of what we need to know to get by in the real world is prescientific (knowledge that is too commonsensical to be included in reference books; for example, animals live for a single solid interval of time, nothing can be in two places at once, animals don’t like pain), dynamic (scripts and rules of thumb for solving problems) and metaknowledge (how to fill in gaps in the knowledge base, how to keep it organized, how to monitor and switch among problem-solving methods, and so on). Perhaps the hardest truth to face, one that AI has been trying to wriggle out of for 34 years, is that there is probably no elegant, effortless way to obtain this immense knowledge base. Rather, the bulk of the effort must (at least initially) be manual entry of assertion after assertion. (D. Lenat and Guha 1990)\n\nI was struck by the same sense of ontological vertigo when looking back at Simon and Newell’s Human Problem Solving, a compendium of their work on decomposing human problem solving into symbolic processes:\n\n\n\n(Newell and Simon 1972, 533)\n\n\n\n\n\n(Newell and Simon 1972, 534)\n\n\nThis sense of vertigo is perhaps best described by Borges in The analytical language of John Wilkins (Borges 2000, 229–32):\n\n… we must examine a problem that is impossible or difficult to postpone: the merit of the forty-part table on which the language is based. Let us consider the eighth category: stones. Wilkins divides them into common (flint, gravel, slate); moderate (marble, amber, coral); precious (pearl, opal); transparent (amethyst, sapphire); and insoluble (coal, fuller’s earth, and arsenic). The ninth category is almost as alarming as the eighth. It reveals that metals can be imperfect (vermilion, quicksilver); artificial (bronze, brass); recremental (filings, rust); and natural (gold, tin, copper). The whale appears in the sixteenth category: it is a viviparous, oblong fish.\nThese ambiguities, redundancies, and deficiencies recall those attributed by Dr. Franz Kuhn to a certain Chinese encyclopedia called the Heavenly Emporium of Benevolent Knowledge. In its distant pages it is written that animals are divided into (a) those that belong to the emperor; (b) embalmed ones; (c) those that are trained; (d) suckling pigs; (e) mermaids; (f) fabulous ones; (g) stray dogs; (h) those that are included in this classification; (i) those that tremble as if they were mad; (j) in­ numerable ones; (k) those drawn with a very fine camel’s-hair brush; (1) etcetera; (m) those that have just broken the flower vase; (n) those that at a distance resemble flies. The Bibliographical Institute of Brussels also exercises chaos: it has parceled the universe into 1,000 subdivisions, of which number 262 corresponds to the Pope, number 282 to the Roman Catholic Church, number 263 to the Lord’s Day, number 268 to Sunday schools, number 298 to Mormonism, and number 294 to Brahmanism, Buddhism, Shintoism, and Taoism. Nor does it disdain the employment of heterogeneous subdivisions, for example, number 179: “Cruelty to animals. Protection of animals. Dueling and suicide from a moral point of view. Various vices and defects. Various virtues and qualities.”"
  },
  {
    "objectID": "blog/posts/perceptron-controversy/index.html#appendix-the-chomskyans",
    "href": "blog/posts/perceptron-controversy/index.html#appendix-the-chomskyans",
    "title": "The Perceptron Controversy",
    "section": "Appendix: the Chomskyans",
    "text": "Appendix: the Chomskyans\nIn the field of psychology, the neural network vs Minsky divide corresponds quite well with the behaviorism vs cognitivism divide.\nAmong the cognitivists, there is no consensus view about the exact role that neural networks must play. Some, like Fodor and Marr, argue that intelligence is just a hardware-independent program, and brains just happen to use neural networks to run the program due to accidents of evolution and biophysics. Others, like Gary Marcus, grant neural networks a more substantial role in constraining the possible programs – that is, each neural network architecture is designed , and each architecture can only run a certain kind of program efficiently – but they still insist that neural networks must have very particular architectures.\nSome might allow simple learning algorithms with simple environmental stimulus (the universal grammar approach of Chomsky), while others might allow complex learning algorithms with complex environmental stimulus (the society of mind approach of Minsky), but what unifies the cognitivists is that they are against simple learning algorithms applied to complex environmental stimulus. They called the this enemy many names, such as “radical behaviorism”, “Skinnerism”, “perceptrons”, “radical connectionism” and now “deep learning”.\n\nNoam Chomsky\nThe cognitivist revolution was led by Noam Chomsky against behaviorism during the 1950s, ending with the victory of cognitivism in “higher psychology”, such as linguistics, though behaviorism survived respectably to this day in other aspects of psychology, such as addiction studies and animal behavior.\nIn a curious parallel, just as neural networks for AI became popular again in the late 1980s, statistical methods for NLP returned during the same period. The watershed moment was the series of IBM alignment models published in 1993 (Brown et al. 1993).\nIn the 1950s, language production was modelled by theoretical machines: finite state automata, stack machines, Markov transition models, and variants thereof. We must understand Chomsky’s two contributions to linguistics. On the first part, he constructed a hierarchy of increasingly powerful kinds of language. This hierarchy was in one-to-one correspondence with the hierarchy of theoretical machines, from finite state automata (type-3, or regular) all the way to Turing machines (type-0, or recursively enumerable). On the second part, he rejected statistical language learning and argued for inborn universal grammar.\nChomsky argued, and subsequent linguists have found, that the syntax of all human languages are at type-2 level, or context-free grammar. None is regular and almost none is context-dependent. Regular languages are modelled by finite state machines and cannot model arbitrarily deep recursion, while context-free languages allow arbitrarily deep recursion such as center embedding. This would come into play later.\nWith the dominance of the Chomsky approach, finite state machines were abandoned, as they are not capable of parsing context-free grammar. You might have heard of the controversy over Pirahã. It is mainly fought over the problem of recursion: does the Pirahã language have recursion or not?1919 Drawing the battle lines, I predicted that Pinker would argue that it must have recursion… and it turns out wrong. Pinker went against Chomsky in this case.\n\n“There’s a lot of strange stuff going on in the Chomskyan program. He’s a guru, he makes pronouncements that his disciples accept on faith and that he doesn’t feel compelled to defend in the conventional scientific manner. Some of them become accepted within his circle as God’s truth without really being properly evaluated, and, surprisingly for someone who talks about universal grammar, he hasn’t actually done the spadework of seeing how it works in some weird little language that they speak in New Guinea.”\nPinker says that his own doubts about the “Chomskyan program” increased in 2002, when Marc Hauser, Chomsky, and Tecumseh Fitch published their paper on recursion in Science. The authors wrote that the distinctive feature of the human faculty of language, narrowly defined, is recursion. Dogs, starlings, whales, porpoises, and chimpanzees all use vocally generated sounds to communicate with other members of their species, but none do so recursively, and thus none can produce complex utterances of infinitely varied meaning. “Recursion had always been an important part of Chomsky’s theory,” Pinker said. “But in Chomsky Mark II, or Mark III, or Mark VII, he all of a sudden said that the only thing unique to language is recursion. It’s not just that it’s the universal that has to be there; it’s the magic ingredient that makes language possible.” (Colapinto 2007)\n\n\nA key principle used by Chomsky was the “poverty of stimulus” argument, which he used to argue that humans must have a universal grammar built in at birth, because there is too little after-birth stimulus for humans to learn languages. For one, true recursion can never be learned empirically, because true recursion can only be conclusively proven by seeing the infinitely many sentences.\nConsider a simple example of the balanced brackets language. A language learner observes sample sentences from the language and try to infer the language. Suppose the learner sees a sequence (), (()), ((())), (((()))), what can they conclude? That it is the balanced brackets language? So we ask them to construct another sentence, and they confidently wrote ((((())))), but we announce to them that they were tricked! The language is the balanced brackets language – except that the brackets only go 4 levels deep. In general, only by seeing all levels of recursion can the balanced brackets language be conclusively learned.\nApplied to linguistics, Chomsky claimed that statistical learning cannot learn syntax, and all attempts have been “colossal failures”.\n\nJust to illustrate, I’ll take one example that was presented back in the 1950s and has become a sort of a classic case because it’s so trivial. ‘Can eagles that fly swim?’ Okay, simple sentence. Everyone understands it. Any young child understands it. There is a question about it. We know that we associate the word ‘can’ with ‘swim,’ not with ‘fly.’ We’re asking ‘Can they swim?’ We’re not asking ‘Can they fly?’ Well, why is that? A natural answer ought to be that you associate ‘can’ with ‘fly.’ After all, ‘fly’ is the word that’s closest to ‘can,’ so why don’t you just take the closest word and interpret it that way? … Well, that property is universal. It holds up in every language. Languages may do it differently, but they’re going to have the same property. It holds in every construction anyone knows, and it’s just a universal property of language.\nWell, this particular example has taken on a life of its own. For one thing, it’s a poverty of stimulus problem, like Descartes’ triangle. There’s been a huge effort to show that it’s not a problem, that if you just do a complex statistical analysis of complex data, you’ll find that that’s what the child will determine from the data. The approaches are odd in several respects. First, every one is not only a failure but a colossal failure. I’m not going to talk about that. I actually have a recent paper about it with a computer scientist at MIT, Bob Berwick, where we run through a lot of the current proposals but it’s easy to show that they’re all just wildly wrong. But they keep coming. Almost every issue of the main cognitive science journals has another proposal of this sort, so that’s one odd fact. There are many efforts to show that there is nothing puzzling about it, they’re all colossal failures. (Chomsky 2010)\n\nWell, finally a testable hypothesis! I asked GPT-4 to Draw a syntax tree for \"Can eagles that fly swim?\", and got this:2020 The code it gave failed on the first try, due to an environment variable issue of the Linux virtual machine it ran on. We fixed it after two more plies of conversation.\n#| code-fold: true\n\n!pip install svgling\nimport nltk\nfrom nltk import Tree\nfrom IPython.display import display\nnltk.download('punkt')\nimport svgling\nfrom IPython.display import SVG, display\n\n# Your tree definition\ntree = Tree.fromstring(\"\"\"\n    (SQ \n        (MD Can) \n        (NP \n            (NNS eagles) \n            (SBAR \n                (WHNP (WDT that)) \n                (S \n                    (VP (VBP fly))\n                )\n            )\n        ) \n        (VP (VB swim))\n    )\n\"\"\")\n\n# Render the tree as an SVG\ntree_svg = svgling.draw_tree(tree)\n\n# Save the SVG to a file\ntree_svg.get_svg().save()\n\n# Display the SVG in the notebook\ndisplay(SVG(tree_svg.get_svg()))\n\n\n\nThe syntax tree for “Can eagles that fly swim?”, drawn by GPT-4.\n\n\nAs expected, Chomsky rejected statistical language learning right until the end.\n\nIt’s true there’s been a lot of work on trying to apply statistical models to various linguistic problems. I think there have been some successes, but a lot of failures. There is a notion of success … which I think is novel in the history of science. It interprets success as approximating unanalyzed data. (Norvig 2017)\n\nPeter Norvig gave a detailed analysis and a rebuttal in (Norvig 2017).\n\n\nThe Chomskyans\nGold’s theorem about language learning in the limit is occasionally quoted in the same context as a justification for the “poverty of stimulus argument”. It seems Chomsky did not consider it a relevant argument (Johnson 2004), and I agree with Chomsky on that account, as Gold’s theorem is extremely generic.\nDuring the second rise of neural networks, there was a bitter controversy that raged during the 1990s, but is essentially forgotten nowadays: the past tense debate. On one side were the connectionists, and on the other side were the cognitivists like Steven Pinker and Gary Marcus (Pinker and Ullman 2002). Tellingly, both Steven Pinker and Gary Marcus were on the side of cognitivists. Steven Pinker is most famous for his other books like The Blank Slate, which applies Chomsky’s linguistics to general psychology.\nHuman language has the distinctive fractal-like structure: for every rule, there are exceptions, and for every exception, there are exceptional exceptions, and so on. This is called “quasi-regularity”. Sejnowski in an interview described how quasi-regularity shows up in phonology, and offered a perfect demonstration project for the power of neural networks against the Chomskyans:\n\nI went to the library and found a book on text-to-speech that had probably one thousand rules in it. It was just lists of rules and exceptions. After I went to the library and found a book on text-to-speech that had probably one thousand rules in it. It was just lists of rules and exceptions. After every rule, there were dozens of exceptions. I figured that either they’re doing it the wrong way, or this is a really tough problem. We considered finding something that was simpler after consulting the experts in linguistics. They said, “Chomsky worked on the problem, and it is well beyond the capability of anything that you could imagine trying to do.” (Rosenfeld and Anderson 2000, 324–25)\n\n(Sejnowski 2018, 75–78) recounts an anecdote about Jerry Fodor, another prominent cognitivist. While Fodor is no longer a hot name in AI nowadays, this anecdote illustrates the Chomsky way of thinking.\n\nIn 1988, I served on a committee for the McDonnell and Pew Foundations that interviewed prominent cognitive scientists and neuroscientists to get their recommendations on how to jump-start a new field called “cognitive neuroscience”. … [Fodor] started by throwing down the gauntlet, “Cognitive neuroscience is not a science and it never will be.” … Fodor explained why the mind had to be thought of as a modular symbol-processing system running an intelligent computer program. [Patricia Churchland] asked him whether his theory also applied to cats. “Yes,” said Fodor, “cats are running the cat program.” But when Mortimer Mishkin, an NIH neuroscientist studying vision and memory, asked him to tell us about discoveries made in his own lab, Fodor mumbled something I couldn’t follow about using event-related potentials in a language experiment. Mercifully, at that moment, a fire drill was called and we all filed outside. Standing in the courtyard, I overheard Mishkin say to Fodor: “Those are pretty small potatoes.” When the drill was over, Fodor had disappeared.\n\nSimilarly, Gary Marcus has been consistently critical of neural network language models, at least since 1992 in (G. F. Marcus et al. 1992). His theory of intelligence is essentially Chomskyan: neural networks can be intelligent, but only if they implement symbolic manipulation rules.21 Furthermore, a lot of symbolic rules must be built in at birth, as the poverty of stimulus precludes learning them empirically.21 This brief sketch suffices. A book-length treatment is (G. F. Marcus 2003).\nFor example, here is him saying in 1993:\n\nWhether children require “negative evidence” (i.e., information about which strings of words are not grammatical sentences) to eliminate their ungrammatical utterances is a central question in language acquisition because, lacking negative evidence, a child would require internal mechanisms to unlearn grammatical errors. … There is no evidence that noisy feedback is required for language learning, or even that noisy feedback exists. Thus internal mechanisms are necessary to account for the unlearning of ungrammatical utterances. (G. F. Marcus 1993)\n\nAnd here is him saying in 2018, just in time to miss the Transformer revolution in natural language processing:\n\nHuman beings can learn abstract relationships in a few trials. If I told you that a schmister was a sister over the age of 10 but under the age of 21, perhaps giving you a single example, you could immediately infer whether you had any schmisters, whether your best friend had a schmister, whether your children or parents had any schmisters, and so forth. (Odds are, your parents no longer do, if they ever did, and you could rapidly draw that inference, too.) In learning what a schmister is, in this case through explicit definition, you rely not on hundreds or thousands or millions of training examples, but on a capacity to represent abstract relationships between algebra-like variables. Humans can learn such abstractions, both through explicit definition and more implicit means (Marcus, 2001). Indeed even 7-month old infants can do so, acquiring learned abstract language-like rules from a small number of unlabeled examples, in just two minutes. (G. Marcus 2018)\n\nNot one to give up, he continued the same criticisms into the age of Transformer language models. If anything, I would grant that he is conveniently predictable22, and we would be unsurprised by his recent criticisms of deep learning (G. Marcus 2018) and large language models, repeatedly.22 It would be funny if someone can train a language model to pass the “Gary Marcus test”: pretend to be Gary Marcus in a Turing test setup. If such a model passes, then either Marcus would admit that the language model makes sense, or that what he says is indistinguishable from nonsense."
  },
  {
    "objectID": "blog/posts/web-design-notes/index.html#punctuation",
    "href": "blog/posts/web-design-notes/index.html#punctuation",
    "title": "Notes on Web Design",
    "section": "Punctuation",
    "text": "Punctuation\n\nComma\nThe comma separates parts with equal syntax roles.\nEqual noun-phrases.\n  I have 3 trees in my backyard: a jujube tree, a jujube tree, and a peach tree.\n\nEqual adjectives.\n  He is a strong, healthy man.\n\nAdjectives at unequal levels.\n* We stayed at an expensive, summer resort.\n  We stayed at an {expensive {summer resort}}.\nThey denote typed multisets.\nThey are typed multisets, as in:\nI have 3 trees in my backyard: a jujube tree, a jujube tree, and a peach tree.\n=&gt;\nI have {(jujube tree, 2), (peach tree, 1)} in my backyard.\n\nThey are not typed sets, because:\nI have 3 trees in my backyard: a jujube tree, a jujube tree, and a peach tree.\n=/=&gt;\nI have {jujube tree, peach tree} in my backyard.\n\nThey are not typed lists, because:\nI have 3 trees in my backyard: a jujube tree, a jujube tree, and a peach tree.\n=\nI have 3 trees in my backyard: a jujube tree, a peach tree, and a jujube tree.\nbut\n(jujube tree, jujube tree, peach tree) =/= (jujube tree, peach tree, jujube tree)\n\nThey are not untyped multisets, because:\n* I have in my backyard a jujube tree and sing.\n  Backyard = {(jujube tree, 1), (sing, 1)}\nThe “and” before the last item on the list seems to be necessary in speech, as in “Heads up! The list is about to be over!”, even though it is not necessary in writing.\nThe “Oxford comma” is a rather odd hack that allows a tiny amount of nesting. In fact, English does not have recursion, and 2 seems to be the deepest nesting we can get. According to Wikipedia, the most common arguments for/against the Oxford comma are: convention, disambiguation. As for convention, it cannot be argued with, only dealt with. As for disambiguation, I recommend using brackets.11 It seems like a lot of weird English syntax is centered around an abhorrence of brackets. Like, I understand why brackets can get annoying (as anyone can see by looking at LISP code), but there is no reason to avoid brackets when you really need to disambiguate something. For example, instead of requiring the Oxford comma, why not just put in a freaking bracket? Don’t say that “oh you can’t speak brackets” like damn you you can’t speak commas either!\nNo Oxford comma:\nTo my parents, Ayn Rand and God.\nTo my mother, Ayn Rand, and God.\n\nOxford comma:\nTo my parents, Ayn Rand, and God.\nTo my mother, Ayn Rand and God.\n\nMathematics:\nTo {my parents, Ayn Rand, God}.\nTo {my mother Ayn Rand, God}.\nTo {my mother, Ayn Rand, God}.\nFor clauses, commas are weird. There is no clear rule.\nEqual clauses.\n  He walked all the way home, and he shut the door.\n\nEqual clauses, though the second lost the pronoun.\n  I saw that she was busy, and [I] prepared to leave.\n\nDependent clause (except that-clause).\n  Because I could not stop for Death, He kindly stopped for me.\n\nNo comma between that-clause and main-clause, or vice versa.\n* We paused before a House, that seemed a swelling of the Ground\n  We paused before a House that seemed a swelling of the Ground\n* That is not dead, which can eternal lie.\n  That is not dead which can eternal lie.\nA pair of commas can replace a pair of parentheses, though there is no benefit, other than avoiding the wrath of English teachers.\nA comma, like an em-dash ---, can indicate pauses and stutters in quotations, such as “Daisy, Daisy… give me—e— your, ans, ser, do…”. This is an edge case and there are no fixed rules here. If you ever have to write something like this, use your imagination.\n\n\nQuotation\nEnglish quotation standards are extremely annoying, because while theoretically quotation is meant to be verbatim, it is anything but. I recommend verbatim quotation. It is consistent with usage in computer programming and formal logic. In particular, the English quotation method would be a syntax error in computer programming, and would also break the proofs of Gödel’s incompleteness theorems, which use the Quine quotation.\nFor nested quotations, alternate between double \" and single ' quotation marks. In other words, inside a level-\\(n\\) writing, use \" if \\(n \\equiv 0\\mod 2\\), and ' otherwise. It is context-free, and its quotient by {\" ~ '} is the Dyck language.\nHowever, if you need to pass the Turing test, then study carefully the following edge cases:\n* Did Hal say \"Good morning, Dave.\"?\n  Did Hal say, \"Good morning, Dave\"?\n\n* No, he said \"Where are you, Dave?\".\n  No, he said, \"Where are you, Dave?\"\n\n  To be perfectly exact, I heard \"Wh--- are you, Da---\".\n\n* \"cat\" is in lowercase.\n* \"Cat\" is in lowercase.\n* \"Cat\" is in uppercase.\n  \"Cat\" is capitalized.\n* In lowercase \"cat\" is.\n* \"cat\" is in lowercase, while \"CAT\" is in uppercase.\n  \"CAT\" is in uppercase, while \"cat\" is in lowercase.\n  What we cannot speak about we must pass over in silence.\n\n* \"sed\" (\"stream editor\") is a Unix utility that parses and transforms text.\n* \"Sed\" (\"stream editor\") is a Unix utility that parses and transforms text.\n  The Unix utility \"sed\" parses and transforms text.\nThe two senses of verbatim (verbatim verbal speech, verbatim written text) are inconsistent with the capitalization constraint:\nThis violates \"first letter capitalization\".\n* \"... 'Vim' stands for 'vi improved'. 'sed' stands for 'stream editor'. ...\" \n\nThis violates \"verbatim written text\".\n* \"... 'Vim' stands for 'vi improved'. 'Sed' stands for 'stream editor'. ...\"\n\nThis violates \"verbatim verbal speech\".\n* \"... 'Vim' stands for 'vi improved', whereas 'sed' stands for 'stream editor'. ...\"\n\nThis violates the constraint of grammar.\n* \"... 'Vim' stands for 'vi improved', 'sed' stands for 'stream editor'. ...\"\n\nThis works, but only by a dubious insertion.\n  \"... 'Vim' stands for 'vi improved'[, whereas] 'sed' stands for 'stream editor'. ...\"\n\nThis works, but just barely.\n  \"... 'Vim' stands for 'vi improved'; 'sed' stands for 'stream editor'. ...\"\n\nIt would not work if the two sentences are not \"closely related\".\n* \"... The Unix coreutils were written mostly in the 1970s; 'sed',\n       which stands for 'stream editor', was written by Lee McMahon in 1973. ...\"\n\n\nWhitespace\nFor most cases, the single whitespace works.\n (U+00A0 no-break space) says: “Do not break line here.”. Similarly for &NoBreak; (U+2060 word joiner). The opposite is &lt;wbr&gt; (U+200B zero width space), which says: “You can break line here.”.\n  (U+2007 figure space),   (U+2008 punctuation space), are only for numerical tabulations, to ensure alignment across rows.\nThe thin space and hair space are not used except by fastidious typographers.\n\n\nConnecting words\nUse - to connect two non-equal parts into a compound noun-phrase, except when one of the parts is an open compound, in which case, use --.\nUse -- to connect two equal parts into a compound noun-phrase.\n\npre-WWII, pre--World War Two, ex--Prime Minister, water-based solution, non--water-based solution, the anti-choice--anti-life debate\n\n\n\nInterruptions\nUse em-dash --- without spacing.\n\nI---no, YOU son of a---\n---and as I was saying---\n\n\n\nDeletions\nTo cut out letters or words by redaction, use the em-dash ---. Imagine that you wrote out the text normally, then you replace the section to be redacted with a single ---, and in this way you will find the right spacing. However, I recommend the black block character █, which looks just like real redactions on secret documents.\n\nThat f--- son of a b███!\n\nThe ellipsis symbol can be done either with ... or with a single … (U+2026 horizontal ellipsis). It is used with spacing on both ends, except that there is no space between it and the quotation mark.\n\n\"Consider this ... , and no more ... be said. ... and so on and so forth ...\", said Mr. ---.\n\n\n\nArithmetics\nIf you cannot use LaTeX, then you could use the hyphen - for minus sign, though it’s more correct to use −. Similarly, the letter x can work, though × is better."
  }
]