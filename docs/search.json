[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "If you thought the Stroop test is hard, just wait until you have tried this.\nPlay online | Play offline"
  },
  {
    "objectID": "projects/index.html#double-stroop",
    "href": "projects/index.html#double-stroop",
    "title": "Projects",
    "section": "",
    "text": "If you thought the Stroop test is hard, just wait until you have tried this.\nPlay online | Play offline"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n  \n  \nYuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Introduction\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMixture of Experts\n\n\n\n\n\n\n\nAI\n\n\nscaling\n\n\n\n\nReview of the mixture-of-experts approach, past and present.\n\n\n\n\n\n\nJan 18, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nWhen will AI pass the Turing Test?\n\n\n\n\n\n\n\nAI\n\n\nscaling\n\n\n\n\nThe forecast chain: scale, perplexity, Turing test, AGI.\n\n\n\n\n\n\nJan 18, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nMathematical Interpretations of Quantum Mechanics\n\n\n\n\n\n\n\nphysics\n\n\n\n\nQuantum mechanics: what it all means, mathematically speaking.\n\n\n\n\n\n\nJan 10, 2024\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nBook Reviews\n\n\n\n\n\n\n\nbook-review\n\n\n\n\nBook reviews.\n\n\n\n\n\n\nDec 16, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\n  \n\n\n\n\nNeural scaling law by data manifold dimensions\n\n\n\n\n\n\n\nAI\n\n\nscaling\n\n\n\n\nNeural networks scale they way they do, purely because of data.\n\n\n\n\n\n\nNov 1, 2023\n\n\nYuxi Liu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "Yuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks."
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#history",
    "href": "blog/posts/mixture-of-experts/index.html#history",
    "title": "Mixture of Experts",
    "section": "History",
    "text": "History"
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#future",
    "href": "blog/posts/mixture-of-experts/index.html#future",
    "title": "Mixture of Experts",
    "section": "Future",
    "text": "Future\n\nAnd then it finally worked. And I think the biggest difference was the computing power. Definitely there were advances in data. So we could do image net because Fei-Fei Li and others gathered this large database, and that was really important. There are certainly differences in the algorithm, right? We’ve got a slightly different squashing function. Instead of shaped like this, it’s shaped like this. I mean, I don’t know how big a deal that was, but we learned how to do stochastic gradient dissent a little bit better. We figured that dropout gave you a little bit better robustness.\nSo there were small things, but I think probably the biggest was the computing power. And I mean, I certainly remember Geoff Hinton came to Berkeley when I was a grad student in 1981, I think, when he talked about these neural nets. And we fellow grad students thought that was so cool. So we said, “Let’s go back into the lab and implement it.\nAnd of course, there was absolutely nothing you could download, so we had to build it all from scratch. And we got it to do exclusive or, and then we got it to do something a little bit more complicated. And it was exciting. And then we gave it the first real problem, and it ran overnight, and it didn’t converge, and we let it run one more day, and it still didn’t converge. And then we gave up, and we went back to our sort of knowledge-based systems approach. But if we had the computing power of today, it probably would have converged after five seconds. (Norvig 2021)"
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#a-toy-model",
    "href": "blog/posts/mixture-of-experts/index.html#a-toy-model",
    "title": "Mixture of Experts",
    "section": "A toy model",
    "text": "A toy model\nTo make this concrete, I coded up system in Python in a Jupyter notebook. See\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate the dataset\ndef generate_dataset(num_samples, sharpness=5):\n    X = np.random.randn(num_samples, 2)\n    p = np.minimum(np.exp(sharpness * np.minimum(X[:, 0], X[:, 1])), 1)\n    y = np.random.binomial(1, p, size=num_samples)\n    return X, y\n\n# Generate the dataset\nnum_samples = 1000\nX, y = generate_dataset(num_samples)\n\n# Split the dataset manually\nsplit_ratio = 0.8\nsplit_index = int(split_ratio * num_samples)\n\nX_train, X_test = X[:split_index], X[split_index:]\ny_train, y_test = y[:split_index], y[split_index:]\n\n# Plot the dataset\ndef plot_dataset(X, y, ax):\n    ax.scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], label='Class 0', marker='o', c='blue')\n    ax.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], label='Class 1', marker='x', c='red')\n    ax.set_title('Dataset Scatter Plot')\n    return ax\n\nfig, ax = plt.subplots(figsize=(8, 6))\nplot_dataset(X_test, y_test, ax)\nplt.show()"
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html",
    "href": "blog/posts/mixture-of-experts/index.html",
    "title": "Mixture of Experts",
    "section": "",
    "text": "Mixture of Experts is an old technique dating back to 1991, but it has become a vital component of modern deep learning to get around the memory bottleneck.\nThe classical version of MoE is fairly simple: You start with a few machine learning models and then you ensemble them together.\nIn this example, we train a binary classifier for points in \\(\\mathbb{R}^2\\). The distribution is constructed so that points are more likely to be class 1 in the upper-right quadrant, and more likely to be class 0 in the other 3 quadrants."
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#theory",
    "href": "blog/posts/mixture-of-experts/index.html#theory",
    "title": "Mixture of Experts",
    "section": "",
    "text": "Mixture of Experts is an old technique dating back to 1991, but it has become a vital component of modern deep learning to get around the memory bottleneck.\nThe classical version of MoE is fairly simple: You start with a few machine learning models and then you ensemble them together.\nIn this example, we train a binary classifier for points in \\(\\mathbb{R}^2\\). The distribution is constructed so that points are more likely to be class 1 in the upper-right quadrant, and more likely to be class 0 in the other 3 quadrants."
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#before-deep-learning",
    "href": "blog/posts/mixture-of-experts/index.html#before-deep-learning",
    "title": "Mixture of Experts",
    "section": "Before deep learning",
    "text": "Before deep learning\nIn the beginning was the gaussian. The gaussian is a beautiful distribution, with linearity, the central limit theorem, fast inference, least squares regression, and so on. The problem is that it has just one peak.\nIf one wants to model a complicated distribution with several bumps, one can make one step up the staircase of complexity, and build distributions from a linear sum of several gaussians. This is the mixture of gaussians.\n\n\n\nA mixture of three gaussian bumps. Figure from Wikipedia.\n\n\nA mixture of experts is then a simple generalization, and training a mixture of experts, back in the old days, was mostly thought of as statistical inference. The main problem was simply modelling complex data with a larger family of statistical distribution. Their main worry was that the experts would overfit.\nThey had little data (enough to fit onto a floppy disk), and each expert was usually just a gaussian distibution or a logistic classifier (any more complex and they wouldn’t know how to calculate the integrals and derivatives). Consequently, what they ended up trying to solve was to fit a few thousand datapoints using tens of very simple experts.\nIt is a general fact of classical machine learning that they were very worried about overfitting, and it is reasonable back then to worry, since they had such small datasets (MNIST was in 1994). This, combined with their inability to hand-design learning algorithms for complex machine learning architectures and the slowness of pure gradient descent, meant that machine learning algorithms back then were simple ones fitted onto small datasets.\nThe overall effect is:\n\ngetting training data: expensive (you have to do it yourself)\ndesigning the algorithm: expensive (cheaper if you have graduate students)\ntraining compute: moderate to high (though a few pioneers have bravely pushed to the “very expensive” regime, and failed1)\ninference compute: very cheap (since that you wouldn’t be able to train anything large)\n\n1 Peter Norvig, coauthor of the most popular AI textbook, recalls:\n\nI certainly remember Geoffrey Hinton came to Berkeley when I was a grad student in 1981, I think, when he talked about these neural nets. And we fellow grad students thought that was so cool. So we said, “Let’s go back into the lab and implement it. And of course, there was absolutely nothing you could download, so we had to build it all from scratch. And we got it to do XOR, and then we got it to do something a little bit more complicated. And it was exciting. And then we gave it the first real problem, and it ran overnight, and it didn’t converge, and we let it run one more day, and it still didn’t converge. And then we gave up, and we went back to our sort of knowledge-based systems approach. But if we had the computing power of today, it probably would have converged after five seconds. (Norvig 2021)\n\nThis should be compared to the very different situation with deep learning:\n\ngetting training data: cheap (just download it online)\ndesigning the algorithm: cheap (make a standard network, add a few decorations, then use backprop with Adam optimizer)\ntraining compute: as expensive as you want\ninference compute: as expensive as you want"
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#after-deep-learning",
    "href": "blog/posts/mixture-of-experts/index.html#after-deep-learning",
    "title": "Mixture of Experts",
    "section": "After deep learning",
    "text": "After deep learning\nWhile classical statistics and machine learning was mainly constrained by how many partial derivatives and integrals the statistician can calculate confidently on paper,2 deep learning is mainly constrained by memory and compute budget.2 If you want a taste of the old days, look at the formulas inside (Jordan and Jacobs 1994). They explicitly calculated the expectation-maximization algorithms for learning a hierarchy of linear experts.\nSo when the deep learning era came circa 2012, people immediately started looking into how to perform conditional computing: save computing cost by only calling a small portion of the model. The idea is that you would have different portions of the model be specialized for different forms of input, and for each input, the model would first cheaply find out which expert should handle it, then call upon only the few specialized experts to handle this particular input.\nDeep learning came with AlexNet (2012), and the first paper on applying MoE to deep learning was “Learning Factored Representations in a Deep Mixture of Experts” (2013). Things really started heating up though with sparsely-gated MoE (2017)."
  },
  {
    "objectID": "blog/posts/mixture-of-experts/index.html#why-moe-for-deep-learning",
    "href": "blog/posts/mixture-of-experts/index.html#why-moe-for-deep-learning",
    "title": "Mixture of Experts",
    "section": "Why MoE for deep learning?",
    "text": "Why MoE for deep learning?\nGenerally, one uses a MoE on the frontier, because:\n\nYou really need to push the metric up by a few points.\nYou can’t train a dense model larger than the frontier model, because it simply fails to converge, or the hyperparameter settings for the small models don’t work for the larger one (and you can’t just run a grid search to find it because it costs a million dollars to do a single run).\nYou can train around 10 copies of the frontier model, because while you don’t have the money to do grid search beyond the current frontier, you have the money to train 10 at the frontier.\nYou can’t infer a dense model larger than the frontier one, because one dense model \\(N\\) times as wide would cost you \\(N^2\\) amount of storage and compute, while if you just train \\(N\\) experts, each with roughly the same architecture as the dense model, it would cost you about \\(N\\) amount of storage and about \\(2\\) amount of compute (if only 2 experts are called per query).\nIndeed, if there are too many parameters, then it can’t even be fit onto a good GPU and must be split across GPUs, and then the GPU-GPU communication becomes a serious problem (the “von Neumann bottleneck”).\n\n\n\n\nThe storage hierarchy. Figure from Harvard CS 61: Systems Programming and Machine Organization (2018), Storage 2: Cache model.\n\n\nAll of which are satisfied by Microsoft, Google, etc. This explains the “rumored” (all but certain at this point) that GPT-4 is a MoE made by multiple GPT-3-like models.\nA quick scan of the recent literature shows this, all from Google.\n\nWe present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. (Shazeer et al. 2017)\n\n\nCombining expert, model and data parallelism, we design two large Switch Transformer models, one with 395 billion and 1.6 trillion parameters, respectively.\n(Fedus, Zoph, and Shazeer 2022)\n\n\nwe demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet.\n(Riquelme et al. 2021)\n\n(Shazeer et al. 2017) is not the first paper on MoE in the deep learning era, but it is the most important one. Also notice that it was applied to between “stacked LSTM layers”, because it was published before Transformers, back when neural language models meant stacks of LSTM. Nowadays, of course, MoE usually means MoE layers within Transformers, because only with Transformers do people regularly train models beyond 10 billion parameters."
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "",
    "text": "This is a theory of neural scaling law, proposed by (Bahri et al. 2021; Sharma and Kaplan 2022)\nAccording to this theory, a neural network, when trained to convergence, allocates its \\(N\\) parameters in two parts: * A fixed number of parameters that map the data to an intrinsic data manifold of dim \\(d\\). * All other parameters that handle pieces of this manifold. Loss \\(\\propto\\) the volume of each manifold piece.\nThey argued that the loss function should scale as \\(L \\propto N^{-4/d}\\) for cross-entropy and mean-square losses."
  },
  {
    "objectID": "blog/posts/neural-network-scrapbook/index.html",
    "href": "blog/posts/neural-network-scrapbook/index.html",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "",
    "text": "Marvin Minsky, on how he gave up on neural networks after the 1950s because he could not afford a few million neurons.\n\nI had the naive idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks. Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. (Bernstein 1981)\n\nPeter Norvig, on how he quickly gave up on neural networks in the 1980s due to lack of compute.\n\nAnd then it finally worked. And I think the biggest difference was the computing power. Definitely there were advances in data. So we could do image net because Fei-Fei Li and others gathered this large database, and that was really important. There are certainly differences in the algorithm, right? We’ve got a slightly different squashing function. Instead of shaped like this, it’s shaped like this. I mean, I don’t know how big a deal that was, but we learned how to do stochastic gradient dissent a little bit better. We figured that dropout gave you a little bit better robustness.\nSo there were small things, but I think probably the biggest was the computing power. And I mean, I certainly remember Geoffrey Hinton came to Berkeley when I was a grad student in 1981, I think, when he talked about these neural nets. And we fellow grad students thought that was so cool. So we said, “Let’s go back into the lab and implement it.\nAnd of course, there was absolutely nothing you could download, so we had to build it all from scratch. And we got it to do exclusive or, and then we got it to do something a little bit more complicated. And it was exciting. And then we gave it the first real problem, and it ran overnight, and it didn’t converge, and we let it run one more day, and it still didn’t converge. And then we gave up, and we went back to our sort of knowledge-based systems approach. But if we had the computing power of today, it probably would have converged after five seconds. (Norvig 2021)"
  },
  {
    "objectID": "blog/posts/neural-network-scrapbook/index.html#the-scaling-hypothesis",
    "href": "blog/posts/neural-network-scrapbook/index.html#the-scaling-hypothesis",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "",
    "text": "Marvin Minsky, on how he gave up on neural networks after the 1950s because he could not afford a few million neurons.\n\nI had the naive idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks. Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. (Bernstein 1981)\n\nPeter Norvig, on how he quickly gave up on neural networks in the 1980s due to lack of compute.\n\nAnd then it finally worked. And I think the biggest difference was the computing power. Definitely there were advances in data. So we could do image net because Fei-Fei Li and others gathered this large database, and that was really important. There are certainly differences in the algorithm, right? We’ve got a slightly different squashing function. Instead of shaped like this, it’s shaped like this. I mean, I don’t know how big a deal that was, but we learned how to do stochastic gradient dissent a little bit better. We figured that dropout gave you a little bit better robustness.\nSo there were small things, but I think probably the biggest was the computing power. And I mean, I certainly remember Geoffrey Hinton came to Berkeley when I was a grad student in 1981, I think, when he talked about these neural nets. And we fellow grad students thought that was so cool. So we said, “Let’s go back into the lab and implement it.\nAnd of course, there was absolutely nothing you could download, so we had to build it all from scratch. And we got it to do exclusive or, and then we got it to do something a little bit more complicated. And it was exciting. And then we gave it the first real problem, and it ran overnight, and it didn’t converge, and we let it run one more day, and it still didn’t converge. And then we gave up, and we went back to our sort of knowledge-based systems approach. But if we had the computing power of today, it probably would have converged after five seconds. (Norvig 2021)"
  },
  {
    "objectID": "blog/posts/neural-network-scrapbook/index.html#neural-networks-want-to-work",
    "href": "blog/posts/neural-network-scrapbook/index.html#neural-networks-want-to-work",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "Neural networks want to work",
    "text": "Neural networks want to work\nMarvin Minsky’s SNARC (1951). Designed to simulate one mouse escaping a maze, it ended up simulating multiple mice due to design bugs – which were never debugged. Though the machine had only 40 neurons, and its parts failed all the time, the whole network continued to work.\n\nIt turned out that because of an electronic accident in our design we could put two or three rats in the same maze and follow them all. The rats actually interacted with one another. If one of them found a good path, the others would tend to follow it. We sort of quit science for a while to watch the machine. We were amazed that it could have several activities going on at once in its little nervous system. Because of the random wiring, it had a sort of fail-safe characteristic. If one of the neurons wasn’t working, it wouldn’t make much of a difference—and, with nearly three hundred tubes and the thousands of connections we had soldered, there would usually be something wrong somewhere. In those days, even a radio set with twenty tubes tended to fail a lot. I don’t think we ever debugged our machine completely, but that didn’t matter. By having this crazy random design, it was almost sure to work, no matter how you built it. (Bernstein 1981)\n\nBernard Widrow once built a MADALINE I (circa 1962) in a rush to present at a technical meeting. Despite that only 1/4 of its circuits were defective, it still worked at reduced capacity.\n\nWe discovered the inherent ability of adaptive computers to ignore their own defects while we were rushing through construction of a system called Madaline I for presentation at a technical meeting. The machine was finished late the night before the meeting and the next day we showed some very complex pattern discriminations. Later we discovered that about a fourth of the circuitry was defective. Things were connected backward, there were short circuits, and poor solder joints. We were pretty unhappy until it dawned on us that this system has the ability to adapt around its own internal flaws. The capacity of the system is diminished but it does not fail. (Widrow 1963)\n\nAndrej Karpathy, on how neural network program bugs are very hard to find, because bugged neural networks do not fail, merely degrade.\n\n… perhaps you forgot to flip your labels when you left-right flipped the image during data augmentation. Your net can still (shockingly) work pretty well because your network can internally learn to detect flipped images and then it left-right flips its predictions. Or maybe your autoregressive model accidentally takes the thing it’s trying to predict as an input due to an off-by-one bug. Or you tried to clip your gradients but instead clipped the loss, causing the outlier examples to be ignored during training. Or you initialized your weights from a pretrained checkpoint but didn’t use the original mean. Or you just screwed up the settings for regularization strengths, learning rate, its decay rate, model size, etc. Therefore, your misconfigured neural net will throw exceptions only if you’re lucky; Most of the time it will train but silently work a bit worse. (Karpathy 2019)\n\nResearchers at OpenAI (2018) reported that fixing RL bugs is as important as better algorithms.\n\nBig-picture considerations like susceptibility to the noisy-TV problem are important for the choice of a good exploration algorithm. However, we found that getting seemingly-small details right in our simple algorithm made the difference between an agent that never leaves the first room and an agent that can pass the first level. To add stability to the training, we avoided saturation of the features and brought the intrinsic rewards to a predictable range. We also noticed significant improvements in performance of RND every time we discovered and fixed a bug (our favorite one involved accidentally zeroing an array which resulted in extrinsic returns being treated as non-episodic; we realized this was the case only after being puzzled by the extrinsic value function looking suspiciously periodic). Getting such details right was a significant part of achieving high performance even with algorithms conceptually similar to prior work. This is one reason to prefer simpler algorithms where possible. (Burda and Edwards 2018)\n\nAround 2019, Gwern, Shawn Presser, and others, trained \\(512\\times 512\\) image generation models using the BigGAN architecture. However, they used compare_gan, which had a multiply-by-zero bug. Somehow it still worked, but not well enough compared to the original BigGAN.\n\nOur primary goal was to train & release 512px BigGAN models on not just ImageNet but all the other datasets we had like anime datasets. The compare_gan BigGAN implementation turned out to have a subtle +1 gamma bug which stopped us from reaching results comparable to the model; while we beat our heads against the wall trying to figure out why it was working but not well enough (figuring it out far too late, after we had disbanded) … “Neural nets want to work” – even if they start out being effectively multiplied by zero. (Branwen 2022)\n\nPersonal story at the Berkeley CS 285, Deep Reinforcement Learning, 2022 Fall.\nFor Homework 3, we were asked to implement the soft actor-critic algorithm. We would implement the agent, run the agent on the Half Cheetah environment, and submit the trajectories to Gradescope, where an autograder would check the trajectories and see if the agent achieved a final score above 300. For the Half Cheetah, score means the distance it travels per episode, averaged over several episodes.\nI noticed that the algorithm I implemented did learn, but the learning curve looked like a rollercoaster, jumping up and down around the range of 250 – 300. After many fruitless and paranoid programming sessions I managed to pass the autograder by trying enough random seeds and just submitting the best seeds. The professor, Sergey Levine, offered little help, admitting that RL agents are extremely hard to debug.\nOne day after the assignment deadline, the professor announced that there was a critical one-line bug in the starter code: The correct algorithm should train the model with past game frames in a random order, but the given code always give them in the FIFO order. With the fix, the learning curve would smoothly sigmoid to 350.\n\nThe Neural Net Tank Urban Legend\nA large list of examples in The Neural Net Tank Urban Legend · Gwern.net. I have a few more.\nAccording to Sejnowski, Takeo Kanade did work on detecting tanks in images. This is unconfirmed. I have looked for “Artificial Intelligence Vision: Progress and Non-Progress”, but it is not available online. I looked for your doctoral dissertation of 1974, but it contains only facial recognition. I also cannot find anything about detecting tanks in his publication list.\n\nIn his talk “Artificial Intelligence Vision: Progress and Non-Progress,” Takeo Kanade (from Carnegie Mellon) noted that computer memories back in the 1960s were tiny by today’s standards and could hold only one image at a time. For his doctoral dissertation in 1974, Takeo had shown that, though his program could find a tank in one image, it was too difficult for it to do so in other images where the tank was in a different position and the lighting was different. But, by the time his early students graduated, the programs they designed could recognize tanks under more general conditions because computers were more powerful. Today his students’ programs can recognize tanks in any image. The difference is that today we have access to millions of images that sample a wide range of poses and lighting conditions, and computers are millions of times more powerful. (Sejnowski 2018, 256)\n\nThere was not a lot of actual research on tank recognition. (Kanal and Randall 1964) contains some good pictures. The network was a two-layered perceptron network, of type \\(\\mathbb{R}^{N \\times N} \\to \\{0, 1\\}^{32\\times 32} \\to \\{0, 1\\}^{24} \\to \\{0, 1\\}\\). It works as follows:\n\nThe grayscale photo is down-scaled and binarized by convolution with a discrete Laplace filter: \\(\\mathbb{R}^{N \\times N} \\to \\{0, 1\\}^{32\\times 32}\\).\nThe weights for the 24 hidden perceptrons are constructed by linear discriminant analysis: \\(\\{0, 1\\}^{32\\times 32} \\to \\{0, 1\\}^{24}\\)\nThe output perceptron is learned by the perceptron learning rule: \\(\\{0, 1\\}^{24} \\to \\{0, 1\\}\\).\n\n\nFigure 1: Images from (Kanal and Randall 1964).\n\n\n\n\n\n\n(a) Grayscale photos, some containing tanks, and some not.\n\n\n\n\n\n\n\n(b) A picture of a tank after convolution with a discrete Laplace filter.\n\n\n\n\n\n\n\n\n\n(c) The architecture of the network."
  },
  {
    "objectID": "blog/posts/neural-network-scrapbook/index.html#the-second-neural-network-winter",
    "href": "blog/posts/neural-network-scrapbook/index.html#the-second-neural-network-winter",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "The second neural network winter",
    "text": "The second neural network winter\nThe first neural network winter started around 1965, when the main research centers pivoted away from neural networks: the Stanford Research Institute group turned to symbolic AI; the Bernard Widrow group turned to using single neurons as adaptive filters; the Frank Rosenblatt group died from lack of funds and then the literal death of Rosenblatt in 1971. It rose again around 1985, when backpropagation and improved compute allowed researchers to train neural networks on the order of \\(10^4\\) parameters and \\(4\\) layers.\nSomething strange happened during the 1990 – 2010 period: the neural network research community silently disappeared again for another 20 years. Unlike the previous case, there was no great mythology or drama about this winter, no Perceptron controversy.\nI would like to find out why.\n\nLukas: So I remember Daphne Koller telling me, maybe 2003, that the kind of state-of-the-art handwriting systems were neural nets, but that it was such an ad hoc kind of system that we shouldn’t focus on it. And I wonder if maybe I should have paid more attention to that and tried harder to make neural nets work for the applications I was doing.\nPeter: Yeah, me too. And certainly Yann LeCun had success with the digit database, and I think that was over-engineered in that they looked at exactly the features they needed for that set of digitizations of those digits. And in fact, I remember researchers talking about, “Well, what change are we going to do for sample number 347?” Right?\nLukas: Oh, really? Okay.\nPeter: There were individual data points that they would perform theories on, so that was definitely over-tuning to the data. And it should have been an indication that was a good approach. It was better than other approaches at the time.\nLukas: I guess so. Although that does sound like damming level of over-fitting the data, I suppose.\nPeter: Right. There was only a couple thousand data points. I forget exactly how many. Maybe it was 10,000. Maybe it was even 100,000, but it wasn’t many. (Norvig 2021)"
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html",
    "href": "blog/posts/ai-creativity/index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "An interesting strand of argument runs through discussions on AI: whether they can be creative or not; whether we should build them to be creative or not.\nDuring an online discussion, I explained to someone who is confused about why people are building these art machines that are “stealing art”. I explained the technological-scientific perspective on art. Their argument ran as follows:\n\nMachines cannot be creative. The apparent creativity is fake and mere copying.\nPeople build machines not because they want to make art, but because of something else. Perhaps a greed for money, a hate of artists, or some other nefarious motivation (the post is a bit vague on the precise motivation).\n\nI tried to explain the very different perspective on the other side of the cultural divide, so that they might understand, if not to accept:\n\nThere is no “magic”. Art might feel impossible to build a machine for, but we can.\nWhat I cannot create, I do not understand. (Feynman quote)\nIntrospection is unreliable. Asking artists how art was “really made” is not a reliable way to understand art.\nThus, we can build art machines, and we want to, if we are to ever understand art.\n\nThey repeated the same arguments, but more vehemently.\nIt was frustrating, though I was not surprised. This incident started my thinking: This is such a common response, that there is probably a psychological mechanism behind it. This essay describes some possible mechanisms."
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#introduction",
    "href": "blog/posts/ai-creativity/index.html#introduction",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "An interesting strand of argument runs through discussions on AI: whether they can be creative or not; whether we should build them to be creative or not.\nDuring an online discussion, I explained to someone who is confused about why people are building these art machines that are “stealing art”. I explained the technological-scientific perspective on art. Their argument ran as follows:\n\nMachines cannot be creative. The apparent creativity is fake and mere copying.\nPeople build machines not because they want to make art, but because of something else. Perhaps a greed for money, a hate of artists, or some other nefarious motivation (the post is a bit vague on the precise motivation).\n\nI tried to explain the very different perspective on the other side of the cultural divide, so that they might understand, if not to accept:\n\nThere is no “magic”. Art might feel impossible to build a machine for, but we can.\nWhat I cannot create, I do not understand. (Feynman quote)\nIntrospection is unreliable. Asking artists how art was “really made” is not a reliable way to understand art.\nThus, we can build art machines, and we want to, if we are to ever understand art.\n\nThey repeated the same arguments, but more vehemently.\nIt was frustrating, though I was not surprised. This incident started my thinking: This is such a common response, that there is probably a psychological mechanism behind it. This essay describes some possible mechanisms."
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#the-self-interest-theory",
    "href": "blog/posts/ai-creativity/index.html#the-self-interest-theory",
    "title": "Yuxi on the Wired",
    "section": "The self-interest theory",
    "text": "The self-interest theory\nThe self-interest theory is as follows: “It is hard to get someone to understand something if something they care about depends on their not understanding it.”"
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#the-non-truth-theory",
    "href": "blog/posts/ai-creativity/index.html#the-non-truth-theory",
    "title": "Yuxi on the Wired",
    "section": "The non-truth theory",
    "text": "The non-truth theory\nThe non-truth theory states that some arguments are forever mired in the same controversies, always rehashing the same arguments, because there is no truth to be found underneath the arguments.\nThere are certain social functions that are best served by saying something in language that looks like they talk about objective things. You can think of this as a hack in the programming language of humans. For example,\nThere are some social functions that"
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#why-it-seems-terrible-that-machines-might-create",
    "href": "blog/posts/ai-creativity/index.html#why-it-seems-terrible-that-machines-might-create",
    "title": "Yuxi on the Wired",
    "section": "Why it seems terrible that machines might create",
    "text": "Why it seems terrible that machines might create\nAt the “immortal dinner party” held by Benjamin Haydon on 28 December 1817, the Romantic poet John Keats agreed with Charles Lamb that Newton “had destroyed all the poetry of the rainbow, by reducing it to the prismatic colors”. Later, Keats wrote “Lamia” that included these famous lines:\nDo not all charms fly\nAt the mere touch of cold philosophy?\nThere was an awful rainbow once in heaven:\nWe know her woof, her texture; she is given\nIn the dull catalogue of common things.\nPhilosophy will clip an Angel's wings,\nConquer all mysteries by rule and line,\nEmpty the haunted air, and gnomed mine—\nUnweave a rainbow, as it erewhile made\nThe tender-person'd Lamia melt into a shade\nGPT4: Keats came up with the concept of “negative capability.” This is the ability to dwell in uncertainties, mysteries, doubts, without any compulsive reaching after fact and reason. Keats valued this ability, arguing that it was central to a poet’s creative process."
  },
  {
    "objectID": "blog/posts/ai-creativity/index.html#why-it-seems-incredible-that-machines-might-create",
    "href": "blog/posts/ai-creativity/index.html#why-it-seems-incredible-that-machines-might-create",
    "title": "Yuxi on the Wired",
    "section": "Why it seems incredible that machines might create",
    "text": "Why it seems incredible that machines might create\nHere, the arguments are easier to classify. It seems that there are several common mental models that people use when they think about machines that create. Using any of these would make it seem obvious that machines cannot be creative. So, I just need to classify the mental models!\n\nMachines as monkeys typing randomly\nIn Gulliver’s Travels (1726) by Jonathan Swift, there was a writing machine. It is a 16x16 matrix of little square blocks, with a character on each side. To use it, you turn the 32 handles randomly, then read out the few words that appeared by chance. This allowed:\n\nthe most ignorant person, at a reasonable charge, and with a little bodily labour, might write books in philosophy, poetry, politics, laws, mathematics, and theology, without the least assistance from genius or study.\n\nIt is a clear satire, possibly of Ramon Llull’ s Thinking Machine (3 concentric rotating disks that generate all possible theological arguments):\n\nThe first of these features means that all of these attributes are inherent; the second, that they are systematically interrelated in such a way as to affirm, with impeccable orthodoxy, that glory is eternal or that eternity is glorious; that power is true, glorious, good, great, eternal, powerful, wise, free, and virtuous, or benevolently great, greatly eternal, eternally powerful, powerfully wise, wisely free, freely virtuous, virtuously truthful, etc., etc.\n\n\n\nMachines as pipes for the water of creativity\n\nIt appears to me that if one wants to make progress in mathematics one should study the masters and not the pupils.\n\n— N.H. Abel (1802–1829), quoted from an unpublished source by O. Ore in Niels Henrik Abel, Mathematician Extraordinary, p. 138.\nThere is a common attitude that I can summarize as this: Like drawing water from the unsullied source at the mountain’s peak, so is the experience of returning to the writings of the masters: clear, refreshing, and devoid of later impurities.\n\nAncient Greek theory of creativity\nIn ancient Greece, the Muses were considered the source of the knowledge embodied in the poetry, lyric songs, and myths that were related orally for centuries in ancient Greek culture. Homer began his Iliad with:\n\nSing, Muse, the fatal wrath of Peleus’ son,\nWhich to the Greeks unnumb’red evils brought,\n\nNote that the Muses was doing the real singing, and Homer was a channel for their singing (back then, poetry was sang – the Iliad was written down only after a few centuries). In Plato’s dialog Ion, Socrates (perhaps a sockpuppet of Plato) argued that “it is not by art that poets compose… but by divine apportionment”:\n\nFor the poets tell us that they carry honey to us from every quarter like bees, and they fly as bees do, sipping from honey-flowing fountains in glens and gardens of the Muses. And they tell the truth. For a poet is a delicate thing, winged and sacred, and unable to create until he becomes inspired and frenzied, his mind no longer in him; as long as he keeps his hold on that, no man can compose or chant prophecy. Since, then, it is not by art that poets compose and say many beautiful things about their subjects, as you do about Homer, but by divine apportionment, they each can do well only that to which the Muse directs them-this one dithyrambs, that one odes, or encomia, or dances, or epics, or iambics-each of them worthless in respect to the others.\n\nThe same point was made repeatedly in Plato’s corpus.\n\nJust as the rhapsode says what he says about Homer not by art but by divine apportionment, without intelligence (Ion 534b-c, 536c, 542a), so in the Meno (gge-looa) politicians get their virtue by divine apportionment, without intelligence; they have no more wisdom than seers and soothsayers, who say many fine things but know nothing of what they say; politicians are divine and inspired like poets, and possessed by the god (Meno 9gb-e). The irrational effects of poetry and rhapsody are directly comparable to the irrational effect of vulgar politics, whose servant is vulgar rhetoric (cf. Gorgias 502C).\n\nBoth quotes came from The Dialogues of Plato, Volume 3: Ion, Hippias Minor, Laches, Protagoras, translated by R. Allen. (I decided not to use one of the freely available versions since they tended to mistranslate “gods” as “God”.)\nFor example, in Phaedrus 245a, Socrates claimed that “the poetry of the sane man vanishes into nothingness before that of the inspired madmen”:\n\nAnd a third kind of possession and madness comes from the Muses. This takes hold upon a gentle and pure soul, arouses it and inspires it to songs and other poetry, and thus by adorning countless deeds of the ancients educates later generations. But he who without the divine madness comes to the doors of the Muses, confident that he will be a good poet by art, meets with no success, and the poetry of the sane man vanishes into nothingness before that of the inspired madmen.\n\n\n\nLater manifestations\nIsaac Newton thought he was merely recovering what the ancients have known all along. His friend William Stukeley described Newton as “the Great Restorer of True Philosophy”.\n\n\nApplication to machine creativity\n\n\n\nMachines as flowers for the DNA of creativity\nFrom Lovelace’s “Notes by the Translator”:\n\nThe Analytical Engine has no pretensions whatever to originate any thing. It can do whatever we know how to order it to perform. (source)\n\nIn his seminal paper “Computing machinery and intelligence” (1950), Alan Turing referenced Lovelace’s observation as the sixth objection to the possibility that machines might think. He then objected:\n\nThe view that machines cannot give rise to surprises is due, I believe, to a fallacy to which philosophers and mathematicians are particularly subject. This is the assumption that as soon as a fact is presented to a mind all consequences of that fact spring into the mind simultaneously with it. It is a very useful assumption under many circumstances, but one too easily forgets that it is false. (source).\n\nTuring was led to Lovelace’s objection by debates with Douglas Hartree, who in his book “Calculating Instruments and Machines” (page 70, 1949), quoted Lovelace approvingly. He objected using the phrase “electronic brain” for devices like electronic calculating machines or automatic pilots. He clarified that these machines cannot “think for themselves” and can only execute the instructions provided to them.\nThus, machines, in this view, are akin to flowers—organisms that reproduce and grow according to a predetermined genetic code but do not originate new genetic information on their own. Creativity, like DNA, must be instilled by a designer or operator, who programs the machine with the “genetic code” of what to create.\nAs a short etymological fun fact, the word “development” is a little capsule of the “flower for the DNA” idea:\n\nFirst use 1756, from French développement (“unrolling”). Compare with envelopment (“rolling”).\n\nThe idea is that of “opening up a scroll and showing what has always been written there. In the machines’ creative process can be seen as a similar”unrolling” of predetermined instructions or codes, much like the genetic “unrolling” in a blooming flower.\nThis is most explicitly manifest in the idea of preformationism, prevalent around 17th to 18th century. It seems the same intuitive appeals of preformationism apply to Lovelace’s objection.\n(A brief personal anecdote: When I was a kid, I thought bus cards contained tiny compressed coins inside, and when you “beep” them, those tiny coins fall into the machine through tiny openings on the card. Preformationism in economics!)"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "",
    "text": "The essay is written at the level of two years of undergraduate mathematics. I will keep jargons to a minimum and use as few infinities as possible. For example, instead of particles that can be anywhere on a real-number line, I would talk about particles that can be in one of three boxes."
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#many-world-theory",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#many-world-theory",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Many-world theory",
    "text": "Many-world theory"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#pilot-wave-theory",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#pilot-wave-theory",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Pilot wave theory",
    "text": "Pilot wave theory"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#copenhagen-interpretation",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#copenhagen-interpretation",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Copenhagen interpretation",
    "text": "Copenhagen interpretation"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#relational-quantum-mechanics",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#relational-quantum-mechanics",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "Relational quantum mechanics",
    "text": "Relational quantum mechanics"
  },
  {
    "objectID": "blog/posts/quantum-mechanics-interpretations/index.html#qbism",
    "href": "blog/posts/quantum-mechanics-interpretations/index.html#qbism",
    "title": "Mathematical Interpretations of Quantum Mechanics",
    "section": "QBism",
    "text": "QBism"
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html",
    "href": "blog/posts/perplexity-turing-test/index.html",
    "title": "When will AI pass the Turing Test?",
    "section": "",
    "text": "Alternative title: How much would it cost to train the first AI scientist?\nI encourage you to play with the Direct Approach Interactive Model to explore a mathematical model behind this."
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html#turing-test-as-statistical-hypothesis-testing",
    "href": "blog/posts/perplexity-turing-test/index.html#turing-test-as-statistical-hypothesis-testing",
    "title": "When will AI pass the Turing Test?",
    "section": "Turing test as statistical hypothesis testing",
    "text": "Turing test as statistical hypothesis testing\n\nSequential hypothesis test\nWe need the following equality:\n\\[\\frac 1n E_{X \\sim P(\\cdot | H_0)}\\left[ \\ln\\frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)}\\right] = \\frac 1n\nE_{X \\sim P(\\cdot | H_0)}\\left[ \\ln\\frac{1}{P(X_{1:n}|H_1)}\\right] - \\frac 1n  E_{X \\sim P(\\cdot | H_0)}\\left[ \\frac{1}{\\ln P(X_{1:n}|H_0)}\\right]\\]\nExplain what each of the three terms of the equality mean. Suppose the machine is a perfect replica of the human, what would each term be?\n\\[\\underbrace{\\frac 1n E_{X \\sim P(\\cdot | H_0)}\\left[ \\ln\\frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)}\\right]}_{\\frac 1n D_{KL}(P(\\cdot | H_0)\\| P(\\cdot | H_1)) } = \\underbrace{\\frac 1n\nE_{X \\sim P(\\cdot | H_0)}\\left[ \\ln\\frac{1}{P(X_{1:n}|H_1)}\\right]}_{\\text{negative log-likelihood loss per token}} - \\underbrace{\\frac 1n  E_{X \\sim P(\\cdot | H_0)}\\left[ \\frac{1}{\\ln P(X_{1:n}|H_0)}\\right]}_{\\text{entropy rate of the human itself}}\\]\nThe first term is the KL-divergence between the machine and the human, as two sources of symbolic strings, averaged over sequence length. Roughly speaking, it is how different they are, per symbol emitted. It is an information-theoretic quantity.\nThe second term is negative log-likelihood loss per token. This is what the language model is trained to minimize.\nThe third term is the entropy rate of the human. It is how random the human is, as a source of symbols.\nIf the machine is a perfect replica of the human, then the second term is zero, and the first term equals the third term.\n\\[T^* \\approx \\frac{\\ln 10}{L - L_\\infty}\\]\nHere, each token on average moves the log-probability-ratio away from 0 by another \\((L-L_\\infty)\\). Decision is triggered when it finally moves out of the interval \\([-\\ln 10, \\ln 10]\\).\nWe are not able to simply look at a few tokens, draw a straight line, and call it a day, because the trajectory of log-probability-ratio is much closer to a random walk with drift. Subjectively, if you were a judge and watching the log-probability-ratio moving, you’d see ups and downs, keeping you in suspense, until it finally crosses the decision boundaries.}\n\n\nSlowdown factor\nHumans are not perfect judges. However, as a crude approximation, we can model them as a slowed-down version of the perfect judge. Specifically, if it takes about \\(T\\) tokens for the perfect judge to reach a likelihood ratio of \\(r\\), it would take about \\(sT\\) tokens for a human judge. Here, \\(s\\) is a number larger than one.\nWe do not have good data on what \\(s\\) is, or whether it is even consistently measurable. However, for our current question, let’s assume \\(s=10\\).\nWhat should the language model’s \\(L-L_\\infty\\) be, before it can pass the Turing test against a human judge for 1000 tokens?\n\\[10 \\times \\ln 10 / 1000 = 0.023\\]\nHow do we estimate the slowdown factor?\n(Jannai et al. 2023)\n\n\nEntropy of natural languages\nWe found that \\(L_\\infty\\) should be interpreted as the intrinsic entropy of the source material. In this case, it is the entropy of natural English. Now, the intrinsic entropy of English is not very easy to estimate, but there had been several attempts.\nThe earliest attempt is by Shannon himself, in 1951. He estimated that the entropy of English is about 0.6 – 1.3 bits per character. Now, we cannot use this number directly, because it is not in the right units – loglikelihood loss is in units of nat/token.\nThe conversion between nat and bit is known exactly: \\(1 \\;\\mathrm{nat} = \\ln(2)\\;\\mathrm{nat}= 0.693\\;\\mathrm{nat}\\). The conversion between character and token can be estimated by running a tokenizer over a large natural English corpus. I have estimated this by running the GPT-2 tokenizer on WikiText2 corpus, and found that on average, 0.22 tokens/character, 1.17 tokens/word.\nWhat is Shannon’s estimated entropy of English, in units of nat/token?\n\\[\\ln 2 \\times [0.6, 1.3] / 0.22 = [1.89, 4.09]\\;\\mathrm{nat/token}\\]\nAnother way to estimate is by attempting to compress a large natural English corpus. It is known in basic information theory that the entropy rate is the lower bound on compression rate. In more detail, if you have a source of information emitting symbols, and its symbol stream has an entropy rate of \\(x \\;\\mathrm{bit/symbol}\\), then it takes \\(xl\\) bits to encode a long segment \\(l\\) symbols long.\nThe Hutter prize is a competition for compressing a \\(10^9\\)-byte segment of the English Wikipedia as much as possible. The competition has been ongoing since 2005.\nThe zipped file is only about 300 Mb in size, meaning that the total entropy in the corpus is no more than \\(3\\times 10^8\\) bytes.\nOver the years, the progress has been slow but somewhat steady. If we extrapolate the prize-winning entries over the years, we see that the best possible compression ratio seems to be about 10x.\nI ran the GPT-2 tokenizer through 1/100 of the dataset, and got 2.96 million tokens. So the entire dataset should have about 300 million tokens. Assuming this, what is the entropy of English, in units of nat/token?\nIf we assume the maximal compression ratio of 10x, then the corpus contains entropy\n\\[10^8\\;\\mathrm{byte} = 8\\times 10^8 \\;\\mathrm{bit} = 5.55\\times 10^8 \\;\\mathrm{nat}\\]\n\\[\\frac{5.55\\times 10^8}{3\\times 10^8}= 1.85 \\;\\mathrm{nat/token}\\]\nAnd the third way is to look at what the scaling laws for the largest language models imply what \\(L_\\infty\\) is. According to page 25 of “Training Compute-Optimal Large Language Models” (2022) (hereafter “Chinchilla scaling law”), \\(L_\\infty = 1.69\\;\\mathrm{nat/token}\\).\nThe estimate by compression and language modelling are remarkably close.\nShannon’s estimate of entropy is above the other two estimates by about 2x. Considering that he measured this by asking volunteers to just look at a small passage of text and keep guessing the next character, we can consider his estimate a noisy estimate biased to be too high (since the volunteers would not be able to extract the very last predictabilities of English by only looking at a small preceding passage).\nCompute scaling law\nThe “Chinchilla scaling law” paper reported a series of training runs on language models, trained by Google DeepMind researchers. According to them, if we have a fixed amount of computing budget \\(C\\) (in units of FLOP), by choosing the model and dataset size correctly, the minimal reducible loss achievable is\n\\[L - L_\\infty = \\frac{1070}{C^{0.154}}\\]\nAssuming slowdown factor \\(s=10\\), and Chinchilla scaling law, then, we have a direct method to predict how long a language model can pass a Turing test, according to how many FLOPs it cost to compute:\n\\[T^* \\sim \\frac{10\\ln 10}{1070}C^{0.154}\\]\nThis gives, as a rule of thumb, 100x compute means 2x length of Turing test.\nIf GPT-4 costs 2e25 FLOP in compute, for how many words can it pass the Turing test? Assume 1 word is 1.2 tokens, as described previously.\n\\[T^* \\approx 170 \\text{ tokens} \\approx 150 \\text{ words}\\] meaning it has a good chance of passing the Turing test if limited to only 150 words. For context, the Attention is All You Need paper has an abstract that’s 200 tokens long.\nA typical scientific paper is about 4000 words long. How much training compute would it cost, in units of of GPT-4? Assuming GPT-4 cost 10 million USD to train, how much money would this model cost to train?\n4000 words is 27x more than 150 words, so it would need \\(27^{1/0.153} = 2e9\\) amount of compute. Assuming 1 GPT-4 cost 10 million USD, that would cost 2e16 USD, or 200 years of global GDP (2023).\nSo it seems like it would take a very long time to train the first AI scientist by just scaling up the GPT-like models. We need more than pure language modelling.\nI leave you with the inspirational quote from Edward “the Bomb” Teller:\n\nThe possibilities of developing an atomic weapon and the desirability of doing it secretly were discussed at a Princeton University conference in which I participated in March 1939… Bohr said this rare variety could not be separated from common uranium except by turning the country into a gigantic factory. Bohr was worried that this could be done and that an atomic bomb could be developed—but he hoped that neither could be accomplished. Years later, when Bohr came to Los Alamos, I was prepared to say, “You see…” But before I could open my mouth, he said: “You see, I told you it couldn’t be done without turning the whole country into a factory. You have done just that.” (Teller and Brown 1975)"
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html#sequential-hypothesis-test",
    "href": "blog/posts/perplexity-turing-test/index.html#sequential-hypothesis-test",
    "title": "When will AI pass the Turing Test?",
    "section": "Sequential hypothesis test",
    "text": "Sequential hypothesis test\nWe need the following equality:\n\\[\\frac 1n E_{X \\sim P(\\cdot | H_0)}\\left[ \\ln\\frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)}\\right] = \\frac 1n\nE_{X \\sim P(\\cdot | H_0)}\\left[ \\ln\\frac{1}{P(X_{1:n}|H_1)}\\right] - \\frac 1n  E_{X \\sim P(\\cdot | H_0)}\\left[ \\frac{1}{\\ln P(X_{1:n}|H_0)}\\right]\\]\nExplain what each of the three terms of the equality mean. Suppose the machine is a perfect replica of the human, what would each term be?\n\\[\\underbrace{\\frac 1n E_{X \\sim P(\\cdot | H_0)}\\left[ \\ln\\frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)}\\right]}_{\\frac 1n D_{KL}(P(\\cdot | H_0)\\| P(\\cdot | H_1)) } = \\underbrace{\\frac 1n\nE_{X \\sim P(\\cdot | H_0)}\\left[ \\ln\\frac{1}{P(X_{1:n}|H_1)}\\right]}_{\\text{negative log-likelihood loss per token}} - \\underbrace{\\frac 1n  E_{X \\sim P(\\cdot | H_0)}\\left[ \\frac{1}{\\ln P(X_{1:n}|H_0)}\\right]}_{\\text{entropy rate of the human itself}}\\]\nThe first term is the KL-divergence between the machine and the human, as two sources of symbolic strings, averaged over sequence length. Roughly speaking, it is how different they are, per symbol emitted. It is an information-theoretic quantity.\nThe second term is negative log-likelihood loss per token. This is what the language model is trained to minimize.\nThe third term is the entropy rate of the human. It is how random the human is, as a source of symbols.\nIf the machine is a perfect replica of the human, then the second term is zero, and the first term equals the third term.\n\\[T^* \\approx \\frac{\\ln 10}{L - L_\\infty}\\]\nHere, each token on average moves the log-probability-ratio away from 0 by another \\((L-L_\\infty)\\). Decision is triggered when it finally moves out of the interval \\([-\\ln 10, \\ln 10]\\).\nWe are not able to simply look at a few tokens, draw a straight line, and call it a day, because the trajectory of log-probability-ratio is much closer to a random walk with drift. Subjectively, if you were a judge and watching the log-probability-ratio moving, you’d see ups and downs, keeping you in suspense, until it finally crosses the decision boundaries.}\n\nSlowdown factor\nHumans are not perfect judges. However, as a crude approximation, we can model them as a slowed-down version of the perfect judge. Specifically, if it takes about \\(T\\) tokens for the perfect judge to reach a likelihood ratio of \\(r\\), it would take about \\(sT\\) tokens for a human judge. Here, \\(s\\) is a number larger than one.\nWe do not have good data on what \\(s\\) is, or whether it is even consistently measurable. However, for our current question, let’s assume \\(s=10\\).\nWhat should the language model’s \\(L-L_\\infty\\) be, before it can pass the Turing test against a human judge for 1000 tokens?\n\\[10 \\times \\ln 10 / 1000 = 0.023\\]\nHow do we estimate the slowdown factor?\n(Jannai et al. 2023)"
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html#entropy-of-natural-languages",
    "href": "blog/posts/perplexity-turing-test/index.html#entropy-of-natural-languages",
    "title": "When will AI pass the Turing Test?",
    "section": "Entropy of natural languages",
    "text": "Entropy of natural languages\nWe found that \\(L_\\infty\\) should be interpreted as the intrinsic entropy of the source material. In this case, it is the entropy of natural English. Now, the intrinsic entropy of English is not very easy to estimate, but there had been several attempts.\nThe earliest attempt is by Shannon himself, in 1951. He estimated that the entropy of English is about 0.6 – 1.3 bits per character. Now, we cannot use this number directly, because it is not in the right units – loglikelihood loss is in units of nat/token.\nThe conversion between nat and bit is known exactly: \\(1 \\;\\mathrm{nat} = \\ln(2)\\;\\mathrm{nat}= 0.693\\;\\mathrm{nat}\\). The conversion between character and token can be estimated by running a tokenizer over a large natural English corpus. I have estimated this by running the GPT-2 tokenizer on WikiText2 corpus, and found that on average, 0.22 tokens/character, 1.17 tokens/word.\nWhat is Shannon’s estimated entropy of English, in units of nat/token?\n\\[\\ln 2 \\times [0.6, 1.3] / 0.22 = [1.89, 4.09]\\;\\mathrm{nat/token}\\]\nAnother way to estimate is by attempting to compress a large natural English corpus. It is known in basic information theory that the entropy rate is the lower bound on compression rate. In more detail, if you have a source of information emitting symbols, and its symbol stream has an entropy rate of \\(x \\;\\mathrm{bit/symbol}\\), then it takes \\(xl\\) bits to encode a long segment \\(l\\) symbols long.\nThe Hutter prize is a competition for compressing a \\(10^9\\)-byte segment of the English Wikipedia as much as possible. The competition has been ongoing since 2005.\nThe zipped file is only about 300 Mb in size, meaning that the total entropy in the corpus is no more than \\(3\\times 10^8\\) bytes.\nOver the years, the progress has been slow but somewhat steady. If we extrapolate the prize-winning entries over the years, we see that the best possible compression ratio seems to be about 10x.\nI ran the GPT-2 tokenizer through 1/100 of the dataset, and got 2.96 million tokens. So the entire dataset should have about 300 million tokens. Assuming this, what is the entropy of English, in units of nat/token?\nIf we assume the maximal compression ratio of 10x, then the corpus contains entropy\n\\[10^8\\;\\mathrm{byte} = 8\\times 10^8 \\;\\mathrm{bit} = 5.55\\times 10^8 \\;\\mathrm{nat}\\]\n\\[\\frac{5.55\\times 10^8}{3\\times 10^8}= 1.85 \\;\\mathrm{nat/token}\\]\nAnd the third way is to look at what the scaling laws for the largest language models imply what \\(L_\\infty\\) is. According to page 25 of “Training Compute-Optimal Large Language Models” (2022) (hereafter “Chinchilla scaling law”), \\(L_\\infty = 1.69\\;\\mathrm{nat/token}\\).\nThe estimate by compression and language modelling are remarkably close.\nShannon’s estimate of entropy is above the other two estimates by about 2x. Considering that he measured this by asking volunteers to just look at a small passage of text and keep guessing the next character, we can consider his estimate a noisy estimate biased to be too high (since the volunteers would not be able to extract the very last predictabilities of English by only looking at a small preceding passage)."
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html#compute-scaling",
    "href": "blog/posts/perplexity-turing-test/index.html#compute-scaling",
    "title": "When will AI pass the Turing Test?",
    "section": "Compute scaling",
    "text": "Compute scaling\nThe “Chinchilla scaling law” paper reported a series of training runs on language models, trained by Google DeepMind researchers. According to them, if we have a fixed amount of computing budget \\(C\\) (in units of FLOP), by choosing the model and dataset size correctly, the minimal reducible loss achievable is\n\\[L - L_\\infty = \\frac{1070}{C^{0.154}}\\]"
  },
  {
    "objectID": "blog/posts/perplexity-turing-test/index.html#forecasting-agi",
    "href": "blog/posts/perplexity-turing-test/index.html#forecasting-agi",
    "title": "When will AI pass the Turing Test?",
    "section": "Forecasting AGI",
    "text": "Forecasting AGI\nAssuming slowdown factor \\(s=10\\), and Chinchilla scaling law, then, we have a direct method to predict how long a language model can pass a Turing test, according to how many FLOPs it cost to compute:\n\\[T^* \\sim \\frac{10\\ln 10}{1070}C^{0.154}\\]\nThis gives, as a rule of thumb, 100x compute means 2x length of Turing test.\nIf GPT-4 costs 2e25 FLOP in compute, for how many words can it pass the Turing test? Assume 1 word is 1.2 tokens, as described previously.\n\\[T^* \\approx 170 \\text{ tokens} \\approx 150 \\text{ words}\\] meaning it has a good chance of passing the Turing test if limited to only 150 words. For context, the Attention is All You Need paper has an abstract that’s 200 tokens long.\nA typical scientific paper is about 4000 words long. How much training compute would it cost, in units of of GPT-4? Assuming GPT-4 cost 10 million USD to train, how much money would this model cost to train?\n4000 words is 27x more than 150 words, so it would need \\(27^{1/0.153} = 2e9\\) amount of compute. Assuming 1 GPT-4 cost 10 million USD, that would cost 2e16 USD, or 200 years of global GDP (2023).\nSo it seems like it would take a very long time to train the first AI scientist by just scaling up the GPT-like models. We need more than pure language modelling.\nI leave you with the inspirational quote from Edward “the Bomb” Teller:\n\nThe possibilities of developing an atomic weapon and the desirability of doing it secretly were discussed at a Princeton University conference in which I participated in March 1939… Bohr said this rare variety could not be separated from common uranium except by turning the country into a gigantic factory. Bohr was worried that this could be done and that an atomic bomb could be developed—but he hoped that neither could be accomplished. Years later, when Bohr came to Los Alamos, I was prepared to say, “You see…” But before I could open my mouth, he said: “You see, I told you it couldn’t be done without turning the whole country into a factory. You have done just that.” (Teller and Brown 1975)"
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html#generating-and-measuring-data-manifold-dimensions",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html#generating-and-measuring-data-manifold-dimensions",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Generating and measuring data manifold dimensions",
    "text": "Generating and measuring data manifold dimensions\nConsider the space of all MNIST images. Each MNIST image is a 28x28 grayscale, so the total space is \\(\\mathbb R^{28\\times 28} = \\mathbb R^{784}\\).\nHowever, as you may have seen in experiments with the VAE, most of the MNIST dataset “collapses” onto a much smaller subset of \\(\\mathbb R^{784}\\). This is the “(intrinsic) data manifold”, with a dimension much smaller than \\(784\\). Indeed, the very fact that the intrinsic dimension is small allows us to have meaningful “2D slices” of the dataset:\n\n\n\nFigure from Convolutional Variational Autoencoder - Tensorflow Core Tutorials. More images can be found in Visualizing the Variational Autoencoder, and can be regenerated in about 10 minutes with a Google Colab GPU.\n\n\nReal data can be expensive, though, which is why we often use “toy” datasets with known dimensions, generated by a known random process. For example, the following is the “Swiss roll” dataset. It is generated by first populating a 2D square \\([0, 1]^2\\), then use a function \\(f: \\mathbb R^2 \\to \\mathbb R^3\\) to “roll up” the square into 3D space.\n\n\n\nThe Isomap algorithm (Tenenbaum, Silva, and Langford 2000), popular for constructing data manifolds. Figure from Wikipedia."
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html#generating-data-manifold-by-random-neural-networks",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html#generating-data-manifold-by-random-neural-networks",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Generating data manifold by random neural networks",
    "text": "Generating data manifold by random neural networks\nConsider the simplest data manifold: \\(\\mathbb R^d\\), affinely transformed, then embedded in \\(\\mathbb R^n\\), with \\(n &gt; d\\).\nTo synthesize such a data manifold, we randomly initialize a teacher network, so-called because it implements the function that a student network will fit to by supervised training. Each teacher network is constructed thus:\n\nThe number of neurons in each layer are: \\([d, 9, 600, 600, 1]\\)\nIt has 0 bias.\nThe weights between layers are sampeled from \\(\\mathcal N(0, m^{-1/2})\\) , where \\(m\\) is the input size of the layer (a form of He initialization).\nThe activation function at the second (with 9 neurons) and last layers are identity. All other activation functions are ReLU.\n\nOnce we have constructed a teacher network, we use it to generate a dataset \\(\\{(x_i, y_i)\\}_i\\) thus:\n\nGenerate random gaussian vectors \\(\\{t_i\\}_i\\) in \\(\\mathbb R^d\\), with mean \\(0\\) and std \\(I_{d\\times d}\\).\nFor each \\(t \\in \\{t_i\\}_i\\), push \\(t\\) through the teacher network.\nLet \\(x \\in \\mathbb R^9\\) be the teacher network activation at the second layer, with 9 neurons.\nLet \\(y \\in \\mathbb R\\) be the teacher network output."
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html#learning-data-manifold-by-neural-networks",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html#learning-data-manifold-by-neural-networks",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Learning data manifold by neural networks",
    "text": "Learning data manifold by neural networks\nAccording to the theory, if the data manifold has dimension \\(d\\), then as we scale up a neural network with \\(N\\) parameters, the MSE loss of a fully-trained network would scale like \\(L \\sim N^{-\\alpha}\\), where \\(\\alpha \\approx 4/d\\).\nLet’s test this.\nFirst, we define the “student” neural network architecture:\n\nThe number of neurons in each layer are: \\([9, n, n, 1]\\).\nThe biases are initialized to 0.\nThe weights between layers are sampeled from \\(\\mathcal N(0, m^{-1/2})\\), where \\(m\\) is the input size of the layer (a form of He initialization).\nAll activation functions are ReLU.\n\nA simple computation shows that the network has exactly \\(N = n^2+12n + 1\\) parameters11 \\(N = \\underbrace{(n+n+1)}_{\\text{first layer}} + \\underbrace{(9n + n^2 + n)}_{\\text{second layer}}\\)\n\n\n\nAn example teacher network architecture with \\([9, 5, 5, 1]\\) neurons."
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html#the-manifold-hypothesis",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html#the-manifold-hypothesis",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "The manifold hypothesis",
    "text": "The manifold hypothesis\nConsider the space of all MNIST images. Each MNIST image is a 28x28 grayscale, so the total space is \\(\\mathbb R^{28\\times 28} = \\mathbb R^{784}\\).\nHowever, as you may have seen in experiments with the VAE, most of the MNIST dataset “collapses” onto a much smaller subset of \\(\\mathbb R^{784}\\). This is the “(intrinsic) data manifold”, with a dimension much smaller than \\(784\\). Indeed, the very fact that the intrinsic dimension is small allows us to have meaningful “2D slices” of the dataset:\n\n\n\nFigure from Convolutional Variational Autoencoder - Tensorflow Core Tutorials. More images can be found in Visualizing the Variational Autoencoder, and can be regenerated in about 10 minutes with a Google Colab GPU.\n\n\nReal data can be expensive, though, which is why we often use “toy” datasets with known dimensions, generated by a known random process. For example, the following is the “Swiss roll” dataset. It is generated by first populating a 2D square \\([0, 1]^2\\), then use a function \\(f: \\mathbb R^2 \\to \\mathbb R^3\\) to “roll up” the square into 3D space.\n\n\n\nThe Isomap algorithm (Tenenbaum, Silva, and Langford 2000), popular for constructing data manifolds. Figure from Wikipedia."
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html#synthetic-data-manifolds",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html#synthetic-data-manifolds",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Synthetic data manifolds",
    "text": "Synthetic data manifolds\nConsider the simplest data manifold: \\(\\mathbb R^d\\), affinely transformed, then embedded in \\(\\mathbb R^n\\), with \\(n &gt; d\\).\nTo synthesize such a data manifold, we randomly initialize a teacher network, so-called because it implements the function that a student network will fit to by supervised training. Each teacher network is constructed thus:\n\nThe number of neurons in each layer are: \\([d, 9, 600, 600, 1]\\)\nIt has 0 bias.\nThe weights between layers are sampeled from \\(\\mathcal N(0, m^{-1/2})\\) , where \\(m\\) is the input size of the layer (a form of He initialization).\nThe activation function at the second (with 9 neurons) and last layers are identity. All other activation functions are ReLU.\n\nOnce we have constructed a teacher network, we use it to generate a dataset \\(\\{(x_i, y_i)\\}_i\\) thus:\n\nGenerate random gaussian vectors \\(\\{t_i\\}_i\\) in \\(\\mathbb R^d\\), with mean \\(0\\) and std \\(I_{d\\times d}\\).\nFor each \\(t \\in \\{t_i\\}_i\\), push \\(t\\) through the teacher network.\nLet \\(x \\in \\mathbb R^9\\) be the teacher network activation at the second layer, with 9 neurons.\nLet \\(y \\in \\mathbb R\\) be the teacher network output."
  },
  {
    "objectID": "blog/posts/scaling-law-by-data-manifold/index.html#experiments",
    "href": "blog/posts/scaling-law-by-data-manifold/index.html#experiments",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Experiments",
    "text": "Experiments"
  }
]