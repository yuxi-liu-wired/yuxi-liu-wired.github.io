<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxi Liu">
<meta name="dcterms.date" content="2024-07-01">

<title>Yuxi on the Wired - Statistical Mechanics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Yuxi on the Wired - Statistical Mechanics">
<meta property="og:description" content="Personal website of Yuxi Liu">
<meta property="og:image" content="https://yuxi-liu-wired.github.io/blog/posts/statistical-mechanics/img/blog icon.jpg">
<meta property="og:site_name" content="Yuxi on the Wired">
<meta name="twitter:title" content="Yuxi on the Wired - Statistical Mechanics">
<meta name="twitter:description" content="Personal website of Yuxi Liu">
<meta name="twitter:image" content="https://yuxi-liu-wired.github.io/blog/posts/statistical-mechanics/img/blog icon.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.qmd"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.qmd"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://bair.berkeley.edu/"> <i class="bi bi-folder-symlink" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi.liu.1995@gmail.com"> <i class="bi bi-envelope" role="img" aria-label="email">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Statistical Mechanics</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">math</div>
                <div class="quarto-category">physics</div>
                <div class="quarto-category">philosophy</div>
                <div class="quarto-category">probability</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxi Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 1, 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">July 1, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#quick-reference" id="toc-quick-reference" class="nav-link" data-scroll-target="#quick-reference">Quick reference</a></li>
  </ul></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a>
  <ul class="collapse">
  <li><a href="#philosophical-comments" id="toc-philosophical-comments" class="nav-link" data-scroll-target="#philosophical-comments">Philosophical comments</a></li>
  <li><a href="#principles-of-statistical-mechanics" id="toc-principles-of-statistical-mechanics" class="nav-link" data-scroll-target="#principles-of-statistical-mechanics">Principles of statistical mechanics</a></li>
  <li><a href="#sec-differential-entropy-ill-defined" id="toc-sec-differential-entropy-ill-defined" class="nav-link" data-scroll-target="#sec-differential-entropy-ill-defined">Differential entropy depends on coordinates choice</a></li>
  </ul></li>
  <li><a href="#mathematical-developments" id="toc-mathematical-developments" class="nav-link" data-scroll-target="#mathematical-developments">Mathematical developments</a>
  <ul class="collapse">
  <li><a href="#fundamental-theorems" id="toc-fundamental-theorems" class="nav-link" data-scroll-target="#fundamental-theorems">Fundamental theorems</a></li>
  <li><a href="#microcanonical-ensembles" id="toc-microcanonical-ensembles" class="nav-link" data-scroll-target="#microcanonical-ensembles">Microcanonical ensembles</a></li>
  <li><a href="#canonical-ensembles" id="toc-canonical-ensembles" class="nav-link" data-scroll-target="#canonical-ensembles">Canonical ensembles</a></li>
  <li><a href="#free-entropies" id="toc-free-entropies" class="nav-link" data-scroll-target="#free-entropies">Free entropies</a></li>
  <li><a href="#the-partition-function" id="toc-the-partition-function" class="nav-link" data-scroll-target="#the-partition-function">The partition function</a></li>
  <li><a href="#conditional-entropies" id="toc-conditional-entropies" class="nav-link" data-scroll-target="#conditional-entropies">Conditional entropies</a></li>
  </ul></li>
  <li><a href="#kinetic-gas-theory" id="toc-kinetic-gas-theory" class="nav-link" data-scroll-target="#kinetic-gas-theory">Kinetic gas theory</a>
  <ul class="collapse">
  <li><a href="#ideal-gas" id="toc-ideal-gas" class="nav-link" data-scroll-target="#ideal-gas">Ideal gas</a></li>
  <li><a href="#ideal-gas-again" id="toc-ideal-gas-again" class="nav-link" data-scroll-target="#ideal-gas-again">Ideal gas (again)</a></li>
  <li><a href="#hard-ball-gas-dilute-gas-limit" id="toc-hard-ball-gas-dilute-gas-limit" class="nav-link" data-scroll-target="#hard-ball-gas-dilute-gas-limit">Hard ball gas (dilute gas limit)</a></li>
  <li><a href="#soft-ball-gas-high-temperature-and-dilute-gas-limit" id="toc-soft-ball-gas-high-temperature-and-dilute-gas-limit" class="nav-link" data-scroll-target="#soft-ball-gas-high-temperature-and-dilute-gas-limit">Soft ball gas (high temperature and dilute gas limit)</a></li>
  </ul></li>
  <li><a href="#other-classical-examples" id="toc-other-classical-examples" class="nav-link" data-scroll-target="#other-classical-examples">Other classical examples</a>
  <ul class="collapse">
  <li><a href="#countably-many-states" id="toc-countably-many-states" class="nav-link" data-scroll-target="#countably-many-states">Countably many states</a></li>
  <li><a href="#fluctuation-by-n-12" id="toc-fluctuation-by-n-12" class="nav-link" data-scroll-target="#fluctuation-by-n-12">Fluctuation by <span class="math inline">\(N^{-1/2}\)</span></a></li>
  <li><a href="#blackbody-radiation" id="toc-blackbody-radiation" class="nav-link" data-scroll-target="#blackbody-radiation">Blackbody radiation</a></li>
  <li><a href="#sec-rubber-band" id="toc-sec-rubber-band" class="nav-link" data-scroll-target="#sec-rubber-band">Rubber bands</a></li>
  </ul></li>
  <li><a href="#combinatorial-examples" id="toc-combinatorial-examples" class="nav-link" data-scroll-target="#combinatorial-examples">Combinatorial examples</a>
  <ul class="collapse">
  <li><a href="#multinomials" id="toc-multinomials" class="nav-link" data-scroll-target="#multinomials">Multinomials</a></li>
  <li><a href="#sanov-theorem" id="toc-sanov-theorem" class="nav-link" data-scroll-target="#sanov-theorem">Sanov theorem</a></li>
  <li><a href="#surface-area-of-high-dimensional-spheres" id="toc-surface-area-of-high-dimensional-spheres" class="nav-link" data-scroll-target="#surface-area-of-high-dimensional-spheres">Surface area of high-dimensional spheres</a></li>
  </ul></li>
  <li><a href="#biological-examples" id="toc-biological-examples" class="nav-link" data-scroll-target="#biological-examples">Biological examples</a>
  <ul class="collapse">
  <li><a href="#how-elastic-is-the-skin-of-red-blood-cell" id="toc-how-elastic-is-the-skin-of-red-blood-cell" class="nav-link" data-scroll-target="#how-elastic-is-the-skin-of-red-blood-cell">How elastic is the skin of red blood cell?</a></li>
  <li><a href="#the-lac-operon" id="toc-the-lac-operon" class="nav-link" data-scroll-target="#the-lac-operon">The <code>lac</code> operon</a></li>
  <li><a href="#unzipping-rna-hairpins" id="toc-unzipping-rna-hairpins" class="nav-link" data-scroll-target="#unzipping-rna-hairpins">Unzipping RNA hairpins</a></li>
  <li><a href="#hungry-hungry-bacteria" id="toc-hungry-hungry-bacteria" class="nav-link" data-scroll-target="#hungry-hungry-bacteria">Hungry hungry bacteria</a></li>
  </ul></li>
  <li><a href="#statistical-field-theory" id="toc-statistical-field-theory" class="nav-link" data-scroll-target="#statistical-field-theory">Statistical field theory</a>
  <ul class="collapse">
  <li><a href="#maximum-caliber" id="toc-maximum-caliber" class="nav-link" data-scroll-target="#maximum-caliber">Maximum caliber</a></li>
  <li><a href="#fluctuation-dissipation-relations" id="toc-fluctuation-dissipation-relations" class="nav-link" data-scroll-target="#fluctuation-dissipation-relations">Fluctuation-dissipation relations</a></li>
  <li><a href="#equality-before-the-law" id="toc-equality-before-the-law" class="nav-link" data-scroll-target="#equality-before-the-law">Equality before the law</a></li>
  <li><a href="#one-dimensional-fdr" id="toc-one-dimensional-fdr" class="nav-link" data-scroll-target="#one-dimensional-fdr">One-dimensional FDR</a></li>
  <li><a href="#sound-waves" id="toc-sound-waves" class="nav-link" data-scroll-target="#sound-waves">Sound waves</a></li>
  </ul></li>
  <li><a href="#metastability" id="toc-metastability" class="nav-link" data-scroll-target="#metastability">Metastability</a>
  <ul class="collapse">
  <li><a href="#common-applications" id="toc-common-applications" class="nav-link" data-scroll-target="#common-applications">Common applications</a></li>
  <li><a href="#uncommon-applications" id="toc-uncommon-applications" class="nav-link" data-scroll-target="#uncommon-applications">Uncommon applications</a></li>
  </ul></li>
  <li><a href="#sec-cft" id="toc-sec-cft" class="nav-link" data-scroll-target="#sec-cft">Crooks fluctuation theorem</a>
  <ul class="collapse">
  <li><a href="#in-a-closed-system-microcanonical" id="toc-in-a-closed-system-microcanonical" class="nav-link" data-scroll-target="#in-a-closed-system-microcanonical">In a closed system (microcanonical)</a></li>
  <li><a href="#in-an-energy-bath-canonical" id="toc-in-an-energy-bath-canonical" class="nav-link" data-scroll-target="#in-an-energy-bath-canonical">In an energy bath (canonical)</a></li>
  <li><a href="#other-ensembles" id="toc-other-ensembles" class="nav-link" data-scroll-target="#other-ensembles">Other ensembles</a></li>
  <li><a href="#application" id="toc-application" class="nav-link" data-scroll-target="#application">Application</a></li>
  </ul></li>
  <li><a href="#jarzynski-equality" id="toc-jarzynski-equality" class="nav-link" data-scroll-target="#jarzynski-equality">Jarzynski equality</a>
  <ul class="collapse">
  <li><a href="#worked-example-bouncing-ball" id="toc-worked-example-bouncing-ball" class="nav-link" data-scroll-target="#worked-example-bouncing-ball">Worked example: bouncing ball</a></li>
  <li><a href="#fluctuation-dissipation-relations-1" id="toc-fluctuation-dissipation-relations-1" class="nav-link" data-scroll-target="#fluctuation-dissipation-relations-1">Fluctuation-dissipation relations</a></li>
  <li><a href="#arrow-of-time" id="toc-arrow-of-time" class="nav-link" data-scroll-target="#arrow-of-time">Arrow of time</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<section id="quick-reference" class="level3">
<h3 class="anchored" data-anchor-id="quick-reference">Quick reference</h3>
<ul>
<li>Liouville’s theorem: Hamiltonian dynamics preserves density in phase space.</li>
</ul>
</section>
</section>
<section id="overview" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<section id="philosophical-comments" class="level3">
<h3 class="anchored" data-anchor-id="philosophical-comments">Philosophical comments</h3>
<p>It is fair to say that, although it originated in the 19th century like all other classical fields of physics, statistical mechanics is unsettled.</p>
<p>Trajectory-centric statistical mechanics. In this view, we start with the equations of motion for a physical system, then study statistical properties of individual trajectories, or collections of them. For example, if we have a pendulum hanging in air, being hit by air molecules all the time, we would study the total trajectory <span class="math inline">\((\theta, x_1, y_1, z_1, x_2, y_2, z_2, \dots)\)</span>, where <span class="math inline">\(\theta\)</span> is the angle of the pendulum swing, and <span class="math inline">\((x_i, y_i, z_i)\)</span> is the location of the <span class="math inline">\(i\)</span>-th air molecule. Then we may ask that, over a long enough period, how frequent would the pendulum visit a certain angle range of <span class="math inline">\([\theta_0, \theta_0 + \delta\theta]\)</span>:</p>
<p><span class="math display">\[
Pr(\theta \in [\theta_0, \theta_0 + \delta\theta]) = \lim_{T \to \infty} \frac{1}{2T} \int_{-T}^{+T} 1[\theta \in [\theta_0, \theta_0 + \delta\theta]] dt
\]</span></p>
<p>In the trajectory-centric view, there are the following issues:</p>
<ul>
<li>Problem of ergodicity: When does time-average equal ensemble-average? A system is called “ergodic” iff for almost all starting conditions, the time-average of the trajectory is the ensemble-average over all trajectories.</li>
<li>Problem of entropy: How is entropy defined <em>on a single trajectory</em>?</li>
<li>H-theorem: In what sense, and under what conditions, does entropy increase?</li>
<li>Problem of equilibrium: What does it mean to say that a trajectory is <em>in equilibrium</em>?</li>
<li>Approach to equilibrium: In what sense, and under what conditions, does the trajectory converge to an equilibrium?</li>
<li>Reversibility problem (<em>Umkehreinwand</em>): If individual trajectories are reversible, why does entropy increase instead of decrease?</li>
</ul>
<p>While these philosophical problems are quite diverting, we will avoid them as much as possible, because we will be working with the ensemble-centric equilibrium statistical mechanics. This is the statistical mechanics that every working physicist uses, and this is what we will present. If you are interested in the philosophical issues, read the <a href="https://plato.stanford.edu/entries/statphys-statmech/">Stanford Encyclopedia entry on the <em>Philosophy of Statistical Mechanics</em></a>.</p>
</section>
<section id="principles-of-statistical-mechanics" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="principles-of-statistical-mechanics">Principles of statistical mechanics</h3>
<ul>
<li>A <strong>physical system</strong> is a classical system with a state space, evolving according to some equation of motion.</li>
<li>An <strong>ensemble</strong> of that system is a probability distribution over its state space.</li>
<li>The idea of (ensemble-centric) statistical mechanics is to study the evolution of an entire probability distribution over all possible states.</li>
<li>The <strong>entropy</strong> of a probability distribution <span class="math inline">\(\rho\)</span> is</li>
</ul>
<p><span class="math display">\[S[\rho] := -\int dx\; \rho(x) \ln \rho(x)\]</span></p>
<ul>
<li>Under any constraint, there exists a unique ensemble, named the <strong>equilibrium ensemble</strong>, which maximizes entropy under constraint.</li>
</ul>
<p>Most of the times, the state space is a phase space, and the equation of motion is described by a Hamiltonian function. However, the machinery of statistical mechanics, as given above, is purely mathematical. It can be used to study any problem in probability whatsoever, even those with no physical meaning.</p>
<p>Believe it or not, the above constitutes the entirety of equilibrium statistical mechanics. So far, it is a purely mathematical theory, with no falsifiability (Popperians shouting in the background). To make it falsifiable, we need to add one more assumption, necessarily fuzzy:<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><blockquote class="blockquote"><sup>1</sup>&nbsp;
<p>As far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.</p>
<p>— Albert Einstein, <em>Address to Prussian Academy of Sciences</em> (1921)</p>
</blockquote>
</div></div><ul>
<li>The equilibrium ensemble is physically meaningful and describes the observable behavior of physical systems.</li>
</ul>
<p>In other words, when a physical system is <em>at equilibrium</em>, then everything observable can be found by studying it <em>as if</em> it has the maximum entropy distribution under constraint.</p>
<p>Of course, just what that “is physically meaningful” means, is another source of endless philosophical arguments. I would trust that you will know what is physically meaningful, and leave it at that, while those who have a taste for philosophy can grapple with the <a href="https://plato.stanford.edu/entries/scientific-underdetermination/">Duhem–Quine thesis</a>.</p>
</section>
<section id="sec-differential-entropy-ill-defined" class="level3">
<h3 class="anchored" data-anchor-id="sec-differential-entropy-ill-defined">Differential entropy depends on coordinates choice</h3>
<p>There is a well-known secret among information theorists: differential entropy is ill-defined.</p>
<p>Consider the uniform distribution on <span class="math inline">\([0, 1]\)</span>. It is the maxent distribution on <span class="math inline">\([0, 1]\)</span> – relative to the Lebesgue measure. However, why should we pick the Lebesgue measure, and what happens if we don’t?</p>
<p>Suppose we now stretch the <span class="math inline">\([0, 1]\)</span> interval nonlinearly, by <span class="math inline">\(f(x) = x^2\)</span>, then the maxent distribution relative to <em>that</em> would no longer be the uniform distribution on <span class="math inline">\([0, 1]\)</span>. Instead, it would be the uniform distribution after stretching.</p>
<p>The problem is this: Differential entropy is not coordinate-free. If we change the coordinates, we change the base measure, and the differential entropy changes as well.</p>
<p>To fix this, we need to use the KL-divergence, which is invariant under a change of base measure, as in <span class="math display">\[-D_{KL}(\rho \| \mu) := - \int dx\; \rho(x) \ln\frac{\rho(x)}{\mu(x)}\]</span></p>
<p>In typical situations, we don’t need to worry ourselves with KL-divergence, as we just pick the uniform distribution <span class="math inline">\(\mu\)</span>. When the state space is infinite in volume, the uniform distribution is not a probability measure, but it will work. Bayesians say that it is an <em>improper prior</em>.</p>
<p>In this interpretation, the principle of “maximum entropy distribution under constraint” becomes the principle of “minimal KL-divergence under constraint”, which <em>is</em> Bayesian inference, with exactly the same formulas.</p>
<p>In almost all cases, we use the uniform prior over phase space. This is how Gibbs did it, and he didn’t really justify it other than saying that <em>it just works</em>, and suggesting it has something to do with Liouville’s theorem. Now with a century of hindsight, we know that it works because of quantum mechanics: We <em>should use</em> the uniform prior over phase space, because phase space volume has a natural unit of measurement: <span class="math inline">\(h^N\)</span>, where <span class="math inline">\(h\)</span> is Planck’s constant, and <span class="math inline">\(2N\)</span> is the dimension of phase space. As Planck’s constant is a universal constant, independent of where we are in phase space, we should weight all of the phase space equally, resulting in a uniform prior.</p>
</section>
</section>
<section id="mathematical-developments" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="mathematical-developments">Mathematical developments</h2>
<section id="fundamental-theorems" class="level3">
<h3 class="anchored" data-anchor-id="fundamental-theorems">Fundamental theorems</h3>
<div id="thm-liouville" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Liouville’s theorem)</strong></span> For any phase space and any Hamiltonian over it (which can change with time), phase-space volume is conserved under motion.</p>
<p>For any probability distribution <span class="math inline">\(\rho_0\)</span>, if after time <span class="math inline">\(t\)</span>, it evolves to <span class="math inline">\(\rho_t\)</span>, and a point <span class="math inline">\(x(0)\)</span> evolves to <span class="math inline">\(x(t)\)</span>, then <span class="math inline">\(\rho_0(x(0)) = \rho_t(x(t))\)</span>.</p>
</div>
<p>The proof is found in any textbook, and also <a href="https://en.wikipedia.org/wiki/Liouville's_theorem_(Hamiltonian)">Wikipedia</a>. Since it is already simple enough, and I can’t really improve upon it, I won’t.</p>
<div id="cor-conservation-entropy" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 1 (conservation of entropy)</strong></span> For a Hamiltonian system, with any Hamiltonian (which can change with time), for any probability distribution <span class="math inline">\(\rho\)</span> over its phase space, its entropy is conserved over time.</p>
</div>
<p>In particular, we have the following corollary:</p>
<div id="cor-ensemble-conservation" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 2</strong></span> Given any set of constraints, if the Hamiltonian preserves these constraints over time, then any constrained-maximal entropy distribution remains constrained-maximal under time-evolution.</p>
</div>
<p>In most cases, the constraint is of a particular form: the expectation is known. In that case, we have the following theorem:</p>
<div id="thm-constrained-optimization" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 (maximal entropy under linear constraints)</strong></span> For the following constrained optimization problem</p>
<p><span class="math display">\[
\begin{cases}
\max_\rho S[\rho] \\
\int A_1(x) \rho(x) &amp;= \bar A_1 \\
\cdots &amp;= \cdots \\
\int A_n(x) \rho(x) &amp;= \bar A_n \\
\end{cases}
\]</span></p>
<p>Consider the following ansatz</p>
<p><span class="math display">\[
\rho(x) = \frac{1}{Z(a_1, \dots, a_n)} e^{-\sum_i a_i A_i(x)}
\]</span></p>
<p>where <span class="math inline">\(Z(a_1, \dots, a_n) = \int e^{-\sum_i a_i A_i(x)} dx\)</span>, and <span class="math inline">\(a_1, \dots, a_n\)</span> are chosen such that the constraints <span class="math inline">\(\int A_i(x) \rho(x) = \bar A_i\)</span> are satisfied.</p>
<p><em>If</em> the ansatz exists, then it is the unique solution.</p>
</div>
<p>The ansatz solution is what you get by Lagrangian multipliers. For a refresher, see the <a href="https://yuxi-liu-wired.github.io/essays/posts/analytical-mechanics/index.html#lagranges-devil-at-disneyland"><em>Analytical Mechanics</em>#Lagrange’s devil at Disneyland</a>. The theorem shows that the solution is unique – provided that it exists. Does it exist? Yes, in physics. If it doesn’t exist, then we are clearly not modelling a physically real phenomenon.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In physics, these are “Boltzmann distributions” or “Gibbs distributions”. In statistics, these are <a href="https://en.wikipedia.org/wiki/Exponential_family">exponential families</a>. Because they are everywhere, they have many names.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Define a distribution <span class="math inline">\(\rho\)</span> as given in the statement of the theorem. That is,</p>
<p><span class="math display">\[
\rho(x) = \frac{1}{Z(a_1, \dots, a_n)} e^{-\sum_i a_i A_i(x)}
\]</span></p>
<p>etc.</p>
<p>Now, it remains to prove that for any other <span class="math inline">\(\rho'\)</span> that satisfies the constraints, we have <span class="math inline">\(S[\rho] \geq S[\rho']\)</span>.</p>
<p>By routine calculation, for any probability distribution <span class="math inline">\(\rho'\)</span>,</p>
<p><span class="math display">\[
D_{KL}(\rho' \| \rho) = -S[\rho'] + \sum_i a_i \braket{A_i}_{\rho'} + \ln Z(a_1, \dots, a_n)
\]</span></p>
<p>If <span class="math inline">\(\rho'\)</span> satisfies the given constraints, then <span class="math inline">\(D_{KL}(\rho' \| \rho) = -S[\rho'] + Const\)</span> where the constant does not depend on <span class="math inline">\(\rho'\)</span>, as long as it satisfies the constraints. Therefore, <span class="math inline">\(S[\rho']\)</span> is maximized when <span class="math inline">\(D_{KL}(\rho' \| \rho)\)</span> is minimized, which is exactly <span class="math inline">\(\rho\)</span>.</p>
</div>
</div>
</div>
<p>The following proposition is often used when we want to maximize entropy in a two-step process:</p>
<div id="thm-compound-entropy" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 (compound entropy)</strong></span> If <span class="math inline">\(\rho_{X,Y}\)</span> is a probability distribution over two variables <span class="math inline">\((X, Y)\)</span>, then</p>
<p><span class="math display">\[S[\rho_{X,Y}] = S[\rho_Y] + \braket{S[\rho_{X|y}]}_y\]</span></p>
<p>or more succinctly,</p>
<p><span class="math display">\[S_{X,Y} = S_Y + \braket{S_{X|y}}_y\]</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Notations">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notations
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(\rho_Y\)</span> is the probability distribution over <span class="math inline">\(Y\)</span>, after we integrate/marginalize <span class="math inline">\(X\)</span> away:</p>
<p><span class="math display">\[
\rho_Y(y) := \int \rho_{X,Y}(x,y)dx
\]</span></p>
<p><span class="math inline">\(\rho_{X|y}\)</span> is the conditional probability distribution over <span class="math inline">\(X\)</span>, conditional on <span class="math inline">\(Y=y\)</span>:</p>
<p><span class="math display">\[
\rho_{X|y}(x) := \frac{\rho_{X,Y}(x,y)}{\int \rho_{X,Y}(x,y) dx}
\]</span></p>
<p><span class="math inline">\(\braket{\cdot}_y\)</span> is the expectation over <span class="math inline">\(\rho_Y\)</span>:</p>
<p><span class="math display">\[
\braket{S_{X|y}}_y := \int S_{X|y} \rho_Y(y)dy
\]</span></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consider a compound system in ensemble <span class="math inline">\(\rho(x, y)\)</span>. Its entropy is</p>
<p><span class="math display">\[S[\rho] = -\int dxdy \; \rho(x, y) \ln \rho(x, y)\]</span></p>
<p>We can take the calculation in two steps:</p>
<p><span class="math display">\[S[\rho] = -\int dxdy \; \rho(x|y)\rho(y) (\ln \rho(x|y) + \ln  \rho(y)) = S[\rho_Y] + \mathbb{E}_y[S[\rho_{X|y}]]\]</span></p>
</div>
</div>
</div>
<p>Intuitively, what does <span class="math inline">\(S_{X,Y} = S_Y + \braket{S_{X|y}}_y\)</span> mean? It means that the entropy in <span class="math inline">\((X, Y)\)</span> can be decomposed into two parts: the part due to <span class="math inline">\(Y\)</span>, and the part remaining after we know <span class="math inline">\(Y\)</span>, but not yet knowing <span class="math inline">\(X\)</span>. In the language of information theory, the total information in <span class="math inline">\((X, Y)\)</span> is equal to the information in <span class="math inline">\(Y\)</span>, plus the information of <span class="math inline">\(X\)</span> conditional over <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
I(X, Y) = I(Y) + I(X|Y)
\]</span></p>
</section>
<section id="microcanonical-ensembles" class="level3">
<h3 class="anchored" data-anchor-id="microcanonical-ensembles">Microcanonical ensembles</h3>
<p>If the only constraint is the constant-energy constraint <span class="math inline">\(H(x) = E\)</span>, then the maximal entropy distribution is the uniform distribution on the shell of constant energy <span class="math inline">\(H = E\)</span>. It is uniform, because once we enforce <span class="math inline">\(H(x) = E\)</span>, there are no other constraints, and so by <a href="#thm-constrained-optimization" class="quarto-xref">Theorem&nbsp;2</a>, the distribution is uniform.</p>
<p>Thus, we obtain the <strong>microcanonical ensemble</strong>:</p>
<p><span class="math display">\[\rho_E(x) \propto 1_{H(x) = E}\]</span></p>
<p>It is sometimes necessary to deal with the “thickness” of the energy shell. In that case, <span class="math inline">\(\rho_E(x) \propto \delta(H(x) - E)\)</span>, where <span class="math inline">\(\delta\)</span> is the Dirac delta function.</p>
<p>By <a href="#thm-constrained-optimization" class="quarto-xref">Theorem&nbsp;2</a>, the microcanonical ensemble is the unique maximizer of entropy under the constraint of constant energy. In particular, if the Hamiltonian does not change over time, then any microcanonical ensemble is preserved over time. In words, if we uniformly “dust” the energy shell of <span class="math inline">\(H(x) = E\)</span> with a cloud of system states, and let all of them evolve over time, then though the dust particles move about, the cloud remains exactly the same.</p>
<p>More generally, we can impose more (in)equality constraints, and still obtain a microcanonical ensemble. For example, consider a ball flying around in an empty room with no gravity. The Hamiltonian is <span class="math inline">\(H(q, p) = \frac{p^2}{2m}\)</span>, and its microcanonical ensemble is <span class="math inline">\(\rho(q, p) \propto \delta(p = \sqrt{2mE})1[p \in \text{the room}]\)</span>. That is, its velocity is on the energy shell, while its position is uniform over the entire room.</p>
<p>If we want to specify the number of particles for each chemical species, then that can be incorporated into the microcanonical ensemble as well. For example, if we want the number of species <span class="math inline">\(i\)</span> be exactly <span class="math inline">\(N_{i0}\)</span>, then we multiply <span class="math inline">\(\rho\)</span> by <span class="math inline">\(1[N_i = N_{i0}]\)</span>.</p>
</section>
<section id="canonical-ensembles" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="canonical-ensembles">Canonical ensembles</h3>
<p>If we have a small system connected to a large system, then we typically don’t care about the large system, and only want to study the ensemble of the small system. In this case, we would first find the microcanonical ensemble for the total system, then integrate out of the large system, resulting in an ensemble over just the small system, as in</p>
<p><span class="math display">\[\rho_{\text{small}}(x) = \int \rho_{\text{total}}(x, y) dy\]</span></p>
<p>where <span class="math inline">\(x\)</span> ranges over the states of the small system, and <span class="math inline">\(y\)</span> of the large system.</p>
<p>Assuming that the energy of the compound system is extensive, we obtain the canonical ensemble. Assuming that the energy and volume are both extensive, we obtain the grand canonical ensemble, etc. The following table would be very useful</p>
<table class="table">
<thead>
<tr class="header">
<th>extensive constraint</th>
<th>ensemble</th>
<th>free entropy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>none</td>
<td>microcanonical</td>
<td>entropy</td>
</tr>
<tr class="even">
<td>energy</td>
<td>canonical</td>
<td>Helmholtz free entropy</td>
</tr>
<tr class="odd">
<td>energy, volume</td>
<td>?</td>
<td>Gibbs free entropy</td>
</tr>
<tr class="even">
<td>energy, particle count</td>
<td>grand canonical</td>
<td>Landau free entropy</td>
</tr>
<tr class="odd">
<td>energy, volume, particle count</td>
<td>?</td>
<td>?</td>
</tr>
</tbody>
</table>
<p>There are some question marks in the above table, because there are no consensus names for those question marks. What is more surprising is that there is no name for the ensemble of constrained energy and volume. I would have expected something like the “Gibbs ensemble”, but history isn’t nice to us like that. Well, then I will name it first, as the <em>big canonical ensemble</em>. And while we’re at it, let’s fill the last row as well:</p>
<table class="table">
<thead>
<tr class="header">
<th>extensive constraint</th>
<th>ensemble</th>
<th>free entropy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>none</td>
<td>microcanonical</td>
<td>entropy</td>
</tr>
<tr class="even">
<td>energy</td>
<td>canonical</td>
<td>Helmholtz free entropy</td>
</tr>
<tr class="odd">
<td>energy, volume</td>
<td>big canonical</td>
<td>Gibbs free entropy</td>
</tr>
<tr class="even">
<td>energy, particle count</td>
<td>grand canonical</td>
<td>Landau free entropy</td>
</tr>
<tr class="odd">
<td>energy, volume, particle count</td>
<td>gross canonical</td>
<td>EVN free energy</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-tip callout-titled" title="Extensivity">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Extensivity
</div>
</div>
<div class="callout-body-container callout-body">
<p>In classical thermodynamics, extensivity means that entropy of the compound system can be calculated in a two-step process: calculate the entropy of each subsystem, then add them up. The important fact is that a subsystem still has enough independence to have its own entropy.</p>
<p>This is not always obvious. If we have two galaxies of stars, we can think of each as a “cosmic gas” where each particle is a star. Now, if we put them near each other, then the gravity between the two galaxies would mean it is no longer meaningful to speak of “the entropy of galaxy 1”, but only “the entropy of galaxy-compound 1-2”.</p>
<p>In statistical mechanics, extensivity means a certain property of each subsystem is unaffected by the state of the other subsystems, and the total is the sum of them. So for example, if <span class="math inline">\(A\)</span> is an extensive property, then it means</p>
<p><span class="math display">\[
A(x_1, \dots, x_n) = A_1(x_1) + \dots + A_n(x_n)
\]</span></p>
<p>Like most textbooks, we assume extensivity by default, although as we noted in <a href="https://yuxi-liu-wired.github.io/essays/posts/equilibrium-thermoeconomics/"><em>Classical Thermodynamics and Economics</em></a>, both classical thermodynamics and statistical mechanics do not require extensivity. We assume extensivity because it is mathematically convenient, and good enough for most applications.</p>
</div>
</div>
<p>In the following theorem, we assume that the total system is extensive, and is already in the maximal entropy distribution (microcanonical ensemble)</p>
<div id="thm-canonical-ensembles" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4</strong></span> If the two systems are in energy-contact, and energy is conserved, and energy is extensive, and the compound system is in a microcanonical ensemble, the small system is in a <strong>canonical ensemble</strong></p>
<p><span class="math display">\[
\rho(x) \propto e^{-\beta H(x)}
\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is the marginal entropy of energy of the large system:</p>
<p><span class="math display">\[\beta := \partial_E S[\rho_{bath, E}]\]</span></p>
<p>Similarly, if the two systems are in energy-and-particle-contact, then the small system has the <strong>grand canonical ensemble</strong></p>
<p><span class="math display">\[
\rho(x) \propto e^{-(\beta H(x) + (-\beta \mu) N(x))}
\]</span></p>
<p>where <span class="math inline">\(-\beta\mu\)</span> is the marginal entropy of particle of the large system:</p>
<p><span class="math display">\[-\beta\mu := (\partial_N S[\rho_{bath, E, N}])_{E}\]</span></p>
<p>Most generally, if the two systems are in <span class="math inline">\(q_1, \dots, q_m\)</span> contact, and <span class="math inline">\(q_1, \dots, q_m\)</span> are conserved and extensive quantity, then</p>
<p><span class="math display">\[\rho(x) \propto e^{-\sum_i p_i q_i(x)}\]</span></p>
<p>where <span class="math inline">\(p_i = (\partial_{q_i} S[\rho_{bath, q}])_{q}\)</span> is the marginal entropy of <span class="math inline">\(q_i\)</span> of the large system.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We prove the case for the canonical ensemble. The other cases are similar.</p>
<p>Since the total distribution of the whole system is the maximal entropy distribution, we are faced with a constrained maximization problem:</p>
<p><span class="math display">\[\max_\rho S[\rho]\]</span></p>
<p>By <a href="#thm-compound-entropy" class="quarto-xref">Theorem&nbsp;3</a>,</p>
<p><span class="math display">\[S = S_{\text{system}}(E_{\text{system}}) + \braket{S_{bath|system}(E_{total} - E_{\text{system}})}_{\text{system}}\]</span></p>
<p>Since the bath is so much larger than the system, we can take just the first term in its Taylor expansion:</p>
<p><span class="math display">\[S_{bath|system}(E_{total} - E_{\text{system}}) = S_{\text{bath}}(E_{total}) - \beta E_{\text{system}}\]</span></p>
<p>where <span class="math inline">\(E_{total}\)</span> is the total energy for the compound system, <span class="math inline">\(\beta = \partial_E S_{\text{bath}}|_{E = E_{total}}\)</span> is the marginal entropy per energy, and <span class="math inline">\(E_{\text{system}}\)</span> is the energy of the system.</p>
<p>This gives us the linearly constrained maximization problem of</p>
<p><span class="math display">\[\max_{\rho_{\text{system}}} (S_{\text{system}} - \beta \braket{E_{\text{system}}}_{\rho_{\text{system}}})\]</span></p>
<p>and we apply Lagrange multipliers to finish the proof.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="extensivity">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
extensivity
</div>
</div>
<div class="callout-body-container callout-body">
<p>Extensivitiy in statistical mechanics yields extensivity in thermodynamics. Specifically, writing <span class="math inline">\(S_{\text{bath}}(E)\)</span>, instead of <span class="math inline">\(S_{\text{bath}}(E, E_{\text{system}})\)</span>, requires the assumption of extensivity. Precisely because the bath and the system do not affect each other, we are allowed to calculate the entropy of the bath without knowing anything about the energy of the system.</p>
<p><span class="math inline">\(S_{\text{bath}}\)</span> is the logarithm of the surface area of the energy shell <span class="math inline">\(H_{\text{bath}} = E_{\text{bath}}\)</span>. By extensivity, <span class="math inline">\(H(x_{\text{bath}}, x_{\text{system}}) = H_{\text{bath}}(x_{\text{bath}}) + H_{\text{system}}(x_{\text{system}})\)</span>, so the energy shells of the bath depends on only <span class="math inline">\(E_{\text{bath}}\)</span>, not <span class="math inline">\(E_{\text{system}}\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The proof showed something extra: If the small system is in distribution <span class="math inline">\(\rho\)</span> that does not equal to the equilibrium distribution <span class="math inline">\(\rho_B\)</span>, then the total system’s entropy is</p>
<p><span class="math display">\[S = S_{max} - D_{KL}(\rho \| \rho_B)\]</span></p>
<p>which is related to of Sanov theorem and large deviation theory, though I don’t know how to make this precise.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Enthalpic ensemble">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Enthalpic ensemble
</div>
</div>
<div class="callout-body-container callout-body">
<p>What if we have a system in volume-contact, but not thermal-contact? This might happen when the system is a flexible bag of gas held in an atmosphere, but the bag is thermally insulating. Notice that in this case, the small system still exchanges energy with the large system via <span class="math inline">\(d\braket{E} = -Pd\braket{V}\)</span>. We don’t have <span class="math inline">\(E = -PdV\)</span>, because the small system might get unlucky. During a moment of weakness, all its particles has abandoned their frontier posts, and the bath has taken advantage of this by encroaching on its land. The system loses volume by <span class="math inline">\(\delta V\)</span>, without earning a compensating <span class="math inline">\(\delta E = P \delta V\)</span>. In short, the thermodynamic equality <span class="math inline">\(E = -PdV\)</span> is inexact in statistical mechanics, and only holds true on the ensemble average.</p>
<p>In this case, because pressure is a constant, we have <span class="math inline">\(d(E + PV) = 0\)</span>, and so we have the enthalpic ensemble <span class="math inline">\(\rho \propto e^{-\beta H}\)</span>, where <span class="math inline">\(H := E + PV\)</span> is the <a href="https://en.wikipedia.org/wiki/Enthalpy">enthalpy</a>,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>Specifically, if you work through the same argument, you would end up with the following constrained maximization problem:</p>
<p><span class="math display">\[
\begin{cases}
\max_{\rho_{\text{system}}} (S_{\text{system}} - \beta \braket{E_{\text{system}}}_{\rho_{\text{system}}} - \beta P \braket{V}) \\
\braket{E_{\text{system}}} + P\braket{V_{\text{system}}} = Const
\end{cases}
\]</span></p>
<p>yielding the enthalpic ensemble (or the <a href="https://en.wikipedia.org/wiki/Isoenthalpic%E2%80%93isobaric_ensemble">isoenthalpic-isobaric ensemble</a>).</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Sorry, I know this is not Hamiltonian, but we are running out of letters.</p></div></div></section>
<section id="free-entropies" class="level3">
<h3 class="anchored" data-anchor-id="free-entropies">Free entropies</h3>
<p>Just like in thermodynamics, it is useful to consider free entropies, which are the convex duals of the entropy:</p>
<ul>
<li>Helmholtz free entropy: <span class="math inline">\(f[\rho] := S[\rho] - \beta \braket{E} = \int dx \; \rho(x) (-\ln \rho(x) - \beta E(x))\)</span>;</li>
<li>Gibbs free entropy: <span class="math inline">\(g[\rho] = S[\rho] - \beta \braket{E} - \beta P \braket{V}\)</span>;</li>
<li>Landau free entropy: <span class="math inline">\(\omega[\rho] = S[\rho] - \beta \braket{E} + \beta \mu \braket{N}\)</span>;</li>
</ul>
<p>etc. Of those, we would mostly use the Helmholtz free energy, so I will write it down again:</p>
<p><span class="math display">\[
f[\rho] := S[\rho] - \beta \braket{E} = \int dx \; \rho(x) (-\ln \rho(x) - \beta E(x))
\]</span></p>
<div id="thm-chain-rule" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5 (chain rule for free entropies)</strong></span> <span class="math inline">\(f_X = S_Y + \braket{f_{X|y}}_y\)</span>, and similarly <span class="math inline">\(g_X = S_Y + \braket{g_{X|y}}_y\)</span>, and similarly for all other free entropies.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\begin{aligned}
  f_X &amp;= S_X - \beta \braket{E}_x  \\
  &amp;= S_Y + \braket{S_{X|y}}_y - \beta \braket{\braket{E}_{x \sim X|y}}_y \\
  &amp;= S_Y + \braket{f_{X|y}}_y
\end{aligned}
\]</span></p>
</div>
</div>
</div>
<p>A common trick in statistical mechanics is to characterize the same equilibrium in many different perspectives. For example, the canonical ensemble has at least 4 characterizations at least. “Muscle memory” in statistical mechanics would allow you to nimbly applying the most suitable one for any occasion.</p>
<div id="thm-canonical-characterization" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6 (4 characterizations of the canonical ensemble)</strong></span> &nbsp;</p>
<ol type="1">
<li>(total entropy under fixed energy constraint) The canonical ensemble maximizes total entropy when the system is in contact with an energy bath that satisfies <span class="math inline">\(\partial_E S_{\text{bath}} = \beta\)</span>, and the total energy is fixed.</li>
<li>(entropy under mean energy constraint) A system maximizes its entropy under constraint <span class="math inline">\(\braket{E} = E_0\)</span> when it assumes the canonical ensemble with <span class="math inline">\(\beta\)</span> that is the unique solution to <span class="math inline">\(\int dx \; e^{-\beta E(x)} = E_0\)</span>.</li>
<li>(thermodynamic limit): Take <span class="math inline">\(N\)</span> copies of a system, and connect them by energy-contacts. Inject the system with total energy <span class="math inline">\(NE_0\)</span>, and let the system reach its microcanonical ensemble. Then at the thermodynamic limit of <span class="math inline">\(N\to \infty\)</span>, the distribution of a single system is the canonical distribution with <span class="math inline">\(\beta\)</span> that is the unique solution to <span class="math inline">\(\int dx \; e^{-\beta E(x)} = E_0\)</span>.</li>
<li>(free entropy) A system maximizes its Helmholtz free entropy when it assumes the canonical ensemble. At the optimal distribution <span class="math inline">\(\rho^*\)</span>, the maximal Helmholtz free entropy is <span class="math inline">\(f[\rho^*] = \ln Z\)</span>, where <span class="math inline">\(Z = \int dx \; e^{-\beta E(x)}\)</span> is the partition function.</li>
</ol>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li>We already proved this.</li>
<li>Use the Lagrange multiplier.</li>
<li>Isolate one system, and treat the rest as an energy-bath.</li>
<li><span class="math inline">\(f[\rho] = \ln Z - D_{KL}(\rho \| \rho_B)\)</span>.</li>
</ol>
</div>
</div>
</div>
</section>
<section id="the-partition-function" class="level3">
<h3 class="anchored" data-anchor-id="the-partition-function">The partition function</h3>
<p>When the system is in a canonical ensemble, we can define a convenient variable <span class="math inline">\(Z = \int dx\; e^{-\beta E(x)}\)</span> called the <strong>partition function</strong>. As proven in <a href="#thm-canonical-characterization" class="quarto-xref">Theorem&nbsp;6</a>, the partition function is equal to <span class="math inline">\(e^f\)</span>, where <span class="math inline">\(f\)</span> is the Helmholtz free entropy of the canonical ensemble.</p>
<div id="thm-partition-cumulant" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7 (the partition function is the cumulant generating function of energy)</strong></span> Let a system be in canonical ensemble with inverse temperature <span class="math inline">\(\beta\)</span>, and let <span class="math inline">\(K(t) := \ln \braket{e^{tE}}\)</span> be the <a href="https://en.wikipedia.org/wiki/Cumulant">cumulant generating function</a> of its energy, then <span class="math display">\[K(t) = \ln Z(\beta-t) - \ln Z(\beta)\]</span></p>
<p>In particular, the <span class="math inline">\(n\)</span>-th cumulant of energy is<br>
<span class="math display">\[\kappa_n(E) = K^{(n)}(t) |_{t=0} = (-\partial_\beta)^n (\ln Z)\]</span></p>
<p>A similar proposition applies for the other ensembles and their free entropies.</p>
</div>
<p>The proof is by direct computation.</p>
<p>For example, the first two cumulants are the mean and variance:</p>
<p><span class="math display">\[\braket{E} = (-\partial_\beta) (\ln Z), \quad \mathrm{Var}(E) = \partial_\beta^2 (\ln Z)\]</span></p>
<p>Typical systems are made of <span class="math inline">\(N\)</span> particles, where <span class="math inline">\(N\)</span> is large, and that these particles are only weakly interacting. In this case, the total Helmholtz free entropy per particle converges at the thermodynamic limit of <span class="math inline">\(N \to \infty\)</span>:</p>
<p><span class="math display">\[
\lim_N \frac 1N \ln Z \to \bar f_\beta
\]</span></p>
<p>Thus, for large but finite <span class="math inline">\(N\)</span>, we have</p>
<p><span class="math display">\[\braket{E} \approx -N \partial_\beta \bar f_\beta, \quad \mathrm{Var}(E) = N\partial_\beta^2 \bar f_\beta\]</span></p>
<p>In particular, the relative fluctuation scales like <span class="math inline">\(\frac{\sqrt{\mathrm{Var}(E)}}{\braket{E}} \sim N^{-1/2}\)</span>.</p>
</section>
<section id="conditional-entropies" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="conditional-entropies">Conditional entropies</h3>
<p>Given any two random variable <span class="math inline">\(X, Y\)</span>, and an “observable” variable <span class="math inline">\(Y\)</span> that is determined by <span class="math inline">\(X\)</span> by some function <span class="math inline">\(h\)</span>, such that <span class="math inline">\(Y = h(X)\)</span>. If we know <span class="math inline">\(X\)</span>, we would know <span class="math inline">\(Y\)</span>, but it is not so conversely, as multiple <span class="math inline">\(X\)</span> may correspond to the same <span class="math inline">\(Y\)</span>. Typically, we use <span class="math inline">\(Y\)</span> as a “summary statistic” for the more detailed, but more complicated <span class="math inline">\(X\)</span>. For example, we might have multiple particles in a box, such that <span class="math inline">\(X\)</span> is their individual locations, while <span class="math inline">\(Y\)</span> is their center of mass.</p>
<div id="thm-cond-ent" class="theorem">
<p><span class="theorem-title"><strong>Theorem 8 (conditional entropy)</strong></span> Given any random variable <span class="math inline">\(X\)</span>, and an “observable” variable <span class="math inline">\(Y\)</span> that is determined by <span class="math inline">\(X\)</span>, and some constraints <span class="math inline">\(c\)</span> on <span class="math inline">\(X\)</span>, if <span class="math inline">\(X\)</span> is the distribution that maximizes entropy under constraints <span class="math inline">\(c\)</span>, with entropy <span class="math inline">\(S_X^*\)</span>, then the observable <span class="math inline">\(Y\)</span> is distributed as</p>
<p><span class="math display">\[\rho_Y^*(y) = e^{S_{X|y}^* - S_X^*}, \quad e^{S_X^*} = \int dy\; e^{S_{X|y}^*}\]</span></p>
<p>where <span class="math inline">\(S_{X|y}^*\)</span> is the maximal entropy for <span class="math inline">\(X\)</span> conditional on the same constraints, plus the extra constraint that <span class="math inline">\(Y = y\)</span>.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>By assumption, <span class="math inline">\(X\)</span> is the unique solution to the constrained optimization problem</p>
<p><span class="math display">\[
\begin{cases}
    \max S_X \\
    \text{constraints on $x$}
\end{cases}
\]</span></p>
<p>By <a href="#thm-compound-entropy" class="quarto-xref">Theorem&nbsp;3</a>, the problem is equivalent to:</p>
<p><span class="math display">\[
\begin{cases}
    \max S_Y + \braket{S_{X|y}}_{y\sim Y} \\
    \text{constraints on $x$}
\end{cases}
\]</span></p>
<p>Now, we can solve the original problem in a two-step process: For each possible observable <span class="math inline">\(y\sim Y\)</span>, we solve an extra-constrained problem:</p>
<p><span class="math display">\[
\begin{cases}
    \max S_{X|y} \\
    \text{original constraints on $x$} \\
    \text{$x$ must be chosen such that the observable $Y = y$}
\end{cases}
\]</span></p>
<p>Then, each such problem gives us a maximal conditional<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> entropy <span class="math inline">\(S_{X|y}^*\)</span>, and we can follow it up by solving for <span class="math inline">\(Y\)</span> with</p>
<p><span class="math display">\[\max\left(S_Y + \braket{S_{X|y}^*}_{y \sim Y}\right)\]</span></p>
<p>Again, the solution is immediate once we see it is just the KL-divergence:</p>
<p><span class="math display">\[S_Y + \braket{S_{X|y}^*}_{y \sim Y} = - \int dy \; \rho_Y(y) \ln\frac{\rho_Y(y)}{e^{S_{X|y}^*}} = \ln Z - D_{KL}(\rho_Y \| \rho_Y^*)\]</span></p>
<p>where</p>
<p><span class="math display">\[Z = \int dy\; e^{S_{X|y}^*}, \quad \rho_Y^*(y) = \frac{e^{S_{X|y}^*}}{Z}\]</span></p>
<p>At the optimal point, the entropy for <span class="math inline">\(X\)</span> is maximized at <span class="math inline">\(S_X^* = \ln Z - 0\)</span>, so <span class="math inline">\(Z = e^{S_X^*}\)</span>.</p>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;If you’re a pure mathematician, you can formalize this using <a href="https://en.wikipedia.org/wiki/Disintegration_theorem">measure disintegration</a>.</p></div></div><div class="callout callout-style-default callout-note callout-titled" title="deriving the canonical ensemble yet again">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
deriving the canonical ensemble yet again
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a small system with energy states <span class="math inline">\(E_1, E_2, \dots\)</span> and a large bath system, in energy contact. We can set <span class="math inline">\(X\)</span> to be the combined state of the whole system, and <span class="math inline">\(Y\)</span> to be the state of the small system. Once we observe <span class="math inline">\(y\)</span>, we have fully determined the small system, so the small system has zero entropy, and so all the entropy comes from the bath system: <span class="math display">\[S_{X|y}^* = S_{\text{bath}} = S_{\text{bath}}(E_{total}) - \beta E_y\]</span></p>
<p>Consequently, the distribution of the small system is <span class="math inline">\(\rho_Y(y) \propto e^{-\beta E_y}\)</span>, as we expect.</p>
<p>A similar calculation gives us the grand canonical ensemble, etc.</p>
</div>
</div>
<div id="thm-cond-free-ent" class="theorem">
<p><span class="theorem-title"><strong>Theorem 9 (conditional free entropy)</strong></span> Given any random variable <span class="math inline">\(X\)</span>, and an “observable” variable <span class="math inline">\(Y\)</span> that is determined by <span class="math inline">\(X\)</span>, and some constraints <span class="math inline">\(c\)</span> on <span class="math inline">\(X\)</span>, if <span class="math inline">\(X\)</span> is the distribution that maximizes Helmholtz free entropy under constraints <span class="math inline">\(c\)</span>, with Helmholtz free entropy <span class="math inline">\(f_X^*\)</span>, then the observable <span class="math inline">\(Y\)</span> is distributed as <span class="math display">\[\rho_Y^*(y) = e^{f_{X|y}^* - f_X^*}, \quad e^{f_X^*} = \int dy\; e^{f_{X|y}^*}\]</span></p>
<p>where <span class="math inline">\(f_{X|y}^*\)</span> is the maximal Helmholtz free entropy for <span class="math inline">\(X\)</span> conditional on the same constraints, plus the constraint that <span class="math inline">\(Y = y\)</span>.</p>
<p>Similarly for Gibbs free entropy, and all other free entropies.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>First note that <span class="math inline">\(f_X = S_Y + \braket{f_{X|y}}_y\)</span>, then argue in the same way.</p>
</div>
</div>
</div>
</section>
</section>
<section id="kinetic-gas-theory" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="kinetic-gas-theory">Kinetic gas theory</h2>
<p>Kinetic gas theory is <em>the</em> paradigm for pre-1930 statistical mechanics. Boltzmann devoted his best years to kinetic gas theory. The connection between kinetic gas theory and statistical mechanics was so strong that it was often confused as one. Modern statistical mechanics has grown to be so much more than this, so we will only settle for deriving the van der Waals equation. This strikes a balance between triviality (the ideal gas equation could be derived in literally two lines) and complication (Boltzmann’s monumental <em>Lectures on Gas Theory</em> has 500 pages <span class="citation" data-cites="boltzmannLecturesGasTheory2011">(<a href="#ref-boltzmannLecturesGasTheory2011" role="doc-biblioref">Boltzmann 2011</a>)</span>).</p>
<p>To review, the van der Waals gas equation is</p>
<p><span class="math display">\[P = \frac{N/\beta}{V- bN} - \frac{cN^2}{V^2}\]</span></p>
<p>where <span class="math inline">\(b, c\)</span> are real numbers that depend on the precise properties of the gas molecules. The term <span class="math inline">\(V - bN\)</span> accounts for the fact that each gas molecule excludes some volume, so that, as <span class="math inline">\(N\)</span> grows, it corrects for the ideal gas pressure <span class="math inline">\(P_{ideal}\)</span> by <span class="math inline">\(\sim P_{ideal}\frac{bN}{V}\)</span>. The term <span class="math inline">\(\frac{cN^2}{V^2}\)</span> accounts for overall interaction energy between gas molecules. Suppose the interaction is overall attractive, then we would have <span class="math inline">\(c &gt; 0\)</span>, and otherwise <span class="math inline">\(c &lt; 0\)</span>.</p>
<section id="ideal-gas" class="level3">
<h3 class="anchored" data-anchor-id="ideal-gas">Ideal gas</h3>
<p>Consider a tank of ideal gas consisting of <span class="math inline">\(N\)</span> point-masses, flying around in a free space with volume <span class="math inline">\(V\)</span>. The tank of gas has inverse temperature <span class="math inline">\(\beta\)</span>, so its phase-space distribution is</p>
<p><span class="math display">\[
\rho(q_{1:N}, p_{1:N}) = \prod_{i\in 1:N} \rho(q_i, p_i), \quad \rho(q, p) = \underbrace{\frac{1}{V}}_{\text{free space}} \times \underbrace{\frac{e^{-\beta \frac{\|p_i\|^2}{2m}}}{(2\pi m/\beta)^{3/2}}}_{\text{Boltzmann momentum distribution}}
\]</span></p>
<p>The total energy of the gas has no positional term, so it is all due to momentum. Because the momenta coordinates <span class="math inline">\(p_{1,x}, p_{1,y}, \dots, p_{N,y}, p_{N,z}\)</span> do not interact, their kinetic energies simply sum, giving</p>
<p><span class="math display">\[
U = 3N \times \int_{\mathbb{R}}dp\; \frac{p^2}{2m} \frac{e^{-\frac{p^2}{2m/\beta}}}{\sqrt{2\pi m/\beta}} = \frac{3N}{2\beta}
\]</span></p>
<p>This is the same as Boltzmann’s derivation so far. However, although entropy is exactly defined when there are only finitely or countably many possible states, as <span class="math inline">\(\sum_{j \in \mathbb{N}} -p_j \ln p_j\)</span>, this is not so when state space is uncountably large, like <span class="math inline">\(\mathbb{R}^{6N}\)</span>. When Boltzmann encountered the issue, he solved it by discretizing the phase space into <em>arbitrary but small</em> cubes. The effect is that he could rederive the ideal gas laws, but the entropy has an additive constant that depends on the exact choice of the cube size. This was not a problem for Boltzmann, who was trying to found classical thermodynamics upon statistical mechanics, and in classical thermodynamics, entropy <em>does</em> have an indeterminant additive constant.</p>
<p>Later, Planck in his derivation of the blackbody radiation law, used the same trick. Ironically, Planck did not believe in atoms nor quantized light, but he did make the correct assumption that there is a natural unit of measurement for phase space area, which he called <span class="math inline">\(h\)</span>, and which we know as Planck’s constant. <span class="citation" data-cites="duncanConstructingQuantumMechanics2019">(<a href="#ref-duncanConstructingQuantumMechanics2019" role="doc-biblioref">Duncan and Janssen 2019, chap. 2</a>)</span>.</p>
<p>Following Planck, we discretize the phase space into little cubes of size <span class="math inline">\(h^{3N}\)</span>, and continue:</p>
<p><span class="math display">\[
\begin{aligned}
    S &amp;= -\sum_{i \in\text{Little cubes}} p_i \ln p_i \\
    &amp;\approx -\sum_{i \in\text{Little cubes}} (\rho(i) h^{3N}) \ln (\rho(i) h^{3N}) \\
    &amp;\approx -\int_{\mathbb{R}^{6N}} dp_{1:N}dq_{1:N} \; \rho(p_{1:N}, q_{1:N}) \ln (\rho(p_{1:N}, q_{1:N}) h^{3N}) \\
    &amp;= -\int_{\mathbb{R}^{6N}} dp_{1:N}dq_{1:N} \; \rho(p_{1:N}, q_{1:N}) \ln \rho(p_{1:N}, q_{1:N}) - 3N \ln h \\
    &amp;= -\underbrace{N\int_{\mathbb{R}^{6}} dpdq \; \rho(p, q) \ln \rho(p, q)}_{\text{non-interacting particles}} - 3N \ln h
\end{aligned}
\]</span></p>
<p>Now, the entropy of a single atom <span class="math inline">\(\int_{\mathbb{R}^{6}} dpdq \; \rho(p, q) \ln \rho(p, q)\)</span> factors again into one position space and three momentum spaces:</p>
<p><span class="math display">\[
\begin{aligned}
    -\int_{\mathbb{R}^{6}} dpdq \; \rho(p, q) \ln \rho(p, q) &amp;= -\int_{\mathbb{R}^3} dq \rho(q) \ln \rho(q) - \sum_{i = x, y, z} \int_{\mathbb{R}} dp_i \ln \rho(p_i) \\
    &amp;= \ln V + 3 \times \underbrace{(\text{entropy of }\mathcal N(0, m/\beta))}_{\text{check Wikipedia}} \\
    &amp;= \ln V + \frac 32 \ln(2\pi m/\beta) + \frac 32 \\
\end{aligned}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Does this remind you of our previous discussion about how <a href="#sec-differential-entropy-ill-defined">differential entropy is ill-defined</a>? Finally that discussion is paying off! The choice of a natural unit of measurement in phase space is <em>equivalent</em> to fixing a natural base measure on phase space, such that differential entropy becomes well-defined.</p>
</div>
</div>
<p>The above is not yet correct, because permuting the atoms does not matter. That is, we have grossly inflated the state space. For example, if <span class="math inline">\(N = 2\)</span>, then we have counted the state <span class="math inline">\((q_1, p_1, q_2, p_2)\)</span>, then <span class="math inline">\((q_2, p_2, q_1, p_1)\)</span>, as if they are different, but they must be counted as the same. We must remove this redundancy by “quotienting out” the permutation group over the particles. The effect is dividing the phase space by <span class="math inline">\(\ln N!\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    \frac SN &amp;= \ln V + \frac 32 \ln(2\pi m/\beta) + \frac 32 - 3 \ln h - \underbrace{\frac 1N \ln N!}_{\text{Stirling's approximation}} \\
    &amp;= \ln\left[\frac{V}{N} \left(\frac{2\pi m}{\beta h^2}\right)^{\frac 32}\right] + \frac 52
\end{aligned}
\]</span></p>
<p>giving us the <strong>Sackur–Tetrode formula</strong>:</p>
<p><span class="math display">\[
S(U, V, N) = \ln\left[\frac{V}{N} \left(\frac{4\pi m U}{3N h^2}\right)^{\frac 32}\right] + \frac 52
\]</span></p>
<p>All other thermodynamic quantities can then be derived from this. For example, the pressure is</p>
<p><span class="math display">\[P = \beta^{-1}(\partial_V S)_{U, N} = \frac{1}{\beta V}\]</span></p>
<p>more conventionally written as <span class="math inline">\(PV = \beta^{-1} = Nk_BT\)</span>, the <strong>ideal gas equation</strong>, where we have re-inserted the Boltzmann constant in respect for tradition.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="How Tetrode measured $h$">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How Tetrode measured <span class="math inline">\(h\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>In early 1900s, Walther Nernst proposed the third law of thermodynamics. The history is rather messy, but suffice to say that the version we are going to care about says, “At the absolute zero of temperature the entropy of every chemically homogeneous solid or liquid body has a zero value.”. In support, he studied experimentally the thermodynamic properties of many materials at temperatures approaching absolute zero. He had a hydrogen liquefier and could reach around <span class="math inline">\(20 \;\mathrm{K}\)</span>.</p>
<p>Working on the assumption that <span class="math inline">\(S = 0\)</span> in any chemical at <span class="math inline">\(T = 0\)</span>, he could measure the entropy of any substance by slowing heating up a substance (or cooling down), measuring its heat capacity at all temperatures, then take an integral:</p>
<p><span class="math display">\[
k_B S = \int \frac{CdT}{T}
\]</span></p>
<p>The low-temperature data for mercury was the most available (mercury was also the substance with which Onnes discovered superconductivity). However, mercury is mostly in a liquid form at low temperatures. Fortunately, the latent heat of vaporization <span class="math inline">\(\Delta L\)</span> can be measured, and then we can get</p>
<p><span class="math display">\[
S_{\text{vapor}} = S_{\text{liquid}} + \frac{\Delta L}{k_BT}
\]</span></p>
<p>Back then, <span class="math inline">\(k_B = \frac{\text{Gas constant}}{\text{Avogadro constant}}\)</span>, and the <span class="math inline">\(S_{\text{liquid}}, \Delta L\)</span> of mercury were all measured, so combining these, Tetrode calculated a value of <span class="math inline">\(h\)</span> that is within <span class="math inline">\(30\%\)</span> of modern measurement. <span class="citation" data-cites="grimus100thAnniversarySackur2013">(<a href="#ref-grimus100thAnniversarySackur2013" role="doc-biblioref">Grimus 2013</a>)</span></p>
</div>
</div>
</section>
<section id="ideal-gas-again" class="level3">
<h3 class="anchored" data-anchor-id="ideal-gas-again">Ideal gas (again)</h3>
<p>We rederive the thermodynamic properties of ideal monoatomic gas via Helmholtz free entropy.</p>
<p><span class="math display">\[Z = \int e^{-\beta E} = \underbrace{\frac{1}{N!}}_{\text{identical particles}} \underbrace{V^N}_{\text{position}} \underbrace{(2\pi m/\beta )^{\frac 32 N}}_{\text{momentum}}\]</span></p>
<p>By the same formula from classical thermodynamics,</p>
<p><span class="math display">\[
d\ln Z = -\braket{E}d\beta + \beta\braket{P} dV \implies
\begin{cases}
    \braket{E}   &amp;= \frac 32 \frac{N}{\beta} \\
    \braket{P}V  &amp;= \frac{N}{\beta}
\end{cases}
\]</span></p>
<p>Notice how the <span class="math inline">\(\ln N!\)</span> part simply does not matter in this case.</p>
</section>
<section id="hard-ball-gas-dilute-gas-limit" class="level3">
<h3 class="anchored" data-anchor-id="hard-ball-gas-dilute-gas-limit">Hard ball gas (dilute gas limit)</h3>
<p>In order to refine the approach, we need to account for two effects.</p>
<ol type="1">
<li>Each particle takes up finite volume, which forces the total volume of positional space to be smaller than <span class="math inline">\(V^N\)</span>.</li>
<li>Particle pairs have interactions, which changes the Boltzmann distribution.</li>
</ol>
<p>The first effect can be modelled by assuming each atom is a hard ball of radius <span class="math inline">\(r\)</span>. The particles still have no interaction <em>except</em> that their positions cannot come closer than <span class="math inline">\(2r\)</span>.</p>
<p>Because there is no potential energy, the Boltzmann distribution on momentum space is the same, and so the Helmholtz free entropy <span class="math inline">\(\ln Z\)</span> still splits into the sum of positional entropy and momentous entropy. The momentum part is still <span class="math inline">\(\frac 32 N \ln\frac{2\pi}{\beta m}\)</span>, as the hard balls do not interfere with each other’s momentum, but the position part is smaller, because the balls mutually exclude each other.</p>
<p>Let <span class="math inline">\(a = 8V_{ball} = \frac{32}{3}\pi r^3\)</span> be a constant for the gas.</p>
<p>To measure the volume of the diminished position space, we can add one hard ball at a time. The first hard ball can take one of <span class="math inline">\(V\)</span> possible positions, as before. The next ball’s center cannot be within <span class="math inline">\(2r\)</span> of the center of the first ball, so its position can only take one of <span class="math inline">\((V - a)\)</span> positions, where <span class="math inline">\(a = 8V_{ball} = \frac{32}{3}\pi r^3\)</span> is a constant that depends on the shape of the hard balls. We continue this argument, obtaining the total volume in position space:</p>
<p><span class="math display">\[V(V- a) \cdots (V - (N-1)a) \approx V^N e^{0 -\frac{a}{V}-2\frac{a}{V} -\dots -(N-1)\frac{a}{V}} \approx V^N\left(1- \frac{N^2 a}{2V} \right)\]</span></p>
<p>This gives us</p>
<p><span class="math display">\[\braket{E} = \frac{3N}{2\beta}, \quad \braket{P}V \approx \frac N\beta \left(1 + \frac{a N}{2V}\right) \approx \frac{N/\beta}{V - \frac a2 N}\]</span></p>
<p>The second equation is the van der Waals equation when the term <span class="math inline">\(c = 0\)</span>, meaning there is neither attraction nor repulsion between particles.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="virial expansion">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
virial expansion
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the above derivation, we are assuming that only pairwise exclusion matters. That is, we ignore the possibility that three or more balls may simultaneously intersecting each other. We can make a more accurate counting argument via the <a href="https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle">inclusion-exclusion principle</a>, which would lead us to a <a href="https://en.wikipedia.org/wiki/Virial_expansion">virial expansion</a> for gas.</p>
<p>Specifically, if the balls <span class="math inline">\(A, B\)</span> are intersecting, which has probability <span class="math inline">\(a/V\)</span>, and <span class="math inline">\(B, C\)</span> are also intersecting, also with probability <span class="math inline">\(a/V\)</span>, then <span class="math inline">\(A, C\)</span> are quite likely to be also intersecting, with probability much higher than <span class="math inline">\(a/V\)</span>. Therefore, if we have excluded the cases where <span class="math inline">\(A, B\)</span> are intersecting by subtracting with <span class="math inline">\(a/V\)</span>, and the cases where <span class="math inline">\(B, C\)</span> are intersecting by subtracting another <span class="math inline">\(a/V\)</span>, then we should be subtracting with something less than <span class="math inline">\(a/V\)</span>. The cluster expansion principle makes this precise. Unfortunately, it requires some difficult combinatorics. The interested reader should study <span class="citation" data-cites="andersenClusterMethodsEquilibrium1977">(<a href="#ref-andersenClusterMethodsEquilibrium1977" role="doc-biblioref">Andersen 1977</a>)</span>.</p>
</div>
</div>
</section>
<section id="soft-ball-gas-high-temperature-and-dilute-gas-limit" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="soft-ball-gas-high-temperature-and-dilute-gas-limit">Soft ball gas (high temperature and dilute gas limit)</h3>
<p>In the above derivation, we got one part of van der Waals equation right – the part where particles take up space. However, we have not yet accounted for the force between particles. We expect that if the particles attract each other, then <span class="math inline">\(P\)</span> should be smaller, and if the particles repel each other, then <span class="math inline">\(P\)</span> should be larger.</p>
<p>Let’s assume the gas is made of balls that has a hard core and a soft aura. That is, they repulse or attract each other at a distance, and when a pair comes too close. We also assume the force law depends only on the distances between particles.</p>
<p>That is, we can write such a system as having a gas potential energy <span class="math inline">\(V(q_1, \dots, q_N) = \sum_{i &lt; j} V(\|q_i - q_j\|)\)</span>. To enforce the hard core, we should have <span class="math inline">\(V(r) = \infty\)</span> when <span class="math inline">\(r \in [0, r_0]\)</span>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Lennard-Jones_potential.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Example potential energy field. This is the <a href="https://en.wikipedia.org/wiki/Lennard-Jones_potential">Lennard-Jones potential</a>, with a hard (not perfectly hard, but hard enough!) exclusive core, a soft repelling middle, and an attraction when far away. Figure from <a href="https://commons.wikimedia.org/wiki/File:Graph_of_Lennard-Jones_potential.png">Wikimedia Commons</a></figcaption>
</figure>
</div>
<p>Now, the partition function becomes</p>
<p><span class="math display">\[
Z = \int e^{-\beta\sum_i \frac{p_i^2}{2m} - \beta\sum_{i &lt; j}V(\|q_i - q_j\|)} dqdp
\]</span></p>
<p>The momentum part is still the same <span class="math inline">\((2\pi/\beta m)^{\frac 32 N}\)</span>, but the position part is more difficult now. Still, we hope it will be close to <span class="math inline">\(V^N\)</span>.</p>
<p>That is, we need to calculate:</p>
<p><span class="math display">\[Z = \underbrace{V^N (2\pi/\beta m)^{\frac 32 N} \frac{1}{V^N}}_{\text{ideal gas}} \int_{V^N} e^{ - \beta\sum_{i &lt; j}V(\|q_i - q_j\|)} dq\]</span></p>
<p>The integral <span class="math inline">\(\int_{V^N} e^{ - \beta\sum_{i &lt; j}V(\|q_i - q_j\|)} dq\)</span> can be evaluated piece-by-piece: <span class="math display">\[
\int_{V^N} e^{ - \beta\sum_{i &lt; j}V(\|q_i - q_j\|)} dq = \int dq_1 \left(\int dq_2 \; e^{-\beta V(\| q_1 - q_2 \|)} \left(\int dq_3 \; e^{-\beta (V(\| q_1 - q_3 \|) + V(\| q_2 - q_3 \|))} \cdots\right)\right)
\]</span></p>
<p>Because the chamber is so much larger than the molecular force-field, it is basically infinite. So for almost all of <span class="math inline">\(q_1\)</span> (except when it is right at the walls of the chamber), <span class="math inline">\(\int dq_2 \; e^{-\beta V(\| q_1 - q_2 \|)} \approx V - \delta\)</span>, where <span class="math inline">\(\delta\)</span> is some residual volume:</p>
<p><span class="math display">\[\delta := \int_{V} dq_2 \; (1 - e^{-\beta V(\|q_2 \|)})\]</span></p>
<p>Furthermore, because we are dealing with a dilute gas, the higher-order interactions don’t matter (see previous remark about the virial expansion). Therefore, the integral <span class="math display">\[\int_{V} dq_3 \; e^{-\beta (V(\| q_1 - q_3 \|) + V(\| q_2 - q_3 \|))} \approx \int_{V_1 \cup V_2 \cup V_3} dq_3 \; e^{-\beta (V(\| q_1 - q_3 \|) + V(\| q_2 - q_3 \|))} \]</span></p>
<p>where <span class="math inline">\(V_1\)</span> is the “turf” of particle <span class="math inline">\(1\)</span>, and <span class="math inline">\(V_2\)</span> is the turf of particle <span class="math inline">\(2\)</span>, and <span class="math inline">\(V_3\)</span> is the rest of the volume. Because the gas is dilute, we have basically <span class="math inline">\(V_1\)</span> disjoint from <span class="math inline">\(V_2\)</span>, giving us<br>
<span class="math display">\[\approx \sum_{j = 1, 2, 3}\int_{V_j} dq_3\; e^{-\beta V(\| q_j - q_3 \|)} \approx V - 2\delta\]</span></p>
<p>Together, we have <span class="math display">\[\int_{V^N} e^{ - \beta\sum_{i &lt; j}V(\|q_i - q_j\|)} dq  \approx V(V-\delta) \cdots(V - (N-1)\delta)\]</span></p>
<p>Giving us <span class="math display">\[\ln Z \approx \ln Z_{\text{ideal}} - \frac{N^2 \delta}{2V}\]</span></p>
<p>It remains to calculate the residual volume. It has two parts, one due to the hard core and one due to the soft halo: <span class="math display">\[\delta = \int_{\|q_2 \| \leq r_0} dq_2 \; (1 - e^{-\infty}) + \int_{\|q_2 \| &gt; r_0} dq_2 \; (1 - e^{-\beta V(\|q_2\|)})\]</span></p>
<p>The first part is just <span class="math inline">\(a\)</span>, as calculated previously. The second part depends on the exact shape of the potential well. However, when temperature is high, <span class="math inline">\(\beta\)</span> would be very small, so the second part is approximately <span class="math inline">\(\int dq_2 (\beta V)\)</span>, which is a constant times <span class="math inline">\(\beta\)</span>.</p>
<p>Thus, we have <span class="math display">\[\ln Z \approx \ln Z_{\text{ideal}} - \frac{N^2}{V}(a + b \beta)\]</span></p>
<p>for some constants <span class="math inline">\(a, b\)</span>. This gives us the van der Waals equation: <span class="math display">\[\braket{P} V = \frac{N}{\beta} + \frac{N^2}{\beta V}a + \frac{N^2}{V} b\]</span></p>
</section>
</section>
<section id="other-classical-examples" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="other-classical-examples">Other classical examples</h2>
<section id="countably-many-states" class="level3">
<h3 class="anchored" data-anchor-id="countably-many-states">Countably many states</h3>
<p>In a surprising number of applications, we have a single system in an energy bath. The system has finitely many, or countably infinitely many, distinguishable states, each with a definite energy: <span class="math inline">\(E_0 \leq E_1 \leq E_2 \leq \cdots\)</span>. In particular, this covers most of the basic examples from quantum mechanics. In such a system, the probability of being in state <span class="math inline">\(i\)</span> is <span class="math inline">\(p_i = \frac 1Z e^{-\beta E_i}\)</span> where</p>
<p><span class="math display">\[
Z = \sum_i e^{-\beta E_i}
\]</span></p>
<p>Because I don’t like sections that are literally two paragraphs long, I will reformulate this as multinomial regression in mathematical statistics.</p>
<p>In the problem of classification, we observe some vector <span class="math inline">\(\vec X\)</span>, and we need to classify it into one of finitely many states <span class="math inline">\(\{1, 2, \dots\}\)</span>. With multinomial regression, we construct one vector <span class="math inline">\(\vec b_i\)</span> for each possible state <span class="math inline">\(i\)</span>, and then declare that the probability of being in state <span class="math inline">\(k\)</span> is</p>
<p><span class="math display">\[
Pr(i | X) = \frac{e^{-\vec X \cdot \vec b_i}}{Z(X)}, \quad Z(\vec X) = \sum_j e^{-\vec b_j \cdot \vec X}
\]</span></p>
<p>To make the parallel clearer:</p>
<p><span class="math display">\[
\begin{aligned}
\text{log probability} &amp; &amp; \text{observable } &amp; &amp;\text{ feature} &amp; &amp; \text{normalization constant} \\
    \ln p(i | \beta) &amp;=&amp; -\beta &amp; \; &amp; E_i &amp; &amp; - \ln Z \\
    \ln Pr(i | \vec X) &amp;=&amp;    -\vec X     &amp; \cdot &amp; \vec b_i &amp; &amp; - \ln Z
\end{aligned}
\]</span></p>
<p>We can make the analogy exact by adding multiple observables. Specifically, if we solve the following constrained optimization problem</p>
<p><span class="math display">\[
\begin{cases}
\max S \\
\braket{\vec b} = \vec b_0
\end{cases}
\]</span></p>
<p>then the solution is a multinomial classifier, with <span class="math inline">\(\vec X\)</span> playing the role of <span class="math inline">\(\beta\)</span>.</p>
<p>Interpreting the physics as statistics, we can think of <span class="math inline">\(\beta\)</span> as an “observable”. It is as if we are asking the physical system “What state are you in?” but we can only ask a very crude question “What is your energy on average?” Knowing that, we can make a reasonable guess by using the maximal entropy compatible with that answer.</p>
<p>Interpreting the statistics as physics, we can think of the observable <span class="math inline">\(\vec X\)</span> as “entropic forces”, trying to push the system towards the distribution of maximal entropy. At the equilibrium of zero entropic force, we have a multinomial classifier. This is the prototypical idea of <a href="https://en.wikipedia.org/wiki/Energy-based_model">energy-based statistical modelling</a>.</p>
</section>
<section id="fluctuation-by-n-12" class="level3">
<h3 class="anchored" data-anchor-id="fluctuation-by-n-12">Fluctuation by <span class="math inline">\(N^{-1/2}\)</span></h3>
<p>Suppose we have several tanks of oxygen gas that can exchange energy. They are in a microcanonical ensemble. Now, if we measure the total energy in the first tank, we would get a value <span class="math inline">\(E_1\)</span>. We sample it again after a while, and we would get another value. Averaging them, we would get <span class="math inline">\(\braket{E_1}\)</span>, which ought to match the prediction by classical thermodynamics. However, if thermodynamics is the theory of the average, then to go beyond it, statistical mechanics must predict the variance as well.</p>
<p>In <a href="#thm-partition-cumulant" class="quarto-xref">Theorem&nbsp;7</a>, We had already seen that the partition function generates the mean, the variance, and all other terms. Here we expand on this.</p>
<p>Take several systems, and let them exchange energy, but nothing else. For concreteness, we can imagine taking several copper tanks of gas, and let them touch each other. The tanks hold their shape, not expanding or contracting. The system has total entropy</p>
<p><span class="math display">\[S = \sum_i S_i(E_i, A_i)\]</span></p>
<p>where <span class="math inline">\(A_i\)</span> stand for the other state variables we don’t care about, because they are held constant. Now, there is a single constraint of constant total energy:</p>
<p><span class="math display">\[E = \sum_i E_i\]</span></p>
<p>In the thermodynamical limit, the compound system reaches the maximal entropy state <span class="math inline">\(E_1^*, \dots, E_n^*\)</span>, which solves the following constrained maximization</p>
<p><span class="math display">\[
\begin{cases}
    \max \sum_i S_i(E_i, A_i)\\
    E = \sum_i E_i
\end{cases}
\]</span></p>
<p>By calculus, at the optimal point, all systems satisfy</p>
<p><span class="math display">\[
(\partial_{E_i} S_i)_{A_i} = \beta
\]</span></p>
<p>for some number <span class="math inline">\(\beta\)</span>. This is the <em>zeroth law of thermodynamics</em>.</p>
<p>However, we are in statistical mechanics, so the compound system actually does not stay exactly at the optimal point. Instead, the energy levels fluctuate. Write the <strong>fluctuation vector</strong> as</p>
<p><span class="math display">\[Y = (\Delta E_1, \dots, \Delta E_n)\]</span></p>
<p>which satisfies the constraint <span class="math inline">\(\sum_i \Delta E_i = 0\)</span>.</p>
<p>Suppose we observe the fluctuation vector to be a certain value <span class="math inline">\(Y = y\)</span>, then by <a href="#thm-cond-ent" class="quarto-xref">Theorem&nbsp;8</a>,</p>
<p><span class="math display">\[\rho_Y(y) \propto e^{S^*_{X|y}}\]</span></p>
<p>where <span class="math inline">\(S^*_{X|y}\)</span> is the entropy of the compound system, given <span class="math inline">\(Y = y\)</span>. For small fluctuations, we do a Taylor expansion:</p>
<p><span class="math display">\[S^*_{X|y} = \sum_i S_i(E_i^*) + \underbrace{(\partial_{E_i} S_i)_{A_i}}_{\text{$=\beta$}} \Delta E_i + \frac 12 (\partial_{E_i}^2 S_i)_{A_i} (\Delta E_i)^2 + \cdots\]</span></p>
<p>Since <span class="math inline">\(\sum_i \Delta E_i = 0\)</span> at the equilibrium point,</p>
<p><span class="math display">\[\rho_Y(\Delta E) \propto e^{\sum_i \frac 12 (\partial_{E_i}^2 S_i)_{A_i} (\Delta E_i)^2}\]</span></p>
<p>Now, <span class="math inline">\(\partial_E S = \beta\)</span>, and <span class="math inline">\(\partial_E^2 S = -\frac{1}{T^2 C}\)</span> in typical thermodynamic notation, where <span class="math inline">\(C\)</span> is <span class="math inline">\(\partial_T E\)</span>, the heat capacity (holding all other variables <span class="math inline">\(A\)</span> constant), so we have the following equation:</p>
<p><span class="math display">\[\rho_Y(\Delta E) \propto e^{-\sum_i \frac{1}{2T^2 C_{i}} (\Delta E_i)^2}\]</span></p>
<p>with fluctuation on the order <span class="math inline">\(\Delta E_i \sim \sqrt{T^2 C_i}\)</span>. For most substances studied by 19th century physicists, such as gas, that is <span class="math inline">\(\sim \sqrt{N} k_B T\)</span>. If they could measure the energy of gas at <span class="math inline">\(T = 500 \;\mathrm{K}\)</span> with precision down to <span class="math inline">\(10^{-3} \;\mathrm{J}\)</span>, that would still require a tank of gas with <span class="math inline">\(N = 10^{34} = 10^{10} \;\mathrm{mol}\)</span>. If they wanted to study this in oxygen, they would need 0.1 million tonnes of it.</p>
</section>
<section id="blackbody-radiation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="blackbody-radiation">Blackbody radiation</h3>
<p>Planck’s derivation of the blackbody radiation is the first great success of quantum statistical mechanics. We give a brief presentation here that tracks Planck’s original argument.</p>
<p>Consider a hollow cubic box with side lengths <span class="math inline">\(L\)</span>. The box has perfectly reflecting walls. At thermal equilibrium, the box is full of standing electromagnetic waves. Each standing EM wave has form <span class="math inline">\(\vec E(x, y, z, t) = \vec E_0 \sin(\omega t)\sin(\frac{n_x \pi x}{L})\sin(\frac{n_y \pi y}{L})\sin(\frac{n_z \pi z}{L})\)</span>, for some positive integers <span class="math inline">\(n_x, n_y, n_z\)</span>. Each wave has wavevectors <span class="math inline">\(\vec k = (n_x, n_y, n_z) \frac{\pi}{L}\)</span>. If we draw a region of volume <span class="math inline">\(\delta K\)</span> in the space of wavevectors, then the region would contain about <span class="math inline">\(\delta K \frac{L^3}{\pi^3}\)</span> valid wavevectors. Thus, we say that the wavevector space is <span class="math inline">\([0, +\infty)^3\)</span>, and has <strong>density of states</strong> <span class="math inline">\(\frac{L^3}{\pi^3}\)</span>. We can picture it as <span class="math inline">\([0, +\infty)^3\)</span> with a rectangular grid of points being the valid wavevectors, such that the numerical density of such grid points is <span class="math inline">\(\frac{L^3}{\pi^3}\)</span>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/density_of_states.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="margin-caption">The rectangular grid of valid wavevectors in the space of possible wavevectors.</figcaption>
</figure>
</div>
<p>At this point, we depart from Planck’s derivation. Instead of considering standing waves in a perfectly reflecting chamber, we consider planar waves in a chamber with periodic boundaries. That is, we imagine that we have opened 6 portals, so that its top wall is “ported” to the bottom, etc. In this case, the planar waves have valid wavevectors <span class="math inline">\(\vec k = (n_x, n_y, n_z) \frac{2\pi}{L}\)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Wait, the numerical density of grid points is now just <span class="math inline">\(\frac{L^3}{8\pi^3}\)</span>, which is <span class="math inline">\(1/8\)</span> of what we found previously?</p>
<p>Yes, indeed, but it will work out correctly, because whereas the density of states has dropped to just <span class="math inline">\(1/8\)</span> of previously, the state space has increased <span class="math inline">\(8\times\)</span>, from <span class="math inline">\([0, +\infty)^3\)</span> to <span class="math inline">\(\mathbb{R}^3\)</span>.</p>
</div>
</div>
<p>Now, we need to allow <em>two</em> states at each valid wavevector, to account for polarization.</p>
<p>At this point, we have decomposed the state space into a composition of oscillators. Because there is no interaction between these oscillators,<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> it remains to calculate the partition function of each oscillator.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;That is, two photons do not interact, except when the energy levels are so high that you would need a quantum field theorist to know what is going on.</p>
<p>Light resembles ghosts and spirits in that they are massless, untouchable, moving very fast, bright, and vaguely associated with good feelings. During the 19th century, the best scientific theory for light, that of ether theory, became the foundation of many spiritualist world systems. <span class="citation" data-cites="aspremPonderingImponderablesOccultism2011">(<a href="#ref-aspremPonderingImponderablesOccultism2011" role="doc-biblioref">Asprem 2011</a>)</span> The connection of electromagnetism with <a href="https://en.wikipedia.org/wiki/Animal_magnetism">animal magnetism</a> did not help. Whereas modern spiritualists talk of electromagnetic fields and quantum vibrations, a century ago they talked of subatomic structures and ether vibrations.</p></div></div><p>Planck considered an ensemble of <span class="math inline">\(N\)</span> oscillators, all at the same wavevector and polarization. If they have average energy <span class="math inline">\(\braket{E}\)</span>, the question is to find the total entropy for the whole system, which, when divided by <span class="math inline">\(N\)</span>, should yield the entropy of a single oscillator. Here he used the celebrated <em>quantum hypothesis</em>: The energy levels are divided into integer levels of <span class="math inline">\(nh\nu\)</span>, where <span class="math inline">\(n = 0, 1, 2, \dots\)</span>. By the stars and bars argument, there are <span class="math inline">\(\binom{N + M-1}{M}\)</span> ways do distribute these energy-quanta between these oscillators, where <span class="math inline">\(M = \frac{N\braket{E}}{h\nu}\)</span>.</p>
<p><span class="math display">\[S = \frac 1N \ln \binom{N+M-1}{M} \underbrace{\approx}_{\text{Stirling}} (1 + a) \ln (1+a) - a \ln a, \quad a = \frac{\braket{E}}{h\nu}\]</span></p>
<p>Given the entropy function, he then matched <span class="math inline">\(\braket{E}\)</span> to temperature <span class="math inline">\(\beta\)</span> by the equality <span class="math inline">\(\beta = \partial_{\braket{E}} S\)</span>, giving</p>
<p><span class="math display">\[
\braket{E} = \left(\frac{h\nu}{e^{\beta h\nu}-1}\right)
\]</span></p>
<p>The rest of the derivation is fairly obvious. I will just point out one more interesting fact.</p>
<p>According to Kirchhoff’s law of thermal radiation, a chunk of matter is exactly as absorptive as it is emissive. A black body absorbs all light, and conversely it emits light at the maximal level. A white body absorbs no light, and conversely it does not emit light. This can be understood as a consequence of the second law: If a body emits more light than it absorbs, then it would spontaneously get colder when placed inside a black body radiation chamber.</p>
<p>However, much more can be said than this. Not only is it exactly as absorptive as it is emissive, it is as absorptive as it is emissive at <em>any angle</em>, at any wavelength, and any polarization. So for example, if a piece of leaf is not absorptive when viewed from an angle, at the green light wavelength, of clockwise polarization, then it is not emissive under the same angle, wavelength, polarization.</p>
<p>Why is that? The standard argument <span class="citation" data-cites="reifFundamentalsStatisticalThermal1998">(<a href="#ref-reifFundamentalsStatisticalThermal1998" role="doc-biblioref">Reif 1998, chap. 9.15</a>)</span> uses a time-reversal argument, but I like to think of it as yet more instances of protecting the second law. If you look inside a black body radiation chamber, you would see a maximal entropy state. Light rushes in all directions equally, at all polarizations equally, and the energy is distributed optimally across the spectrum to maximize entropy (because <span class="math inline">\(\beta\)</span> is constant across the whole spectrum). If we have a material that takes in blue light and outputs green light, then it would spontaneously decrease entropy. Similarly, if it can absorb vertically polarized light to emit diagonally polarized light, it would also spontaneously decrease entropy, etc.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Relativistic gas?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Relativistic gas?
</div>
</div>
<div class="callout-body-container callout-body">
<p>A chamber full of black body radiation is also called a photon gas. The photon gas is sometimes treated as the limit of “ultrarelativistic gas”. Start with the relativistic energy function <span class="math inline">\(E = \sqrt{m^2 c^4 + \|p\|^2 c^2}\)</span>, derive its Boltzmann distribution <span class="math inline">\(\rho(q, p) \propto e^{-\beta \sqrt{m^2 c^4 + \|p\|^2 c^2}}\)</span>, then take the <span class="math inline">\(m \to 0\)</span> limit. This gives some correct results, such as the <span class="math inline">\(U = 3PV\)</span>.</p>
<p>However, accounting for the entropy of photon gas, as well as deriving the black body radiation, hinges critically on the photon quantization <span class="math inline">\(E = h\nu, 2h\nu, \dots\)</span>. I guess it can be done correctly by relativistic quantum mechanics, but it is of course beyond the world of classical mechanics.</p>
</div>
</div>
</section>
<section id="sec-rubber-band" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-rubber-band">Rubber bands</h3>
<p>We have a long chain molecule with <span class="math inline">\(N\)</span> joints. Each joint can go forward or backward, with equal energy. Each link between two joints has length <span class="math inline">\(d\)</span>. The total length of the system is <span class="math inline">\(L\)</span>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/rubber_band.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A sample shape from the ensemble of all rubber band shapes.</figcaption>
</figure>
</div>
<p>The entropy of the system, conditional on <span class="math inline">\(L\)</span>, is</p>
<p><span class="math display">\[S = \ln \binom{N}{\frac{N + L/d}{2}}\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="elastic constant">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
elastic constant
</div>
</div>
<div class="callout-body-container callout-body">
<p>The thermodynamic equation for the rubber band is</p>
<p><span class="math display">\[0 = TdS + FdL\]</span></p>
<p>because the internal energy of the rubber band is constant, no matter how the joint turns.</p>
<p>Therefore, the elastic force is</p>
<p><span class="math display">\[F = -T \partial_L S \approx -T \frac{S(L+2d) - S(L)}{2d} \approx \frac{T}{2d }\ln\frac{Nd+L}{Nd - L}\]</span></p>
<p>When <span class="math inline">\(Nd \gg L\)</span>, that is, we have not stretched it close to the breaking point, the elastic force is</p>
<p><span class="math display">\[F \approx \frac{TL}{Nd^2} = k L\]</span></p>
<p>where <span class="math inline">\(k = \frac{T}{Nd^2}\)</span> is the elastic constant, proportional to temperature.</p>
</div>
</div>
<p>Why does the rubber band stiffen when temperature rise? We can interpret it as follows. When we place the rubber band in a chamber of hot air, the air particles would often collide with the links in the rubber band, flipping it. When there are more links going to the right than the left, then the air particles would tend to flip the links to the left, decreasing <span class="math inline">\(L\)</span>, and conversely. The net force is zero only when there are an equal number of links going either way, which is when <span class="math inline">\(L = 0\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="ensemble of lengths">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
ensemble of lengths
</div>
</div>
<div class="callout-body-container callout-body">
<p>Because the rubber band has <span class="math inline">\(dE = TdS + FdL\)</span>, the corresponding free entropy is</p>
<p><span class="math display">\[S + \beta F\braket{L}\]</span></p>
<p>and under the canonical distribution, that free entropy is maximized, meaning that under canonical distribution,</p>
<p><span class="math display">\[\rho(x) = \frac{1}{Z}e^{\beta FL(x)}\]</span></p>
<p>where <span class="math inline">\(x\)</span> is a microstate of the rubber band (i.e.&nbsp;the precise position of each link), and <span class="math inline">\(L(x)\)</span> is the corresponding length (macrostate).</p>
<p>Since for each <span class="math inline">\(L\)</span> there are <span class="math inline">\(\binom{N}{\frac{N + L/d}{2}}\)</span> possible microstates, we have</p>
<p><span class="math display">\[\rho(L) \propto  \binom{N}{\frac{N + L/d}{2}} e^{\beta F L}\]</span></p>
<p>When <span class="math inline">\(Nd \gg L\)</span>, the binomial coefficient is approximately <span class="math inline">\(\exp{[N H_2(\frac 12 + \frac{L}{2dN})]}\)</span>, where <span class="math inline">\(H_2\)</span> is the binary entropy function, therefore</p>
<p><span class="math display">\[\ln \rho(L) \approx \beta F L + NH_2\left(\frac 12 + \frac{L}{2dN}\right) + Const \approx \beta FL - \frac{L^2}{2Nd^2} + Const\]</span></p>
<p>In particular, we see that <span class="math inline">\(\ln \rho(L)\)</span> is maximized at <span class="math inline">\(\braket{L} = \beta F Nd^2\)</span>, exactly what we have found previously. But this time we have also the variance:</p>
<p><span class="math display">\[\sigma_L^2 = \frac{(Nd)^2}{N}\]</span></p>
<p>which again is an example of the general result that <span class="math inline">\(\sigma \propto N^{-1/2}\)</span>.</p>
</div>
</div>
</section>
</section>
<section id="combinatorial-examples" class="level2">
<h2 class="anchored" data-anchor-id="combinatorial-examples">Combinatorial examples</h2>
<section id="multinomials" class="level3">
<h3 class="anchored" data-anchor-id="multinomials">Multinomials</h3>
<p>Let <span class="math inline">\(p_1, \dots, p_k\)</span> be a discrete probability distribution, and let <span class="math inline">\(n_1, \dots, n_k\)</span> be integers that depend on <span class="math inline">\(N\)</span>, such that</p>
<p><span class="math display">\[\lim_{N \to \infty}n_i/N = p_i\]</span></p>
<p>then we have</p>
<p><span class="math display">\[\lim_{N \to \infty}\frac 1N \ln \binom{N}{n_1, \dots, n_k} = -\sum_i p_i \ln p_i\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="idea">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Imagine we have a system made of a single ball that can go into one out of <span class="math inline">\(k\)</span> labelled boxes. Now, let us copy paste the system <span class="math inline">\(N\)</span> times, so we have <span class="math inline">\(N\)</span> balls and <span class="math inline">\(kN\)</span> boxes. Then we can interpret <span class="math display">\[\frac 1N \ln \binom{N}{n_1, \dots, n_k}\]</span></p>
<p>as the maximal average entropy per ball under the following constraints: There are exactly <span class="math inline">\(n_1\)</span> balls in boxes labelled <span class="math inline">\(1\)</span>, etc.</p>
<p>Thinking thermodynamically, we can pick one ball as the system, and put it in contact with a bath made of <span class="math inline">\(N\)</span> balls. Once we calculate the marginal entropies of the bath, we can infer the Boltzmann distribution for the system, and thus the marginal entropy of the system.</p>
<p>Now, here is the non-rigorous part: We <em>hope</em> that as <span class="math inline">\(N \to \infty\)</span>, all correlations (pairwise, triple-wise, etc) between balls decay fast enough, such that the interaction-entropy between the balls drop to zero, leaving <span class="math display">\[\text{average entropy per ball} = \text{marginal entropy of a ball}\]</span></p>
<p>Justifying this rigorously is generally very difficult, and in fact, the assumption is false at phase transitions, where correlations <em>do not</em> decay fast enough.</p>
<p>If this is so, then we need to only show that the marginal distribution of a single ball converges to <span class="math inline">\((p_1, \dots, p_k)\)</span>.</p>
</div>
</div>
<p>This is an example of how the microcanonical ensemble and the canonical ensemble become indistinguishable at the large particle limit. Indeed, Boltzmann often used this equation to derive the canonical ensemble from microcanonical ensembles.</p>
<div class="callout callout-style-default callout-note callout-titled" title="derivation">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
derivation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Suppose we move the ball from box <span class="math inline">\(i\)</span> to box <span class="math inline">\(j\)</span>, then it would force the bath to change all its balls, changing its entropy by <span class="math display">\[
\ln\binom{N}{n_1', \dots, n_k'} -\ln \binom{N}{n_1, \dots, n_k}
\]</span></p>
<p>where <span class="math inline">\(n_i' = n_i + 1, n_j' = n_j - 1\)</span>, and otherwise unchanged.</p>
<p>By definition of multinomials, this is equal to</p>
<p><span class="math display">\[\ln n_j - \ln(n_i+1)\]</span></p>
<p>which converges to <span class="math inline">\((\ln p_j - \ln p_i)\)</span> at large enough <span class="math inline">\(N\)</span>.</p>
<p>Let <span class="math inline">\(X\)</span> stand for the total state, including the bath and the singled-out system, and let <span class="math inline">\(Y\)</span> stand for the state of the singled-out system. By <a href="#thm-cond-ent" class="quarto-xref">Theorem&nbsp;8</a>, when the entire system is at the maximal entropy distribution, the distribution of the singled-out system is</p>
<p><span class="math display">\[\rho^*_Y(y) \propto e^{S_{X|y}^*}\]</span></p>
<p>From the previous calculation, we have <span class="math display">\[S_{X|j}^* - S_{X|j}^* = \ln p_j - \ln p_i\]</span></p>
<p>yielding <span class="math inline">\(S_{X|i}^* = S_0 + \ln p_i\)</span> for some constant <span class="math inline">\(S_0\)</span>, and so <span class="math inline">\(\rho^*_Y(y) \propto p_i\)</span>, finishing the proof.</p>
</div>
</div>
</div>
</section>
<section id="sanov-theorem" class="level3">
<h3 class="anchored" data-anchor-id="sanov-theorem">Sanov theorem</h3>
<p>Consider a population of particles, all in energy-contact with an energy bath of <span class="math inline">\(\beta = 1\)</span>. Each particle has <span class="math inline">\(n\)</span> states, with state <span class="math inline">\(i\)</span> having energy <span class="math inline">\(-\ln p_i\)</span>. Thus, at the Boltzmann distribution, each particle is precisely sampled from the categorical distribution <span class="math inline">\((p_1, \dots, p_n)\)</span>.</p>
<p>Let <span class="math inline">\(X\)</span> stand for the state of the entire population of particles. That is, <span class="math inline">\(X\)</span> describes the precise state of each particle. Let <span class="math inline">\(Y\)</span> stand for the vector of <span class="math inline">\(N_1, \dots, N_n\)</span>. That is, it counts the number of particles in each state.</p>
<p>The free entropy of a single particle is <span class="math display">\[\bar f_X^* = S - \beta \braket{E} = \sum_i (-p_i \ln p_i) - \sum_i p_i E_i = 0\]</span></p>
<p>Because the particles have no interaction energy, the total free entropy of <span class="math inline">\(X\)</span> is <span class="math inline">\(f_X^* = N \bar f_X^* = 0\)</span>.</p>
<p>By <a href="#thm-cond-free-ent" class="quarto-xref">Theorem&nbsp;9</a>, <span class="math display">\[\rho^*_Y(N_1, \dots, N_n) = e^{f_{X|N_1, \dots, N_n}^*}\]</span></p>
<p>where <span class="math inline">\(f_{X|N_1, \dots, N_n}^*\)</span> is the maximal free entropy of the entire system conditional on <span class="math inline">\(Y = (N_1, \dots, N_n)\)</span>.</p>
<p>Now, by definition of <span class="math inline">\(f_{X|N_1, \dots, N_n}^*\)</span>, it is the maximal free entropy under constraint: <span class="math display">\[
\begin{cases}
\max_\rho \sum_{(s_1, \dots, s_N) \in n^N} \rho(s) (-\ln \rho(s) - \beta E(s))  \\
Y = (N_1, \dots, N_n)
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(s_i\)</span> is the state of particle <span class="math inline">\(i\)</span>.</p>
<p>For any <span class="math inline">\(\rho\)</span> that satisfies the constraint of <span class="math inline">\(Y = (N_1, \dots, N_n)\)</span>, the term is a constant</p>
<p><span class="math display">\[E(s) = \sum_{i=1}^n N_i E_i\]</span></p>
<p>Therefore, the problem reduces to maximizing just the entropy under constraint of <span class="math inline">\(Y = (N_1, \dots, N_n)\)</span>, which is a “microcanonical distribution”, the uniform distribution over all possible <span class="math inline">\(\binom{N}{N_1, \dots, N_n}\)</span> ways to satisfy the <span class="math inline">\(Y = (N_1, \dots, N_n)\)</span> constraint.</p>
<p>Therefore, <span class="math display">\[f_{X|N_1, \dots, N_n}^* = \ln \binom{N}{N_1, \dots, N_n} + \sum_i N_i \ln p_i \approx -ND_{KL}(q \| p)\]</span></p>
<p>where <span class="math inline">\(q = (N_1/N, \dots, N_n/N)\)</span> is the empirical distribution.</p>
<p>Therefore, <span class="math display">\[
\frac 1N \ln \rho^*_Y(N_1, \dots, N_n) \to D_{KL}(q \| p)
\]</span></p>
</section>
<section id="surface-area-of-high-dimensional-spheres" class="level3">
<h3 class="anchored" data-anchor-id="surface-area-of-high-dimensional-spheres">Surface area of high-dimensional spheres</h3>
<p>Let <span class="math inline">\(\Omega_N\)</span> be the surface area of a sphere of radius <span class="math inline">\(\sqrt N\)</span> in <span class="math inline">\(\mathbb{R}^N\)</span>, then</p>
<p><span class="math display">\[\ln\Omega_N = \frac N2 \ln (2\pi e)-\frac 12 \ln (\pi e) + O(N^{-1})\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="derivation">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
derivation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(x_1, \dots, x_N\)</span> be sampled IID from <span class="math inline">\(\mathcal N(0, 1)\)</span>, and let <span class="math inline">\(r_N^2 = \sum_i x_i^2\)</span>. By routine calculation, <span class="math inline">\(\mathbb{E}[r_N^2] = N\)</span>, and <span class="math inline">\(\mathbb{E}[r_N^4] = 2N + N^2\)</span>. Therefore, <span class="math inline">\(r_N^2\)</span> is approximately distributed as <span class="math inline">\(\mathcal N(N, 2N)\)</span>, and so <span class="math inline">\(r_N\)</span> is approximately distributed as <span class="math inline">\(\mathcal N(\sqrt N, 1/2)\)</span>.</p>
<p>Now consider two distributions on <span class="math inline">\(\mathbb{R}^N\)</span>. The first is a microcanonical ensemble: <span class="math inline">\(x_{1:N}\)</span> is distributed uniformly on the thin energy shell of <span class="math inline">\(r_N^2 \in [N, N + \delta N]\)</span>. The second is a canonical ensemble: each <span class="math inline">\(x_i\)</span> is distributed independently according to <span class="math inline">\(\mathcal N(0, 1)\)</span>.</p>
<p>We can think of them as particles in a potential well of form <span class="math inline">\(V(x) = \frac 12 x^2\)</span>. The first ensemble is the microcanonical ensemble where the total energy is fixed, and the second is the canonical ensemble at temperature <span class="math inline">\(\beta = 1\)</span>.</p>
<p>We can calculate the entropy of the canonical ensemble in two ways. We can calculate it by adding up the entropy of each particle, which are the same since there is no interaction energy between particles. We can also calculate it indirectly, by first sampling a random radius, then a random point from the microcanonical ensemble, then multiplying them together.</p>
<p>Because the canonical ensemble is spherically symmetric, the radius and the direction of the vector <span class="math inline">\(x_{1:N}\)</span> are independent. Therefore,</p>
<p><span class="math display">\[S_{\text{canonical}} = S_{\text{microcanonical}} + S_{\text{radius}}\]</span></p>
<p>or</p>
<p><span class="math display">\[\ln \Omega_N = \underbrace{S_{\text{canonical}}}_{\text{$= N S[\mathcal N(0, 1)]$}} - \underbrace{S_{\text{radius}}}_{\text{$\approx S[\mathcal N(0, 1/2)]$}}\]</span></p>
<p>Because the entropy of <span class="math inline">\(\mathcal N(0, \sigma^2)\)</span> is <span class="math inline">\(\frac 12 \ln(2\pi e \sigma^2)\)</span>, we plug them in and obtain the result.</p>
</div>
</div>
</div>
<p>Again, this is an example of a general pattern: the microcanonical ensemble and the canonical ensemble become indistinguishable when the number of particles goes infinite.</p>
</section>
</section>
<section id="biological-examples" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="biological-examples">Biological examples</h2>
<section id="how-elastic-is-the-skin-of-red-blood-cell" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="how-elastic-is-the-skin-of-red-blood-cell">How elastic is the skin of red blood cell?</h3>
<p><span class="citation" data-cites="discherNewInsightsErythrocyte2000">(<a href="#ref-discherNewInsightsErythrocyte2000" role="doc-biblioref">Discher 2000</a>)</span> measured the elasticity of the red blood cell’s skin. To a good approximation, it is just a 2D spring, following Hooke’s law. He attached a tiny particle to the surface of a red blood cell, and measured its thermal motion.</p>
<p>As usual, in the microscopic world, inertia might as well not exist,<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> so the oscillator’s energy is entirely elastic. Let it be of form <span class="math inline">\(\frac 12 k(x^2 + y^2)\)</span>. By equipartition of energy, we would have</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;But just to be sure, I checked the original numbers: It is a particle of diameter <span class="math inline">\(40 \;\mathrm{nm}\)</span>, and moving at about <span class="math inline">\(100 \;\mathrm{nm/s}\)</span>, so its kinetic energy is <span class="math inline">\(\sim 10^{-33}\;\mathrm{J} \sim 10^{-13} k_BT\)</span>.</p></div></div><p><span class="math display">\[\frac 12 k\braket{x^2} = \frac 12 \beta^{-1}\]</span></p>
<p>The data shows that <span class="math inline">\(\braket{x^2} = (35 \;\mathrm{nm})^2\)</span> at temperature <span class="math inline">\(T = 310 \;\mathrm{K}\)</span>, giving us an effective elastic constant of <span class="math inline">\(k \sim 0.004 \;\mathrm{pN/nm}\)</span>.</p>
<p>A red blood cell has diameter <span class="math inline">\(10^4 \;\mathrm{nm}\)</span>, so dragging a patch of its skin all across the surface would take only about <span class="math inline">\(40 \;\mathrm{pN}\)</span>. This is about the force output of 10 kinesins working in concert. Thus we can say that the skin of red blood cell is very slack.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/red_blood_cell_discher_2000.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">(A): A tiny particle attached to long molecules in the skin of the red blood cell. (B): Photo of a red blood cell with attached nanoparticle. (C): An example <span class="math inline">\((x,y)\)</span> trajectory. <span class="citation" data-cites="discherNewInsightsErythrocyte2000">(<a href="#ref-discherNewInsightsErythrocyte2000" role="doc-biblioref">Discher 2000, fig. 6</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="the-lac-operon" class="level3">
<h3 class="anchored" data-anchor-id="the-lac-operon">The <code>lac</code> operon</h3>
<p>This example is from <span class="citation" data-cites="garciaQuantitativeDissectionSimple2011">(<a href="#ref-garciaQuantitativeDissectionSimple2011" role="doc-biblioref">Garcia and Phillips 2011</a>)</span>.</p>
<p><em>E. coli</em> has two main sources of food: glucose and lactose. It prefers glucose, so when it is in an environment rich in glucose, it would start by metabolizing the glucose until it is mostly exhausted, then switch to metabolizing lactose.</p>
<p>To simplify, let’s consider a single gene, called the <code>lac</code>, which codes for a lactose-digesting enzyme (lactases). In front of <code>lac</code> there is a site where <code>repressor</code> protein can bind to, which stops <code>lac</code> transcription. The gene is <code>on</code> iff the <code>repressor</code> is not bound there.</p>
<p>The <code>repressor</code> protein can bind to either that specific site, of which there is only one on the entire <em>E. coli</em> genome, or any other site on the entire DNA sequence (non-specific binding). Write the binding energy on the specific site be <span class="math inline">\(E_S\)</span>, and the binding energy on the non-specific sites be <span class="math inline">\(E_{NS}\)</span>.</p>
<p>The system is in a delicate balance between energy, which favors specific binding, and entropy, which favors non-specific binding, with <span class="math inline">\(E_S &lt; E_{NS}\)</span> but not <span class="math inline">\(E_S \ll E_{NS}\)</span>. That is, specific binding is favored, but not <em>too</em> favored. This soft-favorism is what allows <code>lac</code> to be controlled. If it is not favored at all, then it would rarely bind. If it is too favored, then it would almost always bind.</p>
<p>Suppose there are <span class="math inline">\(R\)</span> <code>repressors</code> in the bacterium, then when none is binding specifically, the Gibbs free energy is</p>
<p><span class="math display">\[G_{\text{on}} = RE_{NS} - \beta^{-1} \ln \binom{N_{NS}}{R}\]</span></p>
<p>Notice how the entropy term <span class="math inline">\(\ln\binom{N_{NS}}{R}\)</span> assumes that the <code>repressor</code> proteins are indistinguishable, just like in gas theory. If one of them is binding specifically, the Gibbs free energy is</p>
<p><span class="math display">\[G_{\text{off}} = E_S + (R-1)E_{NS} - \beta^{-1} \ln \binom{N_{NS}}{R-1}\]</span></p>
<p>Thus, the probability of <code>on</code> vs <code>off</code> satisfies</p>
<p><span class="math display">\[\frac{p_{\text{on}}}{p_{\text{off}}} = e^{-\beta (G_{\text{on}} - G_{\text{off}})} = \frac{N_{NS} - R + 1}{R} e^{\beta (E_{NS} - E_{S})} \approx \frac{N_{NS}}{R} e^{\beta (E_{NS} - E_{S})}\]</span></p>
<p>giving us</p>
<p><span class="math display">\[p_{\text{on}} = \frac{N_{NS}}{N_{NS} + R e^{-\beta (E_{NS} - E_{S})}}\]</span></p>
</section>
<section id="unzipping-rna-hairpins" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="unzipping-rna-hairpins">Unzipping RNA hairpins</h3>
<p>The RNA molecules are polymers made of 4 different “letters” that can pair up as A-U and C-G. A common shape for single-stranded RNA is the “hairpin”, pictured below.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/rna_hairpin.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">RNA hairpin. Figure from <a href="https://commons.wikimedia.org/wiki/File:Stem-loop.svg">Wikimedia Commons</a></figcaption>
</figure>
</div>
<p>Since each basepair has about 100 atoms, an RNA hairpin is a large molecule with <span class="math inline">\(\sim 1000\)</span> atoms. This is large enough for statistical mechanics, but not large enough to smooth out thermal fluctuations, which ought to make it interesting. Now, what happens if we pull on the hairpin? This is the experiment done by <span class="citation" data-cites="liphardtReversibleUnfoldingSingle2001">(<a href="#ref-liphardtReversibleUnfoldingSingle2001" role="doc-biblioref">Liphardt et al. 2001</a>)</span>. They attached a single RNA hairpin to two beads, and pulled the beads apart very slowly, using force-feedback to keep the force stable within <span class="math inline">\(0.1 \;\mathrm{pN}\)</span>. The RNA they used is <code>P5ab</code>, which has about 20 basepairs, each base of length about 5 Angstrom.</p>
<p>If we ignore fluctuation, and treat it by classical thermodynamics, we get <span class="math inline">\(dS = \beta dE - \beta F dx\)</span>, where <span class="math inline">\(F\)</span> is the pulling force, and <span class="math inline">\(x\)</span> is the distance between the two tweezers. Looking at this equation, we immediately see that the problem is best analyzed using the following free entropy</p>
<p><span class="math display">\[
h := \underbrace{S - \beta \braket{E}}_{\text{Helmholtz free entropy}} + \beta F \braket{x}
\]</span></p>
<p>which can be interpreted as “one-dimensional Gibbs free entropy”.</p>
<p>Let <span class="math inline">\(x_0\)</span> be the length of a single RNA unit, <span class="math inline">\(n\)</span> be the number of unzipped RNA basepairs, <span class="math inline">\(2N\)</span> be the total number of RNA bases, and <span class="math inline">\(-E_0\)</span> be the bonding energy of a basepair (assume that the two kinds of basepairs have the same bonding energy).</p>
<p>Because the state of the system is fully determined once we know what <span class="math inline">\(n\)</span> is, the entropy <span class="math inline">\(S\)</span> conditional on <span class="math inline">\(n\)</span> is zero, and so, plugging in the <a href="#thm-cond-free-ent" class="quarto-xref">Theorem&nbsp;9</a>, the probability of the hairpin in state <span class="math inline">\(n\)</span> is: <span class="math display">\[
\rho(n) \propto e^{\beta(2Fx_0 - E_0)n}, \quad 0 \leq 2n \leq 2N
\]</span></p>
<p>a truncated exponential distribution. When <span class="math inline">\(F = E_0/2x_0\)</span>, the bonding force and the pulling force are exactly balanced, and the hairpin is equally likely to be in any state. When the pulling force is larger, then <span class="math inline">\(n\)</span> concentrates on the <span class="math inline">\(n = N\)</span> end.</p>
<p>For reasons they did not explain, and I do not understand, in the <span class="citation" data-cites="liphardtReversibleUnfoldingSingle2001">(<a href="#ref-liphardtReversibleUnfoldingSingle2001" role="doc-biblioref">Liphardt et al. 2001</a>)</span> experiment, the RNA hairpin had <em>no</em> intermediate state. It either fully unfolded, or fully folded. In that case, the hairpin became a two-level system, satisfying</p>
<p><span class="math display">\[
p_{\text{unfolded}} = \frac{e^{\beta( FL - \Delta G )}}{1 + e^{\beta( FL - \Delta G )}}
\]</span></p>
<p>where <span class="math inline">\(L = 18 \;\mathrm{nm}\)</span> is the length increase in unfolding, and <span class="math inline">\(\Delta G\)</span> is the increase in Helmholtz free energy during unfolding. This is exactly what they observed.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/liphardt_2001_1.jpeg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">(Left) As the pulling force increases, the RNA hairpin spends more and more time in the unfolded state. In all cases, the RNA is binary, either fully folded or fully unfolded, never in-between. (Right) The probability of being in an unfolded state increases as a logistic function, as predicted. <span class="citation" data-cites="liphardtReversibleUnfoldingSingle2001">(<a href="#ref-liphardtReversibleUnfoldingSingle2001" role="doc-biblioref">Liphardt et al. 2001, fig. 1</a>)</span></figcaption>
</figure>
</div>
<p>Another interesting finding is that when the pulling force increases slowly, the dissipated energy is small, but when the pulling force increases quickly, the wasted energy is large. In short, we have a form of speed limit second law: fast change requires more entropy production. We will discuss this in much more detail in the section on <a href="#sec-cft">Crooks fluctuation theorem</a>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/liphardt_2001_2.jpeg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="margin-caption">Trajectories of folding and unfolding cycles. The area between the upper and lower curves is the dissipated energy during one cycle. Fast cycles dissipate more energy. <span class="citation" data-cites="liphardtReversibleUnfoldingSingle2001">(<a href="#ref-liphardtReversibleUnfoldingSingle2001" role="doc-biblioref">Liphardt et al. 2001, fig. 1</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="hungry-hungry-bacteria" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="hungry-hungry-bacteria">Hungry hungry bacteria</h3>
<p>This example came from <span class="citation" data-cites="bergRandomWalksBiology1993">(<a href="#ref-bergRandomWalksBiology1993" role="doc-biblioref">Howard C. Berg 1993, chap. 6</a>)</span>. See <span class="citation" data-cites="bergPhysicsChemoreception1977 bergMotileBehaviorBacteria2000">(<a href="#ref-bergPhysicsChemoreception1977" role="doc-biblioref">H. C. Berg and Purcell 1977</a>; <a href="#ref-bergMotileBehaviorBacteria2000" role="doc-biblioref">Howard C. Berg 2000</a>)</span> for a detailed look at how bacteria optimally forage for food molecules.</p>
<p>Consider a bacteria swimming in water. A typical one, such as <em>E. coli</em>, is roughly a rod with length <span class="math inline">\(a = 10^{-6}m\)</span> and swimming at <span class="math inline">\(v = 2\times 10^{-5} m/s\)</span>. That is, it swims 20 body-lengths a second.</p>
<p>Water has density <span class="math inline">\(\rho = 10^3 kg/m^3\)</span> and viscosity <span class="math inline">\(\eta = 10^{-3} kg/m\cdot s\)</span>. As is well-known, the <a href="https://en.wikipedia.org/wiki/Reynolds_number">Reynold number</a> of the system is</p>
<p><span class="math display">\[Re = \frac{\rho v a}{\eta} = 2 \times 10^{-5}\]</span></p>
<p>meaning that as soon as the bacteria stops powering itself, its motion ceases after coasting for a length of <span class="math inline">\(\approx 0.1 a \cdot Re = 2\times 10^{-12}m\)</span>, less than the length of an atom! Thus, the bacteria lives in an essentially inertia-less world.</p>
<p>Assuming that a bacteria is a sphere, then its swimming dissipates power at</p>
<p><span class="math display">\[P = Fv = 6\pi \eta a v^2 \approx 8\times 10^{-18}W \approx 2000 k_BT/s\]</span></p>
<p>where <span class="math inline">\(T = 300 K\)</span> is the standard temperature for biology. Since each glucose at complete metabolism produces <span class="math inline">\(686 kcal/mol \approx 1000 k_BT\)</span>, the bacteria just needs to eat 2 molecules of glucose per second to power its swimming.</p>
<p>By the FDR, the diffusion constant for the bacteria is</p>
<p><span class="math display">\[D = \frac{k_BT}{6\pi \eta a} = 2 \times 10^{-13} m^2/s\]</span></p>
<p>meaning that within 1 second, the bacteria diffuses a distance of <span class="math inline">\(\sim \sqrt{2 D \Delta t} \sim a\)</span>, about 1 body length. This shows that the swimming motion is 20 times faster than its thermal motion.</p>
<p>However, if we look at the trajectory of a bacterium under the microscope, we would notice its direction jumping around rapidly. This is due to the thermal motion of its orientation on the rotational group <span class="math inline">\(SO(3)\)</span>. Just like how each translational degree of freedom gets <span class="math inline">\(\frac 12 k_BT\)</span> of thermal energy, each rotational degree of freedom gets it as well, although, because the space of rotations is not a vector space, making this precise is tricky.</p>
<p>Still, once we make it precise, the FDR is still the same, gives us <span class="math inline">\(D_{\text{rotational}} = \frac{k_B T}{\gamma_{\text{rotational}}}\)</span>. Assuming the bacterium is still a sphere of radius <span class="math inline">\(a\)</span>, then <span class="math inline">\(\gamma_{\text{rotational}} = 8\pi \eta a^3\)</span>, giving <span class="math inline">\(D_{\text{rotational}} = 0.2 \;\mathrm{rad^2/s}\)</span>.</p>
<p>Now, a swimming bacterium does not care about rotation around its velocity axis (“longitude”), but does care about the other two rotational freedoms. The variation in “latitude” angle is the sum:</p>
<p><span class="math display">\[\braket{\theta^2} = 2 \times (2D_{\text{rotational}}\Delta t)\]</span></p>
<p>for small time <span class="math inline">\(\Delta t\)</span>. This is false for large <span class="math inline">\(\Delta t\)</span>, because otherwise we would get <span class="math inline">\(\braket{\theta^2} &gt; \pi^2\)</span>, which is absurd because <span class="math inline">\(\theta \in [0, \pi]\)</span>.</p>
<p>This implies that the bacteria veers by about <span class="math inline">\(\braket{\theta^2} \approx (50^\circ)^2\)</span> after a second of swimming. The actual observation gives <span class="math inline">\(\braket{\theta^2} \approx (30^\circ)^2\)</span>. In particular, the bacteria cannot keep a direction for more than about 3 seconds, as at that point its velocity vector would have diffused by about 90 degrees. As the bacteria has no “vestibular organ”, it essentially “forgets its heading” within a few seconds. Any sense of direction must come from the outside world, such as a chemical gradient.</p>
<p>The standard strategy for <em>E. coli</em> hunting is the “run and tumble” strategy. During the “run” phase, the bacterium keeps swimming forward for 1 – 10 seconds, curving gently due to rotational diffusion. At a random point it stops and tumbles for about 0.1 seconds, changing its direction randomly. Tumbling essentially samples a random direction for the next run, close but not quite uniformly on the sphere. It remembers the average chemical concentration during the first second and last second of each run. If the chemical concentration seem to increase, it would tumble less often (thus going on longer runs). Otherwise, it would tumble more often. The net effect is an extremely simple homing missile that diffuses up the chemical gradient.</p>
<p>There is no point for it to swim more than 10 seconds, because it would have totally lost its heading due to diffusion. There is no point in swimming less than 1 second, because it needs to average the chemical concentration over at least a second to overcome the statistical noise. And so, by simple physics, we have explained an <em>E. coli</em>’s hunting strategy.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/bacterial_swimming_berg_1993.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A spherical bacterium is propelled by spinning flagella. The spherical bacterium swims along the <span class="math inline">\(x\)</span> -axis. Its rotational state diffuses along three possible degrees of freedom. Of those, the one around <span class="math inline">\(x\)</span>-axis is irrelevant, and the ones around <span class="math inline">\(y\)</span>- and <span class="math inline">\(z\)</span>-axes are relevant. <span class="citation" data-cites="bergRandomWalksBiology1993">(<a href="#ref-bergRandomWalksBiology1993" role="doc-biblioref">Howard C. Berg 1993, fig. 6.6</a>)</span></figcaption>
</figure>
</div>
<p><span class="citation" data-cites="bergChemotaxisEscherichiaColi1972">(<a href="#ref-bergChemotaxisEscherichiaColi1972" role="doc-biblioref">Howard C. Berg and Brown 1972</a>)</span> tracked the 3D trajectory of <em>E. coli</em> in a liquid that is <span class="math inline">\(3\times\)</span> more viscous than water. They found that the bacterium alternates between swimming and tumbling. During the swimming phase, the angular diffusion has a diffusion constant of <span class="math inline">\(0.06 \;\mathrm{rad^2/s}\)</span>, as theoretically predicted.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/bacterial_trajectory_berg_1972.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A single 3D trajectory of a single bacterium, projected into the <span class="math inline">\(xy, yz, xz\)</span> planes. Gently winding runs alternate with the tight tumbles. <span class="citation" data-cites="bergChemotaxisEscherichiaColi1972">(<a href="#ref-bergChemotaxisEscherichiaColi1972" role="doc-biblioref">Howard C. Berg and Brown 1972, fig. 1</a>)</span></figcaption>
</figure>
</div>
</section>
</section>
<section id="statistical-field-theory" class="level2">
<h2 class="anchored" data-anchor-id="statistical-field-theory">Statistical field theory</h2>
<section id="maximum-caliber" class="level3">
<h3 class="anchored" data-anchor-id="maximum-caliber">Maximum caliber</h3>
<p>Equilibrium statistical mechanics is typically called a “static” theory: It deals with the ensemble of states, but not with how the states change over time. Maximal caliber statistical mechanics handles trajectories in the most obvious way: Collect all paths into a “path space”, define a measure over path space, then study constrained entropy maximization over path space.</p>
<p><a href="https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes">Edward Jaynes</a>, who proposed the idea, called path space entropy “caliber”, so the name “maximum caliber” stuck, even though it is really just standard equilibrium statistical mechanics in path space. In other words, it is just standard statistical field theory, where the fields are of type <span class="math inline">\(\phi: \mathbb{R}\to \mathcal S\)</span>, where <span class="math inline">\(\mathbb{R}\)</span> stand for time, and <span class="math inline">\(\mathcal S\)</span> stand for state space of the system.</p>
<section id="markov-chains" class="level4">
<h4 class="anchored" data-anchor-id="markov-chains">Markov chains</h4>
<p>The simplest example is when time comes in discrete steps. In this case, we can reconstruct Markov chains as a particular kind of maximum caliber distribution.</p>
<p>Consider trajectories of type <span class="math inline">\(\{0, 1, 2, \dots, N\} \to \{1, 2, \dots, m\}\)</span>, and let <span class="math inline">\(s_t\)</span> is the state of timestep <span class="math inline">\(t\)</span>.</p>
<p>If we fix the singleton probability <span class="math inline">\(p_i\)</span> of each state, then the maximum entropy distribution is the Markov chain with uniform initial probability and <span class="math inline">\(p_{i\to j} = p_i p_j\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="derivation">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
derivation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>If we fix the singleton probability of each state, then the problem is a constrained maximization problem</p>
<p><span class="math display">\[
\begin{cases}
    \max S \\
    \frac 1N \sum_t 1[s_t = k] = p_k, \quad \forall k = 1, \dots, m
\end{cases}
\]</span></p>
<p>This is the same problem as <span class="math inline">\(N\)</span> balls in a certain constrained microcanonical ensemble. When <span class="math inline">\(N\)</span> is large enough, the discrete “macrostate” <span class="math inline">\(\frac 1N \sum_t 1[s_t = k]\)</span> becomes continuous, and we can use the Lagrange multiplier to obtain</p>
<p><span class="math display">\[\rho(s_1, \dots, s_N) = \frac 1Z e^{-\sum_{k=1}^m \lambda_k (\frac 1N \sum_{t=1}^N 1[s_t =k] - p_k)}\]</span></p>
<p>This factors over time <span class="math inline">\(t\)</span>, giving us</p>
<p><span class="math display">\[\rho(s_1, \dots, s_N) = \prod_{t=1}^N \rho(s_t)\]</span></p>
<p>with</p>
<p><span class="math display">\[\rho(s_t=k) \propto e^{-\sum_{k=1}^m\frac{\lambda_k}{N}(1[s_t =k] - p_k)} \propto e^{-\frac{\lambda_k}{N}}\]</span></p>
<p>The multiplier <span class="math inline">\(\lambda_k\)</span> can be found by the typical method of solving <span class="math inline">\(p_k = -\partial_{\lambda_k}\ln Z\)</span>, or we can be clever and notice that the constraint implies <span class="math inline">\(\rho(s_t=k) = p_k\)</span>.</p>
</div>
</div>
</div>
<p>Similarly, if we fix the transition probability <span class="math inline">\(p_{i \to j}\)</span> of each state-pair, then the maximum entropy distribution is the Markov chain with uniform initial probability. If we fix the initial probability, then it’s still the Markov chain with the same transition probabilities.</p>
<p>And more generally, if we fix <span class="math inline">\(n\)</span>-th order transition probability <span class="math inline">\(p_{i_1, \dots, i_n \to j}\)</span>, then we obtain an <span class="math inline">\(n\)</span>-th order Markov chain model.</p>
<div class="callout callout-style-default callout-note callout-titled" title="derivation">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-26-contents" aria-controls="callout-26" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
derivation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-26" class="callout-26-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Similarly as above, the path-space distribution is</p>
<p><span class="math display">\[\rho(s_1, \dots, s_N) \propto \prod_{t=1}^{N-1} e^{-\sum_{k, k' \in 1:m} \frac{\lambda_{k, k'}}{N} 1[s_t = k, s_{t+1} = k']} \propto \prod_{t=1}^{N-1} p_{s_t \to s_{t+1}}\]</span></p>
<p>Because the distribution does not specify <span class="math inline">\(s_1\)</span>, it is uniformly distributed on <span class="math inline">\(s_1\)</span>. Otherwise, we can constrain <span class="math inline">\(s_1\)</span> with yet another set of Lagrange multipliers and obtain</p>
<p><span class="math display">\[\rho(s_1, \dots, s_N) \propto \rho(s_1) \times_{t=1}^{N-1} \rho(s_t, s_{t+1})\]</span></p>
<p>Similarly for higher orders.</p>
</div>
</div>
</div>
</section>
<section id="diffusion" class="level4">
<h4 class="anchored" data-anchor-id="diffusion">Diffusion</h4>
<p>In diffusion, we consider paths of type <span class="math inline">\(x: [0, T] \to \mathbb{R}^n\)</span> with <span class="math inline">\(x(0) = 0\)</span>. The path-space entropy</p>
<p><span class="math display">\[
S = - \int \rho(x) \ln \rho(x) D[x]
\]</span></p>
<p>where <span class="math inline">\(D[x]\)</span> means we integrate over path space. Here we see a proper path-integral, which is roughly speaking what happens when we integrate not over <span class="math inline">\(\mathbb{R}^{10}\)</span> or even <span class="math inline">\(\mathbb{R}^{10^{23}}\)</span>, but literally <span class="math inline">\(\mathbb{R}^\infty\)</span>.</p>
<p>As usual, path integrals cannot be done directly, but can be done by first dropping down to <span class="math inline">\(\mathbb{R}^N\)</span>, where <span class="math inline">\(N\)</span> is large but still finite, then hope that the result is still sensible at the <span class="math inline">\(N \to \infty\)</span> limit. If this disturbs you, sorry. It disturbs me too, but it is necessary for all but the most trivial calculations in field theory.</p>
<p>Discretize continuous-time path <span class="math inline">\(x : [0, T] \to \mathbb{R}^n\)</span> into discrete-time path <span class="math inline">\(x: \{0, 1, 2, \dots, N\} \to \mathbb{R}^n\)</span>. Because we can decompose</p>
<p><span class="math display">\[
\rho(x) = \rho(x_0) \rho(x_1 | x_0) \cdots \rho(x_N | x_{0:N-1})
\]</span></p>
<p>the path-space entropy decomposes sequentially:</p>
<p><span class="math display">\[
S = S[x_0] + E[S[x_1 | x_0]] + E[S[x_2 | x_{0:1}]] + \dots + E[S[x_N | x_{0:N-1}]]
\]</span></p>
<p>To prevent the entropy from diverging, we need to impose some constraints. If we constrain the second moment of each step, as</p>
<p><span class="math display">\[(E[x_t|x_{0:t-1}], E[x_t^2|x_{0:t-1}]) = (0, \sigma^2), \quad \forall t \in 0:N\]</span></p>
<p>by reasoning backwards from <span class="math inline">\(t = N\)</span> to <span class="math inline">\(t=0\)</span>, we find that the maximal entropy distribution is a white noise:</p>
<p><span class="math display">\[
\rho(x) = \prod_{t\in 0:N} \rho(x_t), \quad \rho(x_t) \propto e^{-\frac{\|x_t\|^2}{2\sigma^2}}
\]</span></p>
<p>If you have studied <a href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming</a> and cybernetics, this should look very similar to the argument by which you derived the <a href="https://en.wikipedia.org/wiki/Linear-quadratic_regulator">LQR</a>.</p>
<p>To keep the path from exploding into white noise, we instead impose the constraints on the sizes of the steps</p>
<p><span class="math display">\[(E[x_t - x_{t-1}|x_{0:t-1}], E[\|x_t - x_{t-1}\|^2|x_{0:t-1}]) = (0, \sigma^2), \quad \forall t \in 1:N\]</span></p>
<p>Now, as in dynamic programming, we reason backwards from <span class="math inline">\(t = N\)</span> to <span class="math inline">\(t=0\)</span>, and we find that the maximal entropy distribution is the Brownian motion</p>
<p><span class="math display">\[\rho(x) \propto e^{-\frac{\sum_{t\in 1:N} \| x_t-x_{t-1}\|^2}{2\sigma^2}}\]</span></p>
<p>If we constrain the first <em>and</em> second moments of each step, and also allow them to be affected by the previous step, as in</p>
<p><span class="math display">\[
\begin{cases}
    E[x_t - x_{t-1}|x_{0:t-1}] &amp;= \mu(t, x_{t-1}) \\
    E[(x_t - x_{t-1}) (x_t - x_{t-1})^T|x_{0:t-1}] &amp;= \Sigma(t, x_{t-1})
\end{cases}, \quad \forall t \in 1:N
\]</span></p>
<p>then, reasoning backwards as before, we would obtain the <a href="https://en.wikipedia.org/wiki/Fokker-Planck_equation">Fokker–Planck equation</a>.</p>
<p>Other results, such as the Green-Kubo relation, the Onsager reciprocal relations, etc, can be similarly derived by imposing the right constraints in path space. <span class="citation" data-cites="hazoglouCommunicationMaximumCaliber2015">(<a href="#ref-hazoglouCommunicationMaximumCaliber2015" role="doc-biblioref">Hazoglou et al. 2015</a>)</span></p>
</section>
</section>
<section id="fluctuation-dissipation-relations" class="level3">
<h3 class="anchored" data-anchor-id="fluctuation-dissipation-relations">Fluctuation-dissipation relations</h3>
<p>Imagine if you have a weird liquid. You put a piece of pollen into it, and watch it undergo Brownian motion. Now I flip a switch and suddenly all friction disappears from the liquid. What would happen? The pollen is still being hit by random pieces of molecules, so its velocity is still getting pushed this and that way. However, without friction, the velocity at time <span class="math inline">\(t\)</span> is now obtained by integrating all past impulses, with no dissipation whatsoever. Consequently, its velocity undergoes a random walk, and its position undergoes <span class="math inline">\(\int_0^t W_s ds\)</span>. This is very different from what we actually observe, which is that its position undergoes a random walk, not a time-integral of it.</p>
<p>In order to reproduce the actual observed behavior, each fluctuation of its velocity must be quickly dissipated by friction, in a perfect balance such that <span class="math inline">\(\frac 12 m \braket{v^2} = k_BT\)</span> exactly. This perfect conspiracy is the fluctuation-dissipation relation (FDR), or rather, the <em>family</em> of FDRs, because there have been so many of those.</p>
<p>Each FDR is a mathematical equation of form</p>
<p><span class="math display">\[
\text{something about fluctuation} = \text{something about dissipation}
\]</span></p>
<p>The prototypical FDR is the <a href="https://en.wikipedia.org/wiki/Einstein_relation_(kinetic_theory)">Einstein relation</a>, to be derived below:</p>
<p><span class="math display">\[
\underbrace{(\beta D)^{-1}}_{\text{fluctuation}} = \underbrace{\gamma}_{\text{dissipation}}
\]</span></p>
<p>where the left side can be interpreted as the strength of molecular noise, and the right side as the strength of molecular damping.</p>
</section>
<section id="equality-before-the-law" class="level3">
<h3 class="anchored" data-anchor-id="equality-before-the-law">Equality before the law</h3>
<p>Why should there be a mathematical relation between fluctuation and dissipation? I think the deep reason is “equality before the second law”.</p>
<p>If we pause and think about it, then isn’t it strange that, despite the molecular storm happening within a placid cup of water, it does not actually fall out of equilibrium? That, despite every molecule of water rushing this way and that, the cup of water as a whole continues to stay without a ripple? What is this second law, the invisible hand pushing it towards statistical equilibrium and keeping it stable? And that is not a question I’m going to answer here. Perhaps calling it an “invisible hand” is already a bad metaphor. Nevertheless, the second law requires the stability of equilibrium distributions (if equilibria exist).</p>
<p>Now, a system in a statistical equilibrium would, once in a while, deviate significantly from the maximally likely state, just like how we might occasionally flip 10 heads in a row. Nevertheless, if the second law is to be upheld, then significant deviations must be pushed back. In other words, fluctuation is dissipated.</p>
<p>Assuming that in equilibrium, the system is undergoing a random walk, then by the theory of random walks, internally generated fluctuation must be dissipated according to a mathematical law. The law does not need any further input from physics. Indeed, it has no dependence on any physical observations, and belongs to the pure mathematical theory of random walks.</p>
<p>For example, a little pollen in a dish of water might occasionally have a large momentum, but it is very rare. Because it is rare, if we do observe it, we can predict that the next time we look at a pollen, it would have a much lower momentum.</p>
<p>Now, when physicists use the word “dissipation”, they mean the restoring effect of a system under <em>external</em> dissipation, such as pulling on a pollen by an electrostatic field. However, physical laws are equal, so internal dissipation is external dissipation.</p>
<p>Thus, we see that each FDR manifests as an “equality before the law”:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\text{fluctuation} \\
\underbrace{=}_{\text{random walk theory}} &amp;\text{dissipation (of internal fluctuations)} \\
\underbrace{=}_{\text{equality before the law}} &amp;\text{dissipation (of external fluctuations)}
\end{aligned}
\]</span></p>
</section>
<section id="one-dimensional-fdr" class="level3">
<h3 class="anchored" data-anchor-id="one-dimensional-fdr">One-dimensional FDR</h3>
<p>Consider a simple model of Brownian motion. We have a particle in a sticky fluid, moving on a line. The particle starts at <span class="math inline">\(x = 0, t = 0\)</span>, and at each time-step of <span class="math inline">\(\Delta t\)</span>, it moves by <span class="math inline">\(\Delta x\)</span> to the left or the right.</p>
<p>The fluid is at temperature <span class="math inline">\(\beta^{-1}\)</span>, and we pull on the particle at constant force <span class="math inline">\(F\)</span>. We expect that <span class="math inline">\(F = \gamma \braket{v}\)</span>, where <span class="math inline">\(v\)</span> is the ensemble-average velocity of the particle, and <span class="math inline">\(\gamma\)</span> is the viscosity constant.</p>
<p>Now, we let the particle move for a time <span class="math inline">\(t = N\Delta t\)</span>, where <span class="math inline">\(N\)</span> is a large number. The particle would have arrived at some point <span class="math inline">\(x\)</span>, which is a random variable. The particle’s time-averaged velocity is <span class="math inline">\(v = x/t\)</span>.</p>
<p>The number of possible paths that connect <span class="math inline">\((0, 0)\)</span> with <span class="math inline">\((t, x)\)</span> is <span class="math inline">\(\binom{N}{\frac N2 - \frac{x}{2\Delta x}}\)</span>, therefore, the path-space entropy is</p>
<p><span class="math display">\[S_{\text{path}} = \ln \binom{N}{\frac N2 - \frac{x}{2\Delta x}} \approx N \left(\ln 2 - \left(\frac{x}{N\Delta x}\right)^2\right)\]</span></p>
<p>where the approximation is either by Stirling’s approximation, or the binary entropy function.</p>
<p>Because the external force performs work <span class="math inline">\(Fx\)</span>, which is dissipated into the sticky liquid at temperature <span class="math inline">\(\beta\)</span>, we also have</p>
<p><span class="math display">\[S_{\text{work}} = \beta F x\]</span></p>
<p>Because <span class="math inline">\(N\)</span> is large, <span class="math inline">\(\braket{x}\)</span> should be highly concentrated around the point of maximal entropy. That is, we should have</p>
<p><span class="math display">\[
\braket{x} \approx \mathop{\mathrm{argmax}}_x (S_{\text{path}} + S_{\text{work}})
\]</span></p>
<p>The equation on the right is quadratic in <span class="math inline">\(\braket{x}\)</span>, and achieves maximum at <span class="math inline">\(2\braket{x} = \beta FN(\Delta x)^2\)</span>, which simplifies to the Einstein relation</p>
<p><span class="math display">\[\beta D \gamma = 1\]</span></p>
<p>where <span class="math inline">\(D = \frac{\Delta x^2}{2\Delta t}\)</span> is the diffusion coefficient.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can calculate not just the mean <span class="math inline">\(\braket{x}\)</span>, but also its variance <span class="math inline">\(\braket{x^2}\)</span> if we expand <span class="math inline">\(S_{\text{path}} + S_{\text{work}}\)</span> to second order around its maximum, then apply <a href="#thm-cond-ent" class="quarto-xref">Theorem&nbsp;8</a>.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="rubber band">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
rubber band
</div>
</div>
<div class="callout-body-container callout-body">
<p>We notice how this is identical to the <a href="#sec-rubber-band">rubber band system</a>. We can think of the shape of a rubber band as a timeless image of the trajectory of in time. Dissipation is identical to the entropic force pulling on the rubber band. Fluctuation is the variance in rubber band length. The dependence of the elastic constant on temperature, <span class="math inline">\(F/L = \frac{T}{Nd^2}\)</span>, is then the FDR for the rubber band:</p>
<p><span class="math display">\[
\beta \left(\frac{d^2}{1/N}\right) (F/L) = 1
\]</span></p>
<p>This is the simplest example of the general idea of <a href="https://en.wikipedia.org/wiki/Wick_rotation">Wick rotation</a>, where a problem in <span class="math inline">\(n\)</span>-dimensional space and <span class="math inline">\(1\)</span>-dimensional time is converted to a problem in <span class="math inline">\((n+1)\)</span>-dimensional space.</p>
</div>
</div>
</section>
<section id="sound-waves" class="level3">
<h3 class="anchored" data-anchor-id="sound-waves">Sound waves</h3>
<p>TODO sethna section 6.7</p>
</section>
</section>
<section id="metastability" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="metastability">Metastability</h2>
<p>Often in chemistry and biology, we have a particle that is stuck in a shallow potential well. If it ever gets over an energetic barrier, then it can drop to a much deeper minimum. If the particle is in thermal equilibrium with a bath, then it will eventually gain enough energy by random chance to get over the barrier. Of course, conversely, a particle in the deeper minimum would occasionally gain enough energy to go back to the shallow well.</p>
<p>The point is that “getting over a potential barrier” is inherently a non-equilibrium phenomenon, and therefore does not logically belong to an essay on <em>equilibrium</em> statistical mechanics. Still, if we are willing to approximate, then we can derive it easily.</p>
<p>We model a particle in a shallow potential well as a simple harmonic oscillator. If it ever gains more than <span class="math inline">\(\Delta E\)</span> of energy, then it would escape the well. Thus, we can take a look at the particle. If it has yet to escape, then we wait for time <span class="math inline">\(\tau\)</span> (relaxation time) until the particle has reached thermal equilibrium again with the bath, and take another look. The time-until-escape is <span class="math inline">\(N \tau\)</span>, where <span class="math inline">\(N\)</span> is the number of times we look at the system.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/metastable_potential_well.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Getting over a potential well.</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="relaxation time">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
relaxation time
</div>
</div>
<div class="callout-body-container callout-body">
<p>When we look at the oscillator, its state is totally fixed at some <span class="math inline">\((q, p)\)</span>. If we immediately look again, we would not find it in a Boltzmann distribution over phase space, but still at <span class="math inline">\((q, p)\)</span>. How long must we wait before the oscillator state has reasonably “relaxed” from a pointy distribution to the serene bell-shape of the Boltzmann distribution?</p>
<p>That is beyond the scope. Suffice to say that we should wait a while, not too short, after which the system is close enough to equilibrium. But we cannot wait for too long either, because in the long run, the particle would have escaped the potential well. So there is a time-scale, neither too long nor too short, which we call the relaxation time <span class="math inline">\(\tau\)</span>.</p>
<p>Formalizing all these things requires stochastic calculus, which I might write about later.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Goldilocks and the three chefs">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-29-contents" aria-controls="callout-29" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Goldilocks and the three chefs
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-29" class="callout-29-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Goldilocks, with a rumbling stomach, stumbles upon the house of the three chefs. Each chef is holding a pan in one hand and a bottle of Brownian batter in the other. “Excuse me, might I have some pancakes?”</p>
<p>“Of course!” exclaims the first chef, whose pan holds a small, quivering mound. “It’s just been poured, still brimming with energy!”</p>
<p>Goldilocks frowns. “It hasn’t even reached the edges! This batter needs more time to settle.”</p>
<p>The second chef, whose pan appears curiously empty, sighs. “And what of mine? I’ve given it all the time in the world, allowed it to explore every corner of its potential.” He gestures at the pan, now bereft of batter, a few crumbs clinging to the edges.</p>
<p>“But… there’s nothing left!” exclaims Goldilocks, aghast. “Given too much time, the batter has vanished completely!”</p>
<p>The third chef, whose batter has flowed smoothly to coat the pan, smiles warmly. “Perhaps this will be more to your liking. Given much time, but not too much, it’s achieved perfect consistency.”</p>
<p>“Ah, this is perfect!” exclaims Goldilocks, taking a bite of the fluffy pancake. “It’s had enough time to spread evenly, but not so long that it’s dried out.”</p>
<p>“Indeed,” said the second chef, “timing is everything. Too brief, and the batter remains confined to its starting point, unable to fulfill its pan-sized destiny. But too long, it would have escaped the edge of the pan to reach its true destiny – on the ground. Not too short, not too long, just right… that is meta-stability.”</p>
<p>— Guest entry written by <code>Gemini-1.5-Pro</code>.</p>
</div>
</div>
</div>
</div>
</div>
<div id="thm-arrhenius" class="theorem">
<p><span class="theorem-title"><strong>Theorem 10 (Arrhenius equation)</strong></span> The escape time for a simple harmonic oscillator in contact with an energy bath scales as <span class="math inline">\(e^{\beta \Delta E}\)</span>, where <span class="math inline">\(\Delta E\)</span> is the energy barrier, and <span class="math inline">\(\beta\)</span> is the temperature of the bath.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="derivation">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-31-contents" aria-controls="callout-31" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
derivation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-31" class="callout-31-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let the oscillator have <span class="math inline">\(n\)</span> dimensions, then its energy function is <span class="math inline">\(H = \sum_{i=1}^n \frac{p_i^2}{2m_i} + \frac{k_i q_i^2}{2}\)</span>, where <span class="math inline">\(q_i, p_i\)</span> are the generalized position and momentum, and <span class="math inline">\(m_i, k_i\)</span> are the effective masses and elastic constants. The Boltzmann distribution is <span class="math inline">\(\rho(q, p) = Z^{-1} e^{-\beta H}\)</span>, and the probability that it has enough energy to overcome the barrier is</p>
<p><span class="math display">\[
P = \frac{\int_{H \geq \Delta E} \rho(q, p)dqdp}{\int_{H \geq 0} \rho(q, p)dqdp}
\]</span></p>
<p>Notice that the proportionality constant <span class="math inline">\(Z\)</span> is removed. After a change of variables by <span class="math inline">\(x_i = \frac{p_i}{\sqrt{2m_i}}, y_i = \sqrt{\frac{k_i}{2}} q_i\)</span>, we get</p>
<p><span class="math display">\[
P = \frac{\int_{\sum_i x_i^2 + y_i^2 \geq \Delta E} e^{-\beta \sum_i (x_i^2 + y_i^2)} dxdy}{\int_{\sum_i x_i^2 + y_i^2 \geq 0} e^{-\beta \sum_i (x_i^2 + y_i^2)} dxdy}
\]</span></p>
<p>Integrating in spherical coordinates, and simplifying, we get <span class="math inline">\(P = e^{-\beta \Delta E}\)</span>. Thus, the expected time until escape is</p>
<p><span class="math display">\[
T = \braket{N}\tau = \frac{1}{P}\tau = \tau e^{\beta \Delta E}
\]</span></p>
<p>the Arrhenius equation.</p>
<p>The same calculation, for a system held in contact with an energy-and-volume bath, gives us <span class="math inline">\(T = \tau e^{\beta \Delta G}\)</span>, where <span class="math inline">\(\Delta G\)</span> is the Gibbs free energy barrier.</p>
</div>
</div>
</div>
<p>The argument given above for the Arrhenius equation is quite generic. It only assumes there is a system that is stuck in some kind of potential well, and is held at a constant temperature somehow. There is no requirement for the system to be an actual particle in an actual well. The “particle” can very well be the 100-dimensional configuration of a protein during folding, or even the simultaneous position of <span class="math inline">\(10^{23}\)</span> helium atoms in a helium gas. Indeed, the Arrhenius equation pops up everywhere as the time until a system escapes an energy barrier.</p>
<section id="common-applications" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="common-applications">Common applications</h3>
<p>In practice, the Arrhenius equation is used in the form of <span class="math inline">\(\ln r = - a T^{-1} + Const\)</span>, where <span class="math inline">\(r = 1/T\)</span> is the “reaction rate”, and <span class="math inline">\(a = \frac{\Delta E}{k_B}\)</span> is the slope of the <span class="math inline">\(T^{-1} - \ln f\)</span> plot (“the Arrhenius plot”). Such plots are extensively used in chemistry, but they are not exclusively found in chemistry.</p>
<p>In a glass of water held at constant temperature <span class="math inline">\(300 \;\mathrm{K}\)</span>, each water molecule might occasionally reach enough energy to escape into open air. This is evaporation. By this argument, the rate of evaporation follows the Arrhenius law, and indeed it does.</p>
<p>When a pure glass of water is cooled below freezing, it does not actually freeze immediately, because there is still an energetic barrier. Ice is a crystal, but liquid is not a crystal. The barrier between them, where crystal transitions to non-crystal, is penalized. This energetic penalty is what we call “surface tension”. From thermodynamical arguments, at temperature <span class="math inline">\(\Delta T\)</span> belowe freezing point, the Gibbs free energy required to form a sphere of ice with radius <span class="math inline">\(r\)</span> is</p>
<p><span class="math display">\[
G(r) = (4\pi \sigma_{\text{ice-water}})\cdot r^2  - \left(\frac 43 \pi \rho_{\text{ice}} \frac{L\Delta T}{T_{\text{freezing}}}\right) \cdot r^3
\]</span></p>
<p>where <span class="math inline">\(L\)</span> is the latent heat of ice, and <span class="math inline">\(\sigma_{\text{ice-water}}\)</span> is the surface tension. This creates an energy barrier of height <span class="math inline">\(\sim \Delta T^{-2}\)</span>, so the metastable phase of the system can survive for <span class="math inline">\(\sim e^{C\frac{1}{T(\Delta T)^2}}\)</span>. <span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021, chap. 11.3</a>)</span> One must wait for a long while before random chance creates a large enough “seed”, which then grows without bound. This picture we just described is called “bubble nucleation”, as described in <a href="https://en.wikipedia.org/wiki/Classical_nucleation_theory">classical nucleation theory</a>.</p>
<p>Similarly, slightly <a href="https://en.wikipedia.org/wiki/Supersaturation">supersaturated</a> water vapor can stay metastable for a long time before it rains spontaneously. In real-life clouds, however, there are always enough dust for droplets to form around them, thus taking a shortcut through the energetic barrier.</p>
<p>A similar argument applies for <a href="https://en.wikipedia.org/wiki/Disappearing_polymorphs">disappearing polymorphs</a>. Some chemicals, such as Ritonavir, have two forms (“morphs”) of crystals. Call them A and B. Morph A has higher Gibbs free energy density than morph B, but lower energy barrier of formation. Thus, when a chemist attempts to crystallize it from a solution, by Arrhenius law, morph A would appear much faster than morph B. As soon as one droplet of morph A forms, that would serve as centers around which more of morph A forms. Eventually, however, a small crystal of morph A would spontaneously overcome the energetic barrier, shift all its atomic lattices, and become morph B. That would “infect” all subsequent attempts to crystallize morph A. <span class="citation" data-cites="wardPerilsPolymorphismSize2017">(<a href="#ref-wardPerilsPolymorphismSize2017" role="doc-biblioref">Ward 2017</a>)</span></p>
<p>The matter of the disappearing polymorph has serious legal effects. For example, Ritonavir is a medicine for AIDS. It was marketed in 1996 as Form I crystal, which was thought to be the only crystal form for Ritonavir. In 1998, Form II crystals spontaneously appeared. Any lab in contact with Form II could only produce more of Form II. Eventually the company rediscovered how to create Form I despite infection by Form II, but the delay cost the company 250 million USD. <span class="citation" data-cites="bucarDisappearingPolymorphsRevisited2015">(<a href="#ref-bucarDisappearingPolymorphsRevisited2015" role="doc-biblioref">Bučar, Lancaster, and Bernstein 2015</a>)</span></p>
<blockquote class="blockquote">
<p>In a matter of weeks—maybe five or six weeks, every place the product was became contaminated with Form II crystals.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/ritonavir.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Ritonavir and its two morphs. <span class="citation" data-cites="bucarDisappearingPolymorphsRevisited2015">(<a href="#ref-bucarDisappearingPolymorphsRevisited2015" role="doc-biblioref">Bučar, Lancaster, and Bernstein 2015, fig. 2</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="uncommon-applications" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="uncommon-applications">Uncommon applications</h3>
<p>In the simplest model for biochemical process, we just have one chemical reaction following another, until it is complete. If there is a single biochemical step that is much slower than the other steps, then the waiting time for that step dominates, and the total reaction should depend on the temperature by an Arrhenius law. This might explain the observed Arrhenius-law-like dependence on temperature in biological phenomena like tree cricket chirping, alpha brain wave frequency, etc.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/arrhenius_plots.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">(1) Tree cricket chirping frequency. (2) Firefly flashing frequency. (3) Terrapin heartbeat frequency. (4) Human silent counting rate <span class="citation" data-cites="hoaglandPhysiologicalControlJudgments1933">(<a href="#ref-hoaglandPhysiologicalControlJudgments1933" role="doc-biblioref">Hoagland 1933</a>)</span>. These figures are reproduced in <span class="citation" data-cites="laidlerUnconventionalApplicationsArrhenius1972">(<a href="#ref-laidlerUnconventionalApplicationsArrhenius1972" role="doc-biblioref">Laidler 1972</a>)</span>. (5) evaporation rate of octane <span class="citation" data-cites="brennanEvaporationLiquidsKinetic1974">(<a href="#ref-brennanEvaporationLiquidsKinetic1974" role="doc-biblioref">Brennan, Shapiro, and Watton 1974</a>)</span>. (6) Alpha frequency of brains of normal, <a href="https://en.wikipedia.org/wiki/General_paresis_of_the_insane">syphilitic paretic</a>, and <em>very</em> paretic humans <span class="citation" data-cites="hoaglandPacemakersHumanBrain1936">(<a href="#ref-hoaglandPacemakersHumanBrain1936" role="doc-biblioref">Hoagland 1936</a>)</span>.</figcaption>
</figure>
</div>
<p>While writing this, I suddenly recognized <span class="citation" data-cites="hoaglandPhysiologicalControlJudgments1933">(<a href="#ref-hoaglandPhysiologicalControlJudgments1933" role="doc-biblioref">Hoagland 1933</a>)</span> from when I read Feynman’s book all those years ago!</p>
<blockquote class="blockquote">
<p>When I was in graduate school at Princeton [1939–1942] a kind of dumb psychology paper came out that stirred up a lot of discussion. The author had decided that the thing controlling the “time sense” in the brain is a chemical reaction involving iron… his wife had a chronic fever which went up and down a lot. Somehow he got the idea to test her sense of time. He had her count seconds to herself (without looking at a clock), and checked how long it took her to count up to 60. He had her counting – the poor woman – all during the day: when her fever went up, he found she counted quicker; when her fever went down, she counted slower… he tried to find a chemical reaction whose rates varied with temperature in the same amounts as his wife’s counting did. He found that iron reactions fit the pattern best… it all seemed like a lot of baloney to me – there were so many things that could go wrong in his long chain of reasoning.</p>
<p><span class="citation" data-cites="feynmanWhatYouCare1989">(<a href="#ref-feynmanWhatYouCare1989" role="doc-biblioref">Feynman 1989, 55</a>)</span></p>
</blockquote>
<p>And yes, that is the one!</p>
<blockquote class="blockquote">
<p>My wife, having fallen ill with influenza, was used in the first of several experiments. Without, in any way, hinting to her the nature of the experiment, she was asked to count 60 seconds to herself at what she believed to be a rate of 1 per second. Simultaneously the actual duration of the count was observed with a stop-watch.</p>
<p><span class="citation" data-cites="hoaglandPhysiologicalControlJudgments1933">(<a href="#ref-hoaglandPhysiologicalControlJudgments1933" role="doc-biblioref">Hoagland 1933</a>)</span></p>
</blockquote>
</section>
</section>
<section id="sec-cft" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-cft">Crooks fluctuation theorem</h2>
<section id="in-a-closed-system-microcanonical" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="in-a-closed-system-microcanonical">In a closed system (microcanonical)</h3>
<p>SETUP. The system is a classical-mechanical system, with time-reversible dynamics, and follows Liouville’s theorem.</p>
<p>We have a thermodynamic system held under variable constraints <span class="math inline">\(x\)</span>.</p>
<p>The system starts in microcanonical ensemble of energy <span class="math inline">\(E_1\)</span>. Then we change the constraints <span class="math inline">\(x(t)\)</span>, quickly or slowly, over a time interval <span class="math inline">\(t\in [0, \tau]\)</span>. Let the microstate trajectory of the system be <span class="math inline">\(y(t)\)</span>, arriving at the energy shell of <span class="math inline">\(E_2\)</span>.</p>
<p>During the forward process, if the system undergoes microstate trajectory <span class="math inline">\(y(t)\)</span>, then we have to expend work <span class="math inline">\(W[x(t), y(t)] = E_2 - E_1\)</span>.</p>
<p>Let <span class="math inline">\(S_1^*\)</span> be the maximal entropy of the system when held under the constraints of <span class="math inline">\(x(0)\)</span>, and when the system has energy <span class="math inline">\(E_1\)</span>. Similarly for <span class="math inline">\(S_2^*\)</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(S_1 = S_1^*\)</span>, since the system starts in thermal equilibrium. However, by Liouville’s theorem, entropy is <em>conserved</em>! So we actually have <span class="math inline">\(S_2 = S_1 \neq S_2^*\)</span>, because the system does not end in thermal equilibrium.</p>
</div>
</div>
<p><span class="math inline">\(x', y'\)</span> are <span class="math inline">\(x, y\)</span> time-reversed.</p>
<p>For example, if we have a piston of gas made of only a few gas molecules, then the constraint is the volume <span class="math inline">\(V\)</span>, and we want to study the probability of expending work <span class="math inline">\(W\)</span> if we give the piston head a push. The push can be slow or fast – arbitrarily far from equilibrium. Crooks theorem applies no matter how we push the piston head.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Crooks_sethna_2021_4_10.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021, fig. 4.10</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Crooks_sethna_2021_4_11.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021, fig. 4.11</a>)</span></figcaption>
</figure>
</div>
<p>SETUP. probability density over path-space.</p>
<p>Let <span class="math inline">\(\delta E_1, \delta E_2\)</span> be infinitesimals, and let <span class="math inline">\(E_1, E_2\)</span> be real numbers.</p>
<p>Given a small bundle of microtrajectories <span class="math inline">\(y\)</span>, we can measure its path-space volume as <span class="math inline">\(D[y]\)</span>. Suppose they start on the energy shell <span class="math inline">\([E_1, E_1 + \delta E_1]\)</span>, then they would end up <em>somewhere</em>. If we’re lucky, they would end up on the energy shell <span class="math inline">\([E_2 + \delta E_2]\)</span>.</p>
<p>Suppose the system starts in the microcanonical ensemble on the energy shell <span class="math inline">\([E_1, E_1 + \delta E_1]\)</span>, and we perform the constraint-variation <span class="math inline">\(x\)</span>, then there is a certain probability <span class="math inline">\(\delta P\)</span> that we would sample a trajectory from the small bundle. That small probability is <span class="math display">\[\rho(y | x) D[y]\]</span></p>
<p>where <span class="math inline">\(\rho(y | x)\)</span> is a probability density over path-space. In particular, <span class="math inline">\(\rho(y | x) = 0\)</span> identically, unless <span class="math inline">\(y(0)\)</span> is on the energy shell <span class="math inline">\([E_1, E_1 + \delta E_1]\)</span>.</p>
<p>Running the argument backwards, we can define <span class="math inline">\(\rho'(y' | x')\)</span>, another probability density over paths. This one satisfies <span class="math inline">\(\rho'(y'| x') = 0\)</span> unless <span class="math inline">\(y'(0)\)</span> is on the energy shell <span class="math inline">\([E_2, E_2 + \delta E_2]\)</span>.</p>
<div id="thm-todo" class="theorem">
<p><span class="theorem-title"><strong>Theorem 11 (Crooks fluctuation theorem (microcanonical))</strong></span> For any trajectory <span class="math inline">\(y\)</span> such that it starts on the <span class="math inline">\([E_1, E_1 + \delta E_1]\)</span> energy shell, and ends on the <span class="math inline">\([E_2, E_2 + \delta E_2]\)</span> energy shell,</p>
<p><span class="math display">\[\frac{\rho(y | x)}{\rho'(y' | x')} = e^{\Delta S}\]</span></p>
<p>where <span class="math inline">\(\Delta S= \ln\Omega_2 - \ln\Omega_1\)</span>, <span class="math inline">\(\Omega_1\)</span> is the phase space volume of the first energy shell, and <span class="math inline">\(\Omega_2\)</span> the second.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(y\)</span> does not start on the first energy shell, or does not end on the second energy shell, then either the nominator or the denominator is zero, and so the equation fails to hold.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-34-contents" aria-controls="callout-34" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-34" class="callout-34-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In the forward process, the probability of going along that trajectory is <span class="math display">\[\rho(x|y) D[x] = \frac{\delta V}{\Omega_1}\]</span></p>
<p>where <span class="math inline">\(\delta V\)</span> is the phase-space volume of the shaded set.</p>
<p>In the backward process, the probability of reversing that trajectory is <span class="math display">\[\rho'(x'|y') D[x']= \frac{\delta V'}{\Omega_2}\]</span></p>
<p><span class="math inline">\(\delta V' = \delta V\)</span> by Liouville’s theorem, and <span class="math inline">\(D[x] = D[x']\)</span> because <span class="math inline">\(x'\)</span> is just <span class="math inline">\(x\)</span> time-reversed.</p>
</div>
</div>
</div>
</section>
<section id="in-an-energy-bath-canonical" class="level3">
<h3 class="anchored" data-anchor-id="in-an-energy-bath-canonical">In an energy bath (canonical)</h3>
<p>Now, suppose we take the same piston of gas, and put it in energy-contact with an energy bath, then at thermal equilibrium, the piston of gas would have the Boltzmann distribution <span class="math inline">\(\propto e^{-\beta E}\)</span>. We can then give the piston head a push, which would cause it to undergo</p>
<div id="thm-cft" class="theorem">
<p><span class="theorem-title"><strong>Theorem 12 (Crooks fluctuation theorem)</strong></span> <span class="math display">\[\frac{\rho(y | x)}{\rho'(y' | x')} =  e^{S[x, y]} = e^{\beta (W[x, y] - \Delta F^*)}\]</span></p>
<p>where <span class="math inline">\(S[x, y]\)</span> is the entropy produced during the forward process, after the system has equilibrated, and <span class="math inline">\(\Delta F^* = F^*_2 - F^*_2\)</span> is the increase in <em>equilibrium</em> Helmholtz free energy of the system.</p>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>In both forward and backward cases, we start with a thermal equilibrium, and end with a thermal <em>dis</em>equilibrium.</p>
<p>For example, suppose we have a small tank of a few gas molecules in thermal equilibrium with a large energy bath.</p>
<p>Now, we <em>quickly</em> push the piston head in according to the function <span class="math inline">\(x(t)\)</span>. The trajectory of the system would go through is <span class="math inline">\(y(t)\)</span>, which is <em>determined</em> by both <span class="math inline">\(x(t)\)</span> and the initial state of both the system and the energy bath.</p>
<p>Now, we <em>wait a long time</em>, until the tank is in thermal equilibrium again. Then we pull the piston head out with time-reversed trajectory. Because the forward trajectory was quick, the backward trajectory was also quick.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled" title="equilibrium Helmholtz">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
equilibrium Helmholtz
</div>
</div>
<div class="callout-body-container callout-body">
<p>We wrote <span class="math inline">\(F^*\)</span> instead of <span class="math inline">\(F\)</span>, to emphasize that we are dealing with <em>equilibrium</em> Helmholtz free energy, defined by <span class="math inline">\(F^* = \min_\rho (\braket{E} - TS[\rho])\)</span>, and <em>not</em> the generic version <span class="math inline">\(\braket{E} - TS[\rho]\)</span>.</p>
<p>This is vitally important, because at time <span class="math inline">\(\tau\)</span>, when the constraints have just reached their new values, the system is <em>not</em> in equilibrium. We would have to hold the constraints constant for a while for the system to return to equilibrium with the energy bath. Despite this, Crooks fluctuation theorem uses <span class="math inline">\(\Delta F^*\)</span>, which is computed at equilibrium.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-37-contents" aria-controls="callout-37" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-37" class="callout-37-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Apply the microcanonical version of Crooks theorem to the entire compound system that includes both the bath and the system, then integrate over all possible microstate trajectories of the bath <span class="math inline">\(y_{\text{bath}}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
  S[x, y] &amp;= \Delta S_{\text{bath}} + \Delta S_{\text{system}} \\
  &amp;= \beta \Delta E_{\text{bath}} + \Delta S_{\text{system}} \\
  &amp;= \beta(W[x, y] - \braket{\Delta E_{\text{system}}}_2) + \Delta S_{\text{system}} \\
  &amp;= \beta (W[x, y] - \Delta F^*)
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\braket{\cdot}_2\)</span> means the canonical ensemble average under constraint <span class="math inline">\(x(\tau)\)</span>.</p>
<p>Notice that the work expended/entropy produced depends only on the system’s microtrajectory <span class="math inline">\(y(t)\)</span>, and <em>not</em> on the bath’s microtrajectory <span class="math inline">\(y_{\text{bath}}(t)\)</span>. That is,</p>
<p><span class="math display">\[S[x, y, y_{\text{bath}}] = S[x, y]\]</span></p>
<p>This will be used again in the next step when we integrate over <span class="math inline">\(D[y_{\text{bath}}]\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
  \rho(y|x) &amp;= \int_{y, y_{\text{bath}}, x \text{ is valid}}D[y_{\text{bath}}]\; \rho(y, y_{\text{bath}} | x)  \\
  &amp;=  \underbrace{\int_{y', y'_{\text{bath}}, x' \text{ is valid}}D[y'_{\text{bath}}]}_{\text{reversible dynamics}}\; \underbrace{e^{S[x, y, y_{\text{bath}}]}\rho'(y', y'_{\text{bath}} | x')}_{\text{microcanonical Crooks}}  \\
  &amp;=  \int D[y'_{\text{bath}}] \; e^{\red{S[x, y]}}\rho'(y', y'_{\text{bath}} | x') \\
  &amp;=  e^{S[x,y]} \int D[y'_{\text{bath}}] \; \rho'(y', y'_{\text{bath}} | x') \\
  &amp;=  e^{S[x,y]} \rho'(y'|x')
\end{aligned}\]</span></p>
</div>
</div>
</div>
<div id="cor-todo" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 3</strong></span> <span class="math display">\[\frac{\rho(W | x)}{\rho'(-W | x')} =  e^{\beta (W - \Delta F^*)}\]</span></p>
<p>where <span class="math inline">\(\rho(W|x)\)</span> is the probability density of expending work <span class="math inline">\(W\)</span> in the forward process.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-38-contents" aria-controls="callout-38" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-38" class="callout-38-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Integrate over all forward microtrajectories <span class="math inline">\(y\)</span> satisfying <span class="math inline">\(W[x, y] \in [W, W+\delta W]\)</span>. By reversibility, <span class="math inline">\(W[x', y'] = [-W - \delta W, -W]\)</span> for such microtrajectories.</p>
</div>
</div>
</div>
</section>
<section id="other-ensembles" class="level3">
<h3 class="anchored" data-anchor-id="other-ensembles">Other ensembles</h3>
<p>Looking at the proof for Crooks theorem for the canonical ensemble, we immediately obtain many other possible Crooks theorems, one per free entropy.</p>
<p>EXP. Crooks theorem for Gibbs free energy <span class="math inline">\(G\)</span></p>
<p>Suppose we have a piston of magnetic gas in energy-and-volume contact with a bath. Now suppose the gas is in equilibrium with the bath, and we vary the external magnetic field over a trajectory <span class="math inline">\(x\)</span>. Over the microstate trajectory <span class="math inline">\(x\)</span>, the external world would expend both some energy <span class="math inline">\(W[x, y]\)</span> and some volume <span class="math inline">\(V[x, y]\)</span>.</p>
<p><span class="math display">\[\frac{\rho(y | x)}{\rho'(y' | x')} =  e^{S[x, y]} = e^{\beta (W[x, y] + PV[x, y] - \Delta G^*)}\]</span></p>
<p>EXP. Crooks theorem for Landau free energy <span class="math inline">\(\Omega\)</span></p>
<p>Suppose we have a chemical reaction chamber of fixed volume, and in energy-and-particle contact with a bath with chemical potentials <span class="math inline">\(\mu_i\)</span>.</p>
<p><span class="math display">\[\frac{\rho(y | x)}{\rho'(y' | x')} =  e^{S[x, y]} = e^{\beta (W[x, y] -  \sum_i \mu_i N_i[x, y] - \Delta \Omega^*)}\]</span></p>
</section>
<section id="application" class="level3">
<h3 class="anchored" data-anchor-id="application">Application</h3>
<p><span class="citation" data-cites="liphardtReversibleUnfoldingSingle2001 collinVerificationCrooksFluctuation2005">(<a href="#ref-liphardtReversibleUnfoldingSingle2001" role="doc-biblioref">Liphardt et al. 2001</a>; <a href="#ref-collinVerificationCrooksFluctuation2005" role="doc-biblioref">Collin et al. 2005</a>)</span></p>
</section>
</section>
<section id="jarzynski-equality" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="jarzynski-equality">Jarzynski equality</h2>
<p>Let <span class="math inline">\(W\)</span> be the total work we expended by changing the constraints during the interval <span class="math inline">\([0, \tau]\)</span>. Since the work expended depends on the details of the heat bath and the starting state of the system at <span class="math inline">\(t=0\)</span>, this is a random variable.</p>
<div id="thm-todo" class="theorem">
<p><span class="theorem-title"><strong>Theorem 13 (Jarzynski equality)</strong></span> <span class="math display">\[\braket{e^{-\beta W}} = e^{-\beta \Delta F^*}\]</span></p>
<p>where the expectation is taken over many repeats of the same experiment (ensemble average).</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-39-contents" aria-controls="callout-39" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-39" class="callout-39-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Integrate Crooks over all forward trajectories <span class="math inline">\(D[x]\)</span>.</p>
<p><span class="math display">\[\rho(y | x) e^{-\beta W[x, y] } = \rho'(y' | x') e^{-\beta\Delta F}\]</span></p>
<p>now integrate over <span class="math inline">\(\int D[y]\)</span>, using the fact that <span class="math inline">\(D[y] = D[y']\)</span>.</p>
</div>
</div>
</div>
<div id="cor-todo" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 4 (violation of second law is exponentially unlikely)</strong></span> <span class="math display">\[Pr((W - \Delta F^*) \leq - \delta W) \leq e^{-\beta \delta W}\]</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-40-contents" aria-controls="callout-40" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-40" class="callout-40-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Apply Markov’s inequality.</p>
</div>
</div>
</div>
<p>EXP. high probability of work extraction</p>
<p>Classically, if we have a single system in thermal equilibrium with a single energy-bath, and we perform a cyclic operation on it, then we can’t extract work, lest we violate the second law.</p>
<p>Statistically, <span class="math inline">\(\braket{e^{-\beta W}} = 1\)</span>, and so it is entirely possible for us to extract work with high probability, as long as there is a small probability to lose a large enough amount of work.</p>
<p><span class="citation" data-cites="mailletOptimalProbabilisticWork2019">(<a href="#ref-mailletOptimalProbabilisticWork2019" role="doc-biblioref">Maillet et al. 2019</a>)</span> constructed a quantum mechanical device with a single-electron transistor. The electron can expend work. They managed to extract work from the device with over 75% probability.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/maillet_2019_3_c.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Figure from <span class="citation" data-cites="mailletOptimalProbabilisticWork2019">(<a href="#ref-mailletOptimalProbabilisticWork2019" role="doc-biblioref">Maillet et al. 2019, fig. 3.c</a>)</span></figcaption>
</figure>
</div>
<section id="worked-example-bouncing-ball" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="worked-example-bouncing-ball">Worked example: bouncing ball</h3>
<p>This example is from <span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021</a>, exercise 4.8)</span>, which itself derives from <span class="citation" data-cites="luaPracticalApplicabilityJarzynski2005">(<a href="#ref-luaPracticalApplicabilityJarzynski2005" role="doc-biblioref">Lua and Grosberg 2005</a>)</span>. See also <span class="citation" data-cites="hijarJarzynskiEqualityIllustrated2010">(<a href="#ref-hijarJarzynskiEqualityIllustrated2010" role="doc-biblioref">Híjar and de Zárate 2010</a>)</span> for another solved example, of a chest expander with mass points stuck in the middle of the springs. You might need to read my tutorial on <a href="https://yuxi-liu-wired.github.io/sketches/posts/field-theory-how-to/">field-theoretic calculations</a> before attempting that example.</p>
<p>We have a one-dimensional system, of a single ball bounding between two walls of a piston. The only control we have is that we can move one of the piston heads. At the start, the piston has length <span class="math inline">\(L\)</span>, and the system is in thermal equilibrium at inverse temperature <span class="math inline">\(\beta\)</span>. We plunge the piston head at velocity <span class="math inline">\(v\)</span> for time <span class="math inline">\(\Delta L / v\)</span>, then immediately reverse it, taking another <span class="math inline">\(\Delta L / v\)</span>. We explicitly calculate that <span class="math inline">\(\braket{e^{-\beta W}} = 1\)</span>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Jarzynski_bouncing_ball_sethna_2021_4_12.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021, fig. 4.12</a>)</span></figcaption>
</figure>
</div>
<p>The phase space of the ball has 2 dimensions, <span class="math inline">\((p, x)\)</span>. The Boltzmann distribution is</p>
<p><span class="math display">\[\rho(p, x) = \rho(p) \rho(x) = \frac{1}{\sqrt{2\pi m/\beta}}e^{-\frac{\beta}{2m}p^2} \times \frac{1}{L}\]</span></p>
<p>We assume that <span class="math inline">\(L\)</span> is large enough, such that the ball hits the piston head at most once. There are three possibilities:</p>
<ol type="1">
<li>If the piston head hits the ball during the in-stroke, then the ball’s velocity increases by <span class="math inline">\(2v\)</span>, and its kinetic energy increases by <span class="math display">\[W = \Delta KE = 2v(mv - p)\]</span></li>
<li>If the piston head hits the ball during the out-stroke, then the ball’s velocity decreases by <span class="math inline">\(2v\)</span>, and its kinetic energy increases by<br>
<span class="math display">\[W = 2v(mv+p)\]</span></li>
<li>Otherwise, the piston head avoids the ball, and we have <span class="math inline">\(W = 0\)</span>.</li>
</ol>
<p>If at <span class="math inline">\(t=0\)</span>, the ball is in the phase space region labelled “in region”, then it will be hit in the in-stroke. If at <span class="math inline">\(t=\Delta L/v\)</span>, the ball is in the phase space region labelled “out region”, then it will be hit in the out-stroke. Otherwise, it will not be hit.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Jarzynski_bouncing_ball_1.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="margin-caption">Jarzynski_bouncing_ball_1.jpg</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Jarzynski_bouncing_ball_2.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="margin-caption">Jarzynski_bouncing_ball_2.jpg</figcaption>
</figure>
</div>
<p>Therefore,</p>
<p><span class="math display">\[
\begin{aligned}
    \braket{e^{-\beta W}} &amp;= \int_{in} \rho dpdx \; e^{-\beta 2v(mv-p)} + \int_{out} \rho dpdx \; e^{-\beta 2v(mv+p)} + \int_{other} \rho dpdx \; 1 \\
    &amp;= e^{-2\beta mv^2} \left(\int_{in} \rho dpdx \; e^{2\beta vp} + \int_{out} \rho dpdx \; e^{-2\beta vp}\right) +  \int_{other} \rho dpdx \; 1
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\rho(p, x) = \rho(-p, x - \Delta L)\)</span>, the first two integrals can be combined by flipping the “out region”, then moving it by <span class="math inline">\(\Delta L\)</span>, to “out’ region”.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Because <span class="math inline">\(L\)</span> is large, this is <em>mostly</em> correct, as the regions where this is incorrect has <span class="math inline">\(\rho\)</span> so small that it is negligible, as seen in the figure.</p>
</div>
</div>
<p>Now we continue:</p>
<p><span class="math display">\[
\begin{aligned}
\braket{e^{-\beta W}} &amp;\approx e^{-2\beta mv^2} \int_{in, out'} \rho dpdx \; e^{2\beta vp} +  \int_{other} \rho dpdx \; 1 \\
&amp;= \frac{1}{L\sqrt{2\pi m/\beta}} \left(\int_{in, out'} e^{-\frac{\beta}{2m}(p - 2mv)^2} dpdx + \int_{other} e^{-\frac{\beta}{2m}p^2} dpdx\right)
\end{aligned}
\]</span></p>
<p>Because the “in-out’ region” is symmetric across the <span class="math inline">\(p = mv\)</span> line, we can reflect the first integral across the <span class="math inline">\(p=mv\)</span> line and obtain <span class="math display">\[
= \frac{1}{L\sqrt{2\pi m/\beta}} \left(\int_{in, out'} e^{-\frac{\beta}{2m}p^2} dpdx + \int_{other} e^{-\frac{\beta}{2m}p^2} dpdx\right) = 1
\]</span></p>
</section>
<section id="fluctuation-dissipation-relations-1" class="level3">
<h3 class="anchored" data-anchor-id="fluctuation-dissipation-relations-1">Fluctuation-dissipation relations</h3>
<div id="cor-todo" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 5 (Fluctuation-dissipation relations)</strong></span> Since <span class="math inline">\(e^t\)</span> is convex, we have <span class="math inline">\(\Delta F^* \leq \braket{W}\)</span>, meaning that the average work expended is more than the increase in Helmholtz free energy. This has better be true, else we would be violating the second law of thermodynamics on average.</p>
<p>Since <span class="math inline">\(\Delta F^* = -\frac{1}{\beta} \ln\braket{e^{-\beta W}}\)</span>, we find that to second order, <span class="math display">\[\underbrace{\braket{W} - \Delta F^*}_{\text{work dissipation}} = \frac 12 \beta \underbrace{\sigma_W^2}_{\text{work fluctuation}}\]</span></p>
<p>It is more familiarly written as <span class="math display">\[\mu = D\beta\]</span></p>
<p>where <span class="math inline">\(D = \frac 12 \sigma_W^2\)</span> is the fluctuation coefficient, and <span class="math inline">\(\mu = (\braket{W} - \Delta F^*)\)</span> is the dissipation coefficient.</p>
</div>
<p>For periodic forcing, the CFT has a simpler form.</p>
<p>Consider a time-reversible dynamical system immersed in an energy bath with inverse temperature <span class="math inline">\(\beta\)</span> , driven by periodically varying constraints. For example, a pendulum in a sticky fluid subjected to a periodic driving torque, or a water-cooled electric circuit driven by a periodic voltage.</p>
<p>Such a system will settle into a “dynamical equilibrium” ensemble, much like a canonical ensemble. If it is driven by the same process time-reversed, then, it will settle into another dynamical equilibrium ensemble.</p>
<div id="thm-gallavotti-cohen" class="theorem">
<p><span class="theorem-title"><strong>Theorem 14 (Gallavotti–Cohen fluctuation theorem)</strong></span> <span class="math display">\[\frac{\rho(Q)}{\rho'(-Q)} = e^{\beta Q}\]</span></p>
<p>where <span class="math inline">\(\rho(Q)\)</span> is the probability density that a forward cycle, randomly sampled from the dynamical equilibrium ensemble, emits energy <span class="math inline">\(Q\)</span> into the energy bath. Similarly, <span class="math inline">\(\rho'\)</span> is for the backward cycle.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-42-contents" aria-controls="callout-42" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-42" class="callout-42-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Construct a process that starts at equilibrium, then mount up the periodic driving, runs it for <span class="math inline">\(NT\)</span> time where <span class="math inline">\(N\)</span> is a large integer, then remove the driving. At the <span class="math inline">\(N\to\infty\)</span> limit, the CFT reduces to the result.</p>
</div>
</div>
</div>
</section>
<section id="arrow-of-time" class="level3">
<h3 class="anchored" data-anchor-id="arrow-of-time">Arrow of time</h3>
<p><span class="citation" data-cites="jarzynskiEqualitiesInequalitiesIrreversibility2011">(<a href="#ref-jarzynskiEqualitiesInequalitiesIrreversibility2011" role="doc-biblioref">Jarzynski 2011</a>)</span></p>
<p>Suppose a scientist has recorded a microscopic movie of pulling on an RNA, flipped a coin to decide whether to reverse the movie, then given you the movie. Your task is to guess whether the movie is reversed.</p>
<p>By Bayes theorem, <span class="math display">\[Pr(\text{forward}|x, y) = \frac{1}{1 + e^{-S[x, y]}}\]</span></p>
<p>where <span class="math inline">\(S[x, y] = \beta(W[x, y] - \Delta F^*)\)</span>.</p>
<p>In words, the higher the entropy production, the more likely it is that the movie is playing forward. This is one way to say that entropy production provides an arrow of time.</p>
<p><span class="citation" data-cites="parrondoThermodynamicsInformation2015">(<a href="#ref-parrondoThermodynamicsInformation2015" role="doc-biblioref">Parrondo, Horowitz, and Sagawa 2015</a>)</span></p>
<p><span class="math display">\[\braket{e^{-\beta W - I}} = e^{-\beta \Delta F^*}\]</span></p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-andersenClusterMethodsEquilibrium1977" class="csl-entry" role="listitem">
Andersen, Hans C. 1977. <span>“Cluster <span>Methods</span> in <span>Equilibrium Statistical Mechanics</span> of <span>Fluids</span>.”</span> In <em>Statistical <span>Mechanics</span>: <span>Part A</span>: <span>Equilibrium Techniques</span></em>, edited by Bruce J. Berne, 1–45. Boston, MA: Springer US. <a href="https://doi.org/10.1007/978-1-4684-2553-6_1">https://doi.org/10.1007/978-1-4684-2553-6_1</a>.
</div>
<div id="ref-aspremPonderingImponderablesOccultism2011" class="csl-entry" role="listitem">
Asprem, Egil. 2011. <span>“Pondering <span>Imponderables</span>: <span>Occultism</span> in the Mirror of Late Classical Physics.”</span> <em>Aries-Journal for the Study of Western Esotericism</em> 11 (2): 129.
</div>
<div id="ref-bergPhysicsChemoreception1977" class="csl-entry" role="listitem">
Berg, H C, and E M Purcell. 1977. <span>“<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1473391">Physics of Chemoreception.</a>”</span> <em>Biophysical Journal</em> 20 (2): 193–219.
</div>
<div id="ref-bergRandomWalksBiology1993" class="csl-entry" role="listitem">
Berg, Howard C. 1993. <em>Random Walks in Biology</em>. Expanded ed. Princeton, N.J: Princeton University Press.
</div>
<div id="ref-bergMotileBehaviorBacteria2000" class="csl-entry" role="listitem">
———. 2000. <span>“Motile <span>Behavior</span> of <span>Bacteria</span>.”</span> <em>Physics Today</em> 53 (1): 24–29. <a href="https://doi.org/10.1063/1.882934">https://doi.org/10.1063/1.882934</a>.
</div>
<div id="ref-bergChemotaxisEscherichiaColi1972" class="csl-entry" role="listitem">
Berg, Howard C., and Douglas A. Brown. 1972. <span>“Chemotaxis in <span>Escherichia</span> Coli Analysed by <span class="nocase">Three-dimensional Tracking</span>.”</span> <em>Nature</em> 239 (5374): 500–504. <a href="https://doi.org/10.1038/239500a0">https://doi.org/10.1038/239500a0</a>.
</div>
<div id="ref-boltzmannLecturesGasTheory2011" class="csl-entry" role="listitem">
Boltzmann, Ludwig. 2011. <em>Lectures on <span>Gas Theory</span></em>. Reprint edition. New York: Dover Publications.
</div>
<div id="ref-brennanEvaporationLiquidsKinetic1974" class="csl-entry" role="listitem">
Brennan, J. F., J. S. Shapiro, and E. C. Watton. 1974. <span>“Evaporation of Liquids: <span>A</span> Kinetic Approach.”</span> <em>Journal of Chemical Education</em> 51 (4): 276. <a href="https://doi.org/10.1021/ed051p276">https://doi.org/10.1021/ed051p276</a>.
</div>
<div id="ref-bucarDisappearingPolymorphsRevisited2015" class="csl-entry" role="listitem">
Bučar, Dejan-Krešimir, Robert W. Lancaster, and Joel Bernstein. 2015. <span>“Disappearing <span>Polymorphs Revisited</span>.”</span> <em>Angewandte Chemie International Edition</em> 54 (24): 6972–93. <a href="https://doi.org/10.1002/anie.201410356">https://doi.org/10.1002/anie.201410356</a>.
</div>
<div id="ref-collinVerificationCrooksFluctuation2005" class="csl-entry" role="listitem">
Collin, D., F. Ritort, C. Jarzynski, S. B. Smith, I. Tinoco, and C. Bustamante. 2005. <span>“Verification of the <span>Crooks</span> Fluctuation Theorem and Recovery of <span>RNA</span> Folding Free Energies.”</span> <em>Nature</em> 437 (7056): 231–34. <a href="https://doi.org/10.1038/nature04061">https://doi.org/10.1038/nature04061</a>.
</div>
<div id="ref-discherNewInsightsErythrocyte2000" class="csl-entry" role="listitem">
Discher, Dennis E. 2000. <span>“New Insights into Erythrocyte Membrane Organization and Microelasticity.”</span> <em>Current Opinion in Hematology</em> 7 (2): 117.
</div>
<div id="ref-duncanConstructingQuantumMechanics2019" class="csl-entry" role="listitem">
Duncan, Anthony, and Michel Janssen. 2019. <em>Constructing Quantum Mechanics. <span>Volume</span> 1: <span>The</span> Scaffold 1900-1923</em>. First edition. Oxford, United Kingdom ; New York, NY: Oxford University Press.
</div>
<div id="ref-feynmanWhatYouCare1989" class="csl-entry" role="listitem">
Feynman, Richard P. 1989. <em>"<span>What</span> Do You Care What Other People Think?": Further Adventures of a Curious Character</em>. Edited by Ralph Leighton. New York: Bantam Books.
</div>
<div id="ref-garciaQuantitativeDissectionSimple2011" class="csl-entry" role="listitem">
Garcia, Hernan G., and Rob Phillips. 2011. <span>“Quantitative Dissection of the Simple Repression Input–Output Function.”</span> <em>Proceedings of the National Academy of Sciences</em> 108 (29): 12173–78. <a href="https://doi.org/10.1073/pnas.1015616108">https://doi.org/10.1073/pnas.1015616108</a>.
</div>
<div id="ref-grimus100thAnniversarySackur2013" class="csl-entry" role="listitem">
Grimus, Walter. 2013. <span>“100th Anniversary of the <span>Sackur</span>–<span>Tetrode</span> Equation.”</span> <em>Annalen Der Physik</em> 525 (3). <a href="https://doi.org/10.1002/andp.201300720">https://doi.org/10.1002/andp.201300720</a>.
</div>
<div id="ref-hazoglouCommunicationMaximumCaliber2015" class="csl-entry" role="listitem">
Hazoglou, Michael J., Valentin Walther, Purushottam D. Dixit, and Ken A. Dill. 2015. <span>“Communication: <span>Maximum</span> Caliber Is a General Variational Principle for Nonequilibrium Statistical Mechanics.”</span> <em>The Journal of Chemical Physics</em> 143 (5): 051104. <a href="https://doi.org/10.1063/1.4928193">https://doi.org/10.1063/1.4928193</a>.
</div>
<div id="ref-hijarJarzynskiEqualityIllustrated2010" class="csl-entry" role="listitem">
Híjar, Humberto, and José M. Ortiz de Zárate. 2010. <span>“Jarzynski’s Equality Illustrated by Simple Examples.”</span> <em>European Journal of Physics</em> 31 (5): 1097.
</div>
<div id="ref-hoaglandPhysiologicalControlJudgments1933" class="csl-entry" role="listitem">
Hoagland, Hudson. 1933. <span>“The <span>Physiological Control</span> of <span>Judgments</span> of <span>Duration</span>: <span>Evidence</span> for a <span>Chemical Clock</span>.”</span> <em>The Journal of General Psychology</em> 9 (2): 267–87. <a href="https://doi.org/10.1080/00221309.1933.9920937">https://doi.org/10.1080/00221309.1933.9920937</a>.
</div>
<div id="ref-hoaglandPacemakersHumanBrain1936" class="csl-entry" role="listitem">
———. 1936. <span>“Pacemakers of Human Brain Waves in Normals and in General Paretics.”</span> <em>American Journal of Physiology-Legacy Content</em> 116 (3): 604–15. <a href="https://doi.org/10.1152/ajplegacy.1936.116.3.604">https://doi.org/10.1152/ajplegacy.1936.116.3.604</a>.
</div>
<div id="ref-jarzynskiEqualitiesInequalitiesIrreversibility2011" class="csl-entry" role="listitem">
Jarzynski, Christopher. 2011. <span>“Equalities and <span>Inequalities</span>: <span>Irreversibility</span> and the <span>Second Law</span> of <span>Thermodynamics</span> at the <span>Nanoscale</span>.”</span> <em>Annual Review of Condensed Matter Physics</em> 2 (Volume 2, 2011): 329–51. <a href="https://doi.org/10.1146/annurev-conmatphys-062910-140506">https://doi.org/10.1146/annurev-conmatphys-062910-140506</a>.
</div>
<div id="ref-laidlerUnconventionalApplicationsArrhenius1972" class="csl-entry" role="listitem">
Laidler, Keith J. 1972. <span>“Unconventional Applications of the <span>Arrhenius</span> Law.”</span> <em>Journal of Chemical Education</em> 49 (5): 343. <a href="https://doi.org/10.1021/ed049p343">https://doi.org/10.1021/ed049p343</a>.
</div>
<div id="ref-liphardtReversibleUnfoldingSingle2001" class="csl-entry" role="listitem">
Liphardt, Jan, Bibiana Onoa, Steven B. Smith, Ignacio Tinoco, and Carlos Bustamante. 2001. <span>“Reversible <span>Unfolding</span> of <span>Single RNA Molecules</span> by <span>Mechanical Force</span>.”</span> <em>Science</em> 292 (5517): 733–37. <a href="https://doi.org/10.1126/science.1058498">https://doi.org/10.1126/science.1058498</a>.
</div>
<div id="ref-luaPracticalApplicabilityJarzynski2005" class="csl-entry" role="listitem">
Lua, Rhonald C., and Alexander Y. Grosberg. 2005. <span>“Practical <span>Applicability</span> of the <span>Jarzynski Relation</span> in <span>Statistical Mechanics</span>:  <span>A Pedagogical Example</span>.”</span> <em>The Journal of Physical Chemistry B</em> 109 (14): 6805–11. <a href="https://doi.org/10.1021/jp0455428">https://doi.org/10.1021/jp0455428</a>.
</div>
<div id="ref-mailletOptimalProbabilisticWork2019" class="csl-entry" role="listitem">
Maillet, Olivier, Paolo A. Erdman, Vasco Cavina, Bibek Bhandari, Elsa T. Mannila, Joonas T. Peltonen, Andrea Mari, et al. 2019. <span>“Optimal <span>Probabilistic Work Extraction</span> Beyond the <span>Free Energy Difference</span> with a <span>Single-Electron Device</span>.”</span> <em>Physical Review Letters</em> 122 (15): 150604. <a href="https://doi.org/10.1103/PhysRevLett.122.150604">https://doi.org/10.1103/PhysRevLett.122.150604</a>.
</div>
<div id="ref-parrondoThermodynamicsInformation2015" class="csl-entry" role="listitem">
Parrondo, Juan M. R., Jordan M. Horowitz, and Takahiro Sagawa. 2015. <span>“Thermodynamics of Information.”</span> <em>Nature Physics</em> 11 (2): 131–39. <a href="https://doi.org/10.1038/nphys3230">https://doi.org/10.1038/nphys3230</a>.
</div>
<div id="ref-reifFundamentalsStatisticalThermal1998" class="csl-entry" role="listitem">
Reif, Frederick. 1998. <em>Fundamentals of Statistical and Thermal Physics</em>. 43. [pr.]. <span>McGraw-Hill</span> Series in Fundamentals of Physics. New York: McGraw-Hill.
</div>
<div id="ref-sethnaStatisticalMechanicsEntropy2021" class="csl-entry" role="listitem">
Sethna, James P. 2021. <em>Statistical Mechanics: Entropy, Order Parameters, and Complexity</em>. Second Edition. Oxford Master Series in Statistical, Computational, and Theoretical Physics. Oxford, United Kingdom ; New York, NY: Oxford University Press.
</div>
<div id="ref-wardPerilsPolymorphismSize2017" class="csl-entry" role="listitem">
Ward, Michael D. 2017. <span>“Perils of <span>Polymorphism</span>: <span>Size Matters</span>.”</span> <em>Israel Journal of Chemistry</em> 57 (1-2): 82–92. <a href="https://doi.org/10.1002/ijch.201600071">https://doi.org/10.1002/ijch.201600071</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/yuxi-liu-wired\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block">Everything ©<a href="https://www.apache.org/licenses/LICENSE-2.0.html">Apache License 2.0</a></span></p>
</div>   
    <div class="nav-footer-center">
<p><span class="faux-block">Yuxi on the Wired</span></p>
</div>
    <div class="nav-footer-right">
<p><span class="faux-block"><a href="../../../sitemap.xml">sitemap</a></span></p>
</div>
  </div>
</footer>




</body></html>