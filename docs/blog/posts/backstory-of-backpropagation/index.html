<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxi Liu">
<meta name="dcterms.date" content="2023-12-26">
<meta name="keywords" content="backpropagation, control theory, history, history of neural networks, history of AI, 1960s, 1950s, Frank Rosenblatt, Bernard Widrow, Paul Werbos, deep learning, perceptron, machine learning, artificial intelligence, Hamiltonian, Jacobian, Lagrangian, optimal control theory, Marvin Minsky, Seymour Papert">
<meta name="description" content="What took them so long to just use the chain rule?">

<title>Yuxi on the Wired - The Backstory of Backpropagation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta property="og:title" content="Yuxi on the Wired - The Backstory of Backpropagation">
<meta property="og:description" content="What took them so long to just use the chain rule?">
<meta property="og:image" content="https://yuxi-liu-wired.github.io/blog/posts/backstory-of-backpropagation/figure/banner_cropped.png">
<meta property="og:site-name" content="Yuxi on the Wired">
<meta property="og:image:height" content="652">
<meta property="og:image:width" content="1024">
<meta name="twitter:title" content="Yuxi on the Wired - The Backstory of Backpropagation">
<meta name="twitter:description" content="What took them so long to just use the chain rule?">
<meta name="twitter:image" content="https://yuxi-liu-wired.github.io/blog/posts/backstory-of-backpropagation/figure/banner_cropped.png">
<meta name="twitter:image-height" content="652">
<meta name="twitter:image-width" content="1024">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html" rel="" target="">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://bair.berkeley.edu/" rel="" target=""><i class="bi bi-folder-symlink" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi.liu.1995@gmail.com" rel="" target=""><i class="bi bi-envelope" role="img" aria-label="email">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">The Backstory of Backpropagation</h1>
                  <div>
        <div class="description">
          What took them so long to just use the chain rule?
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">AI</div>
                <div class="quarto-category">math</div>
                <div class="quarto-category">physics</div>
                <div class="quarto-category">history</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxi Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 26, 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">December 27, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#the-backpropagation-algorithm" id="toc-the-backpropagation-algorithm" class="nav-link" data-scroll-target="#the-backpropagation-algorithm">The backpropagation algorithm</a>
  <ul class="collapse">
  <li><a href="#discrete-logical-time" id="toc-discrete-logical-time" class="nav-link" data-scroll-target="#discrete-logical-time">Discrete logical time</a></li>
  <li><a href="#continuous-logical-time" id="toc-continuous-logical-time" class="nav-link" data-scroll-target="#continuous-logical-time">Continuous logical time</a></li>
  <li><a href="#hybrid-logical-time" id="toc-hybrid-logical-time" class="nav-link" data-scroll-target="#hybrid-logical-time">Hybrid logical time</a></li>
  <li><a href="#optimal-control-theory" id="toc-optimal-control-theory" class="nav-link" data-scroll-target="#optimal-control-theory">Optimal control theory</a></li>
  </ul></li>
  <li><a href="#leibniz" id="toc-leibniz" class="nav-link" data-scroll-target="#leibniz">Leibniz</a></li>
  <li><a href="#mcculloch-and-pitts" id="toc-mcculloch-and-pitts" class="nav-link" data-scroll-target="#mcculloch-and-pitts">McCulloch and Pitts</a></li>
  <li><a href="#frank-rosenblatt" id="toc-frank-rosenblatt" class="nav-link" data-scroll-target="#frank-rosenblatt">Frank Rosenblatt</a></li>
  <li><a href="#bernard-widrow-and-marcian-hoff" id="toc-bernard-widrow-and-marcian-hoff" class="nav-link" data-scroll-target="#bernard-widrow-and-marcian-hoff">Bernard Widrow and Marcian Hoff</a></li>
  <li><a href="#seppo-linnainmaa" id="toc-seppo-linnainmaa" class="nav-link" data-scroll-target="#seppo-linnainmaa">Seppo Linnainmaa</a></li>
  <li><a href="#rumelhart-and-sejnowski" id="toc-rumelhart-and-sejnowski" class="nav-link" data-scroll-target="#rumelhart-and-sejnowski">Rumelhart and Sejnowski</a></li>
  <li><a href="#geoffrey-hinton" id="toc-geoffrey-hinton" class="nav-link" data-scroll-target="#geoffrey-hinton">Geoffrey Hinton</a></li>
  <li><a href="#werbos" id="toc-werbos" class="nav-link" data-scroll-target="#werbos">Werbos</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>The backpropagation algorithm could have been invented by a slight generalization of the adjoint method, or the costate method, known widely among control theorists in the 1950s. Hybrid control theory leads almost inexorably to the backpropagation algorithm.</p>
<p>The backpropagation algorithm was reinvented multiple times during the 1970 – 1986. It was finally no longer being reinvented after the popularity of connectionism finally taught everyone how backpropagation works.</p>
<p>Seppo Linnainmaa’s claim of priority is his 1970 master thesis – in Finnish, and never published. An English paper in 1976 was published that may have been relevant to automatic differentiation research, but not to neural network research. He has priority, but no paternity.</p>
<p>Paul Werbos’ claim of priority is that he developed the backpropagation algorithm in 1971, but was frustrated at every turn whenever he tried publishing it, not managing until around 1982. After that, it was taken up by many neural network researchers. He has priority and paternity.</p>
<p>David Rumelhart freely admits that he was not the first to develop it, but he did develop it independently in 1982 and taught others in his research circle. The 1986 paper he coauthored <span class="citation" data-cites="rumelhartLearningRepresentationsBackpropagating1986">(<a href="#ref-rumelhartLearningRepresentationsBackpropagating1986" role="doc-biblioref">Rumelhart, Hinton, and Williams 1986</a>)</span> became popular enough that nobody would need to reinvent it again. He has no priority, but paternity.</p>
<p>I am unable to explain why none of the first wave of neural network researchers developed backpropagation. The best hypothesis is that everyone was misled by the then current understanding of real neurons. Back in the days, everyone “knew” that real neurons operated by discrete spikes. Possibly they were also misled by the McCulloch–Pitts neuron model, as well as a mistaken attempt to achieve parity with digital computers, which were discrete.</p>
</section>
<section id="the-backpropagation-algorithm" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-backpropagation-algorithm">The backpropagation algorithm</h2>
<p>To set the stage, we should at least quickly derive the backpropagation algorithm. In one sentence, backpropagation is an algorithm that calculates the gradient of every parameter in an acyclic directed graph with respect to a single final parameter.</p>
<div class="page-columns page-full"><p>The graph can be finite or infinite, but in all cases, the acyclic directedness allows us to assign a “logical time” to each node of the graph.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Some nodes are independent variables: they point at other nodes, but no nodes point at <em>them</em>. Other nodes are dependent. If we know the independent variables, we can propagate their values <em>forward</em> in logical time and obtain the values of every node. This is the “forward pass”. Backpropagation goes backwards in logical time.</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;Because anything that happens, happens in the real world, and the real world has a single direction in time, any computation that can happen must have a logical time that is identical with physical time, even though logical time does not have to coincide with physical time. This is an existence proof only.</p></li></div></div>
<p>We use the convention of putting the derivative on the rows. So for example, for <span class="math inline">\(f: \mathbb{R}^2\to\mathbb{R}^2\)</span>, we have</p>
<p><span class="math display">\[
\nabla_x f = \frac{df}{dx} = \begin{bmatrix}
\frac{df_1}{dx_1} &amp; \frac{df_1}{dx_2} \\
\frac{df_2}{dx_1} &amp; \frac{df_2}{dx_2}
\end{bmatrix}
\]</span></p>
<p>This convention simplifies a lot of equations, and completely avoids transposing any matrix.</p>
<section id="discrete-logical-time" class="level3">
<h3 class="anchored" data-anchor-id="discrete-logical-time">Discrete logical time</h3>
<p>Consider an acyclic directed graph with a finite number of nodes. Each graph node is a finite-dimensional real vector, though they may have different dimensions We order the acyclic graph on the number line, so that each node depends only on the earlier nodes. Now denote the graph nodes as <span class="math inline">\(x_0, x_1, ..., x_T\)</span>. By our ordering, <span class="math inline">\(x_1\)</span> depends on only <span class="math inline">\(x_0\)</span>, and <span class="math inline">\(x_2\)</span> depends on only <span class="math inline">\(x_1, x_2\)</span>, and so on:</p>
<p><span class="math display">\[
\begin{aligned}
x_0 &amp;= x_0 \\
x_1 &amp;= f_1(x_0) \\
&amp;\cdots \\
x_T &amp;= f_T(x_0, x_1, ..., x_{T-1})
\end{aligned}
\]</span></p>
<p>Now we perform an infinitesimal perturbation on every of its independent variables. A forward propagation then causes an infinitesimal perturbation on every variable, independent or dependent. Denote these as <span class="math inline">\(dx_0, dx_1, ..., dx_T\)</span>. We can now compute the derivative of <span class="math inline">\(dx_T\)</span> with respect to every other variable by backpropagating the perturbation. Suppose we can see only <span class="math inline">\(dx_T\)</span>, then the change in <span class="math inline">\(x_T\)</span> due to <span class="math inline">\(dx_T\)</span> is the identity. That is,</p>
<p><span class="math display">\[
\frac{dx_T}{dx_T} = I
\]</span></p>
<p>Now suppose we can see only <span class="math inline">\(dx_{T-1}\)</span>, then the change in <span class="math inline">\(x_T\)</span> due to <span class="math inline">\(dx_{T-1}\)</span> can only come from the final step in the forward propagation. Therefore</p>
<p><span class="math display">\[
\frac{dx_T}{dx_{T-1}} = \nabla_{x_{T-1}} f_T(x_0, x_1, ..., x_{T-1})
\]</span></p>
<p>Similarly, the change in <span class="math inline">\(x_T\)</span> due to <span class="math inline">\(dx_{T-2}\)</span> can come from either directly changing <span class="math inline">\(x_T\)</span>, or changing <span class="math inline">\(x_{T-1}\)</span> and thereby changing <span class="math inline">\(x_T\)</span>. Therefore,</p>
<p><span class="math display">\[
\frac{dx_T}{dx_{T-2}} =
    \nabla_{x_{T-1}} f_{T}(x_0, x_1, ..., x_{T-2}) +
    \underbrace{\frac{dx_T}{dx_{T-1}}\nabla_{x_{T-2}} f_{T-1}(x_0, x_1, ..., x_{T-2})}_{\text{the chain rule}}
\]</span></p>
<p>This generalizes to the rest of the steps.</p>
<p>The above computation is fully general, but this makes it inefficient. In practical applications of backpropagation, the computation graph is usually very sparse, meaning that each <span class="math inline">\(x_t\)</span> only directly influence a few more nodes down the line. In standard neural networks, typically <span class="math inline">\(x_{t}\)</span> only directly influences <span class="math inline">\(x_{t+1}, x_{t+2}\)</span>. Thus sparsity is vital for backpropagation to be relevant.</p>
<p>As a side note, we could in fact compute <em>all</em> derivatives, not just the first, in one single backward pass. Other than the second derivatives <span class="math inline">\(\nabla^2_{x_t}x_T\)</span>, there are rarely any use for the other derivatives, such as <span class="math inline">\(\nabla_{x_t}\nabla_{x_s}x_T, \nabla^3_{x_t}x_T\)</span>, etc. The second derivative was occasionally used for neural networks circa 1990, with the further simplification that they only keep the positive diagonal entries of <span class="math inline">\(\nabla^2_{x_t}x_T\)</span> and set all other entries to zero.<span class="citation" data-cites="lecunGeneralizationNetworkDesign1989">(<a href="#ref-lecunGeneralizationNetworkDesign1989" role="doc-biblioref">LeCun 1989</a>)</span></p>
</section>
<section id="continuous-logical-time" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="continuous-logical-time">Continuous logical time</h3>
<p>Consider the problem of controlling a car along a highway. At a sufficient level of simplification, the car is just a single dot on a line, and the state of the car is determined by its speed and velocity. To avoid waiting forever, we require time <span class="math inline">\(t\in [0, T]\)</span>. Write the state variable as follows:</p>
<p><span class="math display">\[
x_t = (\text{location at time }t, \text{velocity at time }t)
\]</span></p>
<p>It might be confusing to use <span class="math inline">\(x_t\)</span> for the state at time <span class="math inline">\(t\)</span>, instead of location, but it makes the notation consistent.</p>
<p>Suppose the only thing we can influence is how much we press the pedal. Write <span class="math inline">\(u_t\)</span> to be the resulting acceleration we inflict on the car by pressing on the pedal. Further, assume the car is slowed down by friction that is proportional to velocity. We then have:</p>
<p><span class="math display">\[
\dot x_t = f(x_t, u_t)
\]</span></p>
<div class="page-columns page-full"><p>where <span class="math inline">\(f(x_t, u_t) = (x_{t, 1}, -\mu x_{t, 1} + u_t)\)</span> is the dynamics equation of the system, and <span class="math inline">\(\mu\)</span> is the friction coefficient.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;To allow for time-varying dynamics, simply replace <span class="math inline">\(f(x_t, u_t)\)</span> with <span class="math inline">\(f(t, x_t, u_t)\)</span>. This clutters the notation without involving new ideas.</p></li></div></div>
<p>Now, we can draw the computation graph as before. The computation graph has a continuum of nodes, but it has a very simple structure. The graph has two kinds of nodes: the <span class="math inline">\(x_t\)</span> nodes and the <span class="math inline">\(u_t\)</span> nodes. Its independent variables are <span class="math inline">\(x_0\)</span> and all the <span class="math inline">\(u_t\)</span> nodes. Each <span class="math inline">\(x_{t+dt}\)</span> depends on only <span class="math inline">\(x_{t}\)</span> and <span class="math inline">\(u_t\)</span>. This makes the two propagations particularly simple.</p>
<p>The forward propagation is obtained by integration, which we can unwrap into a form resembling the case of the discrete logical time:</p>
<p><span class="math display">\[
\begin{aligned}
x_0 &amp;= x_0 \\
x_{dt} &amp;= x_0 + f(x_0, u_0) dt \\
&amp;\cdots
x_{T} &amp;= x_{T-dt} + f(x_{T-dt}, u_{T-dt}) dt \\
\end{aligned}
\]</span></p>
<p>The backpropagation is similarly obtained. By inspecting the computation graph, we can see that each <span class="math inline">\(u_t\)</span> only directly influences <span class="math inline">\(x_{t+dt}\)</span>, thus</p>
<p><span class="math display">\[
\frac{dx_T}{du_t} = \frac{dx_T}{dx_{t+dt}} \nabla_{u_t}f(x_t, u_t) dt
\]</span></p>
<p>It remains to compute <span class="math inline">\(\frac{dx_T}{dx_t}\)</span>. This can be found by backpropagation too, since each <span class="math inline">\(x_t\)</span> only directly influences <span class="math inline">\(x_{t+dt}\)</span>, thus</p>
<p><span class="math display">\[
\frac{dx_T}{dx_t} = \frac{dx_T}{dx_{t+dt}} \left[I + \nabla_{x_t} f(x_t, u_t) dt\right]
\]</span></p>
<p>If we denote the gradient as <span class="math inline">\(g_t := \frac{dx_T}{dx_t}\)</span>, then we find an equation for <span class="math inline">\(g_t\)</span>:</p>
<p><span class="math display">\[
g_t = \left[I + (g_t + \dot g_t dt) \nabla_{x_t} f(x_t, u_t) dt\right]\implies \dot g_t = -g_t\nabla_{x_t} f(x_t, u_t)
\]</span></p>
<p>This equation bottoms out at the end time, <span class="math inline">\(t=T\)</span>, for which <span class="math inline">\(g_T = \frac{dx_T}{dx_T} = I\)</span>. Thus we have the <a href="https://en.wikipedia.org/wiki/Costate_equation">costate equations</a>:</p>
<p><span class="math display">\[
\begin{cases}
g_T &amp;= I \\
\dot g_t &amp;= - g_t \nabla_{x_t} f(x_t, u_t)
\end{cases}
\]</span></p>
<p>which must, as you can see, be integrated <em>backwards in time</em> – backpropagation again! Indeed, control theory practically compels us to find backpropagation.</p>
</section>
<section id="hybrid-logical-time" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="hybrid-logical-time">Hybrid logical time</h3>
<div class="page-columns page-full"><p>When the computation graph has both nodes with discrete logical times, and nodes with continuous logical times, we call such a system as having hybrid logical time.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;The name “hybrid” comes from “<a href="https://en.wikipedia.org/wiki/Hybrid_system">hybrid control theory</a>”, which are systems with both discrete changes, or jumps, and continuous changes, or flow. They appear whenever a digital device is used to control a continuous system, such as a computer piloting an airplane.</p></li></div></div>
<p>The idea can be illustrated by the fundamental problem in control theory: how to optimize control? You cannot optimize without optimizing a real number, so we write down the number as <span class="math inline">\(J\)</span>, representing the “cost” of the trajectory. For example, in driving a car, we might want to optimize comfort, time-until-arrival, and fuel cost. We can then write <span class="math inline">\(J\)</span> down as something like</p>
<p><span class="math display">\[
J = \underbrace{(x_{T, 0} - x_{goal})^2}_{\text{location should be at the goal location at the end time}}
    + \underbrace{(x_{T, 1} - 0)^2}_{\text{speed should be zero at the end time}} + \int_0^T u_t^2 dt
\]</span></p>
<p>More generally, the objective to be optimized is in the form</p>
<p><span class="math display">\[
J = A(x_T) + \int_0^T L(x_t, u_t)dt
\]</span></p>
<p>for some real-valued functions <span class="math inline">\(A, L\)</span>.</p>
<p>We can of course care about more than the state at the last time-step. We can care about multiple time-steps <span class="math inline">\(t_0, t_1, ..., t_n\)</span> by writing down a cost function <span class="math inline">\(J = \sum_{i=0}^n A_i(x_{t_i}) + \int_0^T L(x_T, u_T)dt\)</span>, but this merely creates complications without introducing new ideas. Even more ornate computation graphs, weaving together multiple strands of continuous and discrete logical times, can be backpropagated in the same way without involving new ideas.</p>
<p>Define the costate <span class="math inline">\(\lambda_t := \nabla_{x_t} J\)</span>, then the costate backpropagates as:</p>
<p><span class="math display">\[
\lambda_t = L(x_t, u_t) dt + \lambda_{t+dt}(I + \nabla_{x_t}f(x_t, u_t)dt)
\]</span></p>
<p>and simplifying, we have the costate equation:</p>
<p><span class="math display">\[
\begin{cases}
\lambda_T &amp;= \nabla_{x_T} A(x_T) \\
\dot \lambda_t &amp;= - \nabla_{x_t} L(x_t, u_t) - \lambda_t \nabla_{x_t} f(x_t, u_t)
\end{cases}
\]</span></p>
<p>which can be solved by integrating backwards in time.</p>
<p>Once we have obtained all the costates, we can compute <span class="math inline">\(\nabla_{u_t} J\)</span>. Since <span class="math inline">\(u_t\)</span> can only influence <span class="math inline">\(J\)</span> either directly via <span class="math inline">\(L(x_t, u_t)\)</span> or indirectly via <span class="math inline">\(x_t\)</span>, we have</p>
<p><span class="math display">\[
\nabla_{u_t} J = \left[\nabla_{u_t}f(x_t, u_t) \lambda_t + \nabla_{u_t}L(x_t, u_t)\right]dt
\]</span></p>
<p>Here we note that <span class="math inline">\(\nabla_{u_t} J\)</span> is an infinitesimal in <span class="math inline">\(dt\)</span>, which is qualitatively different from <span class="math inline">\(\nabla_{x_t}J = g_t\)</span>, which is not an infinitesimal, but a mere matrix. Why is it so? The simplest example suffices to illustrate.</p>
<p>Consider a mass point sliding on a frictionless plane. When we perturb <span class="math inline">\(x_t\)</span>, we would push it to the side by <span class="math inline">\(\delta x_{t, 0}\)</span> and also change its velocity by <span class="math inline">\(\delta x_{t, 1}\)</span>, and so at the end time <span class="math inline">\(T\)</span>, we would have changed <span class="math inline">\(x_T\)</span> by <span class="math inline">\((\delta x_{t, 0} + (T-t)\delta x_{t, 1}, \delta x_{t, 1})\)</span>, which is the same order of infinitesimal. Now, we can control the mass point by applying a force <span class="math inline">\(u_t\)</span>, which gives us the dynamics equation</p>
<p><span class="math display">\[
\dot x_t = (x_{t, 1}, u_t)
\]</span></p>
<p>To “perturb” <span class="math inline">\(u_t\)</span> by <span class="math inline">\(\delta u_t\)</span> does not make sense on its own, as a “spike” of <span class="math inline">\(\delta u_t\)</span> that lasts for zero amount of time does not show up in the system trajectory. One can apply a durationless jump in state, but one cannot apply a durationless force. Therefore, it only makes sense to perturb <span class="math inline">\(u_t\)</span> by <span class="math inline">\(\delta u_t\)</span> and persist the perturbation for <span class="math inline">\(dt\)</span> time. This then perturbs the state at the end time by <span class="math inline">\(((T-t)\delta u_t dt , \delta u_t dt)\)</span>, which gives <span class="math inline">\(\nabla_{u_t}x_T \propto dt\)</span>.</p>
</section>
<section id="optimal-control-theory" class="level3">
<h3 class="anchored" data-anchor-id="optimal-control-theory">Optimal control theory</h3>
<p>An optimal trajectory must have <span class="math inline">\(\nabla_{u_t} J = 0\)</span>, since otherwise we can shave off a little piece of cost by giving <span class="math inline">\(u_t\)</span> a little boost in the other direction (infinitesimal gradient descent!). This gives us two equations that any optimal trajectory must satisfy</p>
<p><span class="math display">\[
\begin{cases}
- \dot \lambda_t &amp;= \nabla_{x_t} L(x_t, u_t) + \lambda_t \nabla_{x_t} f(x_t, u_t) \\
0                &amp;= \nabla_{u_t} L(x_t, u_t) + \lambda_t \nabla_{u_t} f(x_t, u_t) \\
\end{cases}
\]</span></p>
<p>Now, what is the effect of perturbing <span class="math inline">\(u_t\)</span> by <span class="math inline">\(du_t\)</span>? It would perturb <span class="math inline">\(x_{t+dt}\)</span> by <span class="math inline">\(\nabla_{u_t} f(x_t, u_t) du_t dt\)</span>, a second-order infinitesimal. Consequently, it would perturb <span class="math inline">\(x_T\)</span> by only a second-order infinitesimal, and thus <span class="math inline">\(\lambda\)</span> too. Therefore, we have</p>
<p><span class="math display">\[
\nabla_{u_t}\lambda_t = 0
\]</span></p>
<p>giving us simplified equations for optimality:</p>
<p><span class="math display">\[
\begin{cases}
- \dot \lambda_t &amp;= \nabla_{x_t} L(x_t, u_t) + \lambda_t \nabla_{x_t} f(x_t, u_t) \\
0                &amp;= \nabla_{u_t} (L(x_t, u_t) + \lambda_t f(x_t, u_t)) \\
\end{cases}
\]</span></p>
<p>Unfortunately, the first equation cannot be simplified similarly, because <span class="math inline">\(\nabla_{x_t}\lambda_t \neq 0\)</span>. Still, it seems <span class="math inline">\(L(x_t, u_t) + \lambda_t f(x_t, u_t)\)</span> should be an important quantity:</p>
<p><span class="math display">\[
H(x_t, u_t, \lambda_t) := L(x_t, u_t) + \lambda_t f(x_t, u_t)
\]</span></p>
<p>The letters are meaningful. <span class="math inline">\(L\)</span> is the “Lagrangian”, and <span class="math inline">\(H\)</span> is the “Hamiltonian”. Indeed, classical Hamiltonian mechanics is a special case of (first order) optimal control theory.</p>
<p>If we interpret economically the quantities, then <span class="math inline">\(J\)</span> is the cost of the entire trajectory, <span class="math inline">\(\lambda_t\)</span> is the marginal cost of the point <span class="math inline">\(x_t\)</span> in the trajectory, and <span class="math inline">\(L(x_t, u_t)\)</span> is the cost-rate at time <span class="math inline">\(t\)</span>. The second equation of optimality <span class="math inline">\(\nabla_{u_t} H(x_t, u_t, \lambda_t) = 0\)</span> states that when the trajectory is optimal, the marginal cost of control is zero: there is no saving in perturbing the control in any direction.</p>
<p>Therefore, define the “optimized” Hamiltonian and optimized control relative to it:</p>
<p><span class="math display">\[
\begin{cases}
H^*(x_t, \lambda_t) &amp;:= \min_{u_t}H(x_t, u_t, \lambda_t) = \min_{u_t} \left(L(x_t, u_t) + \lambda_t f(x_t, u_t)\right) \\
u^*(x_t, \lambda_t) &amp;:= \mathop{\mathrm{argmin}}_{u_t}H(x_t, u_t, \lambda_t)
\end{cases}
\]</span></p>
<p>Then, by <a href="https://en.wikipedia.org/wiki/Hotelling's_lemma">Hotelling’s lemma</a>, we obtain the <a href="https://en.wikipedia.org/wiki/Hamiltonian_mechanics">Hamiltonian equations of motion</a>:</p>
<p><span class="math display">\[
\begin{cases}
- \dot \lambda_t &amp;= \nabla_{x_t} L(x_t, u_t^*) + \lambda_t \nabla_{x_t} f(x_t, u_t^*) &amp;= \nabla_{x_t} H^*(x_t, \lambda_t) \\
  \dot x_t       &amp;= f(x_t, u_t^*)                                                     &amp;= \nabla_{\lambda_t} H^*(x_t, \lambda_t) \\
\end{cases}
\]</span></p>
<p>This is often called the <a href="https://en.wikipedia.org/wiki/Pontryagin%27s_maximum_principle">Pontryagin’s maximum principle</a>, as Pontryagin’s school of optimal control theory is based on this principle and its extensions. Control theory was a national security issue during the Cold War, as it was used from the <a href="https://en.wikipedia.org/wiki/Space_Race">space race</a> to the <a href="https://en.wikipedia.org/wiki/Nuclear_arms_race">missile race</a>.</p>
<p>In classical control theory, the equation is sometimes solved in closed form, as in the case of <a href="https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator">linear quadratic control</a>. When it cannot be solved in closed form, it can still be numerically solved using a continuous form of the Bellman equation. Similar to how the Hamiltonian equations have the alternative form of the <a href="https://en.wikipedia.org/wiki/Hamilton-Jacobi_equation">Hamilton–Jacobi equation</a>, the Pontryagin equations have an alternative form of the of the <a href="https://en.wikipedia.org/wiki/Hamilton-Jacobi_equation">Hamilton–Jacobi–Bellman equation</a>. Possibly, the name “dynamic programming” appears later in Paul Werbos’ invention of backpropagation, which he named “dynamic feedback”.</p>
<p>In economics, one can modify this framework slightly to allow discounting cost over time, yielding theories about “optimal investment” such as the <a href="https://en.wikipedia.org/wiki/Ramsey%E2%80%93Cass%E2%80%93Koopmans_model">Ramsey optimal growth theory</a>.</p>
</section>
</section>
<section id="leibniz" class="level2">
<h2 class="anchored" data-anchor-id="leibniz">Leibniz</h2>
<p>The chain rule dates back to <em>Calculus Tangentium differentialis</em> [Differential calculus of tangents], a manuscript by Leibniz dated 1676 November <span class="citation" data-cites="childManuscriptsLeibnizHis1917">(<a href="#ref-childManuscriptsLeibnizHis1917" role="doc-biblioref">Child 1917</a>)</span>. It says</p>
<blockquote class="blockquote">
<p>it does not matter, whether or no the letters <span class="math inline">\(x, y, z\)</span> have any known relation, for this can be substituted afterward.</p>
</blockquote>
<p>In mathematical notation, he found that <span class="math inline">\(dy = dx \frac{dy}{dx}\)</span>, or in his notation, <span class="math inline">\(\overline{dy} = \overline{dx} \frac{dy}{dx}\)</span>, where the overbar denotes the thing to be differentiated. You can read it as a bracket: <span class="math inline">\(d(y) = d(x) \frac{dy}{dx}\)</span>.</p>
<p>He then gave the following examples (Yes, there is a sign error. No, I’m not going to fix it. He just have to live with his mistakes.):</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \overline{d \sqrt[2]{a+b z+c}} z^2 \text {. Let } a+b z+c z^2=x \text {; } \\
\text{Then} \quad  &amp; \overline{d \sqrt[2]{x}}=-\frac{1}{2 \sqrt{x}} \text {, and } \frac{d x}{d z}=b+2 c z \text {; } \\
\text{Therefore} \quad  &amp; \overline{d \sqrt{a+b z+c z^2}}=-\frac{b+2 c z}{2 \overline{d z} \sqrt{a+b z+c z^2}} \\
&amp;
\end{aligned}
\]</span></p>
</section>
<section id="mcculloch-and-pitts" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="mcculloch-and-pitts">McCulloch and Pitts</h2>
<p>In the famous paper <span class="citation" data-cites="mccullochLogicalCalculusIdeas1943">(<a href="#ref-mccullochLogicalCalculusIdeas1943" role="doc-biblioref">McCulloch and Pitts 1943</a>)</span>, McCulloch and Pitts proposed that</p>
<blockquote class="blockquote">
<p>Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes.</p>
</blockquote>
<p>The McCulloch and Pitts paper is difficult to read, and is rooted in early 20th century mathematical logic. In fact, it uses the same abstruse notations of the <a href="https://en.wikipedia.org/wiki/Principia_Mathematica"><em>Principia Mathematica</em></a>, which is cited in the paper.</p>
<div class="quarto-layout-panel page-columns page-full">
<div class="quarto-layout-row quarto-layout-valign-top page-columns page-full">
<div class="quarto-layout-cell page-columns page-full" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/principia_mathematica.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">The infamous proof of 1+1=2 in Principia Mathematica</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell page-columns page-full" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/principia_mathematica_McCulloch_and_Pitts.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">The same notation is used by McCulloch and Pitts</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>The McCulloch and Pitts paper, like the <em>Perceptron</em> book, is something often cited but rarely read. The best introduction to the McCulloch and Pitts neural network is still <span class="citation" data-cites="minskyComputationFiniteInfinite1967">(<a href="#ref-minskyComputationFiniteInfinite1967" role="doc-biblioref">Minsky 1967, chap. 3</a>)</span>, which explicitly designs, neuron by neuron, threshold by threshold, how to construct neural networks that perform various tasks, such as simulating arbitrary Turing machines, process signals, perform additions, etc.</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="figure/minsky_1976_finite_state_machine.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="figure/minsky_1976_serial_binary_addition_network.png" class="img-fluid"></p>
</div>
</div>
</div>
</section>
<section id="frank-rosenblatt" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="frank-rosenblatt">Frank Rosenblatt</h2>
<p>Frank Rosenblatt is the originator of the term “backpropagation”, or more precisely, “back-propagating error-correction procedure” <span class="citation" data-cites="rosenblattPrinciplesNeurodynamicsPerceptrons1962">(<a href="#ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" role="doc-biblioref">Rosenblatt 1962, vol. 55, chap. 13</a>)</span>, although it was quite far from our current use of backpropagation. He is the originator of many concepts in neural networks, and he was even popularly famous during his own time. During the period of 1957 – 1962, he did a lot of the theory and experiments with perceptron networks. Some experiments ran on digital computer simulations, and others on the Mark I Perceptron, a bespoke machine that takes up a whole room.</p>
<p>A perceptron is a function of form <span class="math inline">\(\theta(w^T x + b)\)</span>, where <span class="math inline">\(\theta\)</span> is the 0-1 step function, and <span class="math inline">\(w \in \mathbb{R}^n, b \in \mathbb{R}\)</span> are its learnable parameters. A perceptron network was a network of perceptrons. Rosenblatt experimented with a large number of perceptron network architectures, but most often, he experimented with a certain 2-layered feedforward architecture, which is sometimes called “<em>the</em> perceptron machine”.</p>
<p><em>The</em> perceptron machine is a machine that computes a function of type <span class="math inline">\(\{0, 1\}^n \to \{0, 1\}\)</span>. Its input layer is composed of units named “Stimulus units” or “S units”. The S units do not perform any computation, but merely pass on binary outputs to the hidden layer. In the Mark I perceptron, the S units were 20×20 cadmium sulfide photocells, making a 400-pixel image. The hidden layer is composed of perceptrons named “Association units” or “A units”. Their outputs pass on to the output layer, composed of perceptrons named “Response units” or “R units”.</p>
<p>We can describe <em>the</em> perceptron machine in one equation:</p>
<p><span class="math display">\[
f(x) = \theta\left(b^{R} + \sum_i w^{AR}_i \theta\left((w^{SA, i})^T x + b^{A}_i\right)\right)
\]</span></p>
<div class="page-columns page-full"><p>Rosenblatt proved some mathematical theorems, the most important of which is the <a href="https://en.wikipedia.org/wiki/Perceptron#Learning_algorithm_for_a_single-layer_perceptron">perceptron convergence theorem</a>, which shows that assuming the dataset is linearly separable, then the perceptron learning algorithm converges after making a bounded number of errors<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. He also performed experiments on the Mark I Perceptron machine, IBM computers, and some other computers.</p><div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;There is another technical assumption on the dataset, but it is omitted for space. Suffice to say it is satisfied for all finite datasets.</p></li></div></div>
<p>His theorems and experiments were exhaustively documented in his book <span class="citation" data-cites="rosenblattPrinciplesNeurodynamicsPerceptrons1962">(<a href="#ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" role="doc-biblioref">Rosenblatt 1962</a>)</span>. Its breadth is quite astonishing. It contains:</p>
<ul>
<li>perceptrons with continuous activation functions (section 10.2);</li>
<li>perceptron convergence theorem for perceptrons with continuous activation functions that are also monotonic and sign-preserving (page 249);</li>
<li>perceptron layers with random delay in transmission time (chapter 11);</li>
<li>layers with connections between units within the same layer, with possibly closed loops (chapter 17–19);</li>
<li>layers with connections from a later layer to a previous layer (chapter 20);</li>
<li>perceptron networks that learns to associate image and audio inputs ()</li>
<li>program-learning perceptrons (chapter 22)</li>
<li>perceptron networks that analyze videos and audios (chapter 23)</li>
</ul>
<p>From our vantage point, we can fairly say that he has invented randomization, weight initiation schemes, neural Turing machines, recurrent neural networks, vision models, time delay neural networks, multimodal neural networks…</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/rosenblatt_figure_58.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="figure-caption margin-caption">Figure 58. The first multimodal neural network?</figcaption>
</figure>
</div>
<p>What is even more astonishing is that as far as I can see, he did not make use of gradient descent learning at all, not even on a single layer like Widrow and Hoff. The perceptron learning rule converges, but only for a single-layered perceptron network on linearly separable data. Thus, when it came to two-layered perceptron networks, he fixed the wiring in the first layer randomly, then adapted the second layer. This would prove to be a turning point in the “<a href="https://yuxi-liu-wired.github.io/blog/posts/backstory-of-backpropagation/neural-network-winter">perceptron controversy</a>”.</p>
<p>In the chapter where he talked about backpropagation <span class="citation" data-cites="rosenblattPrinciplesNeurodynamicsPerceptrons1962">(<a href="#ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" role="doc-biblioref">Rosenblatt 1962, vol. 55, chap. 13</a>)</span>, he was trying to train both layers in the two-layered perceptron network by extending the perceptron learning algorithm. The algorithm works on each individual perceptron unit, but only if the desired output is known. The desired outputs for the output units are supplied by the experimenter, but the desired outputs for the hidden units could not be known, so Rosenblatt tried some hacks. Essentially, he took the error signals at the output units, and used heuristic rules to “backpropagate the error” to synthesize error signals at the hidden units. The precise details are hacky, and no longer of any relevance.</p>
<p>One last thing about his backpropagation rule: he also discovered the per-layer learning rate trick. Because his backpropagation algorithm was so hacky, he found that he had to stabilize it by making the first layer learn at a much lower rate than the second.</p>
<blockquote class="blockquote">
<p>It is found that if the probabilities of changing the S-A connections are large, and the threshold is sufficiently small, the system becomes unstable, and the rate of learning is hindered rather than helped by the variable S-A network. Under such conditions, the S-A connections are apt to change into some new configuration while the system is still trying to adjust its values to a solution which might be perfectly possible with the old configuration. Better performance is obtained if the rate of change in the S-A network is sufficiently small to permit an attempt at solving the problem before drastic changes occur.</p>
</blockquote>
</section>
<section id="bernard-widrow-and-marcian-hoff" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="bernard-widrow-and-marcian-hoff">Bernard Widrow and Marcian Hoff</h2>
<p>Widrow and Hoff worked on multilayered perceptrons in the early 1960s. They trained a single-layered perceptron with gradient descent on the squared loss, then proceeded to not try gradient descent after a year of trying to train a network with <em>two layers</em>.</p>
<p>The Widrow–Hoff machine, which they called the ADALINE (“ADAptive Linear NEuron”), is a function of type <span class="math inline">\(\mathbb{R}^n \to \{0,1\}\)</span> defined by</p>
<p><span class="math display">\[
f(x) = \theta(w^T x + b)
\]</span></p>
<p>and here it is again, that dreaded Heaviside step-function that was the bane of every neural network before backpropagation. What was odd about this one is that ADALINE was <em>trained</em> by gradient descent with the squared loss function <span class="math inline">\(\frac 12 (w^T x + b - y)^2\)</span>, which is <em>continuous</em>, not discrete:</p>
<p><span class="math display">\[
w \leftarrow w - \alpha (w^T x + b - y) w, \quad b \leftarrow b - \alpha (w^T x + b - y) b
\]</span></p>
<p>The first ADALINE machine was a box that learned to classify binary patterns on a <span class="math inline">\(4 \times 4\)</span> grid. It was pretty amusing, as everything was manual. The patterns were inputted by flipping 16 switches by hand. The error <span class="math inline">\(w^T x + b - y\)</span> was read from a voltmeter, and the parameters <span class="math inline">\(w, b\)</span> were individually adjusted by turning knobs that controlled rheostats inside the box. They then automated the knob-turning process using electrochemical devices called memistors, though the pattern input <span class="math inline">\(x\)</span> and the desired output <span class="math inline">\(y\)</span> were still entered by manual switches.</p>
<div class="quarto-layout-panel page-columns page-full" width="80%">
<div class="quarto-layout-row quarto-layout-valign-top page-columns page-full">
<div class="quarto-layout-cell page-columns page-full" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/widrow_2022_memistor_adaline.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">A memistor ADALINE with glass-sealed memistors. <span class="citation" data-cites="widrowAncientHistory2023">(<a href="#ref-widrowAncientHistory2023" role="doc-biblioref">Widrow 2023, fig. 26.12</a>)</span></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell page-columns page-full" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/widrow_2022_learning_curve.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Test patterns, and a learning curve, for the ADALINE machine. <span class="citation" data-cites="widrowAncientHistory2023">(<a href="#ref-widrowAncientHistory2023" role="doc-biblioref">Widrow 2023, fig. 26.4</a>)</span></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Widrow recounts an amusing encounter with Rosenblatt:</p>
<blockquote class="blockquote">
<p>I just put the pattern in and the Adaline went “phut,” and the needle was reading to the right or to the left. So I just held the adapt button down so some of the cells are plating while others are deplating, depending on the direction of the error signal. Rosenblatt’s students put the pattern into the percept ron. You could see it in the lights on the percept ron. You could hear the potentiometers grinding away. We put another pattern into the Adaline, and it went “blip ,” and there it was, adapted. They put it in the perceptron, and it’s still grinding away. We put in a couple more patterns. Then we test the Adaline and test the percept ron to see whether the patterns are still in there.</p>
<p>They’re in the Adaline. In the perceptron, they’re all gone. I don’t know whether the machine was temperamental or what, but it was difficult to train. I argued with Rosenblatt about that first random layer. I said, “You’d be so much better off if you just took the signal from the pixels and ran them straight into the weights of the second layer.” He insisted that a perceptron had to be built this way because the human retina is built that way. That is, there’s a first layer that’s randomly connected to the retina. He said the reason why you can get something to interpret and make sense of random connections is because it’s adaptive. You can unravel all this random scrambling. What I was trying to do was to not model nature. I was trying to do some engineering.</p>
</blockquote>
<div class="page-columns page-full"><p>After the ADALINE showed good behavior, they went on and on trying to train a two-layered ADALINE (“MADALINE”, or “many ADALINE”), which of course cannot be trained by gradient descent, because the hidden layer ADALINE units used the Heaviside step function! It is almost inconceivable nowadays, but they simply refused to use a continuous activation function, but tried every other trick <em>except</em> that. They ended up with the <a href="https://en.wikipedia.org/wiki/ADALINE#MADALINE">MADALINE I rule</a>. In short, it was a heuristic rule to synthesize supervision signals for the hidden layer, much like <a href="#frank-rosenblatt">Rosenblatt’s heuristic rule</a>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn5"><p><sup>5</sup>&nbsp;The MADALINE I machine consists of multiple hidden ADALINE units, and a single output layer consisting of a single unit. The output unit merely takes the majority vote from the hidden units. If the desired output is <span class="math inline">\(+1\)</span>, but the actual output is <span class="math inline">\(-1\)</span>, then the machine picks those ADALINE units that are negative, but closest to being positive, and make them update their weights, according to the ADALINE learning rule. It was thought of as a form of “minimal disturbance principle”.</p></li></div></div>
<div class="page-columns page-full"><p>Frustrated by the difficulty, they left neural network research. Hoff went to Intel to coinvent the microprocessor, and Widrow set about applying the ADALINE to small problems that it can solve well<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, such as adaptive antennas, adaptive noise filtering in telephone lines, adaptive filtering in medical scanning, etc. Even with just a single ADALINE, he brought breakthroughs to those fields.</p><div class="no-row-height column-margin column-container"><li id="fn6"><p><sup>6</sup>&nbsp;Marvin Minsky would approve</p>
<blockquote class="blockquote">
<p>[The perceptron] is such a simple machine that it would be astonishing if nature did not make use of it somewhere. It may be that one of the best things a neuron can have is a tiny Perceptron, since you get so much from it for so little. You can’t get one big Perceptron to do very much, but for some things it remains one of the most elegant and simple learning devices I know of.” <span class="citation" data-cites="bernsteinMarvinMinskyVision1981">(<a href="#ref-bernsteinMarvinMinskyVision1981" role="doc-biblioref">Bernstein 1981</a>)</span></p>
</blockquote>
</li></div></div>
<blockquote class="blockquote">
<p>Apple have told me that every iPhone, since the iPhone 5, uses the LMS algorithm all over the device. They could not tell me where because, if they did, they would have to shoot me. Apple keeps secrets. <span class="citation" data-cites="widrowCyberneticsGeneralTheory2022">(<a href="#ref-widrowCyberneticsGeneralTheory2022" role="doc-biblioref">Widrow 2022</a>, preface, page xix)</span></p>
</blockquote>
<p>Perhaps Widrow and Hoff were limited to considering only learning rules they could implement electrochemically? But it does not seem so, in the words of Widrow himself, it was a blind spot for all researchers:</p>
<blockquote class="blockquote">
<p>The Madaline had an adaptive first layer and a fixed-logic second layer. What Rosenblatt had was a fixed-logic mst layer and an adaptive-logic second layer or output layer. Now, it is easy to adapt on output layer. But it’s very difficult to adapt a hidden layer. … We could adapt an adaptive first layer with a fixed second layer as long as we knew what the second layer was. But we never succeeded in developing an algorithm for adapting both layers, so that the second layer is not fixed and both layers are adaptive. It wasn’t that we didn’t try. I mean we would have given our eye teeth to come up with something like backprop.</p>
<p>Backprop would not work with the kind of neurons that we were using because the neurons we were using all had quantizers that were sharp. In order to make backprop work, you have to have sigmoids; <strong>you have to have a smooth nonlinearity … no one knew anything about it at that time.</strong> This was long before Paul Werbos. <strong>Backprop to me is almost miraculous</strong>. <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000</a>)</span></p>
</blockquote>
<p>When he heard about the “miraculous” backpropagation in the 1980s, he immediately started writing papers in neural networks again.</p>
<p>If this was an individual case, then I might have dismissed it as Widrow being misled by his background. Before entering a PhD program Widrow took an internship at the military <span class="citation" data-cites="widrowOralHistoryBernard1997">(<a href="#ref-widrowOralHistoryBernard1997" role="doc-biblioref">Widrow 1997</a>)</span>:</p>
<blockquote class="blockquote">
<p>… the sky was divided up into a grid, with one mile by one mile regions, and all the computer sensed was that there was a target within that one mile by one mile square. … The radar pulses were transmitted periodically, and then periodically you got samples of the flight path. Say you take a piece of graph paper and draw and nice, smooth flight path for an airplane, and then at uniform points in time put a mark on the actual path based on the radar sample. If you don’t show anybody that nice, smooth line but only the sequence of grid squares that the airplane was in, one could still reconstruct the smooth track.</p>
</blockquote>
<p>The problem of reconstructing a smooth flight path from discretized radar signals got him interested in quantization, and he did his PhD thesis (1954) in the statistical theory of quantization noise. In the interview 43 years later, he called it “the best piece of work I ever did in my whole life”. He is still the world expert on quantization noise, and wrote an entire textbook on quantization noise <span class="citation" data-cites="widrowQuantizationNoiseRoundoff2008">(<a href="#ref-widrowQuantizationNoiseRoundoff2008" role="doc-biblioref">Widrow and Kollár 2008</a>)</span>.</p>
<p>So regarding this background, one might be tempted to say that Widrow was misled by his infatuation with quantization, and attempted to use quantized neural networks. However, this does not explain why all the <em>other</em> pioneers went down the same wrong path.</p>
</section>
<section id="seppo-linnainmaa" class="level2">
<h2 class="anchored" data-anchor-id="seppo-linnainmaa">Seppo Linnainmaa</h2>
<p>It’s said that Seppo Linnainmaa’s masters thesis in 1970 contains the backpropagation algorithm, but it is in Finnish, and not available online either. A bibliography search shows that his thesis were basically never cited before the 2010s. It is one of those papers that people cite for ritual completion only. I think we can safely say that his thesis has priority but no paternity. <span class="citation" data-cites="griewankWhoInventedReverse2012">(<a href="#ref-griewankWhoInventedReverse2012" role="doc-biblioref">Griewank 2012</a>)</span> describes in detail what is in the thesis, according to which he developed it to analyze the cumulative effect of round-off error in long chains of computer computations.</p>
<p>I checked all his English papers during the 1970s, and it seems only <span class="citation" data-cites="linnainmaaTaylorExpansionAccumulated1976">(<a href="#ref-linnainmaaTaylorExpansionAccumulated1976" role="doc-biblioref">Linnainmaa 1976</a>)</span> has been cited non-negligibly before 2000, and may have had some impact on automatic differentiation, but I cannot tell by how much. It contains two algorithms, one for computing the first derivative, one for computing the first two. Both were special cases of the general backpropagation algorithm for computing arbitrary orders of derivatives.</p>
</section>
<section id="rumelhart-and-sejnowski" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="rumelhart-and-sejnowski">Rumelhart and Sejnowski</h2>
<p>Rumelhart rediscovered backpropagation around 1982 and immediately set about publishing and telling others about it. Some still resisted, such as Geoffrey Hinton, but others immediately grasped it and set about using it, such as Terrence Sejnowski. In the interview, he was rather unbothered by the priority dispute <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000, chap. 12</a>)</span>:</p>
<blockquote class="blockquote">
<p>I had no idea that Paul Werbos had done work on it. … There are other examples of work in the control literature in the ’60s [the <a href="https://en.wikipedia.org/wiki/Adjoint_state_method">adjoint method</a>]. … it’s just no one had really done anything with them. My view is that we not only discovered them, but we realized what we could do with them and did a lot of different things. I consider them independent discoveries, at least three and maybe more. [Shun’ichi] Amari, I think, suggests that he had another discovery, and you know, I believe that. You can look at his paper. He had, but he didn’t do anything with it. I think that was in the late ’60s. I don’t feel any problem. You know, maybe we should have done better scholarship and searched out all of the precursors to it, but we didn’t know there were any.</p>
</blockquote>
<p>In 1983, Rumelhart showed Sejnowski backpropagation, who immediately tried it, and found that it was much faster than Boltzmann machine. What a refreshing change from all those others who stubbornly refused to try it.</p>
<blockquote class="blockquote">
<p>… I was still working on the Boltzmann machine and was beginning to do simulations using backprop. I discovered very rapidly that backprop was about an order of magnitude faster than anything you could do with the Boltzmann machine. And if you let it run longer, it was more accurate, so you could get better solutions. <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000, chap. 14</a>)</span></p>
</blockquote>
<div class="page-columns page-full"><p>This was vitally important later, when Sejnowski used backpropagation to train <a href="https://en.wikipedia.org/wiki/NETtalk_(artificial_neural_network)">NETtalk</a>, a <em>huge</em> network with 18,629 parameters.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> The model was a popular hit and appeared on prime-time television.<span class="citation" data-cites="sejnowskiDeepLearningRevolution2018">(<a href="#ref-sejnowskiDeepLearningRevolution2018" role="doc-biblioref">Sejnowski 2018, 112–15</a>)</span></p><div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;People told him that it had too many parameters and would hopelessly overfit, but the network just needed enough data and it generalized uncannily well. It really does seem like scaling skepticism is a researcher universal, if not a human universal.</p>
<blockquote class="blockquote">
<p>There were 18,629 weights in the network, a large number by the standards of 1986, and impossibly large by the standards of mathematical statistics of the time. With that many parameters, we were told that we would overfit the training set, and the network would not be able to generalize. <span class="citation" data-cites="sejnowskiDeepLearningRevolution2018">(<a href="#ref-sejnowskiDeepLearningRevolution2018" role="doc-biblioref">Sejnowski 2018, 113</a>)</span></p>
</blockquote>
</li></div></div>
</section>
<section id="geoffrey-hinton" class="level2">
<h2 class="anchored" data-anchor-id="geoffrey-hinton">Geoffrey Hinton</h2>
<p>The interview with Geoffrey Hinton is hilarious, mostly about how he <em>spent an entire year refusing to use backpropagation</em>. This section is mostly made of quotations from <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000, chap. 16</a>)</span>.</p>
<p>After learning about Hopfield networks and simulated annealing both in 1982, he tried combining them, and discovered the Boltzmann learning algorithm in 1983, which he published in 1985 <span class="citation" data-cites="ackleyLearningAlgorithmBoltzmann1985">(<a href="#ref-ackleyLearningAlgorithmBoltzmann1985" role="doc-biblioref">Ackley, Hinton, and Sejnowski 1985</a>)</span>.</p>
<blockquote class="blockquote">
<p>I remember I had to give a talk, a sort of research seminar, at Carnegie Mellon in either February or March of ’83 … I wanted to talk about this simulated annealing in Hopfield nets, but I figured I had to have a learning algorithm. I was terrified that I didn’t have a learning algorithm.</p>
<p>Then we got very excited because now there was this very simple local-learning rule. On paper it looked just great. I mean, you could take this great big network, and you could train up all the weights to do just the right thing, just with a simple local learning rule. It felt like we’d solved the problem. That must be how the brain works.</p>
<p>I guess if it hadn’t been for computer simulations, I’d still believe that, but the problem was the noise. It was just a very, very slow learning rule. It got swamped by the noise because in the learning rule, you take the difference between two noisy variables, two sampled correlations, both of which have sampling noise. The noise in the difference is terrible.</p>
<p>I still think that’s the nicest piece of theory I’ll ever do. It worked out like a question in an exam where you put it all together and a beautiful answer pops out.</p>
</blockquote>
<p>And now, the hilarity we have been waiting for. When Rumelhart told him in 1982 about backpropagation,</p>
<blockquote class="blockquote">
<p>I first of all explained to him why it wouldn’t work, based on an argument in Rosenblatt’s book, which showed that essentially it was an algorithm that couldn’t break symmetry. … the standard way of doing learning is to have random initial weights, or you could just put a little bit of noise in the learning rule …</p>
<p>The next argument I gave him was that it would get stuck in local minima. There was no guarantee you’d find a global optimum. Since you’re bound to get stuck in local minima, it wasn’t really worth investigating.</p>
<p>Then I tried to use it to get a very obscure effect. I couldn’t get this very obscure effect with it, so I lost interest in backpropagation. I managed to get this very obscure effect later with a Boltzmann machine. I’d realized that if you’ve got a net to learn something, and then you add noise to the weights by making it learn something else, it should be much faster at relearning what it had previously learned. You should get very fast relearning of stuff that it previously knew, as compared to the initial learning. We programmed a backpropagation net, and we tried to get this fast relearning. <strong>It didn’t give fast relearning, so I made one of these crazy inferences that people make – which was, that backpropagation is not very interesting</strong>.</p>
</blockquote>
<p>After one year of failing to get Boltzmann machines to train, he discovered that Boltzmann machines <em>also</em> got stuck in local minima. Desperate times call for desperate measures, and he finally resorted to gradient descent.</p>
<blockquote class="blockquote">
<p>After initially getting them to work, I dedicated over a year to refining their performance. I experimented with various techniques, including weight decay, which helped in preventing large energy barriers. We attempted several other approaches, but none of them yielded satisfactory results. I distinctly recall a point where we observed that training a Boltzmann machine could model certain functions effectively, but prolonged training caused its performance to deteriorate, a phenomenon we referred to as “going sour.” We couldn’t initially comprehend why this was happening. I generated extensive reports, filling stacks of paper about three inches thick, as I couldn’t believe that these networks would degrade as you acquired more knowledge.</p>
<p>It took me several weeks to realize the root of the problem. The learning algorithm operated under the assumption that it could reach thermal equilibrium. However, as the weights grew larger, the annealing process failed to achieve thermal equilibrium, trapping the system in local minima. Consequently, the learning algorithm started behaving incorrectly. This realization led us to introduce weight decay as a solution to prevent this issue.</p>
<p><strong>After investing over a year</strong> in attempting to optimize these methods, I ultimately concluded that they were unlikely to deliver the desired results. In a moment of desperation, I considered revisiting [backpropagation].</p>
</blockquote>
<p>Then he asked the research group for volunteers. None of the ten students took up the offer (and thus giving up a place as the coauthor of the backpropagation paper??):</p>
<blockquote class="blockquote">
<p>They’d all been thoroughly indoctrinated by then into Boltzmann machines. … They all said, “You know, why would you want to program that?” We had all the arguments: It’s assuming that neurons can send real numbers to each other; of course, they can only send bits to each other; you have to have stochastic binary neurons; these real-valued neurons are totally unrealistic. It’s ridiculous.” So they just refused to work on it, not even to write a program, so I had to do it myself.</p>
<p>I went off and I spent a weekend. I wrote a LISP program to do it.</p>
</blockquote>
<p>Hinton almost had one last chance at giving up on backpropagation.</p>
<blockquote class="blockquote">
<p>I almost blew it because the first thing I tried it on was an 8-3-8 encoder; that is, you have eight input units, three hidden units, and eight output units. You want to turn on one of the input units and get the same output unit to be on.</p>
<p>In a Boltzmann machine, since the units are binary, the states of the three hidden units have to be the eight different binary codes, so you know what the hidden units will look like when they’ve solved the problem. So I ran backpropagation on this problem and looked at the weights of the hidden units. They didn’t have the right structure of weights. … I thought, “Oh well, it turns out backpropagation’s not that good after all.” Then I looked at the error, and the error was zero. I was amazed.</p>
</blockquote>
<p>And so one year of excuses later, Hinton finally surrendered to backpropagation. They published the algorithm along with several examples, which became widely cited.</p>
<blockquote class="blockquote">
<p>That was at the stage when we were just completing the PDP books, so we’d already agreed on what was going to be in the books. The final chapters were being edited. We decided we’d just slip in an extra chapter on backpropagation, so it was a late addition to the book. But I was always a bit disappointed. I mean, intellectually, backpropagation wasn’t nearly as satisfying as Boltzmann machines. … it didn’t have the nice probabilistic interpretation.</p>
</blockquote>
</section>
<section id="werbos" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="werbos">Werbos</h2>
<p>After reviewing the story of multiple people, my personal opinion is that Paul Werbos most deserves the title of “originator of the backpropagation algorithm”, as he both has the priority and paternity of the algorithm. In 1993, Paul Werbos gave an interview, where he described the backstory of his invention of backpropagation <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000, chap. 15</a>)</span>. I will let him speak, only interjecting with brief comments.</p>
<p>Before entering Harvard, he tried implementing Hebbian learning, but noticed it wouldn’t work.</p>
<blockquote class="blockquote">
<p>It was obvious to me from a statistical point of view that Hebbian learning was going to be measuring correlation coefficients, and for multivariate problems it would not work. I never gave the talk. I said, “Now I’m going to figure out something with the same flavor that does work.”</p>
<p>[Understanding human learning] will help us understand the human mind; it will help human beings understand themselves. Therefore, they will make better political decisions, and better political decisions are vital for the future of humanity.</p>
<p>Minsky was one of my major influences. Well, Minsky and Hebb and Asimov and Freud.</p>
</blockquote>
<p>Sometime before 1968, he was inspired to do backpropagation from reading Freud.</p>
<blockquote class="blockquote">
<p>I talk in there about the concept of translating Freud into mathematics. This is what took me to backpropagation, so the basic ideas that took me to backpropagation were in this journal article in ’68 <span class="citation" data-cites="werbosElementsIntelligence1968">(<a href="#ref-werbosElementsIntelligence1968" role="doc-biblioref">P. Werbos 1968</a>)</span>. I talked a lot about what was wrong with the existing [two state] McCulloch-Pitts neuron model, and how it was only “1” and “0.” I wanted to make that neuron probabilistic. I was going to apply Freudian notions to an upper-level, associative drive reinforcement system. When</p>
</blockquote>
<p>For his masters (1969), he majored in mathematical physics, and minored in decision and control. During his PhD at Harvard, he had to take some compulsory courses, which he didn’t want to. To salvage the situation, he took computer courses which would provide him with some computer hours, a scarce resource at the time. I would guess the following event happened early 1971.</p>
<blockquote class="blockquote">
<p>Initially, I was going to do something on quantum physics. I learned something about partial differential equations, but not enough. I couldn’t produce a really useful product at the end of <span class="math inline">\(x\)</span> number of months.</p>
<p>So I went back to the committee, and I said, “Gee, I can’t do that, <strong>but I have this little method for adapting multilayer perceptrons. It’s really pretty trivial.</strong> It’s just a by-product of this model of intelligence I developed. And I’d like to do it for my paper for this computer course.”</p>
<p>[Larry] Ho’s position was, “I understand you had this idea, and we were kind of open-minded. But look, at this point, you’ve worked in this course for three months, admittedly on something else. I’m sorry, you’re just going to have to take an incomplete in the course.”</p>
<p>And I said, “You mean I can’t do it?”</p>
<p>“No, no, you’ll have to take an incomplete because, basically, the first thing didn’t work. We’re very skeptical this new thing is going to work.”</p>
<p>“But look, the mathematics is straightforward.”</p>
<p>“Yeah, yeah, but you know, <strong>we’re not convinced it’s so straightforward</strong>. You’ve got to prove some theorems first.”</p>
<p>So they wouldn’t let me do it. One of the reasons that is amusing to me is that there are now some people who are saying backprop was invented by Bryson and Ho. They don’t realize it was the same Larry Ho, who was on my committee and who said this wasn’t going to work.</p>
</blockquote>
<p>I am not sure if this is sarcastic or not. It reminds me of the “summer vision project” <span class="citation" data-cites="papertSummerVisionProject1966">(<a href="#ref-papertSummerVisionProject1966" role="doc-biblioref">Papert 1966</a>)</span> that expected some undergraduate students to construct “a significant part of a visual system” in a single summer.</p>
<blockquote class="blockquote">
<p>By the time my orals came around, it was clear to me that the nature of reality is a hard problem, that I’d better work on that one later and finish my Ph.D.&nbsp;thesis on something small – something I can finish by the end of a few years, like a complete mathematical model of human intelligence.</p>
</blockquote>
<p>The oral was amusing, and touched on the still-hot issue of <a href="https://en.wikipedia.org/wiki/Recent_human_evolution">recent human evolution</a>.</p>
<blockquote class="blockquote">
<p>… I made a statement that there might be parameters affecting utility functions in the brain, parameters that vary from person to person. You could actually get a significant amount of adaptation in ten generations’ time. I was speculating that maybe the rise and fall of human civilizations, as per Toynbee and Spengler, might correlate with these kinds of things. The political scientist on the committee, <a href="https://en.wikipedia.org/wiki/Karl_Deutsch">Karl Deutsch</a>, raised his hand. … His book, <em>The Nerves of Government</em>, which compares governments to neural networks, is one of the classic, accepted, authoritative books in political science.</p>
<p>He raised his hand and he said, “Wait a minute, you can’t get significant genetic change in ten generations. That cannot be a factor in the rise and fall of civilizations. That’s crazy.”</p>
<p>Next to him was a mathematical biologist by the name of <a href="https://en.wikipedia.org/wiki/William_H._Bossert">Bossert</a>, who was one of the world’s authorities on population biology. He raised his hand and said, “What do you mean? In our experiments, we get it in seven generations. This guy is understating it. Let me show you the experiments.”</p>
<p>And Deutsch said, “What do you mean, it’s common knowledge? All of our political theories are based on the assumption this cannot happen.” And Bossert said, “Well, it happens. Here’s the data.”</p>
<p>… I passed the orals having said about two sentences and not having discussed models of intelligence.</p>
</blockquote>
<p>It turns out Werbos interpreted backpropagation as the mathematical interpretation of Freudian psychic energy flow. The thesis committee was not amused.</p>
<blockquote class="blockquote">
<p>But the backpropagation was not used to adapt a supervised learning system; it was to translate Freud’s ideas into mathematics, to implement a flow of what Freud called “psychic energy” through the system. I translated that into derivative equations, and I had an adaptive critic backpropagated to a critic, the whole thing, in ’71 or ’72. … The thesis committee said, “We were skeptical before, but this is just unacceptable … You have to find a patron. You must find a patron anyway to get a Ph.D.&nbsp;That’s the way Ph.D.s work.</p>
</blockquote>
<p>The committee gave him three acceptable patrons. He first went to <a href="https://en.wikipedia.org/wiki/Stephen_Grossberg">Stephen Grossberg</a>.</p>
<blockquote class="blockquote">
<p>… he said, ’Well, you’re going to have to sit down. Academia is a tough business, and you have to develop a tough stomach to survive in it. I’m sure you can pull through in the end, but you’re going to have to do some adaptation. So I want you to sit down and hold your stomach, maybe have an antacid. The bottom line is, this stuff you’ve done, it’s already been done before. Or else it’s wrong. I’m not sure which of the two, but I know it’s one of the two.”</p>
</blockquote>
<p>Thanks, Grossberg, for using the law of excluded middle to crush Werbos’ dream.</p>
<p>He then went to <a href="https://en.wikipedia.org/wiki/Marvin_Minsky">Marvin Minsky</a>, who gave us some new clues about why backpropagation took so long to discover: “everybody knows a neuron is a 1-0 spike generator”!</p>
<blockquote class="blockquote">
<p>“I’ve got a way now to adapt multilayer perceptrons, and the key is that they’re not Heaviside functions; they are differentiable. And I know that action potentials, nerve spikes, are 1 or 0, as in McCulloch-Pitts neurons, but here in this book that I had for my first course in neurophysiology are some actual tracings. If you look at these tracings in Rosenblith, they show volleys of spikes, and volleys are the unit of analysis. This is an argument for treating this activity as differentiable, at least as piecewise linear. If you look at that, I can show you how to differentiate through it.”</p>
<p>Minsky basically said, “Look, <strong>everybody knows a neuron is a 1-0 spike generator. That is the official model from the biologists</strong>. Now, you and I are not biologists. If you and I come out and say the biologists are wrong, and this thing is not producing 1s and 0s, nobody is going to believe us. It’s totally crazy. I can’t get involved in anything like this.”</p>
<p>He was probably right, I guess, but he was clearly very worried about his reputation and his credibility in his community.</p>
</blockquote>
<p>Out of curiosity, I looked up the “Rosenblith” book <span class="citation" data-cites="rosenblithSensoryCommunicationContributions2012">(<a href="#ref-rosenblithSensoryCommunicationContributions2012" role="doc-biblioref">Rosenblith 2012</a>)</span>, and indeed there were a few tracings that show continuously varying neural activation.</p>
<div id="fig-rosenblith" class="quarto-layout-panel page-columns page-full">
<p></p><figcaption>Figure&nbsp;1: <img src="figure/no_bg_goldsmith_4.png" class="img-fluid" alt="Page 366 of the book. The physiological basis of wavelength discrimination in the eye of the honeybee, Figure 4."></figcaption><p></p>
<figure class="figure page-columns page-full">
<div class="quarto-layout-row quarto-layout-valign-top page-columns page-full">
<div class="quarto-layout-cell page-columns page-full" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/no_bg_beidler_2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Page 146 of the book. <em>Mechanisms of gustatory and olfactory receptor stimulation</em>, Figure 2.</figcaption>
</figure>
</div>
</div>
</div>
</figure>
</div>
<p>Then Minsky dunked on reinforcement learning as well, because he had an unpublished “jitters machine” that failed to optimize its reward. Presumably the name “jitters machine” refers to how it would jitter in place, not able to move towards the goal.</p>
<blockquote class="blockquote">
<p>Minsky also said, “You know, I used to believe in all this kind of stuff with reinforcement learning because I knew reinforcement learning would work. I knew how to implement it. I had a nice guy named Oliver Selfridge who came in and acted as my patron and gave me permission to do it. We coauthored a paper, but it was really my idea, and he was just acting as patron on the Jitters machine. I’ll hand you the tech report, which we have deliberately never published.”</p>
<p>It was his bad experience with the Jitters machine that turned him off on reinforcement learning and all the neural net ideas. It just didn’t work. I later looked at that paper … He had a system that was highly multivariate with a single reinforcement signal. The system can’t learn efficiently with that. At any rate, he was totally turned off.</p>
</blockquote>
<p>The brief description of the unpublished jitters machine was not making sense to me, so I looked around the literature. It was <em>so</em> unpublished that I found only two more references in the entire literature <span class="citation" data-cites="werbosApplicationsAdvancesNonlinear1982 werbosBuildingUnderstandingAdaptive1987">(<a href="#ref-werbosApplicationsAdvancesNonlinear1982" role="doc-biblioref">P. J. Werbos 1982</a>, <a href="#ref-werbosBuildingUnderstandingAdaptive1987" role="doc-biblioref">1987</a>)</span>, both by Werbos. <span class="citation" data-cites="werbosBuildingUnderstandingAdaptive1987">(<a href="#ref-werbosBuildingUnderstandingAdaptive1987" role="doc-biblioref">P. J. Werbos 1987</a>)</span> described it as:</p>
<blockquote class="blockquote">
<p>There are also some technical reasons to expect better results with the new approach. Almost all of the old perceptron work was based on the McCulloch–Pitts model of the neuron, which was both discontinuous and non-differentiable. It was felt, in the 1950’s, that the output of a brain cell had to be 1 or 0, because the output consisted of sharp discrete “spikes.” More recent work in neurology has shown that higher brain cells output “bursts” or “volleys” of spikes, and that the strength of a volley may vary continuously in intensity. This suggests the CLU model to be discussed in the Appendix. In any event, to exploit basic numerical methods, one must use differentiable processing units. Minsky once experimented with a “jitters” machine, which estimated one derivative per time cycle by making brute-force changes in selected variables; however, the methods to be discussed in the Appendix yield derivatives of all variables and parameters in one major time cycle and thereby multiply efficiency in proportion to the number of parameters <span class="math inline">\((N)\)</span>, which may be huge.</p>
</blockquote>
<p>This makes things perfectly clear. Minsky’s jitters machine was an RL agent with multiple continuously adjustable parameters running a very primitive form of policy gradient. During every episode, it would would estimate <span class="math inline">\(\partial_{\theta_i} R(\theta)\)</span> using finite difference, but since it used finite difference, they could only estimate the partial derivative for <em>only one</em> of the parameters <span class="math inline">\(\theta_i\)</span>! No wonder it never managed to learn. It is almost comical how much they failed to just use gradient descent. It sometimes feels as if did everything to <em>avoid</em> just taking the gradient. In the case of Minsky, he made it very clear, in the new additions to the 1988 version of <em>Perceptrons</em>, that he did not believe in gradient descent, period. But what explains the gradient-phobia of all the others…?</p>
<p>Anyway, back to the interview. Werbos went to <a href="https://en.wikipedia.org/wiki/Jerome_Lettvin">Jerome Lettvin</a>, the neuroscientist famous for <em>What the Frog’s Eye Tells the Frog’s Brain</em>. Turns out he was a proto-<a href="https://en.wikipedia.org/wiki/Eliminativism">eliminativist</a>. While I’m an eliminativist too, Werbos was a Freudian, which can only collide badly with eliminativism.</p>
<blockquote class="blockquote">
<p>“Oh yeah, well, you’re saying that there’s motive and purpose in the human brain.” He said, “That ‘s not a good way to look at brains. I’ve been telling people, ’You cannot take an anthropomorphic view of the human brain.’ In fact, people have screwed up the frog because they’re taking bachtriomorphic views of the frog. If you really want to understand the frog, you must learn to be objective and scientific.”</p>
</blockquote>
<p>Without patrons, he faced the committee again.</p>
<blockquote class="blockquote">
<p>I tried to simplify it. I said, “Look, I’ll pull out the backprop part and the multilayer perceptron part.” I wrote a paper that was just that - that was, I felt, childishly obvious. I didn’t even use a sigmoid [non-linearity]. I used piecewise linear. I could really rationalize that to the point where it looked obvious. I handed that to my thesis committee. I had really worked hard to write it up. They said, “Look, this will work, <strong>but this is too trivial</strong> and simple to be worthy of a Harvard Ph.D.&nbsp;thesis.”</p>
</blockquote>
<p>Oh, now it’s too trivial?? I swear the backstory of backpropagation gets more and more ridiculous by the day.</p>
<blockquote class="blockquote">
<p>… they had discontinued support because they were not interested, so I had no money. … Not even money to buy food. A generous guy, who was sort of a Seventh Day Adventist and a Harvard Ph.D.&nbsp;candidate in ethnobotany, had a slum apartment that rented for about $40 a month in Roxbury in the ghetto. He let me share a room in his suite, in his suite with the plaster falling off, and didn’t ask for rent in advance. I had no money at that time for food. There was a period of three months when I was living there in the slums. To conserve the little bit of money I had, I remember eating soybean soup and chicken neck soup. I remember getting the shakes from inadequate nutrition.</p>
<p>Finally, they said, “Look, you know, we’re not going to allow this.” There was this meeting where we sat around the table. The chairman of the applied math department at that time was a numerical analyst D. G. M. Anderson. He said, “We can’t even allow you to stay as a student unless you do something. You’ve got to come up with a thesis, and it can’t be in this area.”</p>
</blockquote>
<p>Karl Deutsch, who believed in Werbos, sponsored his PhD thesis on a “respectable” problem: fitting an ARMA model to a time-series data of political regimes, and use it to forecast nationalism and political assimilation. <a href="https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method">Box–Jenkins method</a> ran too slowly, so Werbos programmed in the backpropagation which worked, and he finally obtained his PhD in 1974. This is the original reference point for modern neural network backpropagation algorithm.</p>
<blockquote class="blockquote">
<p>Deutsch said, “You ’re saying we need an application to believe this stuff? I have an application that we could believe. I have apolitical forecasting problem. I have this model, this theory of nationalism and social communications? What causes war and peace between nations? I have used up ten graduate students who’ve tried to implement this model on real-world data I’ve collected, and they’ve never been able to make it work. Now, do you think your model of intelligence could solve this problem and help us predict war and peace?”</p>
<p>The first application of backpropagation in the world in a generalized sense was a command that was put into the <a href="https://en.wikipedia.org/wiki/TSP_(econometrics_software)">TSP</a> at MIT , available to the whole MIT community as part of their standard software. It was published as part of MIT ’s report to the DOD [the Department of Defense] and part of the DOD’s report to the world . It was part of the computer manual, so the first publication of backpropagation was in a computer manual from MIT for a working command for people to use in statistical analysis. I was a second author of that manual. It was Brode, Werbos, and Dunn.</p>
<p>… Once I started doing software for them, they discovered I was pretty good at it. Once I had put backpropagation into a TSP command, they offered me a full-time job at the Harvard Cambridge Project, so I did the Ph.D.&nbsp;thesis while having a full-time job. While I was doing these things at MIT, I remember going to one party. … one of the people there said, ’We have heard through the grapevine that somebody has developed this thing called continuous feedback that allows you to calculate all the derivatives in a single pass.”</p>
</blockquote>
<p>But the saga is not over. after the Ph.D., Werbos was hired by the DoD, and promptly got sucked into several more years of misadventure, which meant that he could not even talk about backpropagation in a public journal until 1978 – and the algorithm got removed anyway due to page limits. This annoyed the DoD and he had to move to the Department of Energy.</p>
<blockquote class="blockquote">
<p>I found out that DARPA had spent a lot of effort building a worldwide conflict forecasting model for the Joint Chiefs of Staff that was used in the long-range strategy division of the Joint Chiefs. … I wound up sending a couple of graduate students to create a really good database of Latin America. I said, “You want variance, high variance. Something hard to predict.” I thought conflict in Latin America would be the most beautiful case. I figured there were enough cultural homogeneities that it would be a single stochastic process, but with lots and lots of variance in that process. So we got truly annual data going back, I don’t know, twenty or thirty years for all the countries in Latin America and then reestimated the Joint Chief’s model on it. It had an r2 of about .0016 at forecasting conflict. By jiggling and jiggling we could raise it to about .05. It could predict GNP and economic things decently. Conflict prediction we couldn’t improve, though; it was hopeless.</p>
<p>DARPA wasn’t happy when I published that result. They wanted me to do something real and practical and useful for the United States. This was useful, but it was an exposé. They didn’t like that. Therefore, the report, which included backpropagation in detail and other advanced methods, was not even entered into DOCSUB. Every time I tried to enter it into DOCSUB, somebody jiggled influence to say, “No, no, no, we can’t publish this. This is too hot.”</p>
<p>It was published in <span class="citation" data-cites="werbosEmpiricalTestNew1978">(<a href="#ref-werbosEmpiricalTestNew1978" role="doc-biblioref">P. J. Werbos and Titus 1978</a>)</span> anyway because they couldn’t block the journals, but it didn’t include the appendices. So that paper in 1978 said, “We’ve got this great thing called dynamic feedback, which lets you estimate these things. It calculates derivatives in a single swoop, and you can use it for lots of things, like AI.” … <strong>the appendix on how to do it was not there because of page limits</strong> … At that point, DARPA was no longer happy.</p>
</blockquote>
<p>So he went to the Department of Energy and used backpropagation to make another model, and managed to get silenced again, unable to publish that report until 1988 <span class="citation" data-cites="werbosGeneralizationBackpropagationApplication1988">(<a href="#ref-werbosGeneralizationBackpropagationApplication1988" role="doc-biblioref">P. J. Werbos 1988</a>)</span>.</p>
<blockquote class="blockquote">
<p>They had a million-dollar contract at Oak Ridge National Laboratory to study that model. They wanted me for several things. One, they wanted me to be a translator between engineering and economics. Two, they wanted a critique. They wanted exposés. They wanted me to rip apart the models of the Department of Energy in a very scientific, objective way that didn’t look like I was trying to rip them apart but was anyway. That’s exactly what they wanted to hire me for, and I didn’t really know that was the motive. These particular people didn’t like modeling very much.</p>
<p>So at some point, they wanted sensitivity analysis. And I said, “You know, I know a little bit about calculating derivatives.” … I applied it to a natural gas model at the Department of Energy. In the course of applying it, I did indeed discover dirt. They didn’t want me to publish it because it was too politically sensitive. It was a real-world application, but the problem was that it was too real. At DOE, you know, you don’t have First Amendment rights. That’s one of the terrible things somebody’s got to fix in this country. The reality of the First Amendment has deteriorated. Nobody’s breaking the law, but the spirit of the First Amendment has decayed too far for science. At any rate, they finally gave me permission to publish it around ’86 and ’87. I sent it to the journal Neural Nets - that is, how you do simultaneous recurrent nets and time-lag recurrent nets together. Then the editorial process messed around with it, made the paper perhaps worse, and it finally got published in ’88, which makes me very sad because now I gotta worry about, ’Well, gee, didn’t Pineda do this in ’88?</p>
</blockquote>
<p>As a side note, one might have felt that Werbos’ “turning Freud into mathematics” seem rather strange. This feeling is completely justified. I found a recent paper by him <span class="citation" data-cites="werbosIntelligenceBrainTheory2009">(<a href="#ref-werbosIntelligenceBrainTheory2009" role="doc-biblioref">P. J. Werbos 2009</a>)</span> with this crackpot illustration:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/werbos_2009.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">The text at the end of the arrow says “quantum and collective intelligence (Jung, Dao, Atman…)?”</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-ackleyLearningAlgorithmBoltzmann1985" class="csl-entry" role="listitem">
Ackley, David H., Geoffrey E. Hinton, and Terrence J. Sejnowski. 1985. <span>“A Learning Algorithm for <span>Boltzmann</span> Machines.”</span> <em>Cognitive Science</em> 9 (1): 147–69.
</div>
<div id="ref-bernsteinMarvinMinskyVision1981" class="csl-entry" role="listitem">
Bernstein, Jeremy. 1981. <span>“Marvin <span>Minsky</span>’s <span>Vision</span> of the <span>Future</span>.”</span> <em>The New Yorker</em>, December.
</div>
<div id="ref-childManuscriptsLeibnizHis1917" class="csl-entry" role="listitem">
Child, J. M. 1917. <span>“The <span>Manuscripts</span> of <span>Leibniz</span> on <span>His Discovery</span> of the <span>Differential Calculus</span>. <span>Part II</span> (<span>Continued</span>).”</span> <em>The Monist</em> 27 (3): 411–54. <a href="https://doi.org/10.5840/monist191727324">https://doi.org/10.5840/monist191727324</a>.
</div>
<div id="ref-griewankWhoInventedReverse2012" class="csl-entry" role="listitem">
Griewank, Andreas. 2012. <span>“Who Invented the Reverse Mode of Differentiation.”</span> <em>Documenta Mathematica, Extra Volume ISMP</em> 389400.
</div>
<div id="ref-lecunGeneralizationNetworkDesign1989" class="csl-entry" role="listitem">
LeCun, Yann. 1989. <span>“Generalization and Network Design Strategies.”</span> <em>Connectionism in Perspective</em> 19 (143-155): 18.
</div>
<div id="ref-linnainmaaTaylorExpansionAccumulated1976" class="csl-entry" role="listitem">
Linnainmaa, Seppo. 1976. <span>“Taylor Expansion of the Accumulated Rounding Error.”</span> <em>BIT</em> 16 (2): 146–60. <a href="https://doi.org/10.1007/BF01931367">https://doi.org/10.1007/BF01931367</a>.
</div>
<div id="ref-mccullochLogicalCalculusIdeas1943" class="csl-entry" role="listitem">
McCulloch, Warren S., and Walter Pitts. 1943. <span>“A Logical Calculus of the Ideas Immanent in Nervous Activity.”</span> <em>The Bulletin of Mathematical Biophysics</em> 5 (4): 115–33. <a href="https://doi.org/djsbj6">https://doi.org/djsbj6</a>.
</div>
<div id="ref-minskyComputationFiniteInfinite1967" class="csl-entry" role="listitem">
Minsky, Marvin. 1967. <em>Computation: Finite and Infinite Machines</em>. <span>Englewood Cliffs, NJ</span>: <span>Prentice-Hall</span>.
</div>
<div id="ref-papertSummerVisionProject1966" class="csl-entry" role="listitem">
Papert, Seymour A. 1966. <span>“The Summer Vision Project.”</span>
</div>
<div id="ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" class="csl-entry" role="listitem">
Rosenblatt, Frank. 1962. <em>Principles of Neurodynamics: <span>Perceptrons</span> and the Theory of Brain Mechanisms</em>. Vol. 55. <span>Spartan books Washington, DC</span>.
</div>
<div id="ref-rosenblithSensoryCommunicationContributions2012" class="csl-entry" role="listitem">
Rosenblith, Walter A., ed. 2012. <em>Sensory Communication: Contributions to the <span>Symposium</span> on <span>Principles</span> of <span>Sensory Communication</span>, <span>July</span> 19-<span>August</span> 1, 1959, <span>Endicott House</span>, <span>M</span>.<span>I</span>.<span>T</span></em>. <span>Cambridge, Massachusetts</span>: <span>The M.I.T. Press, Massachusetts Institute of Technology</span>.
</div>
<div id="ref-rosenfeldTalkingNetsOral2000" class="csl-entry" role="listitem">
Rosenfeld, Edward, and James A. Anderson, eds. 2000. <em>Talking <span>Nets</span>: <span>An Oral History</span> of <span>Neural Networks</span></em>. Reprint edition. <span>The MIT Press</span>.
</div>
<div id="ref-rumelhartLearningRepresentationsBackpropagating1986" class="csl-entry" role="listitem">
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. <span>“Learning Representations by Back-Propagating Errors.”</span> <em>Nature</em> 323 (6088): 533–36. <a href="https://doi.org/10.1038/323533a0">https://doi.org/10.1038/323533a0</a>.
</div>
<div id="ref-sejnowskiDeepLearningRevolution2018" class="csl-entry" role="listitem">
Sejnowski, Terrence J. 2018. <em>The <span>Deep Learning Revolution</span></em>. Illustrated edition. <span>Cambridge, Massachusetts London, England</span>: <span>The MIT Press</span>.
</div>
<div id="ref-werbosElementsIntelligence1968" class="csl-entry" role="listitem">
Werbos, P. 1968. <span>“The Elements of Intelligence.”</span> <em>Cybernetica (Namur)</em> 3: 131–78.
</div>
<div id="ref-werbosApplicationsAdvancesNonlinear1982" class="csl-entry" role="listitem">
Werbos, Paul J. 1982. <span>“Applications of Advances in Nonlinear Sensitivity Analysis.”</span> In <em>System <span>Modeling</span> and <span>Optimization</span></em>, edited by R. F. Drenick and F. Kozin, 38:762–70. <span>Berlin/Heidelberg</span>: <span>Springer-Verlag</span>. <a href="https://doi.org/10.1007/BFb0006203">https://doi.org/10.1007/BFb0006203</a>.
</div>
<div id="ref-werbosBuildingUnderstandingAdaptive1987" class="csl-entry" role="listitem">
———. 1987. <span>“Building and Understanding Adaptive Systems: <span>A</span> Statistical/Numerical Approach to Factory Automation and Brain Research.”</span> <em>IEEE Transactions on Systems, Man, and Cybernetics</em> 17 (1): 7–20. <a href="https://doi.org/10.1109/TSMC.1987.289329">https://doi.org/10.1109/TSMC.1987.289329</a>.
</div>
<div id="ref-werbosGeneralizationBackpropagationApplication1988" class="csl-entry" role="listitem">
———. 1988. <span>“Generalization of Backpropagation with Application to a Recurrent Gas Market Model.”</span> <em>Neural Networks</em> 1 (4): 339–56. <a href="https://doi.org/10.1016/0893-6080(88)90007-X">https://doi.org/10.1016/0893-6080(88)90007-X</a>.
</div>
<div id="ref-werbosIntelligenceBrainTheory2009" class="csl-entry" role="listitem">
———. 2009. <span>“Intelligence in the Brain: <span>A</span> Theory of How It Works and How to Build It.”</span> <em>Neural Networks</em>, Goal-<span>Directed Neural Systems</span>, 22 (3): 200–212. <a href="https://doi.org/10.1016/j.neunet.2009.03.012">https://doi.org/10.1016/j.neunet.2009.03.012</a>.
</div>
<div id="ref-werbosEmpiricalTestNew1978" class="csl-entry" role="listitem">
Werbos, Paul J., and Jim Titus. 1978. <span>“An Empirical Test of New Forecasting Methods Derived from a Theory of Intelligence: <span>The</span> Prediction of Conflict in <span>Latin America</span>.”</span> <em>IEEE Transactions on Systems, Man, and Cybernetics</em> 8 (9): 657–66. <a href="https://doi.org/10.1109/TSMC.1978.4310051">https://doi.org/10.1109/TSMC.1978.4310051</a>.
</div>
<div id="ref-widrowOralHistoryBernard1997" class="csl-entry" role="listitem">
Widrow, Bernard. 1997. <span>“Oral <span>History</span>: <span>Bernard Widrow</span>.”</span>
</div>
<div id="ref-widrowCyberneticsGeneralTheory2022" class="csl-entry" role="listitem">
———. 2022. <em>Cybernetics 2.0: <span>A General Theory</span> of <span>Adaptivity</span> and <span>Homeostasis</span> in the <span>Brain</span> and in the <span>Body</span></em>. Vol. 14. <span>Springer Nature</span>.
</div>
<div id="ref-widrowAncientHistory2023" class="csl-entry" role="listitem">
———. 2023. <span>“Ancient <span>History</span>.”</span> In <em>Cybernetics 2.0: <span>A General Theory</span> of <span>Adaptivity</span> and <span>Homeostasis</span> in the <span>Brain</span> and in the <span>Body</span></em>, 277–307. Springer <span>Series</span> on <span>Bio-</span> and <span>Neurosystems</span>. <span>Cham</span>: <span>Springer International Publishing</span>. <a href="https://doi.org/10.1007/978-3-030-98140-2_26">https://doi.org/10.1007/978-3-030-98140-2_26</a>.
</div>
<div id="ref-widrowQuantizationNoiseRoundoff2008" class="csl-entry" role="listitem">
Widrow, Bernard, and István Kollár. 2008. <em>Quantization Noise: Roundoff Error in Digital Computation, Signal Processing, Control and Communications</em>. 1. publ. <span>Cambridge</span>: <span>Cambridge Univ. Press</span>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left"><span class="faux-block">Everything ©<a href="https://www.apache.org/licenses/LICENSE-2.0.html">Apache License 2.0</a></span></div>   
    <div class="nav-footer-center"><span class="faux-block">Yuxi on the Wired</span></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>