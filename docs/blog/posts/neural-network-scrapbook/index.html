<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxi Liu">
<meta name="dcterms.date" content="2023-11-01">
<meta name="description" content="Lightly curated list of stories, anecdotes, and other various bits from neural network research.">

<title>Yuxi on the Wired - A Scrapbook of Neural Network Lores</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta property="og:title" content="Yuxi on the Wired - A Scrapbook of Neural Network Lores">
<meta property="og:description" content="Lightly curated list of stories, anecdotes, and other various bits from neural network research.">
<meta property="og:image" content="https://yuxi-liu-wired.github.io/blog/posts/neural-network-scrapbook/figure/kanal_1964_fig_tank_nontank_mosaic.png">
<meta property="og:site-name" content="Yuxi on the Wired">
<meta property="og:image:height" content="679">
<meta property="og:image:width" content="1106">
<meta name="twitter:title" content="Yuxi on the Wired - A Scrapbook of Neural Network Lores">
<meta name="twitter:description" content="Lightly curated list of stories, anecdotes, and other various bits from neural network research.">
<meta name="twitter:image" content="https://yuxi-liu-wired.github.io/blog/posts/neural-network-scrapbook/figure/kanal_1964_fig_tank_nontank_mosaic.png">
<meta name="twitter:image-height" content="679">
<meta name="twitter:image-width" content="1106">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html" rel="" target="">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://bair.berkeley.edu/" rel="" target=""><i class="bi bi-folder-symlink" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi.liu.1995@gmail.com" rel="" target=""><i class="bi bi-envelope" role="img" aria-label="email">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">A Scrapbook of Neural Network Lores</h1>
                  <div>
        <div class="description">
          Lightly curated list of stories, anecdotes, and other various bits from neural network research.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">AI</div>
                <div class="quarto-category">scaling</div>
                <div class="quarto-category">NN</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxi Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 1, 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">January 20, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-scaling-hypothesis" id="toc-the-scaling-hypothesis" class="nav-link active" data-scroll-target="#the-scaling-hypothesis">The scaling hypothesis</a></li>
  <li><a href="#neural-networks-want-to-work" id="toc-neural-networks-want-to-work" class="nav-link" data-scroll-target="#neural-networks-want-to-work">Neural networks want to work</a>
  <ul class="collapse">
  <li><a href="#the-neural-net-tank-urban-legend" id="toc-the-neural-net-tank-urban-legend" class="nav-link" data-scroll-target="#the-neural-net-tank-urban-legend">The Neural Net Tank Urban Legend</a></li>
  </ul></li>
  <li><a href="#the-second-neural-network-winter" id="toc-the-second-neural-network-winter" class="nav-link" data-scroll-target="#the-second-neural-network-winter">The second neural network winter</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<section id="the-scaling-hypothesis" class="level2">
<h2 class="anchored" data-anchor-id="the-scaling-hypothesis">The scaling hypothesis</h2>
<p>Marvin Minsky, on how he gave up on neural networks after the 1950s because he could not afford a few million neurons.</p>
<blockquote class="blockquote">
<p>I had the naive idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks. Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. <span class="citation" data-cites="bernsteinMarvinMinskyVision1981">(<a href="#ref-bernsteinMarvinMinskyVision1981" role="doc-biblioref">Bernstein 1981</a>)</span></p>
</blockquote>
<p>Peter Norvig, on how he quickly gave up on neural networks in the 1980s due to lack of compute.</p>
<blockquote class="blockquote">
<p>And then it finally worked. And I think the biggest difference was the computing power. Definitely there were advances in data. So we could do image net because Fei-Fei Li and others gathered this large database, and that was really important. There are certainly differences in the algorithm, right? We’ve got a slightly different squashing function. Instead of shaped like this, it’s shaped like this. I mean, I don’t know how big a deal that was, but we learned how to do stochastic gradient dissent a little bit better. We figured that dropout gave you a little bit better robustness.</p>
<p>So there were small things, but I think probably the biggest was the computing power. And I mean, I certainly remember Geoffrey Hinton came to Berkeley when I was a grad student in 1981, I think, when he talked about these neural nets. And we fellow grad students thought that was so cool. So we said, “Let’s go back into the lab and implement it.</p>
<p>And of course, there was absolutely nothing you could download, so we had to build it all from scratch. And we got it to do exclusive or, and then we got it to do something a little bit more complicated. And it was exciting. And then we gave it the first real problem, and it ran overnight, and it didn’t converge, and we let it run one more day, and it still didn’t converge. And then we gave up, and we went back to our sort of knowledge-based systems approach. But if we had the computing power of today, it probably would have converged after five seconds. <span class="citation" data-cites="norvigSingularityEyeBeholder2021">(<a href="#ref-norvigSingularityEyeBeholder2021" role="doc-biblioref">Norvig 2021</a>)</span></p>
</blockquote>
</section>
<section id="neural-networks-want-to-work" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="neural-networks-want-to-work">Neural networks want to work</h2>
<p>Marvin Minsky’s SNARC (1951). Designed to simulate one mouse escaping a maze, it ended up simulating multiple mice due to design bugs – which were never debugged. Though the machine had only 40 neurons, and its parts failed all the time, the whole network continued to work.</p>
<blockquote class="blockquote">
<p>It turned out that because of an electronic accident in our design we could put two or three rats in the same maze and follow them all. The rats actually interacted with one another. If one of them found a good path, the others would tend to follow it. We sort of quit science for a while to watch the machine. We were amazed that it could have several activities going on at once in its little nervous system. Because of the random wiring, it had a sort of fail-safe characteristic. If one of the neurons wasn’t working, it wouldn’t make much of a difference—and, with nearly three hundred tubes and the thousands of connections we had soldered, there would usually be something wrong somewhere. In those days, even a radio set with twenty tubes tended to fail a lot. I don’t think we ever debugged our machine completely, but that didn’t matter. By having this crazy random design, it was almost sure to work, no matter how you built it. <span class="citation" data-cites="bernsteinMarvinMinskyVision1981">(<a href="#ref-bernsteinMarvinMinskyVision1981" role="doc-biblioref">Bernstein 1981</a>)</span></p>
</blockquote>
<p>Bernard Widrow once built a MADALINE I (circa 1962) in a rush to present at a technical meeting. Despite that only 1/4 of its circuits were defective, it still worked at reduced capacity.</p>
<blockquote class="blockquote">
<p>We discovered the inherent ability of adaptive computers to ignore their own defects while we were rushing through construction of a system called Madaline I for presentation at a technical meeting. The machine was finished late the night before the meeting and the next day we showed some very complex pattern discriminations. Later we discovered that about a fourth of the circuitry was defective. Things were connected backward, there were short circuits, and poor solder joints. We were pretty unhappy until it dawned on us that this system has the ability to adapt around its own internal flaws. The capacity of the system is diminished but it does not fail. <span class="citation" data-cites="widrowAdalineSmarterSweet1963">(<a href="#ref-widrowAdalineSmarterSweet1963" role="doc-biblioref">Widrow 1963</a>)</span></p>
</blockquote>
<p>Andrej Karpathy, on how neural network program bugs are very hard to find, because bugged neural networks do not fail, merely degrade.</p>
<blockquote class="blockquote">
<p>… perhaps you forgot to flip your labels when you left-right flipped the image during data augmentation. Your net can still (shockingly) work pretty well because your network can internally learn to detect flipped images and then it left-right flips its predictions. Or maybe your autoregressive model accidentally takes the thing it’s trying to predict as an input due to an off-by-one bug. Or you tried to clip your gradients but instead clipped the loss, causing the outlier examples to be ignored during training. Or you initialized your weights from a pretrained checkpoint but didn’t use the original mean. Or you just screwed up the settings for regularization strengths, learning rate, its decay rate, model size, etc. Therefore, your misconfigured neural net will throw exceptions only if you’re lucky; Most of the time it will train but silently work a bit worse. <span class="citation" data-cites="karpathyRecipeTrainingNeural2019">(<a href="#ref-karpathyRecipeTrainingNeural2019" role="doc-biblioref">Karpathy 2019</a>)</span></p>
</blockquote>
<p>Researchers at OpenAI (2018) reported that fixing RL bugs is as important as better algorithms.</p>
<blockquote class="blockquote">
<p>Big-picture considerations like susceptibility to the noisy-TV problem are important for the choice of a good exploration algorithm. However, we found that getting seemingly-small details right in our simple algorithm made the difference between an agent that never leaves the first room and an agent that can pass the first level. To add stability to the training, we avoided saturation of the features and brought the intrinsic rewards to a predictable range. We also noticed <strong>significant improvements in performance of RND every time we discovered and fixed a bug</strong> (our favorite one involved accidentally zeroing an array which resulted in extrinsic returns being treated as non-episodic; we realized this was the case only after being puzzled by the extrinsic value function looking suspiciously periodic). Getting such details right was a significant part of achieving high performance even with algorithms conceptually similar to prior work. This is one reason to prefer simpler algorithms where possible. <span class="citation" data-cites="burdaReinforcementLearningPredictionbased2018">(<a href="#ref-burdaReinforcementLearningPredictionbased2018" role="doc-biblioref">Burda and Edwards 2018</a>)</span></p>
</blockquote>
<p>Around 2019, Gwern, Shawn Presser, and others, trained <span class="math inline">\(512\times 512\)</span> image generation models using the BigGAN architecture. However, they used <a href="https://github.com/google/compare_gan"><code>compare_gan</code></a>, which had a multiply-by-zero bug. Somehow it still worked, but not well enough compared to the original <a href="https://github.com/ajbrock/BigGAN-PyTorch"><code>BigGAN</code></a>.</p>
<blockquote class="blockquote">
<p>Our primary goal was to train &amp; release 512px BigGAN models on not just ImageNet but all the other datasets we had like anime datasets. The compare_gan BigGAN implementation turned out to have a subtle +1 gamma bug which stopped us from reaching results comparable to the model; while we beat our heads against the wall trying to figure out why it was working but not well enough (figuring it out far too late, after we had disbanded) … “Neural nets want to work” – even if they start out being effectively multiplied by zero. <span class="citation" data-cites="branwenGANsDidnFail2022">(<a href="#ref-branwenGANsDidnFail2022" role="doc-biblioref">Branwen 2022</a>)</span></p>
</blockquote>
<p>Personal story at the <a href="https://rail.eecs.berkeley.edu/deeprlcourse-fa22/">Berkeley CS 285, <em>Deep Reinforcement Learning</em>, 2022 Fall</a>.</p>
<p>For <a href="https://web.archive.org/web/20230305152623/https://rail.eecs.berkeley.edu/deeprlcourse/static/homeworks/hw3.pdf">Homework 3</a>, we were asked to implement the soft actor-critic algorithm. We would implement the agent, run the agent on the <a href="https://gymnasium.farama.org/environments/mujoco/half_cheetah/"><code>Half Cheetah</code></a> environment, and submit the trajectories to <a href="https://en.wikipedia.org/wiki/Gradescope">Gradescope</a>, where an autograder would check the trajectories and see if the agent achieved a final score above 300. For the <code>Half Cheetah</code>, score means the distance it travels per episode, averaged over several episodes.</p>
<p>I noticed that the algorithm I implemented did learn, but the learning curve looked like a rollercoaster, jumping up and down around the range of 250 – 300. After many fruitless and paranoid programming sessions I managed to pass the autograder by trying enough random seeds and just submitting the best seeds. The professor, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>, offered little help, admitting that RL agents are extremely hard to debug.</p>
<p>One day after the assignment deadline, the professor announced that there was <a href="https://web.archive.org/web/20240118234154/https://github.com/berkeleydeeprlcourse/homework_fall2022/commit/d2227e86fb1faf02c115c30c3762f1cfc049c84e">a critical one-line bug in the starter code</a>: The correct algorithm should train the model with past game frames in a random order, but the given code always give them in the FIFO order. With the fix, the learning curve would smoothly sigmoid to 350.</p>
<section id="the-neural-net-tank-urban-legend" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-neural-net-tank-urban-legend">The Neural Net Tank Urban Legend</h3>
<p>A large list of examples in <a href="https://gwern.net/tank">The Neural Net Tank Urban Legend · Gwern.net</a>. I have a few more.</p>
<p>According to Sejnowski, Takeo Kanade did work on detecting tanks in images. This is unconfirmed. I have looked for “Artificial Intelligence Vision: Progress and Non-Progress”, but it is not available online. I looked for your doctoral dissertation of 1974, but it contains only facial recognition. I also cannot find anything about detecting tanks in his publication list.</p>
<blockquote class="blockquote">
<p>In his talk “Artificial Intelligence Vision: Progress and Non-Progress,” Takeo Kanade (from Carnegie Mellon) noted that computer memories back in the 1960s were tiny by today’s standards and could hold only one image at a time. For his doctoral dissertation in 1974, Takeo had shown that, though his program could find a tank in one image, it was too difficult for it to do so in other images where the tank was in a different position and the lighting was different. But, by the time his early students graduated, the programs they designed could recognize tanks under more general conditions because computers were more powerful. Today his students’ programs can recognize tanks in any image. The difference is that today we have access to millions of images that sample a wide range of poses and lighting conditions, and computers are millions of times more powerful. <span class="citation" data-cites="sejnowskiDeepLearningRevolution2018">(<a href="#ref-sejnowskiDeepLearningRevolution2018" role="doc-biblioref">Sejnowski 2018, 256</a>)</span></p>
</blockquote>
<p>There was not a lot of actual research on tank recognition. <span class="citation" data-cites="kanalRecognitionSystemDesign1964">(<a href="#ref-kanalRecognitionSystemDesign1964" role="doc-biblioref">Kanal and Randall 1964</a>)</span> contains some good pictures. The network was a two-layered perceptron network, of type <span class="math inline">\(\mathbb{R}^{N \times N} \to \{0, 1\}^{32\times 32} \to \{0, 1\}^{24} \to \{0, 1\}\)</span>. It works as follows:</p>
<ul>
<li>The grayscale photo is down-scaled and binarized by convolution with a <a href="https://en.wikipedia.org/wiki/Discrete_Laplace_operator">discrete Laplace filter</a>: <span class="math inline">\(\mathbb{R}^{N \times N} \to \{0, 1\}^{32\times 32}\)</span>.</li>
<li>The weights for the 24 hidden perceptrons are constructed by <a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">linear discriminant analysis</a>: <span class="math inline">\(\{0, 1\}^{32\times 32} \to \{0, 1\}^{24}\)</span></li>
<li>The output perceptron is learned by the <a href="https://en.wikipedia.org/wiki/Perceptron#Learning_algorithm_for_a_single-layer_perceptron">perceptron learning rule</a>: <span class="math inline">\(\{0, 1\}^{24} \to \{0, 1\}\)</span>.</li>
</ul>
<div id="fig-kanal-1964-neural-tanks" class="quarto-layout-panel page-columns page-full">
<p></p><figcaption>Figure&nbsp;1: Images from <span class="citation" data-cites="kanalRecognitionSystemDesign1964">(<a href="#ref-kanalRecognitionSystemDesign1964" role="doc-biblioref">Kanal and Randall 1964</a>)</span>.</figcaption><p></p>
<figure class="figure page-columns page-full">
<div class="quarto-layout-row quarto-layout-valign-top page-columns page-full">
<div class="quarto-layout-cell quarto-layout-cell-subref page-columns page-full" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-kanal-1964-neural-tanks-tank-nontank-mosaic" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/kanal_1964_fig_tank_nontank_mosaic.png" class="img-fluid figure-img" data-ref-parent="fig-kanal-1964-neural-tanks"></p>
<figcaption class="figure-caption margin-caption">(a) Grayscale photos, some containing tanks, and some not.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref page-columns page-full" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-kanal-1964-neural-tanks-binary-image-tank" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/kanal_1964_fig_binary_image_tank.png" class="img-fluid figure-img" data-ref-parent="fig-kanal-1964-neural-tanks"></p>
<figcaption class="figure-caption margin-caption">(b) A picture of a tank after convolution with a discrete Laplace filter.</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top page-columns page-full">
<div class="quarto-layout-cell quarto-layout-cell-subref page-columns page-full" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-kanal-1964-neural-tanks-architecture" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/kanal_1964_fig_architecture.png" class="img-fluid figure-img" data-ref-parent="fig-kanal-1964-neural-tanks"></p>
<figcaption class="figure-caption margin-caption">(c) The architecture of the network.</figcaption>
</figure>
</div>
</div>
</div>
</figure>
</div>
</section>
</section>
<section id="the-second-neural-network-winter" class="level2">
<h2 class="anchored" data-anchor-id="the-second-neural-network-winter">The second neural network winter</h2>
<p>The first neural network winter started around 1965, when the main research centers pivoted away from neural networks: the Stanford Research Institute group turned to symbolic AI; the Bernard Widrow group turned to using <em>single</em> neurons as adaptive filters; the Frank Rosenblatt group died from lack of funds and then the literal death of Rosenblatt in 1971. It rose again around 1985, when backpropagation and improved compute allowed researchers to train neural networks on the order of <span class="math inline">\(10^4\)</span> parameters and <span class="math inline">\(4\)</span> layers.</p>
<p>Something strange happened during the 1990 – 2010 period: the neural network research community silently disappeared again for another 20 years. Unlike the previous case, there was no great mythology or drama about this winter, no <a href="https://yuxi-liu-wired.github.io/blog/posts/perceptron-controversy/"><em>Perceptron</em> controversy</a>.</p>
<p>I would like to find out why.</p>
<blockquote class="blockquote">
<p>Lukas: So I remember Daphne Koller telling me, maybe 2003, that the kind of state-of-the-art handwriting systems were neural nets, but that it was such an ad hoc kind of system that we shouldn’t focus on it. And I wonder if maybe I should have paid more attention to that and tried harder to make neural nets work for the applications I was doing.</p>
<p>Peter: Yeah, me too. And certainly Yann LeCun had success with the digit database, and I think that was over-engineered in that they looked at exactly the features they needed for that set of digitizations of those digits. And in fact, I remember researchers talking about, “Well, what change are we going to do for sample number 347?” Right?</p>
<p>Lukas: Oh, really? Okay.</p>
<p>Peter: There were individual data points that they would perform theories on, so that was definitely over-tuning to the data. And it should have been an indication that was a good approach. It was better than other approaches at the time.</p>
<p>Lukas: I guess so. Although that does sound like damming level of over-fitting the data, I suppose.</p>
<p>Peter: Right. There was only a couple thousand data points. I forget exactly how many. Maybe it was 10,000. Maybe it was even 100,000, but it wasn’t many. <span class="citation" data-cites="norvigSingularityEyeBeholder2021">(<a href="#ref-norvigSingularityEyeBeholder2021" role="doc-biblioref">Norvig 2021</a>)</span></p>
</blockquote>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-bernsteinMarvinMinskyVision1981" class="csl-entry" role="listitem">
Bernstein, Jeremy. 1981. <span>“Marvin <span>Minsky</span>’s <span>Vision</span> of the <span>Future</span>.”</span> <em>The New Yorker</em>, December.
</div>
<div id="ref-branwenGANsDidnFail2022" class="csl-entry" role="listitem">
Branwen, Gwern. 2022. <span>“<span>GANs Didn</span>’t <span>Fail</span>, <span>They Were Abandoned</span>.”</span>
</div>
<div id="ref-burdaReinforcementLearningPredictionbased2018" class="csl-entry" role="listitem">
Burda, Yura, and Harri Edwards. 2018. <span>“Reinforcement Learning with Prediction-Based Rewards.”</span> <em>OpenAI</em>.
</div>
<div id="ref-kanalRecognitionSystemDesign1964" class="csl-entry" role="listitem">
Kanal, Laveen N., and Neil C. Randall. 1964. <span>“Recognition System Design by Statistical Analysis.”</span> In <em>Proceedings of the 1964 19th <span>ACM</span> National Conference</em>, 42–501.
</div>
<div id="ref-karpathyRecipeTrainingNeural2019" class="csl-entry" role="listitem">
Karpathy, Andrej. 2019. <span>“A <span>Recipe</span> for <span>Training Neural Networks</span>.”</span> <em>Andrej Karpathy Blog</em>.
</div>
<div id="ref-norvigSingularityEyeBeholder2021" class="csl-entry" role="listitem">
Norvig, Peter. 2021. <span>“Singularity <span>Is</span> in the <span>Eye</span> of the <span>Beholder</span>.”</span>
</div>
<div id="ref-sejnowskiDeepLearningRevolution2018" class="csl-entry" role="listitem">
Sejnowski, Terrence J. 2018. <em>The <span>Deep Learning Revolution</span></em>. Illustrated edition. <span>Cambridge, Massachusetts London, England</span>: <span>The MIT Press</span>.
</div>
<div id="ref-widrowAdalineSmarterSweet1963" class="csl-entry" role="listitem">
Widrow, Bernard. 1963. <span>“Adaline: <span>Smarter</span> Than <span>Sweet</span>.”</span> <em>Stanford Today</em>, no. Autumn 1963.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left"><span class="faux-block">Everything ©<a href="https://www.apache.org/licenses/LICENSE-2.0.html">Apache License 2.0</a></span></div>   
    <div class="nav-footer-center"><span class="faux-block">Yuxi on the Wired</span></div>
    <div class="nav-footer-right"><span class="faux-block"><a href="../../../sitemap.xml">sitemap</a></span></div>
  </div>
</footer>



</body></html>