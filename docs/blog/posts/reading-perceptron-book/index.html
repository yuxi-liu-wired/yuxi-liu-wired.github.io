<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxi Liu">
<meta name="dcterms.date" content="2023-12-21">

<title>Yuxi on the Wired - The Neural Network Winter</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta property="og:title" content="Yuxi on the Wired - The Neural Network Winter">
<meta property="og:description" content="">
<meta property="og:image" content="https://yuxi-liu-wired.github.io/blog/posts/reading-perceptron-book/figure/perceptron_fig_0_2.png">
<meta property="og:site-name" content="Yuxi on the Wired">
<meta name="twitter:title" content="Yuxi on the Wired - The Neural Network Winter">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://yuxi-liu-wired.github.io/blog/posts/reading-perceptron-book/figure/perceptron_fig_0_2.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html" rel="" target="">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://bair.berkeley.edu/" rel="" target=""><i class="bi bi-folder-symlink" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi.liu.1995@gmail.com" rel="" target=""><i class="bi bi-envelope" role="img" aria-label="email">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">The Neural Network Winter</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">AI</div>
                <div class="quarto-category">history</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxi Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 21, 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">December 21, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#setup" id="toc-setup" class="nav-link active" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#perceptron-representation-theory" id="toc-perceptron-representation-theory" class="nav-link" data-scroll-target="#perceptron-representation-theory">Perceptron representation theory</a></li>
  <li><a href="#perceptron-learning-theory" id="toc-perceptron-learning-theory" class="nav-link" data-scroll-target="#perceptron-learning-theory">Perceptron learning theory</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<blockquote class="blockquote">
<p>It would seem that Perceptrons has much the same role as The Necronomicon – that is, often cited but never read.</p>
<p>Marvin Minsky, 1994. Quoted in <span class="citation" data-cites="berkeleyRevisionistHistoryConnectionism1997">(<a href="#ref-berkeleyRevisionistHistoryConnectionism1997" role="doc-biblioref">Berkeley 1997</a>)</span></p>
</blockquote>
<p>Well, you too can read the <em>Necronomicon</em>.</p>
<p>In one sentence, the mathematical portion of the <em>Perceptron</em> book is a theory of two-layered perceptrons, mostly by discrete mathematics. Perceptron representation occupies chapters 0–10, and learning is only studied in chapters 11 and 12.</p>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">Setup</h2>
<p>Let <span class="math inline">\(R\)</span> be a finite set, where “R” could be read as “region”, or “rectangle”.</p>
<div id="def-perceptron" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Perceptron) </strong></span>A perceptron is a binary function of type <span class="math inline">\(\{0, 1\}^R \to \{0, 1\}\)</span>, defined by a weight vector <span class="math inline">\(w\)</span> and a threshold number <span class="math inline">\(b\)</span>:</p>
<p><span class="math display">\[
\Phi(x) := \theta(w^T x + b)
\]</span></p>
<p>where <span class="math inline">\(\theta(t) := 1_{t \geq 0}\)</span> is the 0-1 step function.</p>
</div>
<div id="def-perceptron-machine" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (perceptron machine) </strong></span>A perceptron machine with <span class="math inline">\(k\)</span> hidden neurons is a function of type <span class="math inline">\(\{0, 1\}^R \to \{0, 1\}\)</span>, defined by</p>
<p><span class="math display">\[
\Phi(x) := \psi_{k+1}(\psi_1(x), \psi_2(x), \cdots, \psi_k(x))
\]</span></p>
<p>where <span class="math inline">\(\psi_1, ..., \psi_k\)</span> are (hidden) perceptrons in the hidden layer, and <span class="math inline">\(\psi_{k+1}\)</span> is the single output perceptron.</p>
</div>
<div id="def-predicate-order" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (perceptron orders) </strong></span>The order of a hidden perceptron is the number of nonzero weights.</p>
<p>The order of a perceptron machine is the maximum order of its hidden perceptrons.</p>
<p>The order of a boolean function is the minimum order necessary for a perceptron machine that implements it.</p>
</div>
<p>For example, the constant-0 and constant-1 boolean functions are both of order 0.</p>
<p>A key focus of the perceptron controversy is the concept of “conjunctively local”.</p>
<div id="def-predicate-order" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (conjunctively local) </strong></span>A family of boolean functions is conjunctively local iff their orders are upper bounded.</p>
</div>
</section>
<section id="perceptron-representation-theory" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="perceptron-representation-theory">Perceptron representation theory</h2>
<p>While their definition allows perceptron machines of any real-valued weights and biases, they immediately performed a <a href="https://en.wikipedia.org/wiki/Reduction_(complexity)">computational reduction</a> to the special case of mask perceptron machines. Most of the book considers the problem of classifying subsets of families of planar square grids, such as <span class="math inline">\(\{1, 2, 3\} \times \{1, 2, 3, 4\}\)</span>. A subset is inputted to the perceptron machine by setting inputs in the subset to <span class="math inline">\(1\)</span>, and the rest to <span class="math inline">\(0\)</span>. Consequently, it is natural to consider a special case of perceptron machines.</p>
<div id="def-predicate-order" class="theorem definition page-columns page-full">
<p><span class="theorem-title"><strong>Definition 5 (mask perceptron machine) </strong></span>A mask for <span class="math inline">\(A\subset R\)</span> is a function of type <span class="math inline">\(\{0, 1\}^R \to \R\)</span>, such that <span class="math inline">\(\psi(x) = 1\)</span> if <span class="math inline">\(x_i =1\)</span> for all <span class="math inline">\(i \in A\)</span>, and else <span class="math inline">\(\psi(x) = 0\)</span>.</p>
<p>A mask perceptron machine is a perceptron machine where each hidden perceptron is a mask, and the threshold of the output perceptron is zero. In other words, it is of form</p>
<p><span class="math display">\[
\Phi(x) = \theta\lrb{\sum_{i=1}^k a_i \psi_{A_i}(x)}
\]</span></p>
<p>where each <span class="math inline">\(\psi_{A_i}\)</span> is a mask, each <span class="math inline">\(a_i\in \R\)</span>, and <span class="math inline">\(k\)</span> is the number of hidden perceptrons.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/perceptron_fig_0_2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Figure 0.2</figcaption>
</figure>
</div>
</div>
<div id="thm-predicate-order" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Theorem 1.5.1) </strong></span>Restricting to mask perceptron machines does not change perceptron orders. That is, any boolean function implemented by a perceptron machine of order <span class="math inline">\(k\)</span> can be implemented by a mask perceptron machine of order at most <span class="math inline">\(k\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Take one such perceptron machine. The threshold of the output perceptron can be removed by adding a hidden perceptron that always outputs <span class="math inline">\(1\)</span> – in other words, <span class="math inline">\(\psi_\emptyset\)</span>, the mask of the empty set. Now it remains to decompose each hidden perceptron into masks with the same order, or less.</p>
<p>Let <span class="math inline">\(\psi\)</span> be a hidden perceptron with nonzero weights on the input points <span class="math inline">\(x_{i_1}, ..., x_{i_k}\)</span>, then its output is determined by the values of <span class="math inline">\(x_{i_1}, ..., x_{i_k}\)</span>. Therefore, we can partition the binary set <span class="math inline">\(\{0, 1\}^{i_1, ..., i_k}\)</span> into two subsets <span class="math inline">\(A_0, A_1\)</span>, such that for any input <span class="math inline">\(x\in\{0, 1\}^R\)</span>, we have <span class="math inline">\(\psi(x) = 1\)</span> iff <span class="math inline">\((x_{i_1}, ..., x_{i_k}) \in A_1\)</span>.</p>
<p>In other words, we only need to look at the binary values <span class="math inline">\(x_{i_1}, ..., x_{i_k}\)</span> to know the binary output <span class="math inline">\(\psi(x)\)</span>.</p>
<p>Therefore, we can replace <span class="math inline">\(\psi\)</span> with a boolean formula on <span class="math inline">\(x_{i_1}, ..., x_{i_k}\)</span>, then expand it to obtain up to <span class="math inline">\(2^k\)</span> masks, each of order at most <span class="math inline">\(k\)</span>.</p>
<p>For example, suppose <span class="math inline">\(\psi\)</span> has nonzero weights on <span class="math inline">\(x_1, x_2\)</span>, and is 1 on all odd-sized subsets, then we can write it as a boolean formula:</p>
<p><span class="math display">\[
\left(x_1 \wedge \neg x_2\right) \vee\left(\neg x_1 \wedge x_2\right) = x_1\left(1-x_2\right)+\left(1-x_1\right) x_2 = x_1 + x_2 - 2 x_1 x_2
\]</span></p>
</div>
<p>The next tool they used is symmetry, formulated in the language of <a href="https://en.wikipedia.org/wiki/Group_action">finite group actions</a>.</p>
<p>Let <span class="math inline">\(S_R\)</span> be the permutation group on the elements of <span class="math inline">\(R\)</span>, and <span class="math inline">\(G\)</span> be a subgroup of <span class="math inline">\(S_R\)</span>. We say that a boolean function <span class="math inline">\(\psi\)</span> is <span class="math inline">\(G\)</span>-invariant iff <span class="math inline">\(\psi \circ g=\psi\)</span> for any <span class="math inline">\(g \in G\)</span>. That is, any <span class="math inline">\(X \subset R\)</span>, we have <span class="math inline">\(\psi(X)=\psi(g(X))\)</span>. For example, the parity function is <span class="math inline">\(S_R\)</span>-invariant, since any permutation of the set preserves the size, and thus the parity, of any of its subsets.</p>
<div id="thm-group-invariance" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 (group invariance theorem) </strong></span>If a boolean function is <span class="math inline">\(G\)</span>-invariant, where <span class="math inline">\(G\)</span> is a finite group, then any perceptron machine computing it can be converted to a perceptron machine <span class="math inline">\(\theta(\sum_i a_i \psi_i)\)</span>, such that if <span class="math inline">\(\psi_i=\psi_j \circ g\)</span> for some <span class="math inline">\(g \in G\)</span>, then <span class="math inline">\(a_i=a_j\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Take the group-action average: any mask <span class="math inline">\(\psi\)</span> is equal to <span class="math inline">\(\frac{1}{|G|} \sum_{g\in G} \psi\circ g\)</span>.</p>
</div>
<p>Once the groundwork is laid, they proceeded to prove a wide variety of theorems on the order of particular boolean functions.</p>
<p>Consider the parity function. It is <span class="math inline">\(1\)</span> iff exactly an odd number of inputs are <span class="math inline">\(1\)</span> and the rest are <span class="math inline">\(0\)</span>.</p>
<div id="thm-parity-order" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 (Theorem 3.1) </strong></span>The parity function has order <span class="math inline">\(|R|\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Since the parity function is <span class="math inline">\(S_R\)</span>-invariant, if it is implemented by a perceptron machine of order <span class="math inline">\(k\)</span>, it is implemented by some mask perceptron machine <span class="math inline">\(\theta(\sum_{A_i \subset R} a_{A_i} \psi_{A_i})\)</span>, where each mask is of size <span class="math inline">\(\leq k\)</span>, and each weight <span class="math inline">\(a_{A_i}\)</span> depends only on the size of <span class="math inline">\(A_i\)</span>. Let <span class="math inline">\(b_{|A_i|} = a_{A_i}\)</span> be those coefficients. It remains to show <span class="math inline">\(b_{|R|} \neq 0\)</span>.</p>
<p>For each <span class="math inline">\(X \subset R\)</span>, we have by explicit computation</p>
<p><span class="math display">\[
\theta\lrb{\sum_{A_i \subset R} a_{A_i} \psi_{A_i}} = 1\lrs{f(|X|) \geq 0}
\]</span></p>
<p>where <span class="math inline">\(f(t) := \sum_{i=0}^{|R|}b_i \binom{t}{i}\)</span> is a polynomial in <span class="math inline">\(t\)</span>. Since this perceptron machine implements the parity function, as <span class="math inline">\(t\)</span> increases from <span class="math inline">\(0\)</span> to <span class="math inline">\(|R|\)</span>, the function <span class="math inline">\(f(t) + \epsilon\)</span> must intersect the <span class="math inline">\(x\)</span>-axis at least <span class="math inline">\(|R|\)</span> times for some real value <span class="math inline">\(\epsilon\)</span>. Since <span class="math inline">\(f\)</span> is a polynomial, it must have at least order <span class="math inline">\(|R|\)</span>, thus <span class="math inline">\(b_{|R|} \neq 0\)</span>.</p>
</div>
<div id="thm-one-in-a-box" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4 (Theorem 3.2, one-in-a-box) </strong></span>Let <span class="math inline">\(A_1, A_2, ..., A_m\)</span> be disjoint subsets of <span class="math inline">\(R\)</span> each of size <span class="math inline">\(4 m^2\)</span>, and define the predicate <span class="math inline">\(\psi(X) = \forall i, \left|X \cap A_i\right|&gt;0\)</span>, that is, there is at least one point of <span class="math inline">\(X\)</span> in each <span class="math inline">\(A_i\)</span>. The order of <span class="math inline">\(\psi\)</span> is <span class="math inline">\(\geq m\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let the order of <span class="math inline">\(\psi\)</span> be <span class="math inline">\(k\)</span>.</p>
<p>The predicate <span class="math inline">\(\psi\)</span> is invariant with respect to the group <span class="math inline">\(S_{A_1} \times \cdots \times S_{A_m}\)</span>, so by the same construction as the proof of <a href="#thm-group-invariance">Theorem&nbsp;2</a>, there exists a polynomial <span class="math inline">\(P(t_1, ..., t_m)\)</span>, where <span class="math inline">\(P\)</span> has order <span class="math inline">\(k\)</span>, and</p>
<p><span class="math display">\[
\forall t_1, ..., t_m \in \{0, 1, ..., 4m^2\}, P(t_1, ..., t_m) &lt; 0 \iff t_1 = 0 \vee \cdots \vee t_m = 0
\]</span></p>
<p>Now define <span class="math inline">\(Q(t) := P((t-1)^2, (t-3)^2, \cdots, (t-2m+1)^2)\)</span>. By the above equation, <span class="math inline">\(Q &lt; 0\)</span> at <span class="math inline">\(t=1, 3, \cdots, 2m - 1\)</span> and <span class="math inline">\(Q \geq 0\)</span> at <span class="math inline">\(t = 0, 2, \cdots, 2m\)</span>. Thus, <span class="math inline">\(Q\)</span> has order <span class="math inline">\(\geq 2m\)</span>. Thus, <span class="math inline">\(2k \geq 2m\)</span>.</p>
</div>
<div id="thm-and-or" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5 (Theorem 4.0) </strong></span>There exist predicates <span class="math inline">\(\psi_1\)</span> and <span class="math inline">\(\psi_2\)</span> of order 1 such that <span class="math inline">\(\psi_1 \wedge \psi_2\)</span> and <span class="math inline">\(\psi_1 \vee \psi_2\)</span> are not of finite order. Specifically, if we partition <span class="math inline">\(R\)</span> into three equal subsets <span class="math inline">\(A, B, C\)</span>, then the boolean function does not have bounded order:</p>
<p><span class="math display">\[
(|X \cap A| &gt; |X \cap C|) \wedge (|X \cap B| &gt; |X \cap C|)
\]</span></p>
<p>even though both <span class="math inline">\(|X \cap A| &gt; |X \cap C|\)</span> and <span class="math inline">\(|X \cap B| &gt; |X \cap C|\)</span> are of order <span class="math inline">\(1\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math inline">\(|X \cap A| &gt; |X \cap C|\)</span> is computed by the order-<span class="math inline">\(1\)</span> perceptron machine <span class="math inline">\(\theta\lrb{\sum_{i\in A} x_i - \sum_{i \in C}x_i}\)</span>, and similarly for the other one.</p>
<p>To show the composite boolean function is not of bounded order, suppose otherwise, then by the same argument as the previous proof, we can construct a sequence of polynomials <span class="math inline">\(P_1(a, b, c), P_2(a, b, c), P_3(a, b, c), ...\)</span>, such that each <span class="math inline">\(P_n\)</span> is the polynomial corresponding to the perceptron machine for the case where <span class="math inline">\(|A| = |B| = |C| = n\)</span>, and each of them has order at most <span class="math inline">\(M\)</span>, for some fixed <span class="math inline">\(M\)</span>.</p>
<p>Being the polynomial corresponding to the perceptron machine for the case where <span class="math inline">\(|A| = |B| = |C| = n\)</span> means precisely that</p>
<p><span class="math display">\[
a &gt; c \wedge b &gt; c \implies P_n(a, b, c) \geq 0; \neg (a &gt; c \wedge b &gt; c) \implies P_n(a, b, c) &lt; 0;
\]</span></p>
<p>for all <span class="math inline">\(a, b, c \in \{0, 1, ..., n\}\)</span>. This implies that each <span class="math inline">\(P_1, P_2, ... \neq 0\)</span>. Now, divide each polynomial by the root-sum-square of its coefficients, so that if we interpret the coefficients of each <span class="math inline">\(P_n\)</span> as a point in a high-dimensional space, the point falls on the unit sphere in that space. Since the unit sphere is compact, there exists a limit point, which we write as <span class="math inline">\(P(a, b, c)\)</span>.</p>
<p>By the limit construction, we have</p>
<p><span class="math display">\[
\forall a, b, c \in \N, \quad a &gt; c \wedge b &gt; c \implies P(a, b, c) \geq 0; \neg (a &gt; c \wedge b &gt; c) \implies P(a, b, c) \leq 0;
\]</span></p>
<p>If we color the points <span class="math inline">\(\N^3\)</span> with black for <span class="math inline">\(P &lt; 0\)</span> and white for <span class="math inline">\(P \geq 0\)</span>, then we can see the dusty outlines of a pyramid. We can construct a solid pyramid by zooming.</p>
<p>Let <span class="math inline">\(M'\)</span> be the order of <span class="math inline">\(P\)</span>, then we can “zoom out” by taking the <a href="https://en.wikipedia.org/wiki/Homogeneous_function">projective limit</a> <span class="math inline">\(Q(a, b, c) := \lim_{t \to \infty} t^{-M'} P(ta, tb, tc)\)</span>. This <span class="math inline">\(Q\)</span> is a homogeneous polynomial, and by continuity,</p>
<p><span class="math display">\[
\forall a, b, c \geq 0, \quad a &gt; c \wedge b &gt; c \implies P(a, b, c) \geq 0; a &lt; c \vee b &lt; c \implies P(a, b, c) \leq 0;
\]</span></p>
<p>This implies that <span class="math inline">\(P\)</span> is identically zero on the “creased curve” <span class="math inline">\(\{ a, b, c \geq 0, a = c \vee b = c\}\)</span> in the projective plane, which is not an algebraic curve, therefore it is identically zero, contradiction.</p>
</div>
<div id="thm-connectedness-order" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6 (Theorem 5.1) </strong></span>The connectedness function has order <span class="math inline">\(\Omega(|R|^{1/3})\)</span>.</p>
</div>
<div class="proof page-columns page-full">
<div class="page-columns page-full"><p><span class="proof-title"><em>Proof</em>. </span>If we have a mask perceptron machine that can solve the connectedness problem, then it can be repurposed<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> to solve the one-in-a-box problem of the following kind:</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;Repurposing one machine to solve another problem is a common trick in computational complexity, called “<a href="https://en.wikipedia.org/wiki/Reduction_(complexity)">reduction</a>”. For perceptron machines, they called it “Theorem 5.4.1: The Collapsing Theorem”.</p></li></div></div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/perceptron_fig_5_2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Figure 5.2</figcaption>
</figure>
</div>
<p>In the picture, the rectangle <span class="math inline">\(R\)</span> has width <span class="math inline">\(4m^2\)</span> and height <span class="math inline">\(2m+1\)</span>. We fill in all the odd-numbered rows, and use the machine to solve the one-in-a-box problem for the even-numbered rows. By <a href="#thm-one-in-a-box">Theorem&nbsp;4</a>, the machine has order <span class="math inline">\(\geq m = \Omega(|R|^{1/3})\)</span>.</p>
</div>
<p>More generally, the only locally conjunctive topological invariant is the Euler number <span class="math inline">\(E\)</span> (Theorem 5.9). That is, if <span class="math inline">\(\psi\)</span> is a boolean function, and it depends only on the topology of the input figure, and its order is upper-bounded by some number <span class="math inline">\(k\)</span> that does not grow even as <span class="math inline">\(R\)</span> grows into a larger and larger rectangle, then <span class="math inline">\(\psi\)</span> is of form <span class="math inline">\(f \circ E\)</span>, for some function <span class="math inline">\(f: \N \to 2\)</span>.</p>
<p>The Euler number itself is <span class="math inline">\(E(X) = \sum_{i \in R} x_i - \sum_{i,j \in R} x_ix_j + \sum_{i,j,k, l \in R} x_ix_jx_kx_l\)</span>, where the <span class="math inline">\(i,j\in R\)</span> ranges only over adjacent points, and <span class="math inline">\(i,j,k, l \in R\)</span> ranges only over quadruples that form a square. Thus the Euler number itself has order <span class="math inline">\(4\)</span> (Theorem 5.8.1).</p>
<p>Chapters 6–9 continue in the same style, but moves to the case where the input space is made of one or two copies of the infinite line <span class="math inline">\(\Z\)</span>, or the infinite plane <span class="math inline">\(\Z^2\)</span>, and the predicate to recognize is still translation-invariant. In order to avoid blowing up the perceptron machine with infinities, they restricted to the case where the input figure is of finite size, meaning that <span class="math inline">\(\sum_i x_i\)</span> is finite.</p>
<p>Chapter 6 develops the idea of “spectra” of images. For example, the following picture shows that if we were to design a perceptron machine using only masks of size up to 2, and translation-invariant weights, then it cannot possibly distinguish between the two tetris-like pieces, because both figures contain exactly 4 instances of single-square, 1 instance of two squares side by side, and so on.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/perceptron_chap_6.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">page 100</figcaption>
</figure>
</div>
<p>Sections 6.1–6.5 of the chapter studies the same idea for masks of size up to 4. For example, Section 6.4 shows that “<span class="math inline">\(X\)</span> is the perimeter of a complete circle” is of order <span class="math inline">\(4\)</span>.</p>
<p>Section 6.6 claims that “recognizing figures in context” is generally not locally conjunctive, although they gave only two examples. Specifically, they showed that the predicate “<span class="math inline">\(X\)</span> is a horizontal line across the rectangle” is order 2, the predicate “<span class="math inline">\(X\)</span>$ contains one horizontal line across plane” is not locally conjunctive. The same is true for the case with “a hollow square” instead of “a horizontal line”.</p>
<p>Chapter 7 uses the “stratification” construction to explicitly demonstrate that several binocular predicates are of finite order, but requires exponentially growing weights. It is essentially the same idea as <a href="https://en.wikipedia.org/wiki/G%C3%B6del_numbering">Gödel numbering</a>. A single illustrative example suffices to demonstrate the general point.</p>
<div id="exm-line-symmetry" class="theorem example page-columns page-full">
<p><span class="theorem-title"><strong>Example 1 </strong></span>Given a line <span class="math inline">\(\Z\)</span>, how to construct a perceptron machine that detects that input figure is symmetric?</p>
<p>Suppose we know for certain that the input figure has leftmost point <span class="math inline">\(m\)</span> and rightmost point <span class="math inline">\(n\)</span>, then we can test for symmetry by computing the value of:</p>
<p><span class="math display">\[
f_{m, n}(x) := \sum_{i = 0}^{n-m} x_{m+i}(x_{n-i} - 1)
\]</span></p>
<p>We have that <span class="math inline">\(f_{m, n}(x) = 0\)</span> if the figure is symmetric, and <span class="math inline">\(f_{m, n}(x) \leq -1\)</span> otherwise.</p>
<div class="page-columns page-full"><p>Now we define the entire perceptron machine by <span class="math inline">\(\sum_{m \leq n} M_{n - m} x_m x_n (f_{m, n}(x) - 1/2)\)</span>. If the sequence of <span class="math inline">\(M_0, M_1, ...\)</span> grows as <span class="math inline">\((d!)^2\)</span> roughly <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, then the largest bracket <span class="math inline">\((m, n)\)</span> would “veto” every smaller bracket contained within, and so the entire perceptron machine is effectively first testing for the smallest interval containing the figure, before testing the symmetry within that interval.</p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;Expanding term by term, we have <span class="math inline">\(|(f_{m, n}(x) - 1/2)| \leq 2(n-m) + \frac 12\)</span>. Therefore, in order for <span class="math inline">\(M_d\)</span> to “veto” every other bracket within, we need</p>
<p><span class="math display">\[
M_d \frac 12 &gt; \sum_{d' = 1}^{d-1} \lrb{M_{d'}(\frac 12 + 2d')(d-d' + 1)}
\]</span></p>
<p>Here <span class="math inline">\(d\)</span> should be read as “distance between two ends of a bracket”.</p>
<p>To bound the growth rate, we bound the recurrence relation <span class="math inline">\(M_d = \sum_{d' = 1}^{d-1} \lrb{M_{d'}(4d' + 1)(d-d' + 1)}\)</span>. The sum on the right is bounded by</p>
<p><span class="math display">\[
\sum_{d' = 1}^{d-1} \lrb{M_{d'}(4d' + 1)(d-d' + 1)} \in \Theta{\lrs{
    \sum_{d' = 1}^{d-1} \lrb{M_{d'}d'},
    d^2\sum_{d' = 1}^{d-1} \lrb{M_{d'}}}}
\]</span></p>
<p>The lower bound implies <span class="math inline">\(M_d = \Omega((d!)^2 \times d^{-1})\)</span> and upper bound implies <span class="math inline">\(M_d = O((d!)^2 \times (d+1)^2)\)</span>.</p></li></div></div>
</div>
<p>They made similar constructions for perceptron machines that decide whether two infinite lines or planes are translates of each other, whether two planes are translation and approximate dilations of each other, and whether a line or a plane is the translation of a fixed figure.</p>
<p>Chapter 8 studies diameter-limited perceptron machines, meaning that not only are the hidden perceptrons assumed to be masks of bounded size, those masks are assumed to be contained in a circle of radius <span class="math inline">\(M\)</span> for some finite <span class="math inline">\(M\)</span>. It is intended to be a formalization of the intuition that the perceptron machines should model a real pair of eyes scanning a plane, and each retina is a circular disk with finite radius. No results in chapter 8 were used further in the book, and they resemble the case of finite-order perceptron machines, so we do not discuss the results in detail.</p>
<p>Chapter 9 shows that the connectedness problem is easy to solve using small serial programs, a contrast to the case of perceptron machines. They designed a robot that moves around the plane, and can solve the connectedness problem, with a memory size of just two. Namely, it only need to store up to two locations <span class="math inline">\((x, y), (x', y')\)</span> in its memory during its operation, and it eventually halts in one of three states “empty”, “connected”, and “disconnected”. They then described a few other more exotic computational models. The whole thing is in the same style of solving interesting puzzles in computational complexity, similar to <span class="citation" data-cites="minskyComputationFiniteInfinite1967">(<a href="#ref-minskyComputationFiniteInfinite1967" role="doc-biblioref">Minsky 1967</a>)</span>. The overarching goal is to demonstrate that connectedness is “inherently serial”.</p>
<p>Whereas in the construction <a href="#exm-line-symmetry">Example&nbsp;1</a>, we saw coefficients growing exponentially on an infinite plane, chapter 10 finitizes that this.</p>
<div id="exm-parity-order" class="theorem example">
<p><span class="theorem-title"><strong>Example 2 (Theorem 10.1) </strong></span>Suppose we have a perceptron machine that tests for parity, then by <a href="#thm-parity-order">Theorem&nbsp;3</a>, it must have order <span class="math inline">\(|R|\)</span>. As in the construction given in the proof, we use the group invariance theorem to obtain a group-symmetric machine with form <span class="math inline">\(\theta(\sum_{i=0}^{|R|}\binom{|X|}{i} b_i)\)</span>, where <span class="math inline">\(b_0, b_1, ..., b_{|R|}\)</span> are real numbers. Then, assuming the machine is “reliable”, we can prove that <span class="math inline">\((-1)^{M} b_{M+1 \geq 2^{M}\)</span> for any <span class="math inline">\(M \in \{0, 1, \cdots, |R|-1\}\)</span>.</p>
<p>Since the group-symmetric construction can only average out the most extreme values, this implies that before the group-symmetric construction, our perceptron machine had even more extreme coefficients.</p>
<p>A “reliable” machine is a <a href="https://en.wikipedia.org/wiki/Support_vector_machine">support vector machine</a> with margin <span class="math inline">\(\geq 1/2\)</span>. That is, it is a machine such that <span class="math display">\[
\sum_{i=0}^{|R|}\binom{|X|}{i} b_i \begin{cases}
\geq 1 &amp; \text{ if $|X|$ is odd}
\leq 0 &amp; \text{ if $|X|$ is even}
\end{cases}
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Define <span class="math inline">\(A_n = \sum_{i=0}^n \binom{n}{i}b_i\)</span>. Since the machine is reliable, we have that <span class="math inline">\((-1)^{n}(A_{n+1} - A_n) \geq 1\)</span> for each <span class="math inline">\(n = 0, 1, ..., |R|-1\)</span>. Simplifying the binomial coefficients, we have <span class="math inline">\(A_{n+1} - A_n = \sum_i \binom{n}{i} b_{i+1}\)</span>. Note that we use the convenient convention that <span class="math inline">\(\binom{x}{y} = 0\)</span> if <span class="math inline">\(x &lt; y\)</span>.</p>
<p>Now fix any <span class="math inline">\(M \in \{0, 1, \cdots, |R|-1\}\)</span>, and evaluate the following inequality:</p>
<p><span class="math display">\[
2^{M} = \sum_n \binom{M}{n} \cdot 1 \leq \sum_n \binom{M}{n} (-1)^{n}(A_{n+1} - A_n)
\]</span></p>
<p>By manipulating the binomial coefficients, the right side simplifies to <span class="math inline">\((-1)^M b_{M+1}\)</span>.</p>
</div>
<div class="page-columns page-full"><p>Section 10.2 and 10.3 constructs two pathological examples. By restricting the shapes of hidden perceptron masks, they proved that certain predicates required superexponential weights. Section 10.4 purports to extend the group invariance theorem to the infinite case.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> None of these are used elsewhere in the book. They are virtuoso performances. However, they interpreted this as a serious problem:</p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;They purported to show in Theorem 10.4.1 that, if the weights are bounded, then the group invariance theorem applies again, but the proof is false, with a counterexample being <span class="math inline">\(\sum_{n \in \Z}e^{-n^2}x_n - e^{-n^4}x_nx_{n+1}\)</span>. The theorem <em>might</em> still be correct with another proof, but I cannot find one.</p></li></div></div>
<blockquote class="blockquote">
<p>A proof, in Chapter 10, that coefficients can grow much faster than exponentially with <span class="math inline">\(|R|\)</span> has serious consequences both practically and conceptually: the use of more memory capacity to store the coefficients than to list all the figures strains the idea that the machine is making some kind of abstraction</p>
</blockquote>
</section>
<section id="perceptron-learning-theory" class="level2">
<h2 class="anchored" data-anchor-id="perceptron-learning-theory">Perceptron learning theory</h2>
<p>In Chapter 11, they <em>finally</em> start discussing perceptron learning, which is of a very restrictive form.</p>
<div id="def-perceptron-learning" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6 (Perceptron learning) </strong></span>To train a perceptron machine is to <em>fix</em> its hidden perceptrons, and adjust the weights and threshold of <em>only</em> the single output perceptron by the perceptron learning rule. For this chapter, it is cleaner to change the convention, so that each perceptron outputs <span class="math inline">\(-1, +1\)</span>, instead of <span class="math inline">\(0, 1\)</span>.</p>
<p>Since only the output perceptron is adapted, it suffices to discuss the case where there are <em>no</em> hidden perceptrons, and we only need to adapt the weights of a single perceptron. That is, we have a dataset <span class="math inline">\(D\)</span>, and we sample some <span class="math inline">\((x, y) \in D\)</span>, and verify that <span class="math inline">\(y = \theta(\braket{w, x})\)</span>.</p>
<p>If this is true for all <span class="math inline">\((x, y) \in D\)</span>, then the perceptron learning has converged. Otherwise, we update <span class="math inline">\(w\)</span> by <span class="math inline">\(w \leftarrow w + \alpha y x\)</span>, where <span class="math inline">\(\alpha &gt; 0\)</span> is the learning rate.</p>
</div>
<div id="def-perceptron-learning" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7 (Perceptron learning theorem) </strong></span>Let <span class="math inline">\(D\)</span> be a dataset, with radius <span class="math inline">\(R = \max_{(x, y) \in D} \|x\|\)</span>. If there exists some unit <span class="math inline">\(w^*\)</span> such that <span class="math inline">\(\gamma = \min_{(x, y) \in D} y\braket{w^*, x}\)</span>, then the perceptron learning algorithm converges after making at most <span class="math inline">\((R/\gamma)^2\)</span> updates.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By linearity of the learning rule, we can deal only with the case where <span class="math inline">\(\alpha = 1\)</span>.</p>
<p>By multiplying each <span class="math inline">\(x\)</span> with its <span class="math inline">\(y\)</span>, we can deal only with the case where all <span class="math inline">\(y = +1\)</span>.</p>
<p>By rotating and scaling the space, we can deal only with the case where <span class="math inline">\(w^* = (1, 0, \cdots, 0)\)</span>, and <span class="math inline">\(\gamma = 1\)</span>.</p>
<p>Now, each weight update increases the first coordinate of <span class="math inline">\(w\)</span> by at least <span class="math inline">\(1\)</span>, so after <span class="math inline">\(n\)</span> updates, <span class="math inline">\(\|w\| \geq n\)</span>. However, each weight update of <span class="math inline">\(w \leftarrow w + x\)</span> uses a vector <span class="math inline">\(x\)</span> that is pointing at a direction perpendicular to <span class="math inline">\(w\)</span>, or worse, pointing against <span class="math inline">\(w\)</span>. Therefore, by Pythagorean theorem, <span class="math inline">\(\|w\|^2\)</span> increases by at most <span class="math inline">\(\|x\|^2 \leq R^2\)</span>. So after <span class="math inline">\(n\)</span> updates, <span class="math inline">\(\|w\|^2 \leq nR^2\)</span>.</p>
<p>Combining the two results we have <span class="math inline">\(n \leq R^2\)</span>.</p>
</div>
<p>Modifying the proof slightly, and applying the conclusion of <a href="#exm-parity-order">Example&nbsp;2</a>, we find that starting with the zero weight vector, it takes at least <span class="math inline">\(2^{|R|}/|R|\)</span> steps steps to learn the parity function.</p>
<p>They then gestured that gradient descent is just a somewhat more efficient perceptron learning rule, and cannot escape local optima. No “local learning rule” can escape local optima, unlike symbolic programs that are provably capable of finding global optima.</p>
<p>If the dataset is not linearly separable, then the perceptron weight would not converge. However, the perceptron cycling theorem shows that if the dataset is finite, then the perceptron weight would still be trapped within a large but finite disk, no matter how the dataset is sampled.</p>
<p>Chapter 12 sketches other algorithms for learning (approximate) binary classification algorithms, including naive Bayes, nearest neighbor, ISODATA, a hack used by Arthur Samuel’s checker program, etc. Section 12.6 and 12.7 study the time-space tradeoff using variations on a toy problem: given a subset of <span class="math inline">\(\{0, 1\}^n\)</span>, decide whether a <span class="math inline">\(n\)</span>-bit word is in it or not.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-berkeleyRevisionistHistoryConnectionism1997" class="csl-entry" role="listitem">
Berkeley, Istvan SN. 1997. <span>“A Revisionist History of Connectionism.”</span> <em>Unpublished Manuscript</em>.
</div>
<div id="ref-minskyComputationFiniteInfinite1967" class="csl-entry" role="listitem">
Minsky, Marvin. 1967. <em>Computation: Finite and Infinite Machines</em>. <span>Englewood Cliffs, NJ</span>: <span>Prentice-Hall</span>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left"><span class="faux-block">Everything ©<a href="https://www.apache.org/licenses/LICENSE-2.0.html">Apache License 2.0</a></span></div>   
    <div class="nav-footer-center"><span class="faux-block">Yuxi on the Wired</span></div>
    <div class="nav-footer-right"><span class="faux-block"><a href="../../../sitemap.xml">sitemap</a></span></div>
  </div>
</footer>



</body></html>