<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxi Liu">
<meta name="dcterms.date" content="2023-12-05">
<meta name="description" content="The bitter lesson in bite-sized packets.">

<title>Yuxi on the Wired - Fermi Estimation for Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta property="og:title" content="Yuxi on the Wired - Fermi Estimation for Neural Networks">
<meta property="og:description" content="The bitter lesson in bite-sized packets.">
<meta property="og:image" content="https://yuxi-liu-wired.github.io/blog/posts/neural-scaling-laws/figure/banner_5.png">
<meta property="og:site-name" content="Yuxi on the Wired">
<meta property="og:image:height" content="1024">
<meta property="og:image:width" content="1792">
<meta name="twitter:title" content="Yuxi on the Wired - Fermi Estimation for Neural Networks">
<meta name="twitter:description" content="The bitter lesson in bite-sized packets.">
<meta name="twitter:image" content="https://yuxi-liu-wired.github.io/blog/posts/neural-scaling-laws/figure/banner_5.png">
<meta name="twitter:image-height" content="1024">
<meta name="twitter:image-width" content="1792">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html" rel="" target="">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://bair.berkeley.edu/" rel="" target=""><i class="bi bi-folder-symlink" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi.liu.1995@gmail.com" rel="" target=""><i class="bi bi-envelope" role="img" aria-label="email">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Fermi Estimation for Neural Networks</h1>
                  <div>
        <div class="description">
          The bitter lesson in bite-sized packets.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">AI</div>
                <div class="quarto-category">economics</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxi Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 5, 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">December 5, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#gpt-like-agi" id="toc-gpt-like-agi" class="nav-link active" data-scroll-target="#gpt-like-agi">GPT-like AGI</a></li>
  <li><a href="#chinchilla-scaling-law" id="toc-chinchilla-scaling-law" class="nav-link" data-scroll-target="#chinchilla-scaling-law">Chinchilla Scaling Law</a></li>
  <li><a href="#dataset-size" id="toc-dataset-size" class="nav-link" data-scroll-target="#dataset-size">Dataset size</a></li>
  <li><a href="#memory-requirement" id="toc-memory-requirement" class="nav-link" data-scroll-target="#memory-requirement">Memory requirement</a>
  <ul class="collapse">
  <li><a href="#memory-cost" id="toc-memory-cost" class="nav-link" data-scroll-target="#memory-cost">Memory cost</a></li>
  <li><a href="#memory-bandwidth-and-latency" id="toc-memory-bandwidth-and-latency" class="nav-link" data-scroll-target="#memory-bandwidth-and-latency">Memory bandwidth and latency</a></li>
  <li><a href="#batch-inference" id="toc-batch-inference" class="nav-link" data-scroll-target="#batch-inference">Batch inference</a></li>
  </ul></li>
  <li><a href="#training-cost" id="toc-training-cost" class="nav-link" data-scroll-target="#training-cost">Training cost</a>
  <ul class="collapse">
  <li><a href="#the-difficulty-of-large-scale-training" id="toc-the-difficulty-of-large-scale-training" class="nav-link" data-scroll-target="#the-difficulty-of-large-scale-training">The difficulty of large-scale training</a></li>
  </ul></li>
  <li><a href="#inference-cost" id="toc-inference-cost" class="nav-link" data-scroll-target="#inference-cost">Inference cost</a></li>
  <li><a href="#energetic-cost" id="toc-energetic-cost" class="nav-link" data-scroll-target="#energetic-cost">Energetic cost</a>
  <ul class="collapse">
  <li><a href="#the-lowest-possible-power-for-life" id="toc-the-lowest-possible-power-for-life" class="nav-link" data-scroll-target="#the-lowest-possible-power-for-life">The lowest possible power for life</a></li>
  </ul></li>
  <li><a href="#environmental-cost" id="toc-environmental-cost" class="nav-link" data-scroll-target="#environmental-cost">Environmental cost</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<p><img src="figure/banner_6.png" class="img-fluid"></p>
<p>Fermi estimation is a technique for estimating quantities through rough approximations and educated guesses. Named after physicist Enrico Fermi, this method involves breaking down a problem into simpler, more manageable parts, making reasonable assumptions, and using simple arithmetic to arrive at a good enough answer.</p>
<p>This post walks you through a simple example of Fermi estimation of a deep learning model. Specifically, we will try to estimate the design parameters of a hypothetical GPT-5 with 1 trillion parameters. Along the way, we will touch upon various other items like the Landauer limit (because of course we will).</p>
<section id="gpt-like-agi" class="level2">
<h2 class="anchored" data-anchor-id="gpt-like-agi">GPT-like AGI</h2>
<p>Let’s get into the mood for Fermi estimation, by estimating the number of parameters necessary for achieving human-level Artificial General Intelligence, under two assumptions: that a GPT-like architecture is enough; that it requires only matching the human brain in the "numbers".</p>
<p>Let’s estimate how many layers the hypothetical GPT model should have. When prompted with a question, the first answer comes to mind in about a second, and it is generally a good one, if not the best. Perhaps slow deliberation is nothing but stringing together a long chain of snap judgments, guided by snap judgments about snap judgments.</p>
<p>The characteristic time-scale of a brain is 0.01 seconds – the fastest brainwave, gamma wave, is 100 Hz. This indicates that the brain takes on the order of 100 steps to make a snap decision – the "hundred-step-rule" of <a href="https://en.wikipedia.org/wiki/Jerome_A._Feldman">Jerome Feldman</a><span class="citation" data-cites="feldmanConnectionistModelsTheir1982">(<a href="#ref-feldmanConnectionistModelsTheir1982" role="doc-biblioref">Feldman and Ballard 1982</a>)</span>. This is suspiciously close to the depth of the largest model of GPT-3, which has 96 layers.</p>
<p>How many parameters would such a model require? The brain has <span class="math inline">\(10^{15}\)</span> synapses. It’s unclear how precise each synapse is, but one estimate states that the hippocampal synapse has a precision of about 5 bits <span class="citation" data-cites="bartoljrNanoconnectomicUpperBound2015">(<a href="#ref-bartoljrNanoconnectomicUpperBound2015" role="doc-biblioref">Bartol Jr et al. 2015</a>)</span>, which can be stored within a 16-bit floating point number, with room to spare.</p>
<p>Assuming that, we expect an AGI GPT to have <span class="math inline">\(10^{15}\)</span> parameters, or 1000× that of our hypothetical GPT-5.</p>
</section>
<section id="chinchilla-scaling-law" class="level2">
<h2 class="anchored" data-anchor-id="chinchilla-scaling-law">Chinchilla Scaling Law</h2>
<p>The paper "Training Compute-Optimal Large Language Models" <span class="citation" data-cites="hoffmannTrainingComputeOptimalLarge2022">(<a href="#ref-hoffmannTrainingComputeOptimalLarge2022" role="doc-biblioref">Hoffmann et al. 2022</a>)</span> reported a series of training runs on language models, trained by Google DeepMind researchers. Each training run is characterized by four numbers:</p>
<ul>
<li><span class="math inline">\(L\)</span>: the final loss (negative log-likelihood per token) achieved by the trained model.</li>
<li><span class="math inline">\(N\)</span>: the number of parameters in the model.</li>
<li><span class="math inline">\(D\)</span>: training dataset size, measured in tokens.</li>
<li><span class="math inline">\(C\)</span>: training compute cost, measured in FLOP.</li>
</ul>
<p>After training a few hundred models, they obtained a large dataset of <span class="math inline">\((L, N, D, C)\)</span>, and they fitted a statistical law of the form</p>
<p><span class="math display">\[L = \frac{A}{N^\alpha} + \frac{B}{D^{\beta}} + L_0,\]</span></p>
<p>where the parameters are</p>
<p><span class="math display">\[\alpha = 0.34, \beta = 0.28, A = 406.4, B = 410.7, L_0 = 1.69.\]</span></p>
<p>They also estimated that the cost of training compute <span class="math inline">\(C\)</span> is proportional to <span class="math inline">\(ND\)</span>. This is understandable, because each token must flow through the entire model and "hit" each parameter once, incurring a fixed number of floating point operations. They estimated that it takes 6 FLOPs per parameter per token. That is,</p>
<p><span class="math display">\[C = C_0 ND, \quad C_0 = 6\]</span></p>
<p>Given the assumptions, for each fixed computing budget <span class="math inline">\(C\)</span>, we can solve for the optimal <span class="math inline">\(D\)</span> and <span class="math inline">\(N\)</span>, which is usually referred to as "Chinchilla optimal" training:</p>
<p><span class="math display">\[\begin{cases}
        \min_{N, D} L = \frac{A}{N^\alpha} + \frac{B}{D^{\beta}} + L_0,\\
        \text{such that } C_0 ND = C.
\end{cases}\]</span></p>
<p>Solve the above equations symbolically to find <span class="math inline">\(N_{opt}, D_{opt}\)</span> as a function of <span class="math inline">\(C, C_0, \alpha, \beta, A, B\)</span>. Then, plug in the numerical values of the parameters, to find a numerical expression for <span class="math inline">\(N_{opt}, D_{opt}\)</span> as a function of <span class="math inline">\(C\)</span>.</p>
<details>
<summary>
Solution
</summary>
<p>Since <span class="math inline">\(C = C_0 ND\)</span>, we have <span class="math inline">\(N = \frac{C}{C_0 D}\)</span>. Plug it into <span class="math inline">\(\min_{N, D} L = \frac{A}{N^\alpha} + \frac{B}{D^{\beta}} + L_0\)</span>, we obtain</p>
<p><span class="math display">\[\min_{D} L = \frac{A}{(\frac{C}{C_0 D})^\alpha} + \frac{B}{D^{\beta}} + L_0.\]</span></p>
<p>Take derivative with respect to <span class="math inline">\(D\)</span> and set it to zero. We get an expression for <span class="math inline">\(D_{opt}\)</span>. Plug it back to <span class="math inline">\(C = C_0 ND\)</span>, we get an expression for <span class="math inline">\(D_{opt}\)</span>. These simplify to:</p>
<p><span class="math display">\[N_{o p t}(C)=G\left(\frac{C}{C_0}\right)^a, \quad D_{o p t}(C)=G^{-1}\left(\frac{C}{C_0}\right)^b, \quad \text { where } \quad G=\left(\frac{\alpha A}{\beta B}\right)^{\frac{1}{\alpha+\beta}}, a=\frac{\beta}{\alpha+\beta}, b=\frac{\alpha}{\alpha+\beta}.\]</span></p>
<p>Plugging in the numerical values, we get</p>
<span class="math display">\[\begin{cases}
        N_{opt}(C) = 0.6 \; C^{0.45} \\
        D_{opt}(C) = 0.3 \; C^{0.55} \\
        L_{opt}(C) = 1070 \; C^{-0.154} + 1.7
    \end{cases}
    \]</span>
</details>
<p>In the same paper, they also performed a <em>direct</em> statistical fitting, to find the optimal <span class="math inline">\(N, D\)</span> for a given <span class="math inline">\(C\)</span>, without going through the intermediate steps above. This gives a slightly different result (only slightly different – as you would know after solving the previous problem):</p>
<p><span class="math display">\[N_{opt}(C) = 0.1 C^{0.5}; \quad D_{opt}(C) = 1.7 C^{0.5}.\]</span></p>
<p>For the rest of the essay, we will use these equations as the Chinchilla scaling laws. Do not use the equations you derived for the previous problem.</p>
<p>Suppose we decide that our next AI should have 1 trillion (<span class="math inline">\(N = 10^{12}\)</span>) parameters, and we use Chinchilla scaling laws. How much compute would it cost to train, and how many tokens would its training dataset have?</p>
<details>
<summary>
Solution
</summary>
<span class="math inline">\(N = 0.1 \times C^{0.5} = 10^{12}\)</span>, so <span class="math inline">\(C= 10^{26}\)</span> FLOP, and <span class="math inline">\(D = 1.7 \times 10^{13}\)</span>, or 17 trillion tokens.
</details>
</section>
<section id="dataset-size" class="level2">
<h2 class="anchored" data-anchor-id="dataset-size">Dataset size</h2>
<p>Assuming each English word cost about 1.4 tokens, how many English words would 10 trillion tokens be? Assuming each page has 400 words, and each book has 300 pages, how many books is that? To put it into context, look up the size of Library of Congress, and <a href="https://en.wikipedia.org/wiki/Google_Books">Google Books</a>, and compare with the number we just calculated.</p>
<details>
<summary>
Solution
</summary>
<span class="math inline">\(10 / 1.4 = 7\)</span> trillion words. If each book has <span class="math inline">\(400 \times 300 = 0.12\)</span> million words, then that is 60 million books, if they were all in English.
</details>
<p>Since humans are kind of the same everywhere, book lengths should be kind of the same everywhere – information-dense languages would naturally have books with lower apparent word-count, but the same information (measured in bytes).</p>
</section>
<section id="memory-requirement" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="memory-requirement">Memory requirement</h2>
<p>Typically, a deep learning model has 16-bit floating point parameters. Modern systems sometimes use lower precision (e.g.&nbsp;8-bit) floating point numbers to save space, but generally it is necessary to use at least 16-bit during training, and the model is converted to lower precision after training ("post-training quantization").</p>
<p>Given that each parameter is a 16-bit floating point number, how much memory does it cost to store a model with 1 billion parameters? How about our hypothetical GPT-5, which has 1 trillion parameters? How many A100 GPU (VRAM = 40 GB) would be required to contain the full GPT-5 model?</p>
<details>
<summary>
Solution
</summary>
<p>1 billion parameters is 2 billion bytes, or 2 GB. Our hypothetical GPT-5 would take up 2 TB. It would take 50 A100 to contain it.</p>
Now, 50 A100 GPUs has a running cost of about 100 USD/hr, but not such a terrible problem, but then, during training, you would have to run an optimizer like Adam, which would need three floating point numbers per parameter (gradient, momentum, scale), instantly increasing the memory requirement to 200 A100 GPUs.
</details>
<section id="memory-cost" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="memory-cost">Memory cost</h3>
<div class="page-columns page-full"><p>This table<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> gives the price per megabyte of different storage technology, in price per megabyte (2010 dollars), up to 2018.</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;Source: <a href="https://cs61.seas.harvard.edu/site/2018/Storage2/">Storage 2: Cache model – CS 61 2018</a>.</p></li></div></div>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: right;">Year</th>
<th style="text-align: right;">Memory (DRAM)</th>
<th style="text-align: right;">Flash/SSD</th>
<th style="text-align: right;">Hard disk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">~1955</td>
<td style="text-align: right;">$411,000,000</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$6,230</td>
</tr>
<tr class="even">
<td style="text-align: right;">1970</td>
<td style="text-align: right;">$734,000.00</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$260.00</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1990</td>
<td style="text-align: right;">$148.20</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">$5.45</td>
</tr>
<tr class="even">
<td style="text-align: right;">2003</td>
<td style="text-align: right;">$0.09</td>
<td style="text-align: right;">$0.305</td>
<td style="text-align: right;">$0.00132</td>
</tr>
<tr class="odd">
<td style="text-align: right;">2010</td>
<td style="text-align: right;">$0.019</td>
<td style="text-align: right;">$0.00244</td>
<td style="text-align: right;">$0.000073</td>
</tr>
<tr class="even">
<td style="text-align: right;">2018</td>
<td style="text-align: right;">$0.0059</td>
<td style="text-align: right;">$0.00015</td>
<td style="text-align: right;">$0.000020</td>
</tr>
</tbody>
</table>
<p>The same costs <em>relative</em> to the cost of a hard disk in ~2018:</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: right;">Year</th>
<th style="text-align: right;">Memory</th>
<th style="text-align: right;">Flash/SSD</th>
<th style="text-align: right;">Hard disk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">~1955</td>
<td style="text-align: right;">20,500,000,000,000</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">312,000,000</td>
</tr>
<tr class="even">
<td style="text-align: right;">1970</td>
<td style="text-align: right;">36,700,000,000</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">13,000,000</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1990</td>
<td style="text-align: right;">7,400,000</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">270,000</td>
</tr>
<tr class="even">
<td style="text-align: right;">2003</td>
<td style="text-align: right;">4,100</td>
<td style="text-align: right;">15,200</td>
<td style="text-align: right;">6.6</td>
</tr>
<tr class="odd">
<td style="text-align: right;">2010</td>
<td style="text-align: right;">950</td>
<td style="text-align: right;">122</td>
<td style="text-align: right;">3.6</td>
</tr>
<tr class="even">
<td style="text-align: right;">2018</td>
<td style="text-align: right;">295</td>
<td style="text-align: right;">7.5</td>
<td style="text-align: right;">1</td>
</tr>
</tbody>
</table>
<p>Suppose long-term memory storage can last 1 year before being replaced. How much money does it cost per year to store a 1 trillion parameter model on an SSD, the most expensive form of long-term storage? How much money does it cost to store a 1 trillion parameter model on DRAM memory? Use 2018 prices.</p>
<details>
<summary>
Solution
</summary>
<p>SSD cost 0.00015 USD/MB in 2018, or 0.15 USD/GB. 1 trillion parameters cost 2000 GB of storage, or about 300 USD. So the total cost of storage is 300 USD/year, just a rounding error.</p>
<p>In contrast, DRAM cost 0.006 USD/MB, which is 40x that of SSD, so the total cost is 12000 USD.</p>
<p>Now, compared with the cost of A100 GPU themselves? It takes about 50 A100 GPU to run the model, and each would cost about 20,000 USD. So the cost of long-term memory is essentially zero, and the cost of DRAM is about <span class="math inline">\(\frac{12000}{20000\times 50} = 1\%\)</span> of the total cost of GPU.</p>
So what is the limit? The memory bandwidth, which we will see in the next question.
</details>
</section>
<section id="memory-bandwidth-and-latency" class="level3">
<h3 class="anchored" data-anchor-id="memory-bandwidth-and-latency">Memory bandwidth and latency</h3>
<p>While the memory itself is cheap, moving the data requires expensive high-bandwidth wiring. Indeed, the memory bandwidth between the DRAM (or "VRAM" for "Video RAM") and the little processors on the GPU is a main bottleneck on how good the GPU can perform.</p>
<p>During a single forward pass of a model, the parameters of the model are loaded from the DRAM of the GPU into the fast cache memories, then pushed through the thousands of computing processors on the GPU.</p>
<p>A100 GPU has a memory bandwidth of 1.6 TB/s.</p>
<p>What is the minimal latency, in seconds, for the GPU to perform a single forward pass through our hypothetical GPT-5 model with 1 trillion parameters? How many tokens can it output (autoregressively) in one minute? How about GPT-3 (175 billion parameters)?</p>
<p>Since we are just trying to compute an order-of-magnitude estimate, let’s assume for the problem that the model fits onto a single DRAM on a single GPU. You can also ignore the need to read/write model activations and optimizer states.</p>
<details>
<summary>
Solution
</summary>
<p>Since the model takes up 2 TB of memory, it would take at least 1.3 seconds to even load the model from DRAM, during a single forward pass.</p>
<p>Autoregression means that the next token cannot be generated until the previous one is already generated, so the model can only generate one token per 1.3 seconds, or 46 tokens per minute. A Chat-GPT5 would be a very slow talker!</p>
<p>However, it <em>can</em> run in batch mode. So for example, it might be able to run 1 million conversations in parallel, outputting one token for all conversations every 2 seconds. Latency is the hard limit, but throughput is not.</p>
<p>GPT-3 with 175 billion parameters would run 1000/175 times faster, at 262 tokens per minute, about the speed of a fast human talker. This corresponds well with the general impression that ChatGPT types about 200 words a minute.</p>
</details>
</section>
<section id="batch-inference" class="level3">
<h3 class="anchored" data-anchor-id="batch-inference">Batch inference</h3>
<p>There are some ways to improve latency. For example, the model can be parallelized over several GPUs, which effectively increases the memory bandwidth. For example, "tensor parallelism" splits each layer into several GPUs.</p>
<p>There is also "pipeline parallelism", which splits the model into layers. The first few layers go into one GPU, the next few go into another, and so on. This does NOT decrease latency, but it does increase throughput.</p>
<p>The fundamental bottleneck in an autoregressive model like GPT is that, by design, they have to start a sentence without knowing how it will end. That is, they have to generate the first token before generating the next one, and so on. This can never be parallelized (except by egregious hacks like speculative decoding).</p>
<p>One reason Transformers dominated over RNN is that training and inferring an RNN <em>both</em> must be done one-token-at-a-time. For Transformers, training can be done in parallel over the entire string. Inferring however still cannot be parallelized.</p>
<p>Parallelization tends to be a deeply troublesome business – parallel programming is generally deeply troublesome. However, there is a general trick to improve throughput: batch mode. For example, GPT-5 might be able to run 1 million conversations in parallel, outputting one token for all conversations per forward pass. This trick works until the batch size is so large that the activations due to tokens takes up about as much memory bandwidth as the model parameters.</p>
<p>Concretely, we can estimate what is a good batch size for GPT-3 175B. It has 96 attention layers, each with 96×128-dimension heads. It is typically run with 16-bit precision floating point numbers. For a single token, how many megabytes would all the floating point activations cost?</p>
<details>
<summary>
Solution
</summary>
A single token would cost <span class="math inline">\(96 \times 96 \times 128\)</span> floating point activations, or about 2.4 MB.
</details>
<p>The model itself has 175 billion 16-bit floating point parameters, taking up about 350 GB. How many tokens do we need to put into a batch, before the activations occupy the same amount of memory as the model parameters?</p>
<details>
<summary>
Solution
</summary>
<p>In order for the activations of tokens to occupy the same memory as the GPT-3 parameters itself, we need about <span class="math inline">\(\frac{350 \;\mathrm{GB}}{2.4 \;\mathrm{MB}} = 0.15 \text{million tokens}\)</span>.</p>
<p>If we count the optimizer states for the model during training, then GPT-3 takes up <span class="math inline">\(4 \times 350 \;\mathrm{GB} = 1.4 \;\mathrm{TB}\)</span>, and so we need about 0.6 million tokens.</p>
This explains why during training, the batch sizes of the largest models typically are on the order of 1 million tokens. It also shows why there is a natural drive towards scale – only the largest companies can expect to regularly run 0.2 million chats simultaneously.
</details>
</section>
</section>
<section id="training-cost" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="training-cost">Training cost</h2>
<p>How much money does compute cost? We can work through an example using the current standard computing hardware: Nvidia A100 GPU (40 GB VRAM version).</p>
<p>The most important specifications are:</p>
<ul>
<li>Unit price: 15000 USD.</li>
<li>Rental price: 2 USD/hr.</li>
<li>Speed: 0.3 petaFLOP/s = 3E14 FLOP/s.</li>
<li>Power: 0.3 kW.</li>
<li>Memory bandwidth: 1600 GB/s.</li>
</ul>
<p>In the literature about the largest AI models, the training cost is often reported in units of “petaFLOP-day”, which is equal to 1 petaFLOP/second x 1 day. How many FLOP is 1 petaFLOP-day? What is the equivalent number of A100-hour? If we were to buy 1 petaFLOP-day of compute with rented A100 GPU, how much would it cost?</p>
<details>
<summary>
Solution
</summary>
<p>1 petaFLOP-day = 1 petaFLOP/second x 1 day = 1E15 FLOP/s x 8.64E4 s = 8.64E19 FLOP.</p>
Since 1 A100 can run at 0.3 petaFLOP/s, getting 1 petaFLOP-day requires us to run 1/0.3 A100 for 1 day, or 80 A100-hours. At the price of 2 USD/hr, it would cost 160 USD.
</details>
<p>The largest model of GPT-3 cost 3640 petaFLOP-days to train (according to <a href="https://arxiv.org/pdf/2005.14165v4.pdf#page=46">Table D.1 of the report</a>). How much would it cost if it were trained with A100? How much money does it cost to train our hypothetical GPT-5?</p>
<details>
<summary>
Solution
</summary>
<p>2E25 FLOP = 0.2 amount of GPT-5 = 17 million A100-hours = 33 million USD.</p>
<p>And accounting for the utilization rate of 30%, that would give us 110 million USD.</p>
<p>Oh, and if you want some kind of official confirmation? <a href="https://web.archive.org/web/20230417111136/https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/">OpenAI’s CEO Says the Age of Giant AI Models Is Already Over | WIRED</a></p>
<blockquote class="blockquote">
<p>At the MIT event, Altman was asked if training GPT-4 cost $100 million; he replied, “It’s more than that.”</p>
</blockquote>
</details>
<div class="page-columns page-full"><p>In reality, the GPU cannot be worked to their full speed, and we only use about 30% of its theoretical peak FLOP/s (so for example, for A100, we only get 0.1 petaFLOP/s, instead of 0.3 petaFLOP/s).<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> For this question, we assume that the utilization rate is 100%.</p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;The utilization rate of 30% is <a href="https://epochai.org/blog/estimating-training-compute">according to EpochAI</a>.</p></li></div></div>
<p>Also, we are assuming the training happens in one go, without hardware failures, divergences, and other issues requiring restart at a checkpoint. There is not much published data from large-scale training, but the OPT-175B training run (described later) took 3 months to complete, but would have taken only 33 days if there were no need for the many restarts. This suggests that the restarts would increase the computing cost by 2× to 3×.</p>
<div class="page-columns page-full"><p>For context, here are the costs of <em>development</em> of various items<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>:</p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;Sorry, not adjusted for inflation to the same year, but they are roughly in the range of 2000 – 2020 USD.</p></li></div></div>
<ul>
<li><a href="https://web.archive.org/web/20150213101032/http://archive.wired.com/gadgets/wireless/magazine/16-02/ff_iphone?currentPage=all">iPhone 1: 150 million USD</a>.</li>
<li><a href="https://www.mckinsey.com/industries/industrials-and-electronics/our-insights/semiconductor-design-and-manufacturing-achieving-leading-edge-capabilities">A typical 5 nm chip: 0.5 billion USD</a>.</li>
<li>Airbus A380: 18 billion USD. <span class="citation" data-cites="bowenEconomicGeographyAir2010">(<a href="#ref-bowenEconomicGeographyAir2010" role="doc-biblioref">Bowen 2010</a>, Table 4.3)</span></li>
<li><a href="http://politics.people.com.cn/n/2013/0607/c1001-21776413.html">Three Gorges Dam: 250 billion CNY, or about 30 billion USD</a>.</li>
<li><a href="https://en.wikipedia.org/wiki/Manhattan_Project">Manhattan Project: 24 billion USD (2021 level)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Apollo_program">Apollo Program: 178 billion USD (2022 level)</a></li>
</ul>
<p>Comment on the cost of our hypothetical GPT-5. Is it on the order of a large commercial actor like Google, or a state actor like China?</p>
<details>
<summary>
Solution
</summary>
The cost is on the order of 1 billion USD, which is roughly the largest projects undertaken by private corporations. Larger models, as those contemplated by AGI projects in Google, OpenAI, etc, have historically been undertaken by state actors only.
</details>
<p>Here is another way to estimate. Building a large AI model, according to accountancy, would be investment into a production machine, and thus part of “capital expenditure” (“CapEx”). We can find out what proportion is 1 billion USD in their total CapEx. I looked it up <a href="https://finbox.com/NASDAQGS:GOOG.L/explorer/capex/">here</a>.</p>
<ul>
<li>During the 2020–2022, Microsoft has yearly CapEx on average 25 billion USD.</li>
<li>Google has about 25 billion USD.</li>
<li>Meta, 20.</li>
<li>Amazon, 63.</li>
</ul>
<p>In short, training something like GPT-5 would cost a lot, on the order of 5% of total CapEx, even by the standards of the largest companies. It can probably scale up by another factor of 10× if they really push it, but not more.</p>
<p>In order to train even larger AI models, those AI models <em>must</em> enter production. They must become a productive member of society, otherwise the company would nott have the money to train them.</p>
<p><a href="https://news.microsoft.com/source/features/ai/openai-azure-supercomputer/">Microsoft announces new supercomputer, lays out vision for future AI work (2020)</a>:</p>
<blockquote class="blockquote">
<p>The supercomputer developed for OpenAI is a single system with more than 285,000 CPU cores, 10,000 GPUs and 400 gigabits per second of network connectivity for each GPU server. Compared with other machines listed on the TOP500 supercomputers in the world, it ranks in the top five, Microsoft says.</p>
</blockquote>
<p>The largest companies, like Microsoft have GPU on the order of 10000 A100. What is the wall clock hour of training GPT-5, assuming you have 10000 A100, perfect utilization, and no interruptions?</p>
<details>
<summary>
Solution
</summary>
83 million hours / 10000 = 350 days. Almost exactly 1 year.
</details>
<section id="the-difficulty-of-large-scale-training" class="level3">
<h3 class="anchored" data-anchor-id="the-difficulty-of-large-scale-training">The difficulty of large-scale training</h3>
<p>Large models do end up diverging many times during training runs, requiring restarts. Sometimes the divergence is so bad that the whole training run must be started from scratch. Sometimes the convergence is too slow, requiring fixes to the optimizer and learning rate schedules, etc. All together, we should expect the failures, restarts, deadends… to triple the cost at least, to ~1 billion USD.</p>
<p>It is not easy to find "stories from the trenches" for actually training large-scale neural networks that really show this, but thanks to Meta, we have one. In 2021, a team of 5 engineers from Meta trained a LLM with 175 billion parameters, in 3 months, using 1024 80GB A100 GPUs from. Excluding all the divergences, hardware failures, and other issues that caused lost progress, the final model would have taken about 33 days of continuous training.</p>
<p>They have kept journals during their training. This is now published at <a href="https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles#chronicles-of-opt-development">metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq · GitHub</a>. You can see how difficult it is to train a large model. Selected quotes:</p>
<blockquote class="blockquote">
<p>These notes cover ~90 restarts over the course of training the lineage of this current model (experiments 12.xx).</p>
</blockquote>
<blockquote class="blockquote">
<p>Found issues with the new dataset where perplexity was unreasonably low… After applying as much regex-ing as we could to salvage the dataset, we relaunched another set of experiments to test LPE (experiments 20-29) on the new dataset. We didn’t have time to retrain a new BPE on the final dataset, so we fell back to using the GPT-2 BPE.</p>
</blockquote>
<blockquote class="blockquote">
<p>From experiment 11.4 onward, we saw grad norm explosions / loss explosions / nans after a couple hundred updates after each restart, along with extremely unstable loss scales that would drop to the point of massively underflowing. We started taking more and more drastic actions then, starting with increasing weight decay to 0.1, lowering Adam beta2 to 0.95, lowering LR again, until finally by experiment 11.10 we hot-swapped in ReLU and also switched to a more stable MHA calculation (noting that the x**3 term in GeLU might be a source of instability with FP16).</p>
</blockquote>
<p><img src="figure/run11.jpeg" class="img-fluid"></p>
<blockquote class="blockquote">
<p>On November 11, we started training our 12.00 experiment with all of these changes, and since then, the only restarts we’ve had to make were all related to hardware issues (missing GPUs on instances, training randomly hanging after including a new node, ECC errors, partial checkpoint upload after hardware error, CUDA errors, NCCL errors, etc.).</p>
</blockquote>
<blockquote class="blockquote">
<p>Replacement through the cloud interface can take hours for a single machine, and we started finding that more often than not we would end up getting the same bad machine again.</p>
</blockquote>
<blockquote class="blockquote">
<p>There were also issues with blob store when downloading 1.6TB of a single model checkpoint (992 files, each ~1.7GB) on restarts, at which point the downloads themselves would start hanging nondeterministically, which then delayed training recovery even further.</p>
</blockquote>
<blockquote class="blockquote">
<p>We managed to hit our top three record long runs of the experiment these past two weeks, lasting 1.5, 2.8, and 2 days each! If we were to look at only the runs that have contributed to pushing training further and plot training perplexity against wall clock time, we get the following [The breaks are due to the Thanksgiving holiday]:</p>
</blockquote>
<p><img src="figure/run12_56_perc.jpeg" class="img-fluid"></p>
</section>
</section>
<section id="inference-cost" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="inference-cost">Inference cost</h2>
<p>Inference cost a lot less money than training, but it’s still a substantial cost. For Transformer language models, it costs about 2 FLOPs per parameter to infer on one token.</p>
<p>Given that GPT-3.5 has 175 billion parameters, how many FLOPs would it take to infer on 1 million tokens? How much money would it cost if it were run with A100?</p>
<details>
<summary>
Solution
</summary>
<p><span class="math inline">\(175 \;\mathrm{billion} \times 1 \;\mathrm{million} \times 2 = 4\times 10^{17} \;\mathrm{FLOPs}\)</span>. Now one A100-hour is <span class="math inline">\(8.64\times 10^{19} \;\mathrm{FLOPs}\)</span>, so that is 1/200 A100-hour, or about 0.01 USD.</p>
<p><a href="https://openai.com/pricing">The price offered by OpenAI</a> is 2 USD per 1 million tokens, so it’s a very profitable business… but see next question.</p>
</details>
<p><a href="https://openai.com/pricing">The price offered by OpenAI</a> is 2 USD per 1 million tokens. Assuming that GPT-3.5 cost 10 million USD to develop and train, how many tokens must be sold, just to recoup the cost? Assuming each English word cost about 1.4 tokens, and each essay is 1000 words, how many essays would it be equivalent to?</p>
<details>
<summary>
Solution
</summary>
<p>Since the cost is almost negligible (1/200 of the revenue), we can pretend it’s all profit. So, the profit is 2 USD per 1 million tokens. We need to recoup 10 million USD, so we need <span class="math inline">\(10 \;\mathrm{million} / 2 \times 1 \;\mathrm{million} = 5\times 10^{12} \;\mathrm{tokens}\)</span>, or 4 billion essays.</p>
About one essay per person on earth, or 10 essays per person in America… is that too much to ask?
</details>
<p>Moore’s law applies to both CPU and GPU. From 2006 to 2021, GPU FLOP/s per dollar has been doubling every 2.5 years. We can fairly well assume that the price of petaFLOP-day also decreases at the same rate.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://epochai.org/blog/trends-in-gpu-price-performance"><img src="figure/epoch_empirical_gpu_flops_per_dollar.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="page-columns page-full"><p>Assuming Moore’s law continues to hold indefinitely, and that there will be no algorithmic progress. When would GPT-3 become as cheap as a typical hobbyist project, say, 1000 USD?<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;Since a 2006 GPU and a 2020 GPU both have the same lifespan (1 – 4 years usually), and their cost of electricity is only a small proportion of their running expense, the price of GPU FLOP/s per dollar at the point of purchase is roughly proportional to the rental rate of GPU.</p></li></div></div>
<details>
<summary>
Solution
</summary>
GPT-3 cost 6 million USD to train. For it to cost just 1000 USD, we need to drop its price 6000-fold. Now, <span class="math inline">\(\log_2(6000) \times 2.5\; \mathrm{year} = 30 \; \mathrm{year}\)</span>. So it would be around 2050.
</details>
</section>
<section id="energetic-cost" class="level2">
<h2 class="anchored" data-anchor-id="energetic-cost">Energetic cost</h2>
<p>The Landauer limit states that the cost of erasing one bit of information is <span class="math inline">\(E = k_B T \ln 2\)</span>, where <span class="math inline">\(k_B\)</span> is the Boltzmann constant, and <span class="math inline">\(T\)</span> is the temperature of the computing machinery. At room temperature, <span class="math inline">\(T = 300 K\)</span>, giving us <span class="math inline">\(E = 3\times 10^{-21} J\)</span>.</p>
<p>Now, one FLOP is a floating point operation, meaning that you start with two 32-bit objects and end up with a single 32-bit object. You start with 64 bits and end up with just 32 bits, and so you lose 32 bits. So by the Landauer limit, the minimal energetic cost is <span class="math inline">\(32 k_B T \ln 2\)</span>.</p>
<p>Given this, what is the minimal energy required for performing one FLOP? What is the minimal power (in Watts) required to perform 300 TFLOP/s, as in A100 GPU? Compared this to the actual value of 300 Watts.</p>
<details>
<summary>
Solution
</summary>
<p>The energy per FLOP is <span class="math inline">\(E_{FLOP} = 32 \times 3\times 10^{-21} J = 10^{-19} J\)</span>. At 300 TFLOP/s, we need <span class="math inline">\(P_{A100} = 3\times 10^{14} E_{FLOP}/s = 3\times 10^{-5}W\)</span>. The actual value of 300 Watts is 10 million times more than the theoretical minimum.</p>
There is still <a href="https://en.wikipedia.org/wiki/There's_Plenty_of_Room_at_the_Bottom">plenty of room at the bottom</a>!
</details>
<p>For context, the equivalent FLOP/s for the human brain is controversial, but a brief scan through <a href="https://aiimpacts.org/brain-performance-in-flops/">the review article</a> says that it should be about 1E18 FLOP/s.&nbsp;The energetic power of the brain is well-known: about 30 Watts. This means the brain is about 30000x more energy-efficient than A100 GPU, but still 300x less efficient than the theoretical minimum.</p>
<section id="the-lowest-possible-power-for-life" class="level3">
<h3 class="anchored" data-anchor-id="the-lowest-possible-power-for-life">The lowest possible power for life</h3>
<p>The slowest metabolism found on earth (so far) is in <a href="https://www.quantamagazine.org/zombie-microbes-redefine-lifes-energy-limits-20200812/">microbes living below deep ocean surface</a>. They had to survive on the energy of “marine snow” falling to the surface, then slowly diffusing through the subsurface mud. Their power consumption is estimated at… <span class="math inline">\(10^{-21} W\)</span>. Since their temperature is about the temperature of the ocean water, which is liquid, their temperature is still about <span class="math inline">\(T = 273 K\)</span>, and so the Landauer limit is still about <span class="math inline">\(3\times 10^{-21} J\)</span>. This shows that they can lose at most 500 bits every day.</p>
<p>Most of the operations must be spent on simply repairing biomolecules, since repairing is effectively a 1-bit-losing operation: you start with a molecule that could be “damaged” or “fine”, and end up with a molecule that is “fine”, losing 1 bit. In fact, there are usually many possible ways to be “damaged”, so you would lose several bits.</p>
<p>For example, suppose you have a base-pair-mutation in one copy of the DNA. You want to fix it by checking with the other copies. This then means you start with 4 possible states (ACGT) and end up with just 1 state, losing 2 bits in the process.</p>
</section>
</section>
<section id="environmental-cost" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="environmental-cost">Environmental cost</h2>
<p>According to “Carbon emissions and large neural network training”<span class="citation" data-cites="pattersonCarbonEmissionsLarge2021">(<a href="#ref-pattersonCarbonEmissionsLarge2021" role="doc-biblioref">Patterson et al. 2021</a>)</span>, the carbon emission of training GPT-3 is 552 tCO<sub>2</sub>. According to a <a href="https://www.reuters.com/business/cop/carbon-needs-cost-least-100tonne-now-reach-net-zero-by-2050-2021-10-25/">2021 poll of climate economists</a>, 1 tCO<sub>2</sub> emission should cost somewhere between 50 and 250 USD. Let’s take their geometric average of 112 USD.</p>
<p>If we add all the tCO<sub>2</sub> cost to the training of GPT-3, how much more expensive would it be? Compare that with its A100-GPU cost of training.</p>
<details>
<summary>
Solution
</summary>
<p><span class="math inline">\(112 \times 552 = 62,000 \;\mathrm{USD}\)</span>.</p>
<p>Previously, we calculated the GPU cost of training to be around 6 million USD. So fully carbon-pricing the emission would make it 1% more expensive.</p>
Generally, adding in the tCO<sub>2</sub> cost of training the largest models increase the cost by about 1 to 2 % (I have computed it in several ways). Since the global GDP increases by about 2% a year, this shows that fully accounting for the carbon cost of AI would delay it by perhaps 1 year.
</details>
<details>
<summary>
Side note for economics students
</summary>
<p>You might argue that fully carbon-taxing the global economy would raise not just electricity prices, but also the price of production, etc. For example, the global shipping industry uses fuel that has a high carbon emission.</p>
<p>However, if the global shipping industry were to fully add carbon taxing into the fuel cost, then GPU prices would rise a little, but food prices would rise <em>a lot</em>. Why? Simply put, the shipping cost of a GPU, even if we were to take account of all the trips its components have made around the world, is still small. After all, one A100 GPU is just 1.7 kg, but sells for 15000 USD, so a tonne of A100 GPU can sell for 9 million USD.</p>
<p>To put it into perspective: the price of US maize is about 200 USD/tonne. So if shipping cost were to increase so much as to double the price of maize, it would only increase price of A100 by 1/45000.</p>
<p>Even if we were to assume that producing an A100, being a commodity with a globally scattered supply chain, means shipping its components around the world 1000 times, the total shipping cost of A100 would only be increased by 1/45 = 2.2%.</p>
<p>In other words, properly pricing carbon emissions would only increase the price of AI by up to 5%, even as it doubles the price of food. AI is already green (within 5%), but food is far from green. That’s even before we get to the subsidies! We don’t have GPU subsidies, but we certainly have corn subsidies…</p>
In this regard, it’s not “Green AI” that is important, but Green Corn, Green Timber, and all the other bulk commodities.
</details>
<p>To put the number in another context, compare it with some typical American food. According to <a href="https://ourworldindata.org/carbon-footprint-food-methane">Our World in Data</a>, it cost about 50 kg of CO<sub>2</sub> emission per 1 kg of beef.</p>
<p>Also, <a href="https://web.archive.org/web/20231006035327/https://farmdocdaily.illinois.edu/2021/05/an-overview-of-meat-consumption-in-the-united-states.html">an average American person (<em>not</em> household) consumed 38 kg of beef in 2020</a>.</p>
<p>Compare the CO<sub>2</sub> emission of GPT-3 and CO<sub>2</sub> emission from beef consumption. Assuming each burger ("quarter pounder") contains 1/4 pound (113 grams) of beef. How many burgers would be equivalent to the CO<sub>2</sub> emission of GPT-3?</p>
<details>
<summary>
Solution
</summary>
<p>113 grams of beef emits about 5.6 kg of CO<sub>2</sub>, so GPT-3 emits about (552 ton)/(5.6 kg) = 90,000 burgers. It is about the same amount as 250 people eating one burger everyday, for a whole year.</p>
38 kg of beef gives about 2 tCO<sub>2</sub> emission. So the emission of GPT-3 is equal to the emission due to beef consumption of 276 American persons.
</details>
<p>I think this strongly argues against the conclusion from <span class="citation" data-cites="pattersonCarbonEmissionsLarge2021">(<a href="#ref-pattersonCarbonEmissionsLarge2021" role="doc-biblioref">Patterson et al. 2021</a>)</span>:</p>
<blockquote class="blockquote">
<p>To help reduce the carbon footprint of ML, we believe energy usage and CO<sub>2</sub>e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark… We believe such new incentives could lead to a virtuous cycle where ML practitioners compete to increase accuracy while lowering energy consumption and CO<sub>2</sub>e that could bend the curve of ML carbon footprint growth for computationally intensive NLP models.</p>
</blockquote>
<p>One, the carbon footprint of ML is already minimal, compared to the cost of just making all the GPUs.</p>
<div class="page-columns page-full"><p>Two, accounting for CO<sub>2</sub> is a dreadfully boring business,<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> and should properly be done by carbon taxing by the public officials. The ML practitioners are not climate economists or accountants, and they should not be burdened by accounting for climate change. If the price is right, then they simply need to budget how much money they want to spend on the training run, and the market would optimize the rest (including optimizing the right level of climate change<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>).</p><div class="no-row-height column-margin column-container"><li id="fn5"><p><sup>5</sup>&nbsp;If you don’t believe me, try reading <span class="citation" data-cites="pattersonCarbonEmissionsLarge2021">(<a href="#ref-pattersonCarbonEmissionsLarge2021" role="doc-biblioref">Patterson et al. 2021</a>)</span>.</p></li><li id="fn6"><p><sup>6</sup>&nbsp;The right level of climate change is not "none", but rather "when the marginal cost equals marginal benefit". This might sound controversial, but it is just introductory economics.</p>
<blockquote class="blockquote">
<p><span class="citation" data-cites="krugmanRicardoDifficultIdea2002">(<a href="#ref-krugmanRicardoDifficultIdea2002" role="doc-biblioref">Krugman 2002</a>)</span> Luckily, at this point the orthodoxy of the academic economists is very much a minority position among intellectuals in general; one can seem to be a courageous maverick, boldly challenging the powers that be, by reciting the contents of a standard textbook.</p>
</blockquote>
</li></div></div>
<p>In one sentence: <strong>There need be no new incentive other than the profit motive.</strong></p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-bartoljrNanoconnectomicUpperBound2015" class="csl-entry" role="listitem">
Bartol Jr, Thomas M., Cailey Bromer, Justin Kinney, Michael A. Chirillo, Jennifer N. Bourne, Kristen M. Harris, and Terrence J. Sejnowski. 2015. <span>“Nanoconnectomic Upper Bound on the Variability of Synaptic Plasticity.”</span> <em>Elife</em> 4: e10778.
</div>
<div id="ref-bowenEconomicGeographyAir2010" class="csl-entry" role="listitem">
Bowen, John T. 2010. <em>The Economic Geography of Air Transportation: Space, Time, and the Freedom of the Sky</em>. <span>Routledge</span>.
</div>
<div id="ref-feldmanConnectionistModelsTheir1982" class="csl-entry" role="listitem">
Feldman, Jerome A., and Dana H. Ballard. 1982. <span>“Connectionist Models and Their Properties.”</span> <em>Cognitive Science</em> 6 (3): 205–54. <a href="https://doi.org/10.1207/s15516709cog0603_1">https://doi.org/10.1207/s15516709cog0603_1</a>.
</div>
<div id="ref-hoffmannTrainingComputeOptimalLarge2022" class="csl-entry" role="listitem">
Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. <span>“Training <span>Compute-Optimal Large Language Models</span>.”</span> <span>arXiv</span>. <a href="https://doi.org/10.48550/arXiv.2203.15556">https://doi.org/10.48550/arXiv.2203.15556</a>.
</div>
<div id="ref-krugmanRicardoDifficultIdea2002" class="csl-entry" role="listitem">
Krugman, Paul. 2002. <span>“Ricardo’s Difficult Idea: Why Intellectuals Don’t Understand Comparative Advantage.”</span> In <em>The Economics and Politics of International Trade</em>, 40–54. <span>Routledge</span>.
</div>
<div id="ref-pattersonCarbonEmissionsLarge2021" class="csl-entry" role="listitem">
Patterson, David, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. <span>“Carbon <span>Emissions</span> and <span>Large Neural Network Training</span>.”</span> <span>arXiv</span>. <a href="https://arxiv.org/abs/2104.10350">https://arxiv.org/abs/2104.10350</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left"><span class="faux-block">Everything ©<a href="https://www.apache.org/licenses/LICENSE-2.0.html">Apache License 2.0</a></span></div>   
    <div class="nav-footer-center"><span class="faux-block">Yuxi on the Wired</span></div>
    <div class="nav-footer-right"><span class="faux-block"><a href="../../../sitemap.xml">sitemap</a></span></div>
  </div>
</footer>



</body></html>