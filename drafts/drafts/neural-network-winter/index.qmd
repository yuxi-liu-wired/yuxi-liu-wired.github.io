---
title: "The Neural Network Winter"
author: "Yuxi Liu"
date: "2023-12-21"
date-modified: "2023-12-21"

categories: [AI, economics, history]
format:
    html:
        toc: true

description: "What really killed off the neural networks?"
# image: "figure/"

status: "wip"
confidence: "likely"
importance: 10
---

## The enigma of Marvin Minsky

In an 1993 interview, Robert Hecht-Nielsen described how Minsky stood in relation with the neural network community [@rosenfeldTalkingNetsOral2000, pages 303-305]:

> Minsky had gone to the same New York "science" high school as Frank Rosenblatt, a Cornell psychology Ph.D. whose "perceptron" neural network pattern recognition machine was receiving significant media attention. The wall-to-wall media coverage of Rosenblatt and his machine irked Minsky. One reason was that although Rosenblatt's training was in "soft science," his perceptron work was quite mathematical and quite sound—turf that Minsky, with his "hard science" Princeton mathematics Ph.D., didn't feel Rosenblatt belonged on. Perhaps an even greater problem was the fact that the heart of the perceptron machine was a clever motor-driven potentiometer adaptive element that had been pioneered in the world's first neurocomputer, the "SNARC", which had been designed and built by Minsky several years earlier! In some ways, Minsky's early career was like that of Darth Vader. He started out as one of the earliest pioneers in neural networks but was then turned to the dark side of the force (AI) and became the strongest and most effective foe of his original community. This view of his career history is not unknown to him. When he was invited to give the keynote address at a large neural network conference in the late 1980s to an absolutely rapt audience, he began with the words: "I am not the Devil!"

However, it appears he had changed his mind later. As recounted by Terence Sejnowski in [@sejnowskiDeepLearningRevolution2018, pages 256--258]:

> I was invited to attend the 2006 Dartmouth Artificial Intelligence Conference, “AI\@50,” a look back at the seminal 1956 Summer Research Project on artificial intelligence held at Dartmouth and a look forward to the future of artificial intelligence. ... These success stories had a common trajectory. In the past, computers were slow and only able to explore toy models with just a few parameters. But these toy models generalized poorly to real-world data. When abundant data were available and computers were much faster, it became possible to create more complex statistical models and to extract more features and relationships between the features.
> 
> In his summary talk at the end of the conference \[The AI\@50 conference (2006)\], Marvin Minsky started out by saying how disappointed he was both by the talks and by where AI was going. He explained why: “You’re not working on the problem of general intelligence. You’re just working on applications." ...
> 
> There was a banquet on the last day of AI\@50. At the end of the dinner, the five returning members of the 1956 Dartmouth Summer Research Project on Artificial Intelligence made brief remarks about the conference and the future of AI. In the question and answer period, I stood up and, turning to Minsky, said: “There is a belief in the neural network community that you are the devil who was responsible for the neural network winter in the 1970s. Are you the devil?” Minsky launched into a tirade about how we didn’t understand the mathematical limitations of our networks. I interrupted him—“Dr. Minsky, I asked you a yes or no question. Are you, or are you not, the devil?” He hesitated for a moment, then shouted out, “Yes, I am the devil!”

What are we to make of the enigma of Minsky? Was he the devil, or was he not the devil? Who

### The education of Minsky

SNARC, PhD thesis

@sec-mcculloch-pitts

### Meeting Seymour Papert

The book is one of those things that have passed into the stuff of legend: often quoted, rarely read. However, for its historical importance, I have actually read the book. To save you the trouble of reading the book, I will describe it in @sec-perceptron-book so you don't have to.

### Reading between the lines

the literal lines vs what is written between the lines. Cite Papert's 1980s paper about "alternative ways of knowing" -- a brief description of how he and others tried to bring postmodernism and social justice to hard sciences, and then describe how it all blew up in the 1990s Science Wars and the Sokal affair.


## What does the *Perceptron* book really say? {#sec-perceptron-book}

### Mathematical preliminaries

Definitions

### Some mathematical results

Chapter X: topological computing

### Non-mathematical arguments

Nobody can dispute with the mathematical results. However, the significance of the book does not hinge on the mathematical results, but rather the purported practical implications of such results.

Speaking on official record, Marvin Minsky insisted that he did not set out to kill neural networks, only putting it back to its proper place by showing what they cannot do. Restricted to the context of 1960s neural networks, it was an accurate assessment. Without backpropagation, neural network researchers could not train more than 2 layers, and most often they limited themselves to a single layer of trainable parameters, though they experimented with hand designed or random layers in front and behind the trainable layer. [Rosenblatt empirically investigated](##sec-rosenblatt) learning rules for two trainable layers in a four-layered neural network as early as 1962, but backpropagation only became known to most of neural network researchers in the 1980s.

During the 1980s, as neural networks rose again into prominence, Seymour Papert and Marvin Minsky got together to get the book republished with a new chapter specifically used to address the new neural network.

In the same year, Seymour Papert wrote a solo paper [@TODO One AI or many? (Papert, 1988)], which shows that his opinion is the same as that of Minsky.

The Society of Mind hypothesis is the exact opposite idea as "One big net for everything" [@TODO]. Instead of one big neural network for everything (Schmidhuber's hypothesis), or $\sim 6$ neural networks for everything (my hypothesis), Marvin Minsky and Seymour Papert argued for thousands of neural networks for everything.

[Insert quotes here]

In short, they believed that the brain is a network of large modules, each module is a network of smaller modules, and so on, until we bottom out to the simplest modules each consisting of perhaps tens of thousands of neurons. They did not give precise numerical estimates for how many -- it would be imprudent to -- but from my impression of reading The Society of Mind, they believed it had to have at least a thousand, arranged in at least 4 levels deep.

The agenda, once understood, unmasks the moral conviction behind their theoretical arguments.

Why do they argue vigorously against the possibility of scaling up more than two layers? They argued without mathematical justification, because it is extremely difficult to say anything generic about deep neural networks. After all, two layers is enough for approximating any function, so to say anything interesting about such networks require carefully restricting its architecture. The mathematical difficulty was severe enough that even now we do not have a good theory of deep learning. Why then, without justification, do they still vigorously argue against deep learning?

> [intuition that it's a sterile extension]

They had to, because they had to argue that generic deep learning does not scale.

Why?

Because, if generic deep learning scales up to arbitrary sizes, then the whole basis of a society of mind collapses. The brain would become one big network, not thousands of tiny networks.

Minsky and Papert admitted that neural networks are computationally universal. It is foolish to deny it, as even [@mccullochLogicalCalculusIdeas1943] argued that neural networks are universal.[^turing-complete-mcculoch-pitts] So what did they really mean by this objection?

[^turing-complete-mcculoch-pitts]:
    > every net, if furnished with a tape, scanners connected to afferents, and suitable efferents to perform the necessary motor-operations, can compute only such numbers as can a Turing machine. ... If any number can be computed by an organism, it is computable by these definitions, and conversely.

> [quote about how it is trivial to say that it's computationally universal]

They meant that, while any simple task that a human can do is solved by some neural network, there is no neural network that can solve every simple task that a human can do. Instead, each task has its own peculiarities, and requires a peculiar network that can learn to solve *that particular task* efficiently, but no other task. A generic network must fail -- it must fail to converge, or converge exponentially slowly, or converge to a bad local minima, or fail in some other way. It simply must fail, in order to save the society of mind.

## Language according to Noam Chomsky

In the field of psychology, the neural network vs Minsky divide corresponds quite well with the behaviorism vs cognitivism divide.

The cognitivist revolution was led by Noam Chomsky against behaviorism. It lasted around 1950s to 1960s, ending with the victory of cognitivism in "higher psychology", such as linguistics, though behaviorism survived respectably to this day in other aspects of psychology, such as addiction studies and animal behavior.

As expected, Chomsky rejected statistical language learning right until the end.

[On Chomsky and the two cultures of statistical learning]

[Noam Chomsky: Where Artificial Intelligence Went Wrong - The Atlantic](https://web.archive.org/web/20121102194839/http://www.theatlantic.com/technology/archive/2012/11/noam-chomsky-on-where-artificial-intelligence-went-wrong/261637/)

[Statistical learning and language acquisition | The Oxford Handbook of Language Evolution | Oxford Academic](https://academic.oup.com/edited-volume/37200/chapter-abstract/327378090?login=false)

In a curious parallel, just as neural networks for AI became popular again in the late 1980s, statistical methods for NLP became popular at the same period. The watershed moment was the publication of the [IBM alignment models](https://en.wikipedia.org/wiki/IBM_alignment_models) in 1993 [@TODO The Mathematics of Statistical Machine Translation].

In the 1950s, language production was modelled by theoretical machines: finite state automata, stack machines, Markov transition models, and variants thereof. We must understand Chomsky's two contributions to linguistics. On the first part, he constructed a [hierarchy](https://en.wikipedia.org/wiki/Chomsky_hierarchy) of increasingly powerful kinds of language. This hierarchy was in one-to-one correspondence with the hierarchy of theoretical machines, from finite state automata (type-3, or regular) all the way to Turing machines (type-0, or recursively enumerable). On the second part, he rejected statistical language learning and argued for inborn universal grammar.

Chomsky argued, and subsequent linguists have found, that the syntax of all human languages are at type-2 level, or [context-free grammar](https://en.wikipedia.org/wiki/Context-free_grammar). None is regular and almost none is context-dependent. Regular languages are modelled by finite state machines and cannot model arbitrarily deep recursion, while context-free languages allow arbitrarily deep recursion such as [center embedding](https://en.wikipedia.org/wiki/Center_embedding). This would come into play later.

With the dominance of the Chomsky approach, finite state machines were abandoned, as they are not capable of parsing context-free grammar. You might have heard of the controversy over Pirahã. It is mainly fought over the problem of recursion: does the Pirahã language have recursion or not?[^pinker-piraha]

[^pinker-piraha]: 
    Drawing the battle lines, we can predict that Chomskyans Steven Pinker would argue that it must have recursion... and it turns out the prediction went wrong on this account. Pinker went against Chomsky in this case. ["The Interpreter." NEW YORKER (2007).]

    > Steven Pinker, the Harvard cognitive scientist, who wrote admiringly about some of Chomsky’s ideas in his 1994 best-seller, “The Language Instinct,” told me, “There’s a lot of strange stuff going on in the Chomskyan program. He’s a guru, he makes pronouncements that his disciples accept on faith and that he doesn’t feel compelled to defend in the conventional scientific manner. Some of them become accepted within his circle as God’s truth without really being properly evaluated, and, surprisingly for someone who talks about universal grammar, he hasn’t actually done the spadework of seeing how it works in some weird little language that they speak in New Guinea.” Pinker says that his own doubts about the “Chomskyan program” increased in 2002, when Marc Hauser, Chomsky, and Tecumseh Fitch published their paper on recursion in Science. The authors wrote that the distinctive feature of the human faculty of language, narrowly defined, is recursion. Dogs, starlings, whales, porpoises, and chimpanzees all use vocally generated sounds to communicate with other members of their species, but none do so recursively, and thus none can produce complex utterances of infinitely varied meaning. “Recursion had always been an important part of Chomsky’s theory,” Pinker said. “But in Chomsky Mark II, or Mark III, or Mark VII, he all of a sudden said that the only thing unique to language is recursion. It’s not just that it’s the universal that has to be there; it’s the magic ingredient that makes language possible.”

A key principle used by Chomsky was the "poverty of stimulus" argument, which he used to argue that humans must have a universal grammar built in at birth, because there is too little after-birth stimulus for humans to learn languages. For one, true recursion can never be learned empirically, because true recursion can only be conclusively proven by seeing the infinitely many sentences.

Consider a simple example of the [balanced brackets language](https://en.wikipedia.org/wiki/Dyck_language). A language learner observes sample sentences from the language and try to infer the language. Suppose the learner sees a sequence `(), (()), ((())), (((())))`, what can they conclude? That it is the balanced brackets language? So we ask them to construct another sentence, and they confidently wrote `((((()))))`, but we announce to them that they were tricked! The language is the balanced brackets language -- except that the brackets only go 4 levels deep. Why? We explained that it was produced by a finite state machine, so arbitrary levels of nested brackets would [overflow its finite states](https://en.wikipedia.org/wiki/Pumping_lemma_for_regular_languages). Only by seeing all levels of recursion can the balanced brackets language be conclusively learned.

[Gold's theorem about language learning in the limit](https://en.wikipedia.org/wiki/Language_identification_in_the_limit#Gold's_theorem)[@TODO Gold 1967] is occasionally quoted in the same context as a justification for the "poverty of stimulus argument". It seems Chomsky did not consider it a relevant argument. [@TODO Gold's theorem and cognitive science] I agree with Chomsky on that account, as Gold's theorem is extremely generic.

In the early 1990s, there was a bitter controversy that is all but forgotten nowadays: the past tense debate. On one side were the connectionists, and on the other side were the cognitivists. Tellingly, both Steven Pinker and Gary Marcus were on the side of cognitivists [The past and future of the past tense]. Steven Pinker is most famous for his other books like The Blank Slate, which applies Chomsky'S linguistics to general psychology.

Similarly, Gary Marcus has been consistently critical of neural network language models since 1992 [Overregularization in language acquisition]. His theory of intelligence is essentially Chomsky's: neural networks can be intelligent, but only if they implement symbolic manipulation rules. Furthermore, a lot of symbolic rules must be built in at birth, as the poverty of stimulus precludes learning them empirically. For example, here is him saying in 1993 [@TODO Negative evidence in language acquisition]:

> Whether children require “negative evidence” (i.e., information about which strings of words are not grammatical sentences) to eliminate their ungrammatical utterances is a central question in language acquisition because, lacking negative evidence, a child would require internal mechanisms to unlearn grammatical errors. ... There is no evidence that noisy feedback is required for language learning, or even that noisy feedback exists. Thus internal mechanisms are necessary to account for the unlearning of ungrammatical utterances. 

And here is him saying in 2018 [@TODO Deep learning: A critical appraisal]:

> Human beings can learn abstract relationships in a few trials. If I told you that a schmister was a sister over the age of 10 but under the age of 21, perhaps giving you a single example, you could immediately infer whether you had any schmisters, whether your best friend had a schmister, whether your children or parents had any schmisters, and so forth. (Odds are, your parents no longer do, if they ever did, and you could rapidly draw that inference, too.) In learning what a schmister is, in this case through explicit definition, you rely not on hundreds or thousands or millions of training examples, but on a capacity to represent abstract relationships between algebra-like variables. Humans can learn such abstractions, both through explicit definition and more implicit means (Marcus, 2001). Indeed even 7-month old infants can do so, acquiring learned abstract language-like rules from a small number of unlabeled examples, in just two minutes.

His theory is described in detail in *The Algebraic Mind* [@TODO]. In view of this history, we would be unsurprised by his recent criticisms of deep learning [Deep learning: A critical appraisal] and [large language models](https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/), [repeatedly](https://www.theatlantic.com/technology/archive/2023/03/ai-chatbots-large-language-model-misinformation/673376/).

## The tribulations of Frank Rosenblatt {#sec-rosenblatt}

In the early days of AI, there were mainly three camps: cybernetics, symbolic system, and neural networks.

The founding metaphor of the cybernetic camp was that intelligence is adaptive analog signal processing: Fourier transforms, Kalman filtering, PID control, linear predictive coding, and such. I once read [Gods and Golems Inc], in which Wiener sketched out an analog signal processing circuit that performs self-reproduction. Essentially, it was a quine in analog. The cybernetic camp was essentially the school of Norbert Wiener.

The founding metaphor of the symbolic system camp was that intelligence is symbolic manipulation using preprogrammed symbolic rules: logical inference, heuristic tree search, list processing, syntactic trees, and such. The symbolic camp was not strongly centered, though it had key players like Alan Turing, John McCarthy, Herbert Simon, and Marvin Minsky.

[Insert pictures here from the book by Herbert Simon on Human Problem Solving, and images from the CYC project papers]

The founding metaphor of the neural network camp was that intelligence is what the brain does, and the brain is a network of units each with only a few parameters adjusted by learning. Now that neural networks are in the hopes and fear of every thinking person, such description seems trite, but it was once the astonishing hypothesis held by only a few. In the days before 1960, there were several key players in the neural network camp, but among them, the most famous and influential one is Frank Rosenblatt.

### Mark I



### Tobermory

The last 

## The backstory of backpropagation

### Widrow

> No, no, no. We had failed to develop algorithms beyond what we now call Madaline I , the mst algorithm that we developed for the Madaline. The Madaline had an adaptive first layer and a fixed-logic second layer. What Rosenblatt had was a fixed-logic mst layer and an adaptive-logic second layer or output layer. Now, it is easy to adapt on output layer. But it's very difficult to adapt a hidden layer. We didn't call it a hidden layer; we called it the mst layer. We could adapt an adaptive mst layer with a Axed second layer as long as we knew what the second layer was. But we never succeeded in developing an algorithm for adapting both layers, so that the second layer is not fixed and both layers are adaptive. It wasn't that we didn't try. I mean we would have given our eye teeth to come up with something like backprop.
> 
> Backprop would not work with the kind of neurons that we were using because the neurons we were using all had quantizers that were sharp. In order to make backprop work, you have to have sigmoids; you have to have a smooth nonlinearity, a differentiable nonlinearity. Otherwise, no go. And no one knew anything about it at that time. This was long before Paul Werbos. Backprop to me is almost miraculous.

If this were an individual case, then I could have dismissed it as Widrow being misled by his background. Before entering a PhD program Widrow took an internship at the military [@widrowOralHistoryBernard1997]:

> ... the sky was divided up into a grid, with one mile by one mile regions, and all the computer sensed was that there was a target within that one mile by one mile square. ... The radar pulses were transmitted periodically, and then periodically you got samples of the flight path. Say you take a piece of graph paper and draw and nice, smooth flight path for an airplane, and then at uniform points in time put a mark on the actual path based on the radar sample. If you don't show anybody that nice, smooth line but only the sequence of grid squares that the airplane was in, one could still reconstruct the smooth track.

The problem of reconstructing a smooth flight path from discretized radar signals got him interested in quantization, and he did his PhD thesis (1954) in the statistical theory of quantization noise. In the interview 43 years later, he called it "the best piece of work I ever did in my whole life". He is still the world expert on quantization noise, and wrote an entire textbook on quantization noise [@widrowQuantizationNoiseRoundoff2008].

So regarding this background, one might be tempted to say that Widrow was misled by his infatuation with quantization, and attempted to use quantized neural networks. However, this does not explain why all the *other* pioneers went down the same wrong path.

### McCulloch and Pitts {#sec-mcculloch-pitts}

In the famous paper [@mccullochLogicalCalculusIdeas1943], McCulloch and Pitts proposed that

> Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes.

The McCulloch and Pitts paper is difficult to read, and is rooted in early 20th century mathematical logic. If you have seen [*Principia Mathematica*](https://en.wikipedia.org/wiki/Principia_Mathematica), you would notice that the McCulloch and Pitts paper resembles it in its abstruse notation. This is not coincidental, as the paper directly cites *Principia Mathematica*.

![The infamous proof of 1+1=2 in Principia Mathematica](figure/principia_mathematica.png)

![](figure/principia_mathematica_McCulloch_and_Pitts.png)

The McCulloch and Pitts paper, like the *Perceptron* book, is something often cited but rarely read. For a good introduction to the McCulloch and Pitts, one cannot do worse than read [@minskyComputationFiniteInfinite1967, Chapter 3], which explicitly designs, neuron by neuron, threshold by threshold, how to construct neural networks that perform various tasks, such as simulating arbitrary Turing machines, process signals, perform additions, etc.

![](figure/minsky_1976_finite_state_machine.png)

![](figure/minsky_1976_serial_binary_addition_network.png)



### Werbos

> I didn't find a patron. Nobody would support this crazy stuff. It was very depressing. I tried to simplify it. I said, "Look, I'll pullout the backprop part and the multilayer percept ron part." I wrote a paper that was just that -- that was, I felt, childishly obvious. I didn't even use a sigmoid [non-linearity]. I used piecewise linear. I could really rationalize that to the point where it looked obvious. 10 handed that to my thesis committee. I had really worked hard to write it up. They said, "Look, this will work, but this is too trivial and simple to be worthy of a Harvard PhiD. thesis." I might add, at that point they had discontinued support because they were not interested, so I had no money. Approximately at the same time there were scandals about welfare fraud, about how students were getting food. They cut off all that kind of nonsense, so basically I had no money. NO money. Not even money to buy food.

> I spoke to Minsky. I remember I had my [Walter Rosenblith], and I said, "You know, I've got a way now to adapt multilayer perceptrons, and the key is that they're not Heaviside functions; they are differentiable. And I know that action potentials, nerve spikes, are 1 or 0, as in McCulloch-Pitts neurons, but here in this book that I had for my first course in neurophysiology are some actual tracings. If you look at these tracings in Rosenblith, they show volleys of spikes, and volleys are the unit of analysis. This is an argument for treating this activity as differentiable, at least as piecewise linear. If you look at that, I can show you how to differentiate through it." . I went to Minsky for help, but Minsky would not offer help. Minsky basically said, "Look, everybody knows a neuron is a 1-0 spike generator. That is the official model from the biologists. Now, you and I are not biologists. If you and I come out and say the biologists are wrong, and this thing is not producing Is and Os, nobody is going to believe us. It's totally crazy. I can't get involved in anything like this." He was probably right, I guess, but he was clearly very worried about his reputation and his credibility in his community.


## Triumph of the scale


