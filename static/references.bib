@article{ackleyLearningAlgorithmBoltzmann1985,
  title = {A Learning Algorithm for {{Boltzmann}} Machines},
  author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
  year = {1985},
  journal = {Cognitive science},
  volume = {9},
  number = {1},
  pages = {147--169},
  publisher = {{Elsevier}},
  url = {https://www.sciencedirect.com/science/article/pii/S0364021385800124},
  urldate = {2023-12-28},
  keywords = {Multiple DOI},
  file = {G:\Other computers\My Laptop\Zotero Files\Ackley et al_1985_A learning algorithm for Boltzmann machines.pdf}
}

@article{adlerGreatConversationRevisited1990,
  title = {The Great Conversation Revisited},
  author = {Adler, Mortimer},
  year = {1990},
  journal = {The great conversation: A Peoples guide to great books of the western world},
  keywords = {#nosource,No DOI found}
}

@misc{air-movingdeviceGreatSuggestionControl2022,
  type = {Tweet},
  title = {Great Suggestion for a Control Group {{Andy}}},
  author = {{Air-Moving Device}},
  year = {2022},
  month = nov,
  journal = {Twitter},
  url = {https://twitter.com/AirMovingDevice/status/1597070892021862400},
  urldate = {2024-01-18},
  langid = {english}
}

@misc{air-movingdeviceThreadSearchBeijing2022,
  type = {Tweet},
  title = {Thread: {{Search}} for {{Beijing}}/{{Shanghai}}/Other Cities in {{Chinese}} on {{Twitter}}},
  author = {{Air-Moving Device}},
  year = {2022},
  month = nov,
  journal = {Twitter},
  url = {https://web.archive.org/web/20230926020335/https://threadreaderapp.com/thread/1597034969293271040.html},
  urldate = {2024-01-18},
  langid = {english}
}

@book{andersonNeurocomputingFoundationsResearch1989,
  title = {Neurocomputing: {{Foundations}} of {{Research}}},
  shorttitle = {Neurocomputing},
  editor = {Anderson, James A. and Rosenfeld, Edward},
  year = {1989},
  month = dec,
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  abstract = {Researchers will find Neurocomputing an essential guide to the concepts employed in this field that have been taken from disciplines as varied as neuroscience, psychology, cognitive science, engineering, and physics. A number of these important historical papers contain ideas that have not yet been fully exploited, while the more recent articles define the current direction of neurocomputing and point to future research. Each article has an introduction that places it in historical and intellectual perspective.Included among the 43 articles are the pioneering contributions of McCulloch and Pitts, Hebb, and Lashley; innovative work by Von Neumann, Minsky and Papert, Cooper, Grossberg, and Kohonen; exciting new developments in parallel distributed processing.},
  isbn = {978-0-262-51048-6},
  langid = {english},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\00_introduction.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\01.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\02.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\03.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\04.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\05.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\06.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\07.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\08.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\09.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\10.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\11.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\12.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\13.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\14_15.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\16.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\17.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\18.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\19.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\20.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\21.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\22.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\23.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\24.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\25.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\26.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\27.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\28.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\29.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\30.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\31.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\32.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\33.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\34.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\35.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\36.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\37.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\38.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\39.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\40.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\41_42.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\43.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\44_afterword.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\45_name_index.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\46_subject_index.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Neurocomputing_Foundations_of_Research_1989\\content.pdf}
}

@book{arnoldMathematicalMethodsClassical2001,
  title = {Mathematical Methods of Classical Mechanics},
  author = {Arnol'd, V. I.},
  year = {2001},
  series = {Graduate Texts in Mathematics},
  edition = {2. ed., [7. korr. Nachdr.]},
  number = {60},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-3-540-96890-0 978-0-387-96890-2 978-3-540-98403-0},
  langid = {english},
  annotation = {OCLC: 249977873},
  file = {G:\Other computers\My Laptop\Zotero Files\Arnol πd_2001_Mathematical methods of classical mechanics.djvu}
}

@misc{atkinsonDirectApproachInteractive2023,
  title = {Direct {{Approach Interactive Model}}},
  author = {Atkinson, David},
  year = {2023},
  month = may,
  journal = {Epoch},
  url = {https://epochai.org/blog/direct-approach-interactive-model},
  urldate = {2024-01-20},
  abstract = {The Direct Approach framework bounds the compute requirements for transformative AI by extrapolating neural scaling laws. We combine those estimates with simple models of future progress in algorithms, investment, and compute costs to produce a user-adjustable forecast over the date at which TAI will be achieved.},
  langid = {english},
  file = {C:\Users\lyxkg\Zotero\storage\KXIPL3ZN\direct-approach-interactive-model.html}
}

@misc{bahdanauNeuralMachineTranslation2014,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2022-06-09},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Bahdanau et al_2014_Neural Machine Translation by Jointly Learning to Align and Translate.pdf}
}

@article{bahriExplainingNeuralScaling2021,
  title = {Explaining Neural Scaling Laws},
  author = {Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
  year = {2021},
  journal = {arXiv preprint arXiv:2102.06701},
  eprint = {2102.06701},
  url = {https://arxiv.org/abs/2102.06701},
  urldate = {2024-01-19},
  archiveprefix = {arxiv},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Bahri et al_2021_Explaining neural scaling laws.pdf}
}

@misc{bakkerOutingItThat2011,
  title = {Outing the {{It}} That {{Thinks}}: {{The Collapse}} of an {{Intellectual Ecosystem}}},
  shorttitle = {Outing the {{It}} That {{Thinks}}},
  author = {Bakker, R. Scott},
  year = {2011},
  month = nov,
  journal = {Three Pound Brain},
  url = {https://rsbakker.wordpress.com/essay-archive/outing-the-it-that-thinks-the-collapse-of-an-intellectual-ecosystem/},
  urldate = {2023-12-26},
  abstract = {[Note: The peer reviewed version of this paper can be found in, Digital Dionysus: Nietzsche and the Network-Centric Condition, 144-160] . For psychology is now once again the road to the fundamenta{\dots}},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Bakker_2011_outing-the-it-that-thinks-the-collapse-of-an-intellectual-ecosystem.html}
}

@misc{barnettDirectApproach2023,
  title = {The {{Direct Approach}}},
  author = {Barnett, Matthew and Besiroglu, Tamay},
  year = {2023},
  month = apr,
  journal = {Epoch},
  url = {https://epochai.org/blog/the-direct-approach},
  urldate = {2024-01-19},
  abstract = {Empirical scaling laws can help predict the cross-entropy loss associated with training inputs, such as compute and data. However, in order to predict when AI will achieve some subjective level of performance, it is necessary to devise a way of interpreting the cross-entropy loss of a model. This blog post provides a discussion of one such theoretical method, which we call the Direct Approach.},
  langid = {english},
  file = {C:\Users\lyxkg\Zotero\storage\SGCGTZ8H\the-direct-approach.html}
}

@techreport{barnettScalingTransformativeAutoregressive2023,
  title = {Scaling Transformative Autoregressive Models},
  author = {Barnett, Matthew and Besiroglu, Tamay},
  year = {2023},
  url = {https://epochai.org/files/direct-approach.pdf},
  urldate = {2024-01-20},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Barnett_Besiroglu_2023_Scaling transformative autoregressive models.pdf}
}

@article{bartoljrNanoconnectomicUpperBound2015,
  title = {Nanoconnectomic Upper Bound on the Variability of Synaptic Plasticity},
  author = {Bartol Jr, Thomas M. and Bromer, Cailey and Kinney, Justin and Chirillo, Michael A. and Bourne, Jennifer N. and Harris, Kristen M. and Sejnowski, Terrence J.},
  year = {2015},
  journal = {elife},
  volume = {4},
  pages = {e10778},
  publisher = {{eLife Sciences Publications, Ltd}},
  url = {https://elifesciences.org/articles/10778},
  urldate = {2023-12-06},
  file = {G:\Other computers\My Laptop\Zotero Files\Bartol Jr et al_2015_Nanoconnectomic upper bound on the variability of synaptic plasticity.pdf}
}

@techreport{beerProgressNoteResearch1962,
  title = {A Progress Note on Research into a Cybernetic Analogue of Fabric},
  author = {Beer, Stafford},
  year = {1962},
  pages = {29},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Beer_1962_A progress note on research into a cybernetic analogue of fabric.pdf}
}

@article{behrjrEstimatingComparingEntropy2002,
  title = {Estimating and Comparing Entropy across Written Natural Languages Using {{PPM}} Compression},
  author = {Behr Jr, Frederic H. and Fossum, Victoria and Mitzenmacher, Michael D. and Xiao, David},
  year = {2002},
  url = {https://dash.harvard.edu/handle/1/25104999},
  urldate = {2024-01-20},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Behr Jr et al_2002_Estimating and comparing entropy across written natural languages using PPM.pdf}
}

@article{berkeleyRevisionistHistoryConnectionism1997,
  title = {A Revisionist History of Connectionism},
  author = {Berkeley, Istvan SN},
  year = {1997},
  journal = {Unpublished manuscript},
  url = {http://www.universelle-automation.de/1969_Boston.pdf},
  urldate = {2023-12-27},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Berkeley_1997_A revisionist history of connectionism.pdf}
}

@article{bernsteinMarvinMinskyVision1981,
  title = {Marvin {{Minsky}}'s {{Vision}} of the {{Future}}},
  author = {Bernstein, Jeremy},
  year = {1981},
  month = dec,
  journal = {The New Yorker},
  url = {https://www.newyorker.com/magazine/1981/12/14/a-i},
  urldate = {2023-12-25},
  file = {G:\Other computers\My Laptop\Zotero Files\Bernstein_1981_Marvin Minsky‚Äôs Vision of the Future.html}
}

@article{blockReviewPerceptronsIntroduction1970,
  title = {A Review of "Perceptrons: {{An}} Introduction to Computational Geometry"},
  shorttitle = {A Review of ``perceptrons},
  author = {Block, H. D.},
  year = {1970},
  month = dec,
  journal = {Information and Control},
  volume = {17},
  number = {5},
  pages = {501--522},
  issn = {0019-9958},
  doi = {10.1016/S0019-9958(70)90409-2},
  url = {https://www.sciencedirect.com/science/article/pii/S0019995870904092},
  urldate = {2023-12-30},
  file = {C:\Users\lyxkg\Zotero\storage\KZFLCWIF\S0019995870904092.html}
}

@article{borelTheorieRelativiteCinematique1913,
  title = {La Th{\'e}orie de La Relativit{\'e} et La Cin{\'e}matique},
  author = {Borel, {\'E}mile},
  year = {1913},
  journal = {Comptes Rendus des s{\'e}ances de l'Acad{\'e}mie des Sciences},
  volume = {156},
  pages = {215--217},
  keywords = {#nosource,No DOI found}
}

@book{borgesSelectedNonfictions2000,
  title = {Selected Non-Fictions},
  author = {Borges, Jorge Luis},
  editor = {Weinberger, Eliot},
  year = {2000},
  publisher = {{Penguin Books}},
  address = {{New York}},
  url = {http://books.google.com/books?id=BpUrAQAAMAAJ},
  urldate = {2022-03-02},
  abstract = {A collection of writings includes essays, literary and film criticism, biographical sketches, and lectures.},
  langid = {english},
  annotation = {OCLC: 895122397},
  file = {G:\Other computers\My Laptop\Zotero Files\Borges_2000_Selected non-fictions.pdf}
}

@article{boseTheoryNonlinearSystems1956,
  title = {A Theory of Nonlinear Systems},
  author = {Bose, Amar Gopal},
  year = {1956},
  publisher = {{Research Laboratory of Electronics, Massachusetts Institute of Technology}},
  url = {https://dspace.mit.edu/bitstream/handle/1721.1/4773/RLE-TR-309-04734594.pdf;sequence=1},
  urldate = {2023-12-24},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Bose_1956_A theory of nonlinear systems.pdf}
}

@techreport{boweChinaOverseasUnited2018,
  title = {China's Overseas {{United Front}} Work: {{Background}} and Implications for the {{United States}}},
  shorttitle = {China's Overseas {{United Front}} Work},
  author = {Bowe, Alexander},
  year = {2018},
  institution = {{US-China Economic and Security Review Commission}},
  keywords = {#nosource}
}

@book{bowenEconomicGeographyAir2010,
  title = {The Economic Geography of Air Transportation: Space, Time, and the Freedom of the Sky},
  shorttitle = {The Economic Geography of Air Transportation},
  author = {Bowen, John T.},
  year = {2010},
  publisher = {{Routledge}},
  url = {https://books.google.com/books?hl=en&lr=&id=BlyLAgAAQBAJ&oi=fnd&pg=PR7&dq=economic+geography+of+air+transportation:+space,+time,+and+the+freedom+of+the+sky&ots=xkrlI-WjsQ&sig=PCTVtPx8cVFJUXakhMt8eX7fFag},
  urldate = {2023-12-06},
  keywords = {#nosource}
}

@misc{branwenGANsDidnFail2022,
  title = {{{GANs Didn}}'t {{Fail}}, {{They Were Abandoned}}},
  author = {Branwen, Gwern},
  year = {2022},
  month = oct,
  url = {https://gwern.net/gan},
  urldate = {2024-01-18},
  abstract = {{$<$}p{$>$}Diffusion models supposedly beat GANs because they scale better and stabler. That is unproven, and false. GANs should be revisited.{$<$}/p{$>$}},
  copyright = {https://creativecommons.org/publicdomain/zero/1.0/},
  langid = {american},
  keywords = {No DOI found},
  file = {C:\Users\lyxkg\Zotero\storage\SQPYTD6H\gan.html}
}

@misc{branwenScalingHypothesis2020,
  title = {The {{Scaling Hypothesis}}},
  author = {Branwen, Gwern},
  year = {2020},
  month = may,
  url = {https://www.gwern.net/Scaling-hypothesis},
  urldate = {2021-12-15},
  abstract = {On GPT-3: meta-learning, scaling, implications, and deep theory. The scaling hypothesis: neural nets absorb data \& compute, generalizing and becoming more Bayesian as problems get harder, manifesting new abilities even at trivial-by-global-standards-scale. The deep learning revolution has begun as foretold.},
  copyright = {https://creativecommons.org/publicdomain/zero/1.0/},
  langid = {american},
  keywords = {#nosource,No DOI found}
}

@incollection{breimanReflectionsRefereeingPapers1995,
  title = {Reflections {{After Refereeing Papers}} for {{NIPS}}},
  booktitle = {The {{Mathematics Of Generalization}}},
  author = {Breiman, Leo},
  year = {1995},
  publisher = {{CRC Press}},
  abstract = {The theoretical work by Weiner and others on the spectral analysis of stationary time series penetrated statistics following Tukey's heuristic work on estimation of the spectrum. In refereeing papers for NIPS the author was struck by the growing emphasis on mathematical theory. Mathematical theory is not critical to the development of machine learning. In machine learning, the current panacea is a sigmoid network fitted using backpropagation. The pi-method, for approximating functions using noisy data, was suggested by results in mathematical approximation theory. In spite of intense activity, none of the work has had any effect on the day-to-day practice of statistics, or even on present-day theory. The useful theories was not meant to be inclusive, but even a more inclusive list would be very short. A possible reason is that it is difficult to formulate reasonable analytic models for complex data.},
  isbn = {978-0-429-49252-5},
  file = {G:\Other computers\My Laptop\Zotero Files\Breiman_1995_Reflections After Refereeing Papers for NIPS.pdf}
}

@article{breimanStatisticalModelingTwo2001,
  title = {Statistical Modeling: {{The}} Two Cultures (with Comments and a Rejoinder by the Author)},
  shorttitle = {Statistical Modeling},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Statistical science},
  volume = {16},
  number = {3},
  pages = {199--231},
  publisher = {{Institute of Mathematical Statistics}},
  url = {https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.short},
  urldate = {2023-12-31},
  keywords = {Multiple DOI},
  file = {G:\Other computers\My Laptop\Zotero Files\Breiman_2001_Statistical modeling.pdf}
}

@article{brownEstimateUpperBound1992,
  title = {An Estimate of an Upper Bound for the Entropy of {{English}}},
  author = {Brown, Peter F. and Della Pietra, Stephen A. and Della Pietra, Vincent J. and Lai, Jennifer C. and Mercer, Robert L.},
  year = {1992},
  journal = {Computational Linguistics},
  volume = {18},
  number = {1},
  pages = {31--40},
  url = {https://aclanthology.org/J92-1002.pdf},
  urldate = {2024-01-20},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Brown et al_1992_An estimate of an upper bound for the entropy of English.pdf}
}

@article{brownMathematicsStatisticalMachine1993,
  title = {The Mathematics of Statistical Machine Translation: {{Parameter}} Estimation},
  shorttitle = {The Mathematics of Statistical Machine Translation},
  author = {Brown, Peter F. and Della Pietra, Stephen A. and Della Pietra, Vincent J. and Mercer, Robert L.},
  year = {1993},
  publisher = {{MIT Press}},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Brown et al_1993_The mathematics of statistical machine translation.pdf}
}

@misc{burdaReinforcementLearningPredictionbased2018,
  title = {Reinforcement Learning with Prediction-Based Rewards},
  author = {Burda, Yura and Edwards, Harri},
  year = {2018},
  month = oct,
  journal = {OpenAI},
  url = {https://openai.com/research/reinforcement-learning-with-prediction-based-rewards},
  urldate = {2024-01-18},
  abstract = {We've developed~Random Network Distillation (RND), a prediction-based method for encouraging reinforcement learning agents to explore their environments through curiosity, which for the first time exceeds average human performance on~Montezuma's Revenge.},
  langid = {american},
  file = {C:\Users\lyxkg\Zotero\storage\YJUJL76Z\reinforcement-learning-with-prediction-based-rewards.html}
}

@article{burtonLongrangeConstraintsStatistical1955,
  title = {Long-Range Constraints in the Statistical Structure of Printed {{English}}},
  author = {Burton, N. G. and Licklider, J. C. R.},
  year = {1955},
  journal = {The American Journal of Psychology},
  volume = {68},
  number = {4},
  eprint = {1418794},
  eprinttype = {jstor},
  pages = {650--653},
  publisher = {{JSTOR}},
  doi = {10.2307/1418794},
  url = {https://www.jstor.org/stable/1418794},
  urldate = {2024-01-26},
  file = {G:\Other computers\My Laptop\Zotero Files\Burton_Licklider_1955_Long-range constraints in the statistical structure of printed English.pdf}
}

@book{callcottMajorTransitionsEvolution2011,
  title = {The Major Transitions in Evolution Revisited},
  author = {Callcott, Brett and Sterelny, Kim},
  year = {2011},
  series = {Vienna Series in Theoretical Biology},
  publisher = {{MIT Press}},
  address = {{Cambridge (Mass.)}},
  isbn = {978-0-262-01524-0},
  langid = {english},
  lccn = {576.8},
  file = {G:\Other computers\My Laptop\Zotero Files\Callcott_Sterelny_2011_The major transitions in evolution revisited.pdf}
}

@article{carianiEvolveEarEpistemological1993,
  title = {To Evolve an Ear. {{Epistemological}} Implications of Gordon Pask's Electrochemical Devices},
  author = {Cariani, Peter},
  year = {1993},
  journal = {Systems research},
  volume = {10},
  number = {3},
  pages = {19--33},
  doi = {10/bv3vd8},
  annotation = {ZSCC: 0000142},
  file = {G:\Other computers\My Laptop\Zotero Files\Cariani_1993_To evolve an ear.pdf}
}

@article{childManuscriptsLeibnizHis1917,
  title = {The {{Manuscripts}} of {{Leibniz}} on {{His Discovery}} of the {{Differential Calculus}}. {{Part II}} ({{Continued}})},
  author = {Child, J. M.},
  year = {1917},
  journal = {The Monist},
  volume = {27},
  number = {3},
  eprint = {27900650},
  eprinttype = {jstor},
  pages = {411--454},
  publisher = {{Oxford University Press}},
  issn = {0026-9662},
  doi = {10.5840/monist191727324},
  url = {https://www.jstor.org/stable/27900650},
  urldate = {2023-12-27},
  file = {G:\Other computers\My Laptop\Zotero Files\Child_1917_The Manuscripts of Leibniz on His Discovery of the Differential Calculus2.pdf}
}

@book{chomskyAspectsTheorySyntax2015,
  title = {Aspects of the Theory of Syntax},
  author = {Chomsky, Noam},
  year = {2015},
  series = {Massachusetts {{Institute}} of {{Technology}}. {{Research Laboratory}} of {{Electronics}}. {{Special}} Technical Report},
  edition = {50th Anniversary Edition},
  number = {no. 11},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-52740-8},
  lccn = {P291 .C4 2015},
  keywords = {Grammar Comparative and general,Syntax},
  file = {G:\Other computers\My Laptop\Zotero Files\Chomsky_2015_Aspects of the theory of syntax.pdf}
}

@book{chomskyLanguageMind2006,
  title = {Language and Mind},
  author = {Chomsky, Noam},
  year = {2006},
  edition = {3rd ed},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York}},
  isbn = {978-0-521-85819-9 978-0-521-67493-5},
  lccn = {P106 .C52 2006},
  keywords = {Psycholinguistics,Thought and thinking},
  annotation = {OCLC: ocm62132939},
  file = {G:\Other computers\My Laptop\Zotero Files\Chomsky_2006_Language and mind.pdf}
}

@misc{chomskyPovertyStimulusUnfinished2010,
  title = {Poverty of {{Stimulus}}: {{Unfinished Business}}},
  author = {Chomsky, Noam},
  year = {2010},
  month = mar,
  address = {{Johannes Gutenberg-Universit{\"a}t}},
  url = {https://web.archive.org/web/20230216033431/https://www.blogs.uni-mainz.de/studgen-stiftung-jgsp-eng/files/2019/01/Mainz_transcript_edited2.pdf},
  keywords = {#nosource}
}

@book{chomskySyntacticStructures2002,
  title = {Syntactic Structures},
  author = {Chomsky, Noam},
  year = {2002},
  edition = {2nd ed},
  publisher = {{Mouton de Gruyter}},
  address = {{Berlin ; New York}},
  isbn = {978-3-11-017279-9},
  lccn = {P291 .C5 2002},
  keywords = {Generative grammar,Grammar Comparative and general,Syntax},
  file = {G:\Other computers\My Laptop\Zotero Files\Chomsky_2002_Syntactic structures.djvu}
}

@article{chuangBulkQuantumComputation1998,
  title = {Bulk Quantum Computation with Nuclear Magnetic Resonance: Theory and Experiment},
  shorttitle = {Bulk Quantum Computation with Nuclear Magnetic Resonance},
  author = {Chuang, I. L. and Gershenfeld, N. and Kubinec, M. G. and Leung, D. W.},
  year = {1998},
  month = jan,
  journal = {Proceedings of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences},
  volume = {454},
  number = {1969},
  pages = {447--467},
  issn = {1364-5021, 1471-2946},
  doi = {10.1098/rspa.1998.0170},
  url = {https://royalsocietypublishing.org/doi/10.1098/rspa.1998.0170},
  urldate = {2024-01-22},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Chuang et al_1998_Bulk quantum computation with nuclear magnetic resonance.pdf}
}

@article{churchlandCritiquePureVision1994,
  title = {A Critique of Pure Vision},
  author = {Churchland, Patricia S. and Ramachandran, Vilayanur S. and Sejnowski, Terrence J.},
  year = {1994},
  journal = {Large-scale neuronal theories of the brain},
  volume = {23},
  keywords = {No DOI found,reading},
  file = {G:\Other computers\My Laptop\Zotero Files\Churchland et al_1994_A critique of pure vision.pdf}
}

@article{colapintoInterpreter2007,
  title = {The Interpreter},
  author = {Colapinto, John},
  year = {2007},
  month = apr,
  journal = {The New Yorker},
  pages = {125},
  url = {https://blogs.baruch.cuny.edu/anthropology1001/files/2011/01/NewYorkerPirahaArticle1.pdf},
  urldate = {2023-12-31},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Colapinto_2007_The interpreter.pdf}
}

@article{coverConvergentGamblingEstimate1978,
  title = {A Convergent Gambling Estimate of the Entropy of {{English}}},
  author = {Cover, Thomas and King, Roger},
  year = {1978},
  journal = {IEEE Transactions on Information Theory},
  volume = {24},
  number = {4},
  pages = {413--421},
  publisher = {{IEEE}},
  doi = {10.1109/TIT.1978.1055912},
  url = {https://ieeexplore.ieee.org/abstract/document/1055912/},
  urldate = {2024-01-20},
  file = {G:\Other computers\My Laptop\Zotero Files\Cover_King_1978_A convergent gambling estimate of the entropy of English.pdf}
}

@book{coverElementsInformationTheory2006,
  title = {Elements of Information Theory},
  author = {Cover, T. M. and Thomas, Joy A.},
  year = {2006},
  edition = {2nd ed},
  publisher = {{Wiley-Interscience}},
  address = {{Hoboken, N.J}},
  isbn = {978-0-471-24195-9},
  lccn = {Q360 .C68 2006},
  keywords = {Information theory},
  annotation = {ZSCC: 0000005  OCLC: ocm59879802},
  file = {G:\Other computers\My Laptop\Zotero Files\Cover_Thomas_2006_Elements of information theory.pdf}
}

@article{criadoThomasRotationFoucault2009,
  title = {Thomas Rotation and {{Foucault}} Pendulum under a Simple Unifying Geometrical Point of View},
  author = {Criado, C. and Alamo, N.},
  year = {2009},
  journal = {International Journal of Non-Linear Mechanics},
  volume = {44},
  number = {8},
  pages = {923--927},
  publisher = {{Elsevier}},
  doi = {10.1016/j.ijnonlinmec.2009.06.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0020746209001279},
  urldate = {2023-12-12},
  file = {C:\Users\lyxkg\Zotero\storage\KTB6JDGV\S0020746209001279.html}
}

@misc{deletangLanguageModelingCompression2023,
  title = {Language {{Modeling Is Compression}}},
  author = {Del{\'e}tang, Gr{\'e}goire and Ruoss, Anian and Duquenne, Paul-Ambroise and Catt, Elliot and Genewein, Tim and Mattern, Christopher and {Grau-Moya}, Jordi and Wenliang, Li Kevin and Aitchison, Matthew and Orseau, Laurent and Hutter, Marcus and Veness, Joel},
  year = {2023},
  month = sep,
  number = {arXiv:2309.10668},
  eprint = {2309.10668},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.10668},
  url = {http://arxiv.org/abs/2309.10668},
  urldate = {2024-01-30},
  abstract = {It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4\% and LibriSpeech samples to 16.4\% of their raw size, beating domain-specific compressors like PNG (58.5\%) or FLAC (30.3\%), respectively. Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Theory,Computer Science - Machine Learning},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Del√©tang et al_2023_Language Modeling Is Compression.pdf;C\:\\Users\\lyxkg\\Zotero\\storage\\XQHTPEEH\\2309.html}
}

@book{deleuzeThousandPlateausCapitalism1987,
  title = {A Thousand Plateaus: Capitalism and Schizophrenia},
  shorttitle = {A Thousand Plateaus},
  author = {Deleuze, Gilles and Guattari, F{\'e}lix},
  year = {1987},
  publisher = {{University of Minnesota Press}},
  address = {{Minneapolis}},
  isbn = {978-0-8166-1401-1 978-0-8166-1402-8},
  langid = {english},
  lccn = {B77 .D413 1987},
  keywords = {Philosophy,Postmodernism,Psychoanalysis,Radicalism},
  file = {G:\Other computers\My Laptop\Zotero Files\Deleuze_Guattari_1987_A thousand plateaus.pdf}
}

@book{dertouzosThresholdLogicSynthesis1965,
  title = {Threshold Logic: {{A Synthesis Approach}}},
  author = {Dertouzos, Michael},
  year = {1965},
  url = {https://mitpress.mit.edu/9780262040099/},
  urldate = {2023-12-28},
  abstract = {The subject of this book is threshold elements, and the problem consists of several questions aimed at the logical capabilities of a single threshold element and at the synthesis or design of combinational digital systems (e.g., computers) using these devices. A threshold element is primarily a logical gate or a basic building block, and it is one of the most powerful gates known to us today. The work is important on three counts: It is the first book on the subject of threshold elements; it presents an approach not often encountered in the design of digital systems; and it contains primarily original results which answer the basic problem in a number of ways. These results which include a number of synthesis procedures are presented in detail and are illustrated with examples. In the interest of continuity, some results of other investigators are briefly, though fully, explained. THRESHOLD LOGIC: A Synthesis Approach shows how a basic approach (correlation) can be used in treating the questions of threshold element synthesis. A number of scattered concepts are unified through this approach under one idea (the characteristic-vector language). This monograph should interest students and engineers as well as researchers in the fields of threshold logic, logic design, and switching theory in general, as well as in bionics and in computer design.},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Dertouzos_1965_Threshold logic.djvu}
}

@article{dieudonneInvariantTheoryOld1970,
  title = {Invariant Theory, Old and New},
  author = {Dieudonn{\'e}, Jean A. and Carrell, James B.},
  year = {1970},
  journal = {Advances in mathematics},
  volume = {4},
  number = {1},
  pages = {1--80},
  publisher = {{Academic Press}},
  doi = {10.1016/0001-8708(70)90015-0},
  url = {https://core.ac.uk/download/pdf/81982193.pdf},
  urldate = {2024-01-23},
  file = {G:\Other computers\My Laptop\Zotero Files\Dieudonn√©_Carrell_1970_Invariant theory, old and new.pdf}
}

@article{dobsonEssaysReviewsComputing2022,
  title = {Essays and {{Reviews}} in {{Computing}} and {{Culture}}},
  author = {Dobson, James E.},
  year = {2022},
  journal = {Interfaces},
  volume = {3},
  url = {https://cse.umn.edu/cbi/interfaces},
  urldate = {2023-12-30},
  keywords = {No DOI found},
  file = {C:\Users\lyxkg\Zotero\storage\FSIQX224\interfaces.html}
}

@article{dowlingThereAreNo1989,
  title = {There {{Are No Safe Virus Tests}}},
  author = {Dowling, William F.},
  year = {1989},
  month = nov,
  journal = {The American Mathematical Monthly},
  volume = {96},
  number = {9},
  pages = {835--836},
  issn = {0002-9890, 1930-0972},
  doi = {10.1080/00029890.1989.11972292},
  url = {https://www.tandfonline.com/doi/full/10.1080/00029890.1989.11972292},
  urldate = {2023-12-22},
  langid = {english},
  file = {C:\Users\lyxkg\Zotero\storage\RILJSGC4\openurl.html}
}

@article{drayGeometryRelativity2017,
  title = {The Geometry of Relativity},
  author = {Dray, Tevian},
  year = {2017},
  month = sep,
  journal = {American Journal of Physics},
  volume = {85},
  number = {9},
  pages = {683--691},
  issn = {0002-9505},
  doi = {10.1119/1.4997027},
  url = {https://doi.org/10.1119/1.4997027},
  urldate = {2023-12-12},
  abstract = {Geometric treatments of both special and general relativity are outlined, suitable for students in the sciences who are seeing these topics for the first time. Both the advantages and disadvantages of such an approach, and its relationship to more traditional approaches, are then discussed.},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Dray_2017_The geometry of relativity.pdf;C\:\\Users\\lyxkg\\Zotero\\storage\\IG6ETLJI\\The-geometry-of-relativity.html}
}

@book{drayGeometrySpecialRelativity2021,
  title = {The Geometry of Special Relativity},
  author = {Dray, Tevian},
  year = {2021},
  series = {Advances in Applied Mathematics},
  edition = {Second edition},
  publisher = {{Chapman \& Hall/CRC Press}},
  address = {{Boca Raton}},
  abstract = {"This unique book presents a particularly beautiful way of looking at special relativity. The author encourages students to see beyond the formulas to the deeper structure. The unification of space and time introduced by Einstein's special theory of relativity is one of the cornerstones of the modern scientific description of the universe. Yet the unification is counterintuitive because we perceive time very differently from space. Even in relativity, time is not just another dimension, it is one with different properties The book treats the geometry of hyperbolas as the key to understanding special relativity. The author simplifies the formulas and emphasizes their geometric content. Many important relations, including the famous relativistic addition formula for velocities, then follow directly from the appropriate (hyperbolic) trigonometric addition formulas. Prior mastery of (ordinary) trigonometry is sufficient for most of the material presented, although occasional use is made of elementary differential calculus, and the chapter on electromagnetism assumes some more advanced knowledge. Changes to the Second Edition The treatment of Minkowski space and spacetime diagrams has been expanded. Several new topics have been added, including a geometric derivation of Lorentz transformations, a discussion of three-dimensional spacetime diagrams, and a brief geometric description of "area" and how it can be used to measure time and distance. Minor notational changes were made to avoid conflict with existing usage in the literature"--},
  isbn = {978-1-351-66321-2},
  lccn = {QC173.65},
  keywords = {#nosource,Mathematical models,Space and time,Special relativity (Physics)}
}

@book{dudaPatternClassification2001,
  title = {Pattern Classification},
  author = {Duda, Richard O. and Hart, Peter E. and Stork, David G.},
  year = {2001},
  edition = {2nd ed},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {978-0-471-05669-0},
  lccn = {Q327 .D83 2001},
  keywords = {Pattern recognition systems,Statistical decision},
  file = {G:\Other computers\My Laptop\Zotero Files\Duda et al_2001_Pattern classification.djvu}
}

@book{dudaPatternClassificationScene1973,
  title = {Pattern Classification and Scene Analysis},
  author = {Duda, Richard O. and Hart, Peter E.},
  year = {1973},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {978-0-471-22361-0},
  lccn = {Q327 .D83},
  keywords = {#nosource,Pattern recognition systems,Statistical decision}
}

@article{dysonContinuousFunctionsDefined1951,
  title = {Continuous Functions Defined on Spheres},
  author = {Dyson, Freeman J.},
  year = {1951},
  journal = {Annals of Mathematics},
  eprint = {1969487},
  eprinttype = {jstor},
  pages = {534--536},
  publisher = {{JSTOR}},
  doi = {10.2307/1969487},
  url = {https://www.jstor.org/stable/1969487},
  urldate = {2023-12-25},
  file = {G:\Other computers\My Laptop\Zotero Files\Dyson_1951_Continuous functions defined on spheres.pdf}
}

@article{eigenLearningFactoredRepresentations2013,
  title = {Learning Factored Representations in a Deep Mixture of Experts},
  author = {Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  year = {2013},
  journal = {arXiv preprint arXiv:1312.4314},
  eprint = {1312.4314},
  url = {https://arxiv.org/abs/1312.4314},
  urldate = {2024-01-24},
  archiveprefix = {arxiv},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Eigen et al_2013_Learning factored representations in a deep mixture of experts.pdf}
}

@book{einsteinMeaningRelativityFour1922,
  title = {The Meaning of Relativity: Four Lectures Delivered at {{Princeton University}}, {{May}}, 1921},
  shorttitle = {The Meaning of Relativity},
  author = {Einstein, Albert},
  year = {1922},
  publisher = {{Methuen \& Company Limited}},
  url = {https://books.google.com/books?hl=en&lr=&id=0nIxAQAAMAAJ&oi=fnd&pg=PA1&dq=The+Meaning+of+Relativity&ots=eknjBZkyAQ&sig=daAb3Bm6-Ip-kzb-aTc6fNI3HhI},
  urldate = {2023-12-11},
  file = {C:\Users\lyxkg\Zotero\storage\NSMF8IJP\books.html}
}

@article{einsteinZurElektrodynamikBewegter1905,
  title = {Zur {{Elektrodynamik}} Bewegter {{K{\"o}rper}}},
  author = {Einstein, A.},
  year = {1905},
  month = jan,
  journal = {Annalen der Physik},
  volume = {322},
  number = {10},
  pages = {891--921},
  issn = {0003-3804, 1521-3889},
  doi = {10.1002/andp.19053221004},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/andp.19053221004},
  urldate = {2023-12-12},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Einstein_1905_Zur Elektrodynamik bewegter K√∂rper.pdf}
}

@article{fedusSwitchTransformersScaling2022,
  title = {Switch Transformers: {{Scaling}} to Trillion Parameter Models with Simple and Efficient Sparsity},
  shorttitle = {Switch Transformers},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  year = {2022},
  journal = {The Journal of Machine Learning Research},
  volume = {23},
  number = {1},
  pages = {5232--5270},
  publisher = {{JMLRORG}},
  url = {https://dl.acm.org/doi/abs/10.5555/3586589.3586709},
  urldate = {2024-01-18},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Fedus et al_2022_Switch transformers.pdf}
}

@article{feldmanConnectionistModelsTheir1982,
  title = {Connectionist Models and Their Properties},
  author = {Feldman, Jerome A. and Ballard, Dana H.},
  year = {1982},
  journal = {Cognitive science},
  volume = {6},
  number = {3},
  pages = {205--254},
  publisher = {{Elsevier}},
  doi = {10.1207/s15516709cog0603_1},
  url = {https://www.sciencedirect.com/science/article/pii/S0364021382800013},
  urldate = {2023-12-06},
  file = {G:\Other computers\My Laptop\Zotero Files\Feldman_Ballard_1982_Connectionist models and their properties.pdf}
}

@article{fentonBayesLaw2016,
  title = {Bayes and the {{Law}}},
  author = {Fenton, Norman and Neil, Martin and Berger, Daniel},
  year = {2016},
  journal = {Annual Review of Statistics and Its Application},
  volume = {3},
  number = {1},
  pages = {51--77},
  doi = {10.1146/annurev-statistics-041715-033428},
  url = {https://doi.org/10.1146/annurev-statistics-041715-033428},
  urldate = {2023-12-22},
  abstract = {Although the use of statistics in legal proceedings has considerably grown in the last 40 years, primarily classical statistical methods rather than Bayesian methods have been used. Yet the Bayesian approach avoids many of the problems of classical statistics and is also well suited to a broader range of problems. This article reviews the potential and actual use of Bayes in the law and explains the main reasons for its lack of impact on legal practice. These reasons include misconceptions by the legal community about Bayes' theorem, overreliance on the use of the likelihood ratio, and the lack of adoption of modern computational methods. We argue that Bayesian networks, which automatically produce the necessary Bayesian calculations, provide an opportunity to address most concerns about using Bayes in the law.},
  pmid = {27398389},
  keywords = {Bayes,Bayesian networks,legal arguments,statistics in court},
  file = {G:\Other computers\My Laptop\Zotero Files\Fenton et al_2016_Bayes and the Law.pdf}
}

@article{fermiSullElettrostaticaDi1921,
  title = {Sull'elettrostatica Di Un Campo Gravitazionale Uniforme e Sul Peso Delle Masse Elettromagnetiche},
  author = {Fermi, Enrico},
  year = {1921},
  journal = {Il Nuovo Cimento (1911-1923)},
  volume = {22},
  pages = {176--188},
  publisher = {{Societ{\`a} Italiana di Fisica}},
  doi = {10.1007/BF02959697},
  url = {https://core.ac.uk/download/pdf/212161128.pdf},
  urldate = {2023-12-12},
  file = {G:\Other computers\My Laptop\Zotero Files\Fermi_1921_Sull‚Äôelettrostatica di un campo gravitazionale uniforme e sul peso delle masse.pdf}
}

@article{feutrillReviewShannonDifferential2021,
  title = {A {{Review}} of {{Shannon}} and {{Differential Entropy Rate Estimation}}},
  author = {Feutrill, Andrew and Roughan, Matthew},
  year = {2021},
  month = aug,
  journal = {Entropy},
  volume = {23},
  number = {8},
  pages = {1046},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1099-4300},
  doi = {10.3390/e23081046},
  url = {https://www.mdpi.com/1099-4300/23/8/1046},
  urldate = {2024-01-26},
  abstract = {In this paper, we present a review of Shannon and differential entropy rate estimation techniques. Entropy rate, which measures the average information gain from a stochastic process, is a measure of uncertainty and complexity of a stochastic process. We discuss the estimation of entropy rate from empirical data, and review both parametric and non-parametric techniques. We look at many different assumptions on properties of the processes for parametric processes, in particular focussing on Markov and Gaussian assumptions. Non-parametric estimation relies on limit theorems which involve the entropy rate from observations, and to discuss these, we introduce some theory and the practical implementations of estimators of this type.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {entropy rate,estimation,non-parametric,parametric},
  file = {G:\Other computers\My Laptop\Zotero Files\Feutrill_Roughan_2021_A Review of Shannon and Differential Entropy Rate Estimation.pdf}
}

@article{fisherDeathMathematicalTheory1966,
  title = {The Death of a Mathematical Theory: {{A}} Study in the Sociology of Knowledge},
  shorttitle = {The Death of a Mathematical Theory},
  author = {Fisher, Charles S.},
  year = {1966},
  journal = {Archive for History of exact Sciences},
  volume = {3},
  number = {2},
  eprint = {41133257},
  eprinttype = {jstor},
  pages = {137--159},
  publisher = {{JSTOR}},
  doi = {10.1007/BF00357267},
  url = {https://www.jstor.org/stable/41133257},
  urldate = {2024-01-23},
  file = {G:\Other computers\My Laptop\Zotero Files\Fisher_1966_The death of a mathematical theory.pdf}
}

@article{fodorConnectionismCognitiveArchitecture1988,
  title = {Connectionism and Cognitive Architecture: {{A}} Critical Analysis},
  shorttitle = {Connectionism and Cognitive Architecture},
  author = {Fodor, Jerry A. and Pylyshyn, Zenon W.},
  year = {1988},
  journal = {Cognition},
  volume = {28},
  number = {1-2},
  pages = {3--71},
  publisher = {{Elsevier}},
  doi = {10.1016/0010-0277(88)90031-5},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Fodor_Pylyshyn_1988_Connectionism and cognitive architecture.pdf;C\:\\Users\\lyxkg\\Zotero\\storage\\FHXFWK3X\\openurl.html}
}

@misc{foersterNonlinearComputationDeep2017,
  title = {Nonlinear Computation in Deep Linear Networks},
  author = {Foerster, Jakob},
  year = {2017},
  month = sep,
  url = {https://openai.com/research/nonlinear-computation-in-deep-linear-networks},
  urldate = {2024-01-07},
  langid = {american},
  file = {C:\Users\lyxkg\Zotero\storage\U677NHDU\nonlinear-computation-in-deep-linear-networks.html}
}

@article{fuchsSettheoreticGeology2015,
  title = {Set-Theoretic Geology},
  author = {Fuchs, Gunter and Hamkins, Joel David and Reitz, Jonas},
  year = {2015},
  month = apr,
  journal = {Annals of Pure and Applied Logic},
  volume = {166},
  number = {4},
  pages = {464--501},
  issn = {0168-0072},
  doi = {10.1016/j.apal.2014.11.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0168007214001225},
  urldate = {2023-12-22},
  abstract = {A ground of the universe V is a transitive proper class W{$\subseteq$}V, such that W{$\forcesextra$}ZFC and V is obtained by set forcing over W, so that V=W[G] for some W-generic filter G{$\subseteq$}P{$\in$}W. The model V satisfies the ground axiom GA if there are no such W properly contained in V. The model W is a bedrock of V if W is a ground of V and satisfies the ground axiom. The mantle of V is the intersection of all grounds of V. The generic mantle of V is the intersection of all grounds of all set-forcing extensions of V. The generic HOD, written gHOD, is the intersection of all HODs of all set-forcing extensions. The generic HOD is always a model of ZFC, and the generic mantle is always a model of ZF. Every model of ZFC is the mantle and generic mantle of another model of ZFC. We prove this theorem while also controlling the HOD of the final model, as well as the generic HOD. Iteratively taking the mantle penetrates down through the inner mantles to what we call the outer core, what remains when all outer layers of forcing have been stripped away. Many fundamental questions remain open.},
  keywords = {#nosource,Forcing,Ground model,Inner model,Set theory,Set-theoretic geology}
}

@article{gaborUniversalNonlinearFilter1961,
  title = {A Universal Non-Linear Filter, Predictor and Simulator Which Optimizes Itself by a Learning Process},
  author = {Gabor, D. and Wilby, W.P.L. and Woodcock, R.},
  year = {1961},
  journal = {Proceedings of the IEE Part B: Electronic and Communication Engineering},
  volume = {108},
  number = {40},
  pages = {422},
  issn = {03698890},
  doi = {10.1049/pi-b-2.1961.0070},
  url = {https://digital-library.theiet.org/content/journals/10.1049/pi-b-2.1961.0070},
  urldate = {2023-12-24},
  langid = {english},
  keywords = {#nosource}
}

@article{galisonOntologyEnemyNorbert1994,
  title = {The Ontology of the Enemy: {{Norbert Wiener}} and the Cybernetic Vision},
  shorttitle = {The Ontology of the Enemy},
  author = {Galison, Peter},
  year = {1994},
  journal = {Critical inquiry},
  volume = {21},
  number = {1},
  pages = {228--266},
  publisher = {{University of Chicago Press}},
  doi = {10/fpfd3b},
  file = {G:\Other computers\My Laptop\Zotero Files\Galison_1994_The ontology of the enemy.pdf}
}

@article{gershenfeldBulkSpinResonanceQuantum1997,
  title = {Bulk {{Spin-Resonance Quantum Computation}}},
  author = {Gershenfeld, Neil A. and Chuang, Isaac L.},
  year = {1997},
  month = jan,
  journal = {Science},
  volume = {275},
  number = {5298},
  pages = {350--356},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.275.5298.350},
  url = {https://www.science.org/doi/10.1126/science.275.5298.350},
  urldate = {2024-01-22},
  abstract = {Quantum computation remains an enormously appealing but elusive goal. It is appealing because of its potential to perform superfast algorithms, such as finding prime factors in polynomial time, but also elusive because of the difficulty of simultaneously manipulating quantum degrees of freedom while preventing environmentally induced decoherence. A new approach to quantum computing is introduced based on the use of multiple-pulse resonance techniques to manipulate the small deviation from equilibrium of the density matrix of a macroscopic ensemble so that it appears to be the density matrix of a much lower dimensional pure state. A complete prescription for quantum computing is given for such a system.},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Gershenfeld_Chuang_1997_Bulk Spin-Resonance Quantum Computation.pdf}
}

@article{goldhaber-gordonOverviewNanoelectronicDevices1997,
  title = {Overview of Nanoelectronic Devices},
  author = {{Goldhaber-Gordon}, David and Montemerlo, Michael S. and Love, J. Christopher and Opiteck, Gregory J. and Ellenbogen, James C.},
  year = {1997},
  journal = {Proceedings of the IEEE},
  volume = {85},
  number = {4},
  pages = {521--540},
  publisher = {{IEEE}},
  doi = {10.1109/5.573739},
  url = {https://ieeexplore.ieee.org/abstract/document/573739/},
  urldate = {2024-01-22}
}

@article{goldLanguageIdentificationLimit1967,
  title = {Language Identification in the Limit},
  author = {Gold, E Mark},
  year = {1967},
  month = may,
  journal = {Information and Control},
  volume = {10},
  number = {5},
  pages = {447--474},
  issn = {0019-9958},
  doi = {10/bdrjbq},
  url = {http://www.sciencedirect.com/science/article/pii/S0019995867911655},
  urldate = {2020-03-18},
  abstract = {Language learnability has been investigated. This refers to the following situation: A class of possible languages is specified, together with a method of presenting information to the learner about an unknown language, which is to be chosen from the class. The question is now asked, ``Is the information sufficient to determine which of the possible languages is the unknown language?'' Many definitions of learnability are possible, but only the following is considered here: Time is quantized and has a finite starting time. At each time the learner receives a unit of information and is to make a guess as to the identity of the unknown language on the basis of the information received so far. This process continues forever. The class of languages will be considered learnable with respect to the specified method of information presentation if there is an algorithm that the learner can use to make his guesses, the algorithm having the following property: Given any language of the class, there is some finite time after which the guesses will all be the same and they will be correct. In this preliminary investigation, a language is taken to be a set of strings on some finite alphabet. The alphabet is the same for all languages of the class. Several variations of each of the following two basic methods of information presentation are investigated: A text for a language generates the strings of the language in any order such that every string of the language occurs at least once. An informant for a language tells whether a string is in the language, and chooses the strings in some order such that every string occurs at least once. It was found that the class of context-sensitive languages is learnable from an informant, but that not even the class of regular languages is learnable from a text.},
  langid = {english},
  annotation = {ZSCC: 0004693},
  file = {G:\Other computers\My Laptop\Zotero Files\Gold_1967_Language identification in the limit.pdf}
}

@book{goldsteinClassicalMechanics2008,
  title = {Classical Mechanics},
  author = {Goldstein, Herbert and Poole, Charles P. and Safko, John L.},
  year = {2008},
  edition = {3. ed., [Nachdr.]},
  publisher = {{Addison Wesley}},
  address = {{San Francisco Munich}},
  isbn = {978-0-201-65702-9},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Goldstein et al_2008_Classical mechanics.djvu}
}

@article{gordonNaturalHistoryNetworks1959,
  title = {The Natural History of Networks},
  author = {Gordon, Pask},
  year = {1959},
  journal = {Proceedings of International Tracts In Computer Science and Technology and their Application},
  volume = {2},
  pages = {232--263},
  url = {https://www.pangaro.com/pask/Pask-1960-TheNaturalHistoryofNetworks.pdf},
  urldate = {2023-12-23},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Gordon_1959_The natural history of networks.pdf}
}

@misc{grassbergerDataCompressionEntropy2002,
  title = {Data {{Compression}} and {{Entropy Estimates}} by {{Non-sequential Recursive Pair Substitution}}},
  author = {Grassberger, Peter},
  year = {2002},
  month = jul,
  number = {arXiv:physics/0207023},
  eprint = {physics/0207023},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/physics/0207023},
  urldate = {2024-01-20},
  abstract = {We argue that Non-sequential Recursive Pair Substitution (NSRPS) as suggested by Jim{\textbackslash}'enez-Monta{\textbackslash}{\textasciitilde}no and Ebeling can indeed be used as a basis for an optimal data compression algorithm. In particular, we prove for Markov sequences that NSRPS together with suitable codings of the substitutions and of the substitute series does not lead to a code length increase, in the limit of infinite sequence length. When applied to written English, NSRPS gives entropy estimates which are very close to those obtained by other methods. Using ca. 135 GB of input data from the project Gutenberg, we estimate the effective entropy to be \${\textbackslash}approx 1.82\$ bit/character. Extrapolating to infinitely long input, the true value of the entropy is estimated as \${\textbackslash}approx 0.8\$ bit/character.},
  archiveprefix = {arxiv},
  keywords = {Condensed Matter - Statistical Mechanics,Physics - Computational Physics,Physics - Data Analysis Statistics and Probability},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Grassberger_2002_Data Compression and Entropy Estimates by Non-sequential Recursive Pair.pdf;C\:\\Users\\lyxkg\\Zotero\\storage\\5PS9W3CJ\\0207023.html}
}

@article{griewankWhoInventedReverse2012,
  title = {Who Invented the Reverse Mode of Differentiation},
  author = {Griewank, Andreas},
  year = {2012},
  journal = {Documenta Mathematica, Extra Volume ISMP},
  volume = {389400},
  url = {https://content.ems.press/assets/public/full-texts/books/251/chapters/online-pdf/978-3-98547-540-7-chapter-4949.pdf},
  urldate = {2023-12-27},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Griewank_2012_Who invented the reverse mode of differentiation.pdf}
}

@book{grinsteadIntroductionProbability1997,
  title = {Introduction to Probability},
  author = {Grinstead, Charles M. and Snell, J. Laurie and Snell, J. Laurie},
  year = {1997},
  edition = {2nd rev. ed},
  publisher = {{American Mathematical Society}},
  address = {{Providence, RI}},
  isbn = {978-0-8218-0749-1},
  lccn = {QA273 .S668 1997},
  keywords = {Probabilities},
  file = {G:\Other computers\My Laptop\Zotero Files\Grinstead et al_1997_Introduction to probability.pdf}
}

@inproceedings{guzmanDecompositionVisualScene1968,
  title = {Decomposition of a Visual Scene into Three-Dimensional Bodies},
  booktitle = {Proceedings of the {{December}} 9-11, 1968, Fall Joint Computer Conference, Part {{I}} on - {{AFIPS}} '68 ({{Fall}}, Part {{I}})},
  author = {Guzm{\'a}n, Adolfo},
  year = {1968},
  pages = {291},
  publisher = {{ACM Press}},
  address = {{San Francisco, California}},
  doi = {10.1145/1476589.1476631},
  url = {http://portal.acm.org/citation.cfm?doid=1476589.1476631},
  urldate = {2023-12-29},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Guzm√°n_1968_Decomposition of a visual scene into three-dimensional bodies.pdf}
}

@book{hadamardEssayPsychologyInvention1954,
  title = {An Essay on the Psychology of Invention in the Mathematical Field},
  author = {Hadamard, Jacques},
  year = {1954},
  edition = {Unaltered and unabridged repr. of the enlarged ed. 1949},
  publisher = {{Dover Publications}},
  address = {{New York, NY}},
  isbn = {0-486-20107-4},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Hadamard_1995_An essay on the psychology of invention in the mathematical field.djvu}
}

@book{harrisIdentificationNonlinearSystems1966,
  title = {The Identification of Nonlinear Systems with Two-Level Inputs.},
  author = {Harris, George Henry},
  year = {1966},
  publisher = {{Princeton University}},
  url = {https://search.proquest.com/openview/5600befa79c1313583f0f48d9eac08c9/1.pdf?pq-origsite=gscholar&cbl=18750&diss=y},
  urldate = {2023-12-24},
  keywords = {#nosource}
}

@article{harrisIdentificationNonlinearSystems1967,
  title = {Identification of Nonlinear Systems},
  author = {Harris, G. H. and Lapidus, Leon},
  year = {1967},
  month = jun,
  journal = {Industrial \& Engineering Chemistry},
  volume = {59},
  number = {6},
  pages = {66--81},
  issn = {0019-7866, 1541-5724},
  doi = {10.1021/ie50690a012},
  url = {https://pubs.acs.org/doi/abs/10.1021/ie50690a012},
  urldate = {2023-12-23},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Harris_Lapidus_1967_Identification of nonlinear systems2.pdf}
}

@article{hayMarkPerceptronOperators1960,
  title = {Mark {{I}} Perceptron Operators' Manual},
  author = {Hay, John C. and Lynch, Ben E. and Smith, David R.},
  year = {1960},
  journal = {Cornell Aeronautical Laboratory Inc. Buffalo, NY, Tech. Rep},
  url = {https://apps.dtic.mil/sti/pdfs/AD0236965.pdf},
  urldate = {2023-12-30},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Hay et al_1960_Mark I perceptron operators' manual.pdf}
}

@book{heilbronerWorldlyPhilosophersLives1999,
  title = {The {{Worldly Philosophers}}: {{The Lives}}, {{Times And Ideas Of The Great Economic Thinkers}}, {{Seventh Edition}}},
  shorttitle = {The {{Worldly Philosophers}}},
  author = {Heilbroner, Robert L.},
  year = {1999},
  month = aug,
  edition = {7th Revised edition},
  publisher = {{Touchstone}},
  address = {{New York}},
  isbn = {978-0-684-86214-9},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Heilbroner_1999_The Worldly Philosophers.pdf}
}

@misc{hoffmannTrainingComputeOptimalLarge2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  year = {2022},
  month = mar,
  number = {arXiv:2203.15556},
  eprint = {2203.15556},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.15556},
  url = {http://arxiv.org/abs/2203.15556},
  urldate = {2023-12-06},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Hoffmann et al_2022_Training Compute-Optimal Large Language Models.pdf;C\:\\Users\\lyxkg\\Zotero\\storage\\D37RUW79\\2203.html}
}

@article{hubelReceptiveFieldsSingle1959,
  title = {Receptive Fields of Single Neurones in the Cat's Striate Cortex},
  author = {Hubel, David H. and Wiesel, Torsten N.},
  year = {1959},
  journal = {The Journal of physiology},
  volume = {148},
  number = {3},
  pages = {574},
  publisher = {{Wiley-Blackwell}},
  doi = {10.1113/jphysiol.1959.sp006308},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/},
  urldate = {2023-12-28},
  file = {G:\Other computers\My Laptop\Zotero Files\Hubel_Wiesel_1959_Receptive fields of single neurones in the cat's striate cortex.pdf}
}

@article{hungHowChinaCognitive2022,
  title = {How {{China}}'s {{Cognitive Warfare Works}}: {{A Frontline Perspective}} of {{Taiwan}}'s {{Anti-Disinformation Wars}}},
  shorttitle = {How {{China}}'s {{Cognitive Warfare Works}}},
  author = {Hung, Tzu-Chieh and Hung, Tzu-Wei},
  year = {2022},
  journal = {Journal of Global Security Studies},
  volume = {7},
  number = {4},
  pages = {ogac016},
  publisher = {{Oxford University Press}},
  doi = {10.1093/jogss/ogac016},
  url = {https://academic.oup.com/jogss/article-abstract/7/4/ogac016/6647447},
  urldate = {2024-01-23},
  file = {G:\Other computers\My Laptop\Zotero Files\Hung_Hung_2022_How China's Cognitive Warfare Works.pdf}
}

@article{jacobsAdaptiveMixturesLocal1991,
  title = {Adaptive Mixtures of Local Experts},
  author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  year = {1991},
  journal = {Neural computation},
  volume = {3},
  number = {1},
  pages = {79--87},
  publisher = {{MIT Press}},
  doi = {10.1162/neco.1991.3.1.79},
  url = {https://ieeexplore.ieee.org/abstract/document/6797059/},
  urldate = {2024-01-24},
  file = {G:\Other computers\My Laptop\Zotero Files\Jacobs et al_1991_Adaptive mixtures of local experts.pdf}
}

@article{jannaiHumanNotGamified2023,
  title = {Human or {{Not}}? {{A Gamified Approach}} to the {{Turing Test}}},
  shorttitle = {Human or {{Not}}?},
  author = {Jannai, Daniel and Meron, Amos and Lenz, Barak and Levine, Yoav and Shoham, Yoav},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2305.20010v1},
  urldate = {2024-01-19},
  abstract = {We present "Human or Not?", an online game inspired by the Turing test, that measures the capability of AI chatbots to mimic humans in dialog, and of humans to tell bots from other humans. Over the course of a month, the game was played by over 1.5 million users who engaged in anonymous two-minute chat sessions with either another human or an AI language model which was prompted to behave like humans. The task of the players was to correctly guess whether they spoke to a person or to an AI. This largest scale Turing-style test conducted to date revealed some interesting facts. For example, overall users guessed the identity of their partners correctly in only 68\% of the games. In the subset of the games in which users faced an AI bot, users had even lower correct guess rates of 60\% (that is, not much higher than chance). This white paper details the development, deployment, and results of this unique experiment. While this experiment calls for many extensions and refinements, these findings already begin to shed light on the inevitable near future which will commingle humans and AI.},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Jannai et al_2023_Human or Not.pdf}
}

@article{johnsonGoldTheoremCognitive2004,
  title = {Gold's Theorem and Cognitive Science},
  author = {Johnson, Kent},
  year = {2004},
  journal = {Philosophy of Science},
  volume = {71},
  number = {4},
  pages = {571--592},
  publisher = {{Cambridge University Press}},
  doi = {10.1086/423752},
  url = {https://www.cambridge.org/core/journals/philosophy-of-science/article/golds-theorem-and-cognitive-science/046D4E968548BBAF6059FC155AD26FDF},
  urldate = {2023-12-23},
  file = {G:\Other computers\My Laptop\Zotero Files\Johnson_2004_Gold's theorem and cognitive science.pdf}
}

@article{jordanHierarchicalMixturesExperts1994,
  title = {Hierarchical Mixtures of Experts and the {{EM}} Algorithm},
  author = {Jordan, Michael I. and Jacobs, Robert A.},
  year = {1994},
  journal = {Neural computation},
  volume = {6},
  number = {2},
  pages = {181--214},
  publisher = {{MIT Press}},
  doi = {10.1162/neco.1994.6.2.181},
  url = {https://ieeexplore.ieee.org/abstract/document/6796382/},
  urldate = {2024-01-19},
  file = {G:\Other computers\My Laptop\Zotero Files\Jordan_Jacobs_1994_Hierarchical mixtures of experts and the EM algorithm.pdf}
}

@book{jurafskySpeechLanguageProcessing2023,
  title = {Speech and {{Language Processing}}: {{An Introduction}} to {{Natural Language Processing}}, {{Computational Linguistics}} and {{Speech Recognition}}},
  shorttitle = {Speech and {{Language Processing}}},
  author = {Jurafsky, Dan and Martin, James H.},
  year = {2023},
  edition = {3},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Jurafsky_Martin_2023_Speech and Language Processing.pdf}
}

@inproceedings{kanalRecognitionSystemDesign1964,
  title = {Recognition System Design by Statistical Analysis},
  booktitle = {Proceedings of the 1964 19th {{ACM}} National Conference},
  author = {Kanal, Laveen N. and Randall, Neil C.},
  year = {1964},
  pages = {42--501},
  keywords = {No DOI found},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Kanal_Randall_1964_Recognition system design by statistical analysis.pdf;C\:\\Users\\lyxkg\\Zotero\\storage\\VQATP4YH\\800257.html}
}

@misc{karpathyRecipeTrainingNeural2019,
  title = {A {{Recipe}} for {{Training Neural Networks}}},
  author = {Karpathy, Andrej},
  year = {2019},
  month = apr,
  journal = {Andrej Karpathy blog},
  url = {https://karpathy.github.io/2019/04/25/recipe/},
  urldate = {2024-01-18},
  file = {C:\Users\lyxkg\Zotero\storage\UJZT6QGD\recipe.html}
}

@book{keynesGeneralTheoryEmployment2018,
  title = {The {{General Theory}} of {{Employment}}, {{Interest}}, and {{Money}}},
  author = {Keynes, John Maynard},
  year = {2018},
  address = {{Springer International Publishing}},
  isbn = {978-3-319-70343-5},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Keynes_2018_The General Theory of Employment, Interest, and Money.pdf}
}

@article{kingHowChineseGovernment2017,
  title = {How the {{Chinese}} Government Fabricates Social Media Posts for Strategic Distraction, Not Engaged Argument},
  author = {King, Gary and Pan, Jennifer and Roberts, Margaret E.},
  year = {2017},
  journal = {American political science review},
  volume = {111},
  number = {3},
  pages = {484--501},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/S0003055417000144},
  url = {https://www.cambridge.org/core/journals/american-political-science-review/article/how-the-chinese-government-fabricates-social-media-posts-for-strategic-distraction-not-engaged-argument/4662DB26E2685BAF1485F14369BD137C},
  urldate = {2023-12-16},
  file = {G:\Other computers\My Laptop\Zotero Files\King et al_2017_How the Chinese government fabricates social media posts for strategic.pdf}
}

@misc{kleinbergInherentTradeOffsFair2016,
  title = {Inherent {{Trade-Offs}} in the {{Fair Determination}} of {{Risk Scores}}},
  author = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
  year = {2016},
  month = nov,
  number = {arXiv:1609.05807},
  eprint = {1609.05807},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1609.05807},
  url = {http://arxiv.org/abs/1609.05807},
  urldate = {2024-01-10},
  abstract = {Recent discussion in the public sphere about algorithmic classification has involved tension between competing notions of what it means for a probabilistic classification to be fair to different groups. We formalize three fairness conditions that lie at the heart of these debates, and we prove that except in highly constrained special cases, there is no method that can satisfy these three conditions simultaneously. Moreover, even satisfying all three conditions approximately requires that the data lie in an approximate version of one of the constrained special cases identified by our theorem. These results suggest some of the ways in which key notions of fairness are incompatible with each other, and hence provide a framework for thinking about the trade-offs between them.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Kleinberg et al_2016_Inherent Trade-Offs in the Fair Determination of Risk Scores.pdf;C\:\\Users\\lyxkg\\Zotero\\storage\\PU9GJCXV\\1609.html}
}

@article{kleinComparativeReviewRecent1893,
  title = {A Comparative Review of Recent Researches in Geometry},
  author = {Klein, Felix},
  year = {1893},
  journal = {Bulletin of the American Mathematical Society},
  volume = {2},
  number = {10},
  pages = {215--249},
  doi = {10/bj7wwf},
  annotation = {ZSCC: 0000202},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Klein_1893_A comparative review of recent researches in geometry.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Klein_1893_A comparative review of recent researches in geometry2.pdf}
}

@article{knightAI30Years2016,
  title = {An {{AI}} with 30 {{Years}}' {{Worth}} of {{Knowledge Finally Goes}} to {{Work}}},
  author = {Knight, Will},
  year = {2016},
  month = mar,
  journal = {MIT Technology Review},
  url = {https://www.technologyreview.com/2016/03/14/108873/an-ai-with-30-years-worth-of-knowledge-finally-goes-to-work/},
  urldate = {2023-12-22},
  abstract = {An effort to encode the world's knowledge in a huge database has sometimes seemed impractical, but those behind the technology say it is finally ready.},
  langid = {english},
  file = {C:\Users\lyxkg\Zotero\storage\2GIJCFTQ\an-ai-with-30-years-worth-of-knowledge-finally-goes-to-work.html}
}

@article{krizhevskyImagenetClassificationDeep2012,
  title = {Imagenet Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  journal = {Advances in neural information processing systems},
  volume = {25},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Krizhevsky et al_2012_Imagenet classification with deep convolutional neural networks.pdf}
}

@incollection{krugmanRicardoDifficultIdea2002,
  title = {Ricardo's Difficult Idea: Why Intellectuals Don't Understand Comparative Advantage},
  shorttitle = {Ricardo's Difficult Idea},
  booktitle = {The Economics and Politics of International Trade},
  author = {Krugman, Paul},
  year = {2002},
  pages = {40--54},
  publisher = {{Routledge}},
  url = {https://books.google.com/books?hl=en&lr=&id=GLaGAgAAQBAJ&oi=fnd&pg=PA22&dq=RICARDO%27S+DIFFICULT+IDEA%0A%0A&ots=Z7LSpRydaP&sig=4SjoVkTY0HbnG5MJZkD8hy-acM4},
  urldate = {2023-12-06},
  keywords = {economics},
  file = {C:\Users\lyxkg\Zotero\storage\TLLCQE9S\openurl.html}
}

@article{langfordCOMPBASILISKFAQ1999,
  title = {{{COMP}}.{{BASILISK FAQ}}},
  author = {Langford, David},
  year = {1999},
  month = dec,
  journal = {Nature},
  volume = {402},
  number = {6761},
  pages = {465--465},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/44964},
  url = {https://www.nature.com/articles/44964},
  urldate = {2023-12-21},
  abstract = {Frequently asked questions about basilisks.},
  copyright = {1999 Macmillan Magazines Ltd.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {G:\Other computers\My Laptop\Zotero Files\Langford_1999_Comp.pdf}
}

@article{lauChinaBombardsTaiwan2024,
  title = {China Bombards {{Taiwan}} with Fake News Ahead of Election},
  author = {Lau, Stuart},
  year = {2024},
  month = jan,
  journal = {POLITICO},
  url = {https://www.politico.eu/article/china-bombards-taiwan-with-fake-news-ahead-of-election/},
  urldate = {2024-01-23},
  abstract = {China uses fabricated polls and outlandish claims on social media to bolster its camp in Taiwan's election.},
  langid = {english},
  file = {C:\Users\lyxkg\Zotero\storage\BQNQDI5Y\china-bombards-taiwan-with-fake-news-ahead-of-election.html}
}

@article{lecunGeneralizationNetworkDesign1989,
  title = {Generalization and Network Design Strategies},
  author = {LeCun, Yann},
  year = {1989},
  journal = {Connectionism in perspective},
  volume = {19},
  number = {143-155},
  pages = {18},
  publisher = {{North Holland}},
  url = {https://www.academia.edu/download/30766382/lecun.pdf},
  urldate = {2023-12-27},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\LeCun_1989_Generalization and network design strategies.pdf}
}

@article{lenatCycMidtermReport1990,
  title = {Cyc: {{A}} Midterm Report},
  shorttitle = {Cyc},
  author = {Lenat, Douglas and Guha, Ramanathan V.},
  year = {1990},
  journal = {AI magazine},
  volume = {11},
  number = {3},
  pages = {32--32},
  url = {https://ojs.aaai.org/index.php/aimagazine/article/view/842},
  urldate = {2023-11-01},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Lenat_Guha_1990_Cyc.pdf}
}

@article{lenatCycUsingCommon1985,
  title = {Cyc: {{Using}} Common Sense Knowledge to Overcome Brittleness and Knowledge Acquisition Bottlenecks},
  shorttitle = {{{CYC}}},
  author = {Lenat, Douglas B. and Prakash, Mayank and Shepherd, Mary},
  year = {1985},
  journal = {AI magazine},
  volume = {6},
  number = {4},
  pages = {65--65},
  url = {https://ojs.aaai.org/index.php/aimagazine/article/view/510/0},
  urldate = {2023-11-01},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Lenat et al_1985_Cyc2.pdf}
}

@book{levyHackersHeroesComputer2010,
  title = {Hackers: {{Heroes}} of the {{Computer Revolution}}},
  shorttitle = {Hackers},
  author = {Levy, Steven},
  year = {2010},
  month = jun,
  edition = {1st edition},
  publisher = {{O'Reilly Media}},
  address = {{Sebastopol, CA}},
  abstract = {This 25th anniversary edition of Steven Levy's classic book traces the exploits of the computer revolution's original hackers -- those brilliant and eccentric nerds from the late 1950s through the early '80s who took risks, bent the rules, and pushed the world in a radical new direction. With updated material from noteworthy hackers such as Bill Gates, Mark Zukerberg, Richard Stallman, and Steve Wozniak, Hackers is a fascinating story that begins in early computer research labs and leads to the first home computers.  Levy profiles the imaginative brainiacs who found clever and unorthodox solutions to computer engineering problems. They had a shared sense of values, known as "the hacker ethic," that still thrives today. Hackers captures a seminal period in recent history when underground activities blazed a trail for today's digital world, from MIT students finagling access to clunky computer-card machines to the DIY culture that spawned the Altair and the Apple II.   Amazon.com Exclusive: The Rant Heard Round the World  By Steven Levy   Author Steven Levy When I began researching Hackers--so many years ago that it's scary--I thought I'd largely be chronicling the foibles of a sociologically weird cohort who escaped normal human interaction by retreating to the sterile confines of computers labs. Instead, I discovered a fascinating, funny cohort who wound up transforming human interaction, spreading a culture that affects our views about everything from politics to entertainment to business. The stories of those amazing people and what they did is the backbone of Hackers: Heroes of the Computer Revolution.   But when I revisited the book recently to prepare the 25th Anniversary Edition of my first book, it was clear that I had luckily stumbled on the origin of a computer (and Internet) related controversy that still permeates the digital discussion. Throughout the book I write about something I called The Hacker Ethic, my interpretation of several principles implicitly shared by true hackers, no matter whether they were among the early pioneers from MIT's Tech Model Railroad Club (the Mesopotamia of hacker culture), the hardware hackers of Silicon Valley's Homebrew Computer Club (who invented the PC industry), or the slick kid programmers of commercial game software. One of those principles was ``Information Should Be Free.'' This wasn't a justification of stealing, but an expression of the yearning to know more so one could hack more. The programs that early MIT hackers wrote for big computers were stored on paper tapes. The hackers would keep the tapes in a drawer by the computer so anyone could run the program, change it, and then cut a new tape for the next person to improve. The idea of ownership was alien.  This idea came under stress with the advent of personal computers. The Homebrew Club was made of fanatic engineers, along with a few social activists who were thrilled at the democratic possibilities of PCs. The first home computer they could get their hands on was 1975's Altair, which came in a kit that required a fairly hairy assembly process. (Its inventor was Ed Roberts, an underappreciated pioneer who died earlier this year.) No software came with it. So it was a big deal when 19-year-old Harvard undergrad Bill Gates and his partner Paul Allen wrote a BASIC computer language for it. The Homebrew people were delighted with Altair BASIC, but unhappy that Gates and Allen charged real money for it. Some Homebrew people felt that their need for it outweighed their ability to pay. And after one of them got hold of a ``borrowed'' tape with the program, he showed up at a meeting with a box of copies (because it is so easy to make perfect copies in the digital age), and proceeded to distribute them to anyone who wanted one, gratis.  This didn't sit well with Bill Gates, who wrote what was to become a famous ``Letter to Hobbyists,'' basically accusing them of stealing his property. It was the computer-age equivalent to Luther posting the Ninety-Five Theses on the Castle Church. Gate's complaints would reverberate well into the Internet age, and variations on the controversy persist. Years later, when another undergrad named Shawn Fanning wrote a program called Napster that kicked off massive piracy of song files over the Internet, we saw a bloodier replay of the flap. Today, issues of cost, copying and control still rage--note Viacom's continuing lawsuit against YouTube and Google. And in my own business{\textemdash}journalism--availability of free news is threatening more traditional, expensive new-gathering. Related issues that also spring from controversies in Hackers are debates over the ``walled gardens'' of Facebook and Apple's iPad.  I ended the original Hackers with a portrait of Richard Stallman, an MIT hacker dedicated to the principle of free software. I recently revisited him while gathering new material for the 25th Anniversary Edition of Hackers, he was more hard core than ever. He even eschewed the Open Source movement for being insufficiently noncommercial.  When I spoke to Gates for the update, I asked him about his 1976 letter and the subsequent intellectual property wars. ``Don't call it war,'' he said. ``Thank God we have an incentive system. Striking the right balance of how this should work, you know, there's going to be tons of exploration.'' Then he applied the controversy to my own situation as a journalism. ``Things are in a crazy way for music and movies and books,'' he said. ``Maybe magazine writers will still get paid 20 years from now. Who knows? Maybe you'll have to cut hair during the day and just write articles at night.''  So Amazon.com readers, it's up to you. Those who have not read Hackers,, have fun and be amazed at the tales of those who changed the world and had a hell of time doing it. Those who have previously read and loved Hackers, replace your beat-up copies, or the ones you loaned out and never got back, with this beautiful 25th Anniversary Edition from O'Reilly with new material about my subsequent visits with Gates, Stallman, and younger hacker figures like Mark Zuckerberg of Facebook. If you don't I may have to buy a scissors--and the next bad haircut could be yours! Read Bill Gates' letter to hobbyists},
  isbn = {978-1-4493-8839-3},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Levy_2010_Hackers.pdf}
}

@book{lightmanProblemBookRelativity1975,
  title = {Problem {{Book}} in {{Relativity}} and {{Gravitation}}},
  author = {Lightman, Alan P. and Press, William H. and Price, Richard H. and Teukolsky, Saul A.},
  year = {1975},
  month = dec,
  edition = {First Edition},
  publisher = {{Princeton University Press}},
  address = {{Princeton, NJ}},
  abstract = {An essential resource for learning about general relativity and much more, from four leading expertsImportant and useful to every student of relativity, this book is a unique collection of some 475 problems--with solutions--in the fields of special and general relativity, gravitation, relativistic astrophysics, and cosmology. The problems are expressed in broad physical terms to enhance their pertinence to readers with diverse backgrounds. In their solutions, the authors have attempted to convey a mode of approach to these kinds of problems, revealing procedures that can reduce the labor of calculations while avoiding the pitfall of too much or too powerful formalism. Although well suited for individual use, the volume may also be used with one of the modem textbooks in general relativity.},
  isbn = {978-0-691-08162-5},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Lightman et al_1975_Problem Book in Relativity and Gravitation.djvu}
}

@article{linnainmaaTaylorExpansionAccumulated1976,
  title = {Taylor Expansion of the Accumulated Rounding Error},
  author = {Linnainmaa, Seppo},
  year = {1976},
  month = jun,
  journal = {BIT},
  volume = {16},
  number = {2},
  pages = {146--160},
  issn = {0006-3835, 1572-9125},
  doi = {10.1007/BF01931367},
  url = {http://link.springer.com/10.1007/BF01931367},
  urldate = {2023-12-27},
  langid = {english},
  keywords = {#nosource}
}

@book{marcusAlgebraicMindIntegrating2003,
  title = {The Algebraic Mind: {{Integrating}} Connectionism and Cognitive Science},
  shorttitle = {The Algebraic Mind},
  author = {Marcus, Gary F.},
  year = {2003},
  publisher = {{MIT press}},
  url = {https://books.google.com/books?hl=en&lr=&id=7YpuRUlFLm8C&oi=fnd&pg=PR9&dq=the+algebraic+mind&ots=asKHu3nT8m&sig=mu9Myw9KuXRhKsBa1Ywa0CgKB2g},
  urldate = {2023-12-23},
  keywords = {#nosource}
}

@misc{marcusDeepLearningCritical2018,
  title = {Deep {{Learning}}: {{A Critical Appraisal}}},
  shorttitle = {Deep {{Learning}}},
  author = {Marcus, Gary},
  year = {2018},
  month = jan,
  number = {arXiv:1801.00631},
  eprint = {1801.00631},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1801.00631},
  urldate = {2023-12-23},
  abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
  archiveprefix = {arxiv},
  keywords = {97R40,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.0,I.2.6,Statistics - Machine Learning},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Marcus_2018_Deep Learning.pdf;C\:\\Users\\lyxkg\\Zotero\\storage\\MJ6SKAA2\\1801.html}
}

@article{marcusNegativeEvidenceLanguage1993,
  title = {Negative Evidence in Language Acquisition},
  author = {Marcus, Gary F.},
  year = {1993},
  journal = {Cognition},
  volume = {46},
  number = {1},
  pages = {53--85},
  publisher = {{Elsevier}},
  doi = {10.1016/0010-0277(93)90022-N},
  url = {https://www.sciencedirect.com/science/article/pii/001002779390022N},
  urldate = {2023-12-23},
  file = {G:\Other computers\My Laptop\Zotero Files\Marcus_1993_Negative evidence in language acquisition.pdf}
}

@article{marcusOverregularizationLanguageAcquisition1992,
  title = {Overregularization in Language Acquisition},
  author = {Marcus, Gary F. and Pinker, Steven and Ullman, Michael and Hollander, Michelle and Rosen, T. John and Xu, Fei and Clahsen, Harald},
  year = {1992},
  journal = {Monographs of the society for research in child development},
  eprint = {1166115},
  eprinttype = {jstor},
  pages = {i--178},
  publisher = {{JSTOR}},
  url = {https://www.jstor.org/stable/1166115},
  urldate = {2023-12-23},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Marcus et al_1992_Overregularization in language acquisition.pdf}
}

@inproceedings{marczakAnalysisChinaGreat2015,
  title = {An {{Analysis}} of {{China}}'s ``{{Great Cannon}}''},
  booktitle = {5th {{USENIX Workshop}} on {{Free}} and {{Open Communications}} on the {{Internet}} ({{FOCI}} 15)},
  author = {Marczak, Bill and Weaver, Nicholas and Dalek, Jakub and Ensafi, Roya and Fifield, David and McKune, Sarah and Rey, Arn and {Scott-Railton}, John and Deibert, Ron and Paxson, Vern},
  year = {2015},
  url = {https://www.usenix.org/conference/foci15/workshop-program/presentation/marczak},
  urldate = {2023-12-16},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Marczak et al_2015_An Analysis of China‚Äôs ‚ÄúGreat Cannon‚Äù.pdf}
}

@book{mccorduckMachinesWhoThink2004,
  title = {Machines Who Think: A Personal Inquiry into the History and Prospects of Artificial Intelligence},
  shorttitle = {Machines Who Think},
  author = {McCorduck, Pamela},
  year = {2004},
  edition = {25th anniversary update},
  publisher = {{A.K. Peters}},
  address = {{Natick, Mass}},
  isbn = {978-1-56881-205-2},
  lccn = {Q335 .M23 2004},
  keywords = {Artificial intelligence,History},
  file = {G:\Other computers\My Laptop\Zotero Files\McCorduck_2004_Machines who think.pdf}
}

@article{mccullochLogicalCalculusIdeas1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  year = {1943},
  journal = {The bulletin of mathematical biophysics},
  volume = {5},
  number = {4},
  pages = {115--133},
  publisher = {{Springer}},
  doi = {10/djsbj6},
  annotation = {ZSCC: 0022555},
  file = {G:\Other computers\My Laptop\Zotero Files\McCulloch_Pitts_1943_A logical calculus of the ideas immanent in nervous activity.pdf}
}

@misc{mckenzieSeeingBank2023,
  title = {Seeing like a {{Bank}}},
  author = {McKenzie, Patrick},
  year = {2023},
  month = nov,
  journal = {Bits About Money},
  url = {https://www.bitsaboutmoney.com/archive/seeing-like-a-bank/},
  urldate = {2024-01-18},
  file = {C:\Users\lyxkg\Zotero\storage\UGTQXHL6\seeing-like-a-bank.html}
}

@article{mennChinaMeddlingTaiwan2024,
  title = {China's Meddling in {{Taiwan}} Election Presages Year of Misinformation Threats},
  author = {Menn, Joseph and Nix, Naomi and Zakrzewski, Cat and Verma, Pranshu},
  year = {2024},
  month = jan,
  journal = {Washington Post},
  issn = {0190-8286},
  url = {https://www.washingtonpost.com/technology/2024/01/12/taiwan-election-china-misinformation/},
  urldate = {2024-01-23},
  abstract = {More than half the world's population lives in countries holding elections in 2024 {\textemdash} an unprecedented test of systems to combat disinformation.},
  langid = {american},
  file = {C:\Users\lyxkg\Zotero\storage\TQ4FQX2D\taiwan-election-china-misinformation.html}
}

@article{mennTwitterGrapplesChinese2022,
  title = {Twitter Grapples with {{Chinese}} Spam Obscuring News of Protests},
  author = {Menn, Joseph},
  year = {2022},
  month = nov,
  journal = {Washington Post},
  issn = {0190-8286},
  url = {https://www.washingtonpost.com/technology/2022/11/27/twitter-china-spam-protests/},
  urldate = {2024-01-18},
  abstract = {Links to adult content overwhelmed other posts from Chinese cities where rallies escalated.},
  langid = {american},
  keywords = {#nosource}
}

@book{metzingerBeingNoOne2004,
  title = {Being No One: The Self-Model Theory of Subjectivity},
  shorttitle = {Being No One},
  author = {Metzinger, Thomas},
  year = {2004},
  series = {A {{Bradford}} Book},
  edition = {1. paperback ed},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass.}},
  abstract = {According to Thomas Metzinger, no such things as selves exist in the world: nobody ever had or was a self. All that exists are phenomenal selves, as they appear in conscious experience. The phenomenal self, however, is not a thing but an ongoing process; it is the content of a "transparent self-model." In Being No One, Metzinger, a German philosopher, draws strongly on neuroscientific research to present a representationalist and functional analysis of what a consciously experienced first-person perspective actually is. Building a bridge between the humanities and the empirical sciences of the mind, he develops new conceptual toolkits and metaphors; uses case studies of unusual states of mind such as agnosia, neglect, blindsight, and hallucinations; and offers new sets of multilevel constraints for the concept of consciousness. Metzinger's central question is: How exactly does strong, consciously experienced subjectivity emerge out of objective events in the natural world? His epistemic goal is to determine whether conscious experience, in particular the experience of being someone that results from the emergence of a phenomenal self, can be analysed on subpersonal levels of description. He also asks if and how our Cartesian intuitions that subjective experiences as such can never be reductively explained are themselves ultimately rooted in the deeper representational structure of our conscious minds},
  isbn = {978-0-262-63308-6 978-0-262-13417-0},
  langid = {english},
  annotation = {ZSCC: 0001532  OCLC: 254141765},
  file = {G:\Other computers\My Laptop\Zotero Files\Metzinger_2004_Being no one.pdf}
}

@book{metzingerEgoTunnelScience2009,
  title = {The Ego Tunnel: {{The}} Science of the Mind and the Myth of the Self},
  shorttitle = {The Ego Tunnel},
  author = {Metzinger, Thomas},
  year = {2009},
  publisher = {{Basic Books (AZ)}},
  annotation = {ZSCC: 0000011},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Metzinger_2009_The ego tunnel_review.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Metzinger_2009_The ego tunnel.pdf}
}

@article{millerTaiwanBombardedCyberattacks2024,
  title = {Taiwan Bombarded with Cyberattacks Ahead of Election},
  author = {Miller, Maggie and Gedeon, Joseph},
  year = {2024},
  month = jan,
  journal = {POLITICO},
  url = {https://www.politico.com/news/2024/01/11/taiwan-cyberattacks-election-china-00134841},
  urldate = {2024-01-23},
  abstract = {Cybersecurity groups link the attacks against Taiwanese critical infrastructure to China.},
  langid = {english},
  file = {C:\Users\lyxkg\Zotero\storage\YTPRQK7K\taiwan-cyberattacks-election-china-00134841.html}
}

@article{minkowskiGrundgleichungenFurElektromagnetischen1908,
  title = {Die {{Grundgleichungen}} F{\"u}r Die Elektromagnetischen {{Vorg{\"a}nge}} in Bewegten {{K{\"o}rpern}}},
  author = {Minkowski, Hermann},
  year = {1908},
  journal = {Nachrichten von der Gesellschaft der Wissenschaften zu G{\"o}ttingen, Mathematisch-Physikalische Klasse},
  volume = {1908},
  pages = {53--111},
  url = {https://eudml.org/doc/58707},
  urldate = {2023-12-12},
  keywords = {#nosource,No DOI found}
}

@book{minskyComputationFiniteInfinite1967,
  title = {Computation: Finite and Infinite Machines},
  shorttitle = {Computation},
  author = {Minsky, Marvin},
  year = {1967},
  publisher = {{Prentice-Hall}},
  address = {{Englewood Cliffs, NJ}},
  isbn = {978-0-13-165563-8},
  langid = {english},
  annotation = {OCLC: 899037781},
  file = {G:\Other computers\My Laptop\Zotero Files\Minsky_1967_Computation.djvu}
}

@misc{minskyMyUndergraduateThesis2011,
  title = {My Undergraduate Thesis in Fixed Point Theorems (70/151)},
  author = {Minsky, Marvin},
  year = {2011},
  url = {https://www.youtube.com/watch?v=k6yRP203UaM},
  urldate = {2023-12-25},
  abstract = {To hear more of Marvin Minsky's stories, go to the playlist: ~~~{$\bullet~$}Marvin~Minsky~-~The~amazing~videophon...~~ The scientist, Marvin Minsky (1927-2016) was one of the pioneers of the field of Artificial Intelligence, having founded the MIT AI Lab in 1970. Since the 1950s, his work involved trying to uncover human thinking processes and replicate them in machines. [Listener: Christopher Sykes]; date recorded: 2011] TRANSCRIPT: So then in my senior year{\dots} I was{\dots} I was majoring in physics mostly and with fairly poor grades because I was doing so many other things{\dots} and I thought I would make up for this by doing a good thesis, but it turned out at Harvard, physics students can't{\dots} they don't have an undergraduate thesis, but they do in math. So, I asked Andy Gleason why can't I just switch to math and do a thesis and he said: 'No problem,' signed something and I decided to do a thesis on fixed point theorems which was a beautiful little fragment of mathematics that{\dots} that I was very interested in. And there was a theorem that{\dots} Kakutani was a professor at Yale and here's an example of a fixed point theorem: Of{\dots} consider that at each point in the world on{\dots} on the earth, there's a temperature and there's a humidity, and these vary continuously over{\dots} over continuous functions. So there's two continuous functions on this sphere. Kakutani{\dots} somebody else had proved that{\dots} I forget who{\dots} Whitney, Professor Whitney, was a friend of mine at Harvard actually, had proved that you can find two points on the earth which are exactly opposite points which have the same temperature and the same humidity. This is rather surprising. So that{\dots} that was a famous fixed point theorem. Or you could say there's, if you consider mountains{\dots}  There's two points which have the same altitude and the same temperature, any two functions. It's very strange, why should that be? Anyway, Kakutani had proved that if you take a{\dots} you can find a triangle, an equilateral triangle, that is{\dots} 60¬∫ angles, the same thing is true, for an equilateral triangle you can find three points which will have the same temperature and pressure{\dots} or temperature and humidity. So that was Kakutani's fixed point theorem. He was a professor at Yale {\textendash} I'd never met him actually {\textendash} so that's very interesting because it shows that somehow if you have an extra point you can{\dots} you can do a little more because now they're equal at three points.  Isn't that funny? And so then I said: 'Well, what about other... why does it have to be an equilateral triangle?' And so I worked on that a lot and I managed to prove that if it were a right triangle, it was also true and also if it were a triangle which were three points of a regular pentagon, that would be{\dots} the same thing would be true. And that's as far as I got, I couldn't prove it for any old triangle. But, anyway, my proof of the thing for the pentagon impressed Gleason and that was probably why he told Princeton they had to admit me as a graduate student. That's the next step for a mathematician. So it was a great adventure and{\dots} but it always haunted me that I couldn't show this thing for any old triangle.},
  langid = {english},
  keywords = {#nosource}
}

@article{minskyNeuralanalogueCalculatorBased1952,
  title = {A Neural-Analogue Calculator Based upon a Probability Model of Reinforcement},
  author = {Minsky, Marvin},
  year = {1952},
  journal = {Harvard University Psychological Laboratories, Cambridge, Massachusetts},
  keywords = {#nosource,No DOI found}
}

@book{minskyPerceptronsIntroductionComputational1988,
  title = {Perceptrons: An Introduction to Computational Geometry},
  shorttitle = {Perceptrons},
  author = {Minsky, Marvin and Papert, Seymour},
  year = {1988},
  edition = {Expanded ed},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-63111-2},
  lccn = {Q327 .M55 1988},
  keywords = {Data processing,Geometry,Machine learning,Parallel processing (Electronic computers),Perceptrons},
  file = {G:\Other computers\My Laptop\Zotero Files\Minsky_Papert_1988_Perceptrons.pdf}
}

@misc{minskySelectedPublicationsMarvin,
  title = {Selected {{Publications}} of {{Marvin Minsky}}},
  author = {Minsky, Marvin Lee},
  url = {https://web.media.mit.edu/~minsky/bibliography.html},
  urldate = {2023-12-25},
  file = {C:\Users\lyxkg\Zotero\storage\9N4DJNQM\bibliography.html}
}

@book{minskySocietyMind1988,
  title = {The Society of Mind},
  author = {Minsky, Marvin},
  year = {1988},
  series = {A {{Touchstone}} Book},
  edition = {6. Pb-pr},
  publisher = {{Simon and Schuster}},
  address = {{New York}},
  isbn = {978-0-671-65713-0 978-0-671-60740-1},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Minsky_1988_The society of mind.epub}
}

@book{minskyTheoryNeuralanalogReinforcement1954,
  title = {Theory of Neural-Analog Reinforcement Systems and Its Application to the Brain-Model Problem},
  author = {Minsky, Marvin Lee},
  year = {1954},
  publisher = {{Princeton University}},
  url = {https://search.proquest.com/openview/a4ee1c0c78c1b940b9ec121f0e89cef8/1?pq-origsite=gscholar&cbl=18750&diss=y},
  urldate = {2023-12-26},
  file = {C:\Users\lyxkg\Zotero\storage\S5GRY747\intermediateredirectforezproxy.html}
}

@article{moravecFractalBranchingUltradexterous1996,
  title = {Fractal Branching Ultra-Dexterous Robots (Bush Robots)},
  author = {Moravec, Hans and Easudes, J. and Dellaert, F.},
  year = {1996},
  journal = {Technical report, NASA Advanced Concepts Research Project},
  keywords = {#nosource,No DOI found},
  annotation = {ZSCC: 0000008}
}

@book{moravecMindChildrenFuture1995,
  title = {Mind Children: The Future of Robot and Human Intelligence},
  shorttitle = {Mind Children},
  author = {Moravec, Hans},
  year = {1995},
  edition = {4. print},
  publisher = {{Harvard Univ. Press}},
  address = {{Cambridge}},
  isbn = {978-0-674-57618-6},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]  OCLC: 245755104},
  file = {G:\Other computers\My Laptop\Zotero Files\Moravec_1995_Mind children.pdf}
}

@inproceedings{moravecPigsCyberspace1993,
  title = {Pigs in Cyberspace},
  booktitle = {Vision 21: {{Interdisciplinary Science}} and {{Engineering}} in the {{Era}} of {{Cyberspace}}},
  author = {Moravec, Hans},
  year = {1993},
  address = {{NASA. Lewis Research Center}},
  keywords = {No DOI found},
  annotation = {ZSCC: 0000047},
  file = {G:\Other computers\My Laptop\Zotero Files\Moravec_1993_Pigs in cyberspace.pdf}
}

@book{moravecRobotMereMachine1999,
  title = {Robot: Mere Machine to Transcendent Mind},
  shorttitle = {Robot},
  author = {Moravec, Hans P.},
  year = {1999},
  publisher = {{Oxford University Press}},
  url = {https://books.google.com/books?hl=en&lr=&id=fduW6KHhWtQC&oi=fnd&pg=PP9&dq=Moravec,+H.+(1998)+Robot,+Being++mere+machine+to+transcendent+mind.+(forthcoming)+Oxford+University+Press.%0A%0A&ots=SzlArn1c8m&sig=tgVMeQwNA5mlQ0nogP2-CwvWv4Q},
  urldate = {2024-01-22}
}

@article{moravecWhenWillComputer1998,
  title = {When Will Computer Hardware Match the Human Brain},
  author = {Moravec, Hans},
  year = {1998},
  journal = {Journal of evolution and technology},
  volume = {1},
  number = {1},
  pages = {10},
  abstract = {This paper describes how the performance of AI machines tends to improve at the same pace that AI researchers get access to faster hardware. The processing power and memory capacity necessary to match general intellectual performance of the human brain are estimated. Based on extrapolation of past trends and on examination of technologies under development, it is predicted that the required hardware will be available in cheap machines in the 2020s.},
  keywords = {No DOI found},
  annotation = {ZSCC: 0000375},
  file = {G:\Other computers\My Laptop\Zotero Files\Moravec_1998_When will computer hardware match the human brain.pdf}
}

@article{morozovPlanningMachineProject2014,
  title = {The Planning Machine. {{Project Cybersyn}} and the Origins of the Big Data Nation},
  author = {Morozov, Evgeny},
  year = {2014},
  journal = {The New Yorker},
  keywords = {#nosource,No DOI found}
}

@article{muzaffarChineseJournalistAttempts2022,
  title = {Chinese Journalist Attempts Suicide after Cyber-Bullying over {{Shinzo Abe}} Reportage},
  author = {Muzaffar, Maroosha},
  year = {2022},
  month = jul,
  journal = {The Independent},
  url = {https://www.independent.co.uk/asia/china/zeng-ying-suicide-shinzo-abe-b2128922.html},
  urldate = {2024-01-18},
  abstract = {Zeng Ying allegedly tried to kill herself after almost a week of massive trolling by Chinese netizens},
  chapter = {Asia},
  langid = {english},
  file = {C:\Users\lyxkg\Zotero\storage\AECX6HEC\zeng-ying-suicide-shinzo-abe-b2128922.html}
}

@article{nagyNeuralNetworksthenNow1991,
  title = {Neural Networks-Then and Now},
  author = {Nagy, George},
  year = {1991},
  journal = {IEEE Transactions on Neural Networks},
  volume = {2},
  number = {2},
  pages = {316--318},
  publisher = {{IEEE}},
  doi = {10.1109/72.80343},
  url = {https://ieeexplore.ieee.org/abstract/document/80343/},
  urldate = {2023-12-30},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Nagy_1991_Neural networks-then and now.pdf;C\:\\Users\\lyxkg\\Zotero\\storage\\3H3CZ2E6\\openurl.html}
}

@book{newellHumanProblemSolving1972,
  title = {Human Problem Solving},
  author = {Newell, Allen and Simon, Herbert Alexander},
  year = {1972},
  isbn = {978-1-63561-792-4},
  langid = {english},
  keywords = {#nosource}
}

@book{nilssonQuestArtificialIntelligence2009,
  title = {The {{Quest}} for {{Artificial Intelligence}}},
  author = {Nilsson, Nils J.},
  year = {2009},
  month = oct,
  edition = {1st edition},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York}},
  isbn = {978-0-521-12293-1},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Nilsson_2009_The Quest for Artificial Intelligence.pdf}
}

@incollection{norvigChomskyTwoCultures2017,
  title = {{On Chomsky and the Two Cultures of Statistical Learning}},
  booktitle = {{Berechenbarkeit der Welt? Philosophie und Wissenschaft im Zeitalter von Big Data}},
  author = {Norvig, Peter},
  editor = {Pietsch, Wolfgang and Wernecke, J{\"o}rg and Ott, Maximilian},
  year = {2017},
  pages = {61--83},
  publisher = {{Springer Fachmedien}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-12153-2_3},
  url = {https://norvig.com/chomsky.html},
  urldate = {2023-12-22},
  isbn = {978-3-658-12153-2},
  langid = {ngerman},
  file = {G:\Other computers\My Laptop\Zotero Files\Norvig_2017_On Chomsky and the Two Cultures of Statistical Learning.pdf}
}

@misc{norvigSingularityEyeBeholder2021,
  title = {Singularity {{Is}} in the {{Eye}} of the {{Beholder}}},
  author = {Norvig, Peter},
  year = {2021},
  month = dec,
  url = {https://wandb.ai/wandb_fc/gradient-dissent/reports/Peter-Norvig-Google-s-Director-of-Research-Singularity-is-in-the-eye-of-the-beholder--Vmlldzo2MTYwNjk},
  urldate = {2024-01-18},
  abstract = {0:00 Singularity is in the eye of the beholder 0:32 Introduction 1:09 Project Euler 2:42 Advent of Code/Pytudes 4:55 New sections in the new version of his book 10:32 Unreasonable effectiveness of data Paper 15 years later 14:44 What advice would you give to a young researcher? 16:03 Computing power in the evolution of deep learning 19:19 What's been surprising in the development of AI? 24:21 From AlphaGo to human-like intelligence 28:46 What in AI has been surprisingly hard or easy? 32:11 Synthetic data and language 35:16 Singularity is in the eye of the beholder 38:43 The future of python in ML and why he used it in his book 43:00 Underrated topic in ML and bottlenecks in production},
  collaborator = {Biewald, Lukas},
  langid = {english},
  file = {C:\Users\lyxkg\Zotero\storage\R9IS47P9\Peter-Norvig-Google-s-Director-of-Research-Singularity-is-in-the-eye-of-the-beholder--Vmlldzo2M.html}
}

@article{ohaganPolynomialChaosTutorial2013,
  title = {Polynomial Chaos: {{A}} Tutorial and Critique from a Statistician's Perspective},
  shorttitle = {Polynomial Chaos},
  author = {O'Hagan, Anthony},
  year = {2013},
  journal = {SIAM/ASA J. Uncertainty Quantification},
  volume = {20},
  pages = {1--20},
  url = {http://courses.washington.edu/me333afe/Polynomial-chaos_OHagan.pdf},
  urldate = {2023-12-23},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\O‚ÄôHagan_2013_Polynomial chaos.pdf}
}

@phdthesis{olazaranHistoricalSociologyNeural1991,
  title = {A Historical Sociology of Neural Network Research},
  author = {Olazaran, Mikel},
  year = {1991},
  url = {https://era.ed.ac.uk/handle/1842/20075},
  urldate = {2023-12-30},
  abstract = {It has been argued that science is generated and validated through processes of controversy, and that controversies are 'closed' through 'rhetorical' processes of 'enrolment of heterogeneous allies and resources.' It has also been argued that, once a controversy is closed, it is increasingly difficult for the 'losing' position to maintain the plausibility of its views, arguments, and interpretations (words like 'reification,' 'inertia,' and 'institutionalisation' have been used to refer to this). Controversies have shaped neural network research throughout its history, from the 1950s to the 1980s. In this dissertation I analyse the history of neural network research using a 'controversy/rhetorical tactics/enrolment of allies and resources/closure' scheme. I claim that the result is a useful and powerful interpretation of the main developments of the evolution of neural network research. The neural network controversy is especially interesting because it was once (in the late 1960s) closed against neural networks, and twenty years later (in the late 1980s) it was reopened. The history of neural network research can be seen as the history of the closure and reopening of the neural network controversy.},
  langid = {english},
  school = {The University of Edinburgh},
  keywords = {No DOI found},
  annotation = {Accepted: 2017-02-14T12:22:47Z},
  file = {G:\Other computers\My Laptop\Zotero Files\Mikel_1991_A historical sociology of neural network research.pdf}
}

@incollection{olazaranSociologicalHistoryNeural1993,
  title = {A {{Sociological History}} of the {{Neural Network Controversy}}},
  booktitle = {Advances in {{Computers}}},
  author = {Olazaran, Mikel},
  editor = {Yovits, Marshall C.},
  year = {1993},
  month = jan,
  volume = {37},
  pages = {335--425},
  publisher = {{Elsevier}},
  doi = {10.1016/S0065-2458(08)60408-8},
  url = {https://www.sciencedirect.com/science/article/pii/S0065245808604088},
  urldate = {2023-10-31},
  abstract = {This chapter discusses the scientific controversies that have shaped neural network research from a sociological point of view. It looks at the controversy that surrounded Frank Rosenblatt's perceptron machine in the late 1950s and early 1960s. Rosenblatt was well aware of the main problems of his machine, and that he even insisted on them in his books and papers. Emphasis is given on one of the main problems of early neural network research, namely the issue of training multilayer systems. In the middle of the perceptron controversy, Minsky and Papert embarked on a project aimed at showing the limitations of Rosenblatt's perceptron beyond doubt. The chapter analyzes the main results of that project, and shows that Minsky and Papert, and neural network researchers interpreted those results rather differently. It discusses the processes through which this interpretative flexibility was closed and the effects that the crisis of early neural network research had upon the three most important neural network groups of the time, namely Widrow's group, Rosenblatt's group, and the group at SRI. The chapter also looks at the influence that factors like the emergence of symbolic artificial intelligence (AI) and computer technology had on the closure of the neural network controversy. After the closure of the perceptron controversy, symbol-processing remained the dominant approach to AI over the years, until the early 1980s. Some of the most important aspects of that changing context are reviewed and the history of back-propagation is discussed.},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Olazaran_1993_A Sociological History of the Neural Network Controversy.pdf;C\:\\Users\\lyxkg\\Zotero\\storage\\7J29SFKH\\S0065245808604088.html}
}

@article{olazaranSociologicalStudyOfficial1996,
  title = {A {{Sociological Study}} of the {{Official History}} of the {{Perceptrons Controversy}}},
  author = {Olazaran, Mikel},
  year = {1996},
  journal = {Social Studies of Science},
  volume = {26},
  number = {3},
  eprint = {285702},
  eprinttype = {jstor},
  pages = {611--659},
  publisher = {{Sage Publications, Ltd.}},
  issn = {0306-3127},
  doi = {10.1177/030631296026003005},
  url = {https://www.jstor.org/stable/285702},
  urldate = {2023-10-27},
  abstract = {In this paper, I analyze the controversy within Artificial Intelligence (AI) which surrounded the 'perceptron' project (and neural nets in general) in the late 1950s and early 1960s. I devote particular attention to the proofs and arguments of Minsky and Papert, which were interpreted as showing that further progress in neural nets was not possible, and that this approach to AI had to be abandoned. I maintain that this official interpretation of the debate was a result of the emergence, institutionalization and (importantly) legitimation of the symbolic AI approach (with its resource allocation system and authority structure). At the 'research-area' level, there was considerable interpretative flexibility. This interpretative flexibility was further demonstrated by the revival of neural nets in the late 1980s, and subsequent rewriting of the official history of the debate.},
  file = {G:\Other computers\My Laptop\Zotero Files\Olazaran_1996_A Sociological Study of the Official History of the Perceptrons Controversy.pdf}
}

@book{papertChildrenMachineRethinking1994,
  title = {The {{Children}}'s {{Machine}}: {{Rethinking School In The Age Of The Computer}}},
  shorttitle = {The {{Children}}'s {{Machine}}},
  author = {Papert, Seymour A.},
  year = {1994},
  month = apr,
  edition = {Revised ed. edition},
  publisher = {{Basic Books}},
  address = {{New York}},
  abstract = {In his classsic book, Mindstorms: Children, Computers, and powerful Ideas, Seymour Papert set out a vision of how computers could change school. In The Children's Machine he now looks back over a decade during which American schools acquired more than three million computers and assesses progress and resistance to progress.},
  isbn = {978-0-465-01063-9},
  langid = {english}
}

@article{papertOneAIMany1988,
  title = {One {{AI}} or Many?},
  author = {Papert, Seymour},
  year = {1988},
  journal = {Daedalus},
  eprint = {20025136},
  eprinttype = {jstor},
  pages = {1--14},
  publisher = {{JSTOR}},
  url = {https://www.jstor.org/stable/20025136},
  urldate = {2023-12-23},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Papert_1988_One AI or many.pdf}
}

@article{papertSummerVisionProject1966,
  title = {The Summer Vision Project},
  author = {Papert, Seymour A.},
  year = {1966},
  url = {https://dspace.mit.edu/handle/1721.1/6125},
  urldate = {2023-12-25},
  keywords = {#nosource,No DOI found}
}

@misc{pattersonCarbonEmissionsLarge2021,
  title = {Carbon {{Emissions}} and {{Large Neural Network Training}}},
  author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  year = {2021},
  month = apr,
  number = {arXiv:2104.10350},
  eprint = {2104.10350},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2104.10350},
  urldate = {2023-12-06},
  abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {$<$}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Patterson et al_2021_Carbon Emissions and Large Neural Network Training.pdf;C\:\\Users\\lyxkg\\Zotero\\storage\\GAA6GWGQ\\2104.html}
}

@article{pickeringScienceUnknowableStafford2004,
  title = {The Science of the Unknowable: {{Stafford Beer}}'s Cybernetic Informatics},
  shorttitle = {The Science of the Unknowable},
  author = {Pickering, Andrew},
  year = {2004},
  journal = {Kybernetes},
  volume = {33},
  number = {3/4},
  pages = {499--521},
  doi = {10/dqjsk8},
  annotation = {ZSCC: 0000057},
  file = {G:\Other computers\My Laptop\Zotero Files\Pickering_2004_The science of the unknowable.pdf}
}

@article{pinkerFutureTense2002,
  title = {The Past and Future of the Past Tense},
  author = {Pinker, Steven and Ullman, Michael T.},
  year = {2002},
  journal = {Trends in cognitive sciences},
  volume = {6},
  number = {11},
  pages = {456--463},
  publisher = {{Elsevier}},
  doi = {10.1016/S1364-6613(02)01990-3},
  url = {https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(02)01990-3},
  urldate = {2023-12-23},
  file = {C:\Users\lyxkg\Zotero\storage\89QVNYYK\openurl.html}
}

@book{pinkerSenseStyleThinking2015,
  title = {The Sense of Style: The Thinking Person's Guide to Writing in the 21st Century},
  shorttitle = {The Sense of Style},
  author = {Pinker, Steven},
  year = {2015},
  publisher = {{Penguin Books}},
  address = {{London}},
  isbn = {978-0-241-95771-4},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Pinker_2015_The sense of style.epub}
}

@article{pollackNoHarmIntended1989,
  title = {No Harm Intended: {{Marvin L}}. {{Minsky}} and {{Seymour A}}. {{Papert}}. {{Perceptrons}}: {{An Introduction}} to {{Computational Geometry}}, {{Expanded Edition}}. {{Cambridge}}, {{MA}}: {{MIT Press}}, 1988. {{Pp}}. 292. \$12.50 (Paper)},
  shorttitle = {No Harm Intended},
  author = {Pollack, Jordan B.},
  year = {1989},
  month = sep,
  journal = {Journal of Mathematical Psychology},
  volume = {33},
  number = {3},
  pages = {358--365},
  issn = {0022-2496},
  doi = {10.1016/0022-2496(89)90015-1},
  url = {https://www.sciencedirect.com/science/article/pii/0022249689900151},
  urldate = {2023-10-29},
  file = {G:\Other computers\My Laptop\Zotero Files\Pollack_1989_No harm intended.pdf}
}

@inproceedings{rainaLargescaleDeepUnsupervised2009,
  title = {Large-Scale Deep Unsupervised Learning Using Graphics Processors},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
  year = {2009},
  month = jun,
  pages = {873--880},
  publisher = {{ACM}},
  address = {{Montreal Quebec Canada}},
  doi = {10.1145/1553374.1553486},
  url = {https://dl.acm.org/doi/10.1145/1553374.1553486},
  urldate = {2023-12-30},
  isbn = {978-1-60558-516-1},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Raina et al_2009_Large-scale deep unsupervised learning using graphics processors.pdf}
}

@book{ricardoPrinciplesPoliticalEconomy1817,
  title = {On {{The Principles}} of {{Political Economy}}, and {{Taxation}}},
  author = {Ricardo, David},
  year = {1817},
  address = {{London}},
  url = {https://www.gutenberg.org/ebooks/33310},
  urldate = {2023-01-02},
  copyright = {Public domain in the USA.},
  langid = {english},
  lccn = {EBook-No. 33310},
  keywords = {Economics},
  file = {G:\Other computers\My Laptop\Zotero Files\Ricardo_1817_The Principles of Political Economy, and Taxation.html}
}

@article{riquelmeScalingVisionSparse2021,
  title = {Scaling Vision with Sparse Mixture of Experts},
  author = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\'e} and Keysers, Daniel and Houlsby, Neil},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {8583--8595},
  url = {https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html},
  urldate = {2024-01-18},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Riquelme et al_2021_Scaling vision with sparse mixture of experts.pdf}
}

@book{robbOpticalGeometryMotion1911,
  title = {Optical Geometry of Motion: {{A}} New View of the Theory of Relativity},
  shorttitle = {Optical Geometry of Motion},
  author = {Robb, Alfred Arthur},
  year = {1911},
  publisher = {{W. Heffer}},
  url = {https://books.google.com/books?hl=en&lr=&id=XGBDAAAAIAAJ&oi=fnd&pg=PA1&dq=Alfred+Robb+(1911)+Optical+Geometry+of+Motion&ots=cwyFVfhPKJ&sig=TVyiyZ5nd5jg2ioAiBjuoeOKyWo},
  urldate = {2023-12-12},
  file = {C:\Users\lyxkg\Zotero\storage\SARWSMU2\books.html}
}

@book{rosenblattPrinciplesNeurodynamicsPerceptrons1962,
  title = {Principles of Neurodynamics: {{Perceptrons}} and the Theory of Brain Mechanisms},
  shorttitle = {Principles of Neurodynamics},
  author = {Rosenblatt, Frank},
  year = {1962},
  volume = {55},
  publisher = {{Spartan books Washington, DC}},
  url = {https://apps.dtic.mil/sti/citations/AD0256582},
  urldate = {2023-12-28},
  file = {G:\Other computers\My Laptop\Zotero Files\Rosenblatt_1962_Principles of neurodynamics.pdf}
}

@book{rosenblithSensoryCommunicationContributions2012,
  title = {Sensory Communication: Contributions to the {{Symposium}} on {{Principles}} of {{Sensory Communication}}, {{July}} 19-{{August}} 1, 1959, {{Endicott House}}, {{M}}.{{I}}.{{T}}},
  shorttitle = {Sensory Communication},
  editor = {Rosenblith, Walter A.},
  year = {2012},
  publisher = {{The M.I.T. Press, Massachusetts Institute of Technology}},
  address = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-51842-0},
  lccn = {QP435 .S9 1959},
  keywords = {#nosource,Communication,Senses and sensation}
}

@book{rosenfeldTalkingNetsOral2000,
  title = {Talking {{Nets}}: {{An Oral History}} of {{Neural Networks}}},
  shorttitle = {Talking {{Nets}}},
  editor = {Rosenfeld, Edward and Anderson, James A.},
  year = {2000},
  month = feb,
  edition = {Reprint edition},
  publisher = {{The MIT Press}},
  abstract = {Since World War II, a group of scientists has been attempting to understand the human nervous system and to build computer systems that emulate the brain's abilities. Many of the early workers in this field of neural networks came from cybernetics; others came from neuroscience, physics, electrical engineering, mathematics, psychology, even economics. In this collection of interviews, those who helped to shape the field share their childhood memories, their influences, how they became interested in neural networks, and what they see as its future.The subjects tell stories that have been told, referred to, whispered about, and imagined throughout the history of the field. Together, the interviews form a Rashomon-like web of reality. Some of the mythic people responsible for the foundations of modern brain theory and cybernetics, such as Norbert Wiener, Warren McCulloch, and Frank Rosenblatt, appear prominently in the recollections. The interviewees agree about some things and disagree about more. Together, they tell the story of how science is actually done, including the false starts, and the Darwinian struggle for jobs, resources, and reputation. Although some of the interviews contain technical material, there is no actual mathematics in the book.Contributors : James A. Anderson, Michael Arbib, Gail Carpenter, Leon Cooper, Jack Cowan, Walter Freeman, Stephen Grossberg, Robert Hecht-Neilsen, Geoffrey Hinton, Teuvo Kohonen, Bart Kosko, Jerome Lettvin, Carver Mead, David Rumelhart, Terry Sejnowski, Paul Werbos, Bernard Widrow.},
  isbn = {978-0-262-51111-7},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Rosenfeld_Anderson_2000_Talking Nets.pdf}
}

@misc{rumelhartLearningInternalRepresentations1985,
  title = {Learning Internal Representations by Error Propagation},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1985},
  publisher = {{Institute for Cognitive Science, University of California, San Diego La {\dots}}},
  url = {https://apps.dtic.mil/sti/citations/ADA164453},
  urldate = {2023-12-28},
  file = {G:\Other computers\My Laptop\Zotero Files\Rumelhart et al_1985_Learning internal representations by error propagation.pdf}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  journal = {nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/323533a0},
  file = {G:\Other computers\My Laptop\Zotero Files\Rumelhart et al_1986_Learning representations by back-propagating errors.pdf}
}

@book{rumelhartParallelDistributedProcessing1986,
  title = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition},
  shorttitle = {Parallel Distributed Processing},
  author = {Rumelhart, David E. and McClelland, James L.},
  year = {1986},
  series = {Computational Models of Cognition and Perception},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-18120-4 978-0-262-13218-3},
  lccn = {BF455 .R853 1986},
  keywords = {Cognition,Human information processing},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Rumelhart_McClelland_1986_Parallel distributed processing_vol1.pdf;G\:\\Other computers\\My Laptop\\Zotero Files\\Rumelhart_McClelland_1986_Parallel distributed processing_vol2.pdf}
}

@misc{SACBugFix,
  title = {{{SAC}} Bug Fix {$\cdot$} Berkeleydeeprlcourse/Homework\_fall2022@d2227e8},
  url = {https://github.com/berkeleydeeprlcourse/homework_fall2022/commit/d2227e86fb1faf02c115c30c3762f1cfc049c84e},
  urldate = {2024-01-18},
  file = {C:\Users\lyxkg\Zotero\storage\QR8ABZ3U\d2227e86fb1faf02c115c30c3762f1cfc049c84e.html}
}

@misc{schmidhuberOneBigNet2018,
  title = {One {{Big Net For Everything}}},
  author = {Schmidhuber, Juergen},
  year = {2018},
  month = feb,
  number = {arXiv:1802.08864},
  eprint = {1802.08864},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1802.08864},
  urldate = {2023-12-23},
  abstract = {I apply recent work on "learning to think" (2015) and on PowerPlay (2011) to the incremental training of an increasingly general problem solver, continually learning to solve new tasks without forgetting previous skills. The problem solver is a single recurrent neural network (or similar general purpose computer) called ONE. ONE is unusual in the sense that it is trained in various ways, e.g., by black box optimization / reinforcement learning / artificial evolution as well as supervised / unsupervised learning. For example, ONE may learn through neuroevolution to control a robot through environment-changing actions, and learn through unsupervised gradient descent to predict future inputs and vector-valued reward signals as suggested in 1990. User-given tasks can be defined through extra goal-defining input patterns, also proposed in 1990. Suppose ONE has already learned many skills. Now a copy of ONE can be re-trained to learn a new skill, e.g., through neuroevolution without a teacher. Here it may profit from re-using previously learned subroutines, but it may also forget previous skills. Then ONE is retrained in PowerPlay style (2011) on stored input/output traces of (a) ONE's copy executing the new skill and (b) previous instances of ONE whose skills are still considered worth memorizing. Simultaneously, ONE is retrained on old traces (even those of unsuccessful trials) to become a better predictor, without additional expensive interaction with the enviroment. More and more control and prediction skills are thus collapsed into ONE, like in the chunker-automatizer system of the neural history compressor (1991). This forces ONE to relate partially analogous skills (with shared algorithmic information) to each other, creating common subroutines in form of shared subnetworks of ONE, to greatly speed up subsequent learning of additional, novel but algorithmically related skills.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Schmidhuber_2018_One Big Net For Everything.pdf;C\:\\Users\\lyxkg\\Zotero\\storage\\D4GDJX4D\\1802.html}
}

@book{sejnowskiDeepLearningRevolution2018,
  title = {The {{Deep Learning Revolution}}},
  author = {Sejnowski, Terrence J.},
  year = {2018},
  month = oct,
  edition = {Illustrated edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts London, England}},
  abstract = {How deep learning{\textemdash}from Google Translate to driverless cars to personal cognitive assistants{\textemdash}is changing our lives and transforming every sector of the economy.The deep learning revolution has brought us driverless cars, the greatly improved Google Translate, fluent conversations with Siri and Alexa, and enormous profits from automated trading on the New York Stock Exchange. Deep learning networks can play poker better than professional poker players and defeat a world champion at Go. In this book, Terry Sejnowski explains how deep learning went from being an arcane academic field to a disruptive technology in the information economy.Sejnowski played an important role in the founding of deep learning, as one of a small group of researchers in the 1980s who challenged the prevailing logic-and-symbol based version of AI. The new version of AI Sejnowski and others developed, which became deep learning, is fueled instead by data. Deep networks learn from data in the same way that babies experience the world, starting with fresh eyes and gradually acquiring the skills needed to navigate novel environments. Learning algorithms extract information from raw data; information can be used to create knowledge; knowledge underlies understanding; understanding leads to wisdom. Someday a driverless car will know the road better than you do and drive with more skill; a deep learning network will diagnose your illness; a personal cognitive assistant will augment your puny human brain. It took nature many millions of years to evolve human intelligence; AI is on a trajectory measured in decades. Sejnowski prepares us for a deep learning future.},
  isbn = {978-0-262-03803-4},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Sejnowski_2018_The Deep Learning Revolution.pdf}
}

@article{shannonPredictionEntropyPrinted1951,
  title = {Prediction and Entropy of Printed {{English}}},
  author = {Shannon, Claude E.},
  year = {1951},
  journal = {Bell system technical journal},
  volume = {30},
  number = {1},
  pages = {50--64},
  publisher = {{Wiley Online Library}},
  doi = {10.1002/j.1538-7305.1951.tb01366.x},
  file = {G:\Other computers\My Laptop\Zotero Files\Shannon_1951_Prediction and entropy of printed English.pdf}
}

@article{sharmaScalingLawsData2022,
  title = {Scaling Laws from the Data Manifold Dimension},
  author = {Sharma, Utkarsh and Kaplan, Jared},
  year = {2022},
  journal = {The Journal of Machine Learning Research},
  volume = {23},
  number = {1},
  pages = {343--376},
  publisher = {{JMLRORG}},
  keywords = {No DOI found},
  file = {G\:\\Other computers\\My Laptop\\Zotero Files\\Sharma_Kaplan_2022_Scaling laws from the data manifold dimension.pdf;C\:\\Users\\lyxkg\\Zotero\\storage\\YDAM324Z\\3586589.html}
}

@misc{shazeerOutrageouslyLargeNeural2017,
  title = {Outrageously {{Large Neural Networks}}: {{The Sparsely-Gated Mixture-of-Experts Layer}}},
  shorttitle = {Outrageously {{Large Neural Networks}}},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  year = {2017},
  month = jan,
  number = {arXiv:1701.06538},
  eprint = {1701.06538},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1701.06538},
  url = {http://arxiv.org/abs/1701.06538},
  urldate = {2024-01-18},
  abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Shazeer et al_2017_Outrageously Large Neural Networks.pdf}
}

@article{singhExaminingSocietyMind2003,
  title = {Examining the Society of Mind},
  author = {Singh, Push},
  year = {2003},
  journal = {Computing and Informatics},
  volume = {22},
  number = {6},
  pages = {521--543},
  url = {https://www.cai.sk/ojs/index.php/cai/article/view/467/0},
  urldate = {2023-12-30},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Singh_2003_Examining the society of mind.pdf}
}

@book{sipserIntroductionTheoryComputation2006,
  title = {Introduction to the Theory of Computation},
  author = {Sipser, Michael},
  year = {2006},
  edition = {2nd ed},
  publisher = {{Thomson Course Technology}},
  address = {{Boston}},
  isbn = {978-0-534-95097-2},
  lccn = {QA267 .S56 2006},
  keywords = {Computational complexity,Machine theory},
  annotation = {ZSCC: 0000014},
  file = {G:\Other computers\My Laptop\Zotero Files\Sipser_2006_Introduction to the theory of computation.djvu}
}

@book{smithAnimalSignals2003,
  title = {Animal Signals},
  author = {Smith, John Maynard and Harper, David},
  year = {2003},
  series = {Oxford Series in Ecology and Evolution},
  edition = {1st ed},
  publisher = {{Oxford University Press}},
  address = {{New York}},
  isbn = {978-0-19-852685-8 978-0-19-852684-1},
  lccn = {QL776 .M375 2003},
  keywords = {Animal communication},
  annotation = {OCLC: ocm52991264},
  file = {G:\Other computers\My Laptop\Zotero Files\Maynard Smith_Harper_2003_Animal signals.djvu}
}

@article{smithLogicAnimalConflict1973,
  title = {The Logic of Animal Conflict},
  author = {Smith, J. Maynard and Price, George R.},
  year = {1973},
  journal = {Nature},
  volume = {246},
  number = {5427},
  pages = {15--18},
  publisher = {{Nature Publishing Group UK London}},
  doi = {10.1038/246015a0},
  url = {https://www.nature.com/articles/246015a0},
  urldate = {2023-12-21},
  file = {G:\Other computers\My Laptop\Zotero Files\Smith_Price_1973_The logic of animal conflict.pdf}
}

@book{smithMajorTransitionsEvolution1997,
  title = {The Major Transitions in Evolution},
  author = {Smith, John Maynard and Szathm{\'a}ry, E{\"o}rs},
  year = {1997},
  publisher = {{Oxford University Press}},
  address = {{Oxford}},
  isbn = {978-0-19-850294-4},
  langid = {english},
  lccn = {576.82},
  file = {G:\Other computers\My Laptop\Zotero Files\Maynard Smith_Szathm√°ry_1997_The major transitions in evolution.pdf}
}

@article{smithTextualDeference1991,
  title = {Textual Deference},
  author = {Smith, Barry},
  year = {1991},
  journal = {American Philosophical Quarterly},
  volume = {28},
  number = {1},
  pages = {1--12},
  publisher = {{JSTOR}},
  url = {https://web.archive.org/web/20210201165837/http://ontology.buffalo.edu/smith/articles/texdef.html},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Smith_1991_Textual deference.pdf}
}

@article{smolenskyConnectionistAISymbolic1987,
  title = {Connectionist {{AI}}, Symbolic {{AI}}, and the Brain},
  author = {Smolensky, P.},
  year = {1987},
  journal = {Artificial Intelligence Review},
  volume = {1},
  number = {2},
  pages = {95--109},
  issn = {0269-2821, 1573-7462},
  doi = {10.1007/BF00130011},
  url = {http://link.springer.com/10.1007/BF00130011},
  urldate = {2024-01-02},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Smolensky_1987_Connectionist AI, symbolic AI, and the brain.pdf}
}

@incollection{sobelSignalingGames2020,
  title = {Signaling {{Games}}},
  booktitle = {Complex {{Social}} and {{Behavioral Systems}}},
  author = {Sobel, Joel},
  editor = {Sotomayor, Marilda and {P{\'e}rez-Castrillo}, David and Castiglione, Filippo},
  year = {2020},
  pages = {251--268},
  publisher = {{Springer US}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-0716-0368-0_481},
  url = {http://link.springer.com/10.1007/978-1-0716-0368-0_481},
  urldate = {2024-01-18},
  isbn = {978-1-07-160367-3 978-1-07-160368-0},
  langid = {english},
  file = {C:\Users\lyxkg\Zotero\storage\4DFFHDQU\Sobel - 2020 - Signaling Games.pdf}
}

@misc{suttonBitterLesson2019,
  title = {The {{Bitter Lesson}}},
  author = {Sutton, Richard},
  year = {2019},
  month = mar,
  url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
  urldate = {2021-10-20},
  abstract = {Incomplete Ideas},
  file = {C:\Users\lyxkg\Zotero\storage\3W5YRKHM\BitterLesson.html}
}

@book{tellerLegacyHiroshima1975,
  title = {The Legacy of {{Hiroshima}}},
  author = {Teller, Edward and Brown, Allen},
  year = {1975},
  publisher = {{Greenwood Press}},
  address = {{Westport, Conn}},
  isbn = {978-0-8371-8344-2},
  lccn = {UF767 .T4 1975},
  keywords = {Defenses,Nuclear disarmament,Nuclear warfare,United States},
  file = {G:\Other computers\My Laptop\Zotero Files\Teller_Brown_1975_The legacy of Hiroshima.djvu}
}

@article{tenenbaumGlobalGeometricFramework2000,
  title = {A {{Global Geometric Framework}} for {{Nonlinear Dimensionality Reduction}}},
  author = {Tenenbaum, Joshua B. and de Silva, Vin and Langford, John C.},
  year = {2000},
  month = dec,
  journal = {Science},
  volume = {290},
  number = {5500},
  pages = {2319--2323},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.290.5500.2319},
  url = {https://www.science.org/doi/full/10.1126/science.290.5500.2319},
  urldate = {2023-07-30},
  abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs{\textemdash}30,000 auditory nerve fibers or 106 optic nerve fibers{\textemdash}a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
  file = {G:\Other computers\My Laptop\Zotero Files\Tenenbaum et al_2000_A Global Geometric Framework for Nonlinear Dimensionality Reduction.pdf}
}

@article{thomasMotionSpinningElectron1926,
  title = {The Motion of the Spinning Electron},
  author = {Thomas, Llewellyn H.},
  year = {1926},
  journal = {Nature},
  volume = {117},
  number = {2945},
  pages = {514--514},
  publisher = {{Nature Publishing Group UK London}},
  doi = {10.1038/117514a0},
  url = {https://www.nature.com/articles/117514a0},
  urldate = {2023-12-12},
  file = {G:\Other computers\My Laptop\Zotero Files\Thomas_1926_The motion of the spinning electron.pdf}
}

@article{thompsonAnalysisUnconventionalEvolved1999,
  title = {Analysis of Unconventional Evolved Electronics},
  author = {Thompson, Adrian and Layzell, Paul},
  year = {1999},
  journal = {Communications of the ACM},
  volume = {42},
  number = {4},
  pages = {71--79},
  doi = {10/bpnp42},
  keywords = {evolution,evolutionary algorithm,hardware evolution},
  annotation = {ZSCC: 0000143},
  file = {G:\Other computers\My Laptop\Zotero Files\Thompson_Layzell_1999_Analysis of unconventional evolved electronics.pdf}
}

@inproceedings{thompsonEvolutionRobustnessElectronics2000,
  title = {Evolution of Robustness in an Electronics Design},
  booktitle = {International {{Conference}} on {{Evolvable Systems}}},
  author = {Thompson, Adrian and Layzell, Paul},
  year = {2000},
  pages = {218--228},
  publisher = {{Springer}},
  keywords = {evolution,evolutionary algorithm,hardware evolution,No DOI found},
  annotation = {ZSCC: 0000081},
  file = {G:\Other computers\My Laptop\Zotero Files\Thompson_Layzell_2000_Evolution of robustness in an electronics design.pdf}
}

@inproceedings{thompsonEvolvedCircuitIntrinsic1996,
  title = {An Evolved Circuit, Intrinsic in Silicon, Entwined with Physics},
  booktitle = {International {{Conference}} on {{Evolvable Systems}}},
  author = {Thompson, Adrian},
  year = {1996},
  pages = {390--405},
  publisher = {{Springer}},
  keywords = {No DOI found},
  annotation = {ZSCC: 0000386},
  file = {G:\Other computers\My Laptop\Zotero Files\Thompson_1996_An evolved circuit, intrinsic in silicon, entwined with physics.pdf}
}

@article{thompsonExplorationsDesignSpace1999,
  title = {Explorations in Design Space: {{Unconventional}} Electronics Design through Artificial Evolution},
  shorttitle = {Explorations in Design Space},
  author = {Thompson, Adrian and Layzell, Paul and Zebulum, Ricardo Salem},
  year = {1999},
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {3},
  number = {3},
  pages = {167--196},
  publisher = {{IEEE}},
  doi = {10.1109/4235.788489},
  url = {https://ieeexplore.ieee.org/abstract/document/788489/},
  urldate = {2023-12-23},
  file = {G:\Other computers\My Laptop\Zotero Files\Thompson et al_1999_Explorations in design space.pdf}
}

@book{thompsonHardwareEvolutionAutomatic1998,
  title = {Hardware {{Evolution}}: {{Automatic Design}} of {{Electronic Circuits}} in {{Reconfigurable Hardware}} by {{Artificial Evolution}}},
  shorttitle = {Hardware {{Evolution}}},
  author = {Thompson, Adrian},
  year = {1998},
  edition = {1st edition},
  publisher = {{Springer}},
  abstract = {Evolution through natural selection has been going on for a very long time. Evolution through artificial selection has been practiced by humans for a large part of our history, in the breeding of plants and livestock. Artificial evolution, where we evolve an artifact through artificial selection, has been around since electronic computers became common: about 30 years. Right from the beginning, people have suggested using artificial evolution to design electronics automatically.l Only recently, though, have suitable re\- configurable silicon chips become available that make it easy for artificial evolution to work with a real, physical, electronic medium: before them, ex\- periments had to be done entirely in software simulations. Early research concentrated on the potential applications opened-up by the raw speed ad\- vantage of dedicated digital hardware over software simulation on a general\- purpose computer. This book is an attempt to show that there is more to it than that. In fact, a radically new viewpoint is possible, with fascinating consequences. This book was written as a doctoral thesis, submitted in September 1996. As such, it was a rather daring exercise in ruthless brevity. Believing that the contribution I had to make was essentially a simple one, I resisted being drawn into peripheral discussions. In the places where I deliberately drop a subject, this implies neither that it's not interesting, nor that it's not relevant: just that it's not a crucial part of the tale I want to tell here.},
  langid = {english}
}

@article{tribeTrialMathematicsPrecision1971,
  title = {Trial by {{Mathematics}}: {{Precision}} and {{Ritual}} in the {{Legal Process}}},
  shorttitle = {Trial by {{Mathematics}}},
  author = {Tribe, Laurence H.},
  year = {1971},
  journal = {Harvard Law Review},
  volume = {84},
  number = {6},
  eprint = {1339610},
  eprinttype = {jstor},
  pages = {1329--1393},
  publisher = {{The Harvard Law Review Association}},
  issn = {0017-811X},
  doi = {10.2307/1339610},
  url = {https://www.jstor.org/stable/1339610},
  urldate = {2023-12-22},
  abstract = {Professor Tribe considers the accuracy, appropriateness, and possible dangers of utilizing mathematical methods in the legal process, first in the actual conduct of civil and criminal trials, and then in designing procedures for the trial system as a whole. He concludes that the utility of mathematical methods for these purposes has been greatly exaggerated. Even if mathematical techniques could significantly enhance the accuracy of the trial process, Professor Tribe also shows that their inherent conflict with other important values would be too great to allow their general use.},
  file = {G:\Other computers\My Laptop\Zotero Files\Tribe_1971_Trial by Mathematics.pdf}
}

@article{turingComputingMachineryIntelligence1950,
  title = {Computing {{Machinery}} and {{Intelligence}}},
  author = {Turing, A. M.},
  year = {1950},
  journal = {Mind},
  volume = {59},
  number = {236},
  eprint = {2251299},
  eprinttype = {jstor},
  pages = {433--460},
  publisher = {{[Oxford University Press, Mind Association]}},
  issn = {0026-4423},
  doi = {10/b262dj},
  url = {https://www.jstor.org/stable/2251299},
  urldate = {2021-11-25},
  file = {G:\Other computers\My Laptop\Zotero Files\Turing_1950_Computing Machinery and Intelligence.pdf}
}

@article{turkleEpistemologicalPluralismStyles1990,
  title = {Epistemological {{Pluralism}}: {{Styles}} and {{Voices}} within the {{Computer Culture}}},
  shorttitle = {Epistemological {{Pluralism}}},
  author = {Turkle, Sherry and Papert, Seymour},
  year = {1990},
  month = oct,
  journal = {Signs: Journal of Women in Culture and Society},
  volume = {16},
  number = {1},
  pages = {128--157},
  issn = {0097-9740, 1545-6943},
  doi = {10.1086/494648},
  url = {https://www.journals.uchicago.edu/doi/10.1086/494648},
  urldate = {2023-12-26},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Turkle_Papert_1990_Epistemological Pluralism.pdf}
}

@article{varicakApplicationLobachevskianGeometry1910,
  title = {Application of {{Lobachevskian Geometry}} in the {{Theory}} of {{Relativity}}},
  author = {Varicak, V.},
  year = {1910},
  journal = {Physikalische Zeitschrift},
  volume = {11},
  pages = {93--96},
  keywords = {#nosource,No DOI found}
}

@book{Vision21Interdisciplinary1993,
  title = {Vision 21: {{Interdisciplinary Science}} and {{Engineering}} in the {{Era}} of {{Cyberspace}}},
  shorttitle = {Vision 21},
  year = {1993},
  month = mar,
  url = {https://ntrs.nasa.gov/citations/19940022855},
  urldate = {2024-01-22},
  abstract = {The symposium Vision-21: Interdisciplinary Science and Engineering in the Era of Cyberspace was held at the NASA Lewis Research Center on March 30-31, 1993. The purpose of the symposium was to simulate interdisciplinary thinking in the sciences and technologies which will be required for exploration and development of space over the next thousand years. The keynote speakers were Hans Moravec, Vernor Vinge, Carol Stoker, and Myron Krueger. The proceedings consist of transcripts of the invited talks and the panel discussion by the invited speakers, summaries of workshop sessions, and contributed papers by the attendees.},
  keywords = {Astronautics (General)},
  annotation = {NTRS Report/Patent Number: NAS 1.55:10129 NTRS Document ID: 19940022855 NTRS Research Center: Legacy CDMS (CDMS)},
  file = {G:\Other computers\My Laptop\Zotero Files\1993_Vision 21.pdf}
}

@article{walkerXXIRelativeCoordinates1932,
  title = {{{XXI}}.{\textemdash}{{Relative Co-ordinates}}},
  author = {Walker, Arthur G.},
  year = {1932},
  journal = {Proceedings of the Royal Society of Edinburgh},
  volume = {52},
  pages = {345--353},
  publisher = {{Royal Society of Edinburgh Scotland Foundation}},
  doi = {10.1017/S0370164600019611},
  url = {https://www.cambridge.org/core/journals/proceedings-of-the-royal-society-of-edinburgh/article/xxirelative-coordinates/037D8506E47FEA76AD19654F6BB5BFE0},
  urldate = {2023-12-12},
  file = {G:\Other computers\My Laptop\Zotero Files\Walker_1932_XXI.pdf}
}

@incollection{walterNonEuclideanStyleMinkowskian1999,
  title = {The Non-{{Euclidean}} Style of {{Minkowskian}} Relativity},
  booktitle = {The {{Symbolic Universe}}},
  author = {Walter, Scott},
  editor = {Gray, Jeremy},
  year = {1999},
  month = jul,
  pages = {91--127},
  publisher = {{Oxford University PressOxford}},
  doi = {10.1093/oso/9780198500889.003.0007},
  url = {https://academic.oup.com/book/52904/chapter/421936750},
  urldate = {2023-12-12},
  abstract = {Abstract             The history of relativity is structured for most commentators by two landmark discoveries due to Albert Einstein: the special theory (1905) and the general theory of relativity (1915). To get from one theory to the other, we know that Einstein relied on a certain number of fundamental concepts, such as the equivalence principle, and a few key mathematical techniques, for instance, the absolute tensor calculus of Gregorio Ricci-Curbastro and Tullio Levi Civita. Einstein also had need of a third theory and technique, elaborated by his former mathematics professor, Hermann Minkowski (1864-1909), although he did not recognize this for several years. In this paper, we examine the fortunes of Minkowski ` s space-time theory from 1908 to 1916. Our focus is on the emergence of Minkowski `s four-dimensional formalism as a standard technique in theoretical physics, and we investigate one aspect of this history in some detail: the reformulation and reinterpretation of the laws of special relativity in the language of non-Euclidean geometry. The related work done on the space-time theory, or what we call the `non-Euclidean style ` of Minkowskian relativity, provides an example of the geometrization of physics brought about by Minkowski and his followers.},
  isbn = {978-0-19-850088-9 978-1-383-01995-7},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Walter_1999_The non-Euclidean style of Minkowskian relativity.pdf}
}

@book{wattsFirefall2017,
  title = {{Firefall}},
  author = {Watts, Peter},
  year = {2017},
  publisher = {{Head of Zeus}},
  address = {{London}},
  isbn = {978-1-78669-610-6},
  langid = {Engelsk tekst},
  annotation = {ZSCC: NoCitationData[s0]  OCLC: 1090606876},
  file = {G:\Other computers\My Laptop\Zotero Files\Watts et al_2017_Firefall.pdf}
}

@incollection{werbosApplicationsAdvancesNonlinear1982,
  title = {Applications of Advances in Nonlinear Sensitivity Analysis},
  booktitle = {System {{Modeling}} and {{Optimization}}},
  author = {Werbos, Paul J.},
  editor = {Drenick, R. F. and Kozin, F.},
  year = {1982},
  volume = {38},
  pages = {762--770},
  publisher = {{Springer-Verlag}},
  address = {{Berlin/Heidelberg}},
  doi = {10.1007/BFb0006203},
  url = {http://link.springer.com/10.1007/BFb0006203},
  urldate = {2023-12-25},
  isbn = {978-3-540-11691-2},
  langid = {english},
  file = {C:\Users\lyxkg\Zotero\storage\BSL2IXNU\openurl.html}
}

@article{werbosBuildingUnderstandingAdaptive1987,
  title = {Building and Understanding Adaptive Systems: {{A}} Statistical/Numerical Approach to Factory Automation and Brain Research},
  shorttitle = {Building and Understanding Adaptive Systems},
  author = {Werbos, Paul J.},
  year = {1987},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {17},
  number = {1},
  pages = {7--20},
  publisher = {{IEEE}},
  doi = {10.1109/TSMC.1987.289329},
  url = {https://ieeexplore.ieee.org/abstract/document/4075651/},
  urldate = {2023-12-25},
  keywords = {#nosource}
}

@article{werbosElementsIntelligence1968,
  title = {The Elements of Intelligence},
  author = {Werbos, P.},
  year = {1968},
  journal = {Cybernetica (Namur)},
  volume = {3},
  pages = {131--178},
  keywords = {#nosource,No DOI found}
}

@article{werbosEmpiricalTestNew1978,
  title = {An Empirical Test of New Forecasting Methods Derived from a Theory of Intelligence: {{The}} Prediction of Conflict in {{Latin America}}},
  shorttitle = {An Empirical Test of New Forecasting Methods Derived from a Theory of Intelligence},
  author = {Werbos, Paul J. and Titus, Jim},
  year = {1978},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {8},
  number = {9},
  pages = {657--666},
  publisher = {{IEEE}},
  doi = {10.1109/TSMC.1978.4310051},
  url = {https://ieeexplore.ieee.org/abstract/document/4310051/},
  urldate = {2023-12-25},
  keywords = {#nosource}
}

@article{werbosGeneralizationBackpropagationApplication1988,
  title = {Generalization of Backpropagation with Application to a Recurrent Gas Market Model},
  author = {Werbos, Paul J.},
  year = {1988},
  journal = {Neural networks},
  volume = {1},
  number = {4},
  pages = {339--356},
  publisher = {{Elsevier}},
  doi = {10.1016/0893-6080(88)90007-X},
  url = {https://www.sciencedirect.com/science/article/pii/089360808890007X},
  urldate = {2023-12-25},
  file = {G:\Other computers\My Laptop\Zotero Files\Werbos_1988_Generalization of backpropagation with application to a recurrent gas market.pdf}
}

@article{werbosIntelligenceBrainTheory2009,
  title = {Intelligence in the Brain: {{A}} Theory of How It Works and How to Build It},
  shorttitle = {Intelligence in the Brain},
  author = {Werbos, Paul J.},
  year = {2009},
  month = apr,
  journal = {Neural Networks},
  series = {Goal-{{Directed Neural Systems}}},
  volume = {22},
  number = {3},
  pages = {200--212},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2009.03.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608009000501},
  urldate = {2023-12-26},
  abstract = {This paper presents a theory of how general-purpose learning-based intelligence is achieved in the mammal brain, and how we can replicate it. It reviews four generations of ever more powerful general-purpose learning designs in Adaptive, Approximate Dynamic Programming (ADP), which includes reinforcement learning as a special case. It reviews empirical results which fit the theory, and suggests important new directions for research, within the scope of NSF's recent initiative on Cognitive Optimization and Prediction. The appendices suggest possible connections to the realms of human subjective experience, comparative cognitive neuroscience, and new challenges in electric power. The major challenge before us today in mathematical neural networks is to replicate the ``mouse level'', but the paper does contain a few thoughts about building, understanding and nourishing levels of general intelligence beyond the mouse.},
  keywords = {Adaptive critic,Adaptive dynamic programming,ADP,Approximate dynamic programming,Backpropagation,Cognitive prediction,Comparative neuropsychology,Complexity,Consciousness,Creativity,Freud,Intelligence,Neurocontrol,Reinforcement learning,Robust},
  file = {C:\Users\lyxkg\Zotero\storage\9TWTDAA6\S0893608009000501.html}
}

@book{whittakerHistoryTheoriesAether1910,
  title = {A {{History}} of the {{Theories}} of {{Aether}} and {{Electricity}} from the {{Age}} of {{Descartes}} to the {{Close}} of the {{Nineteenth Century}}},
  author = {Whittaker, Edmund Taylor},
  year = {1910},
  publisher = {{Longmans, Green and Company}},
  url = {https://books.google.com/books?hl=en&lr=&id=CGJDAAAAIAAJ&oi=fnd&pg=PA1&dq=E.+T.+Whittaker+(1910)+A+History+of+the+Theories+of+Aether+and+Electricity,+page&ots=S2ac0rBORb&sig=NwkF-WCaGjk21hR3TISLX_txJXw},
  urldate = {2023-12-12},
  file = {C:\Users\lyxkg\Zotero\storage\6ZE3TLQR\books.html}
}

@article{widrow30YearsAdaptive1990,
  title = {30 Years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation},
  shorttitle = {30 Years of Adaptive Neural Networks},
  author = {Widrow, Bernard and Lehr, Michael A.},
  year = {1990},
  journal = {Proceedings of the IEEE},
  volume = {78},
  number = {9},
  pages = {1415--1442},
  publisher = {{IEEE}},
  url = {https://ieeexplore.ieee.org/abstract/document/58323/},
  urldate = {2023-12-30},
  keywords = {#nosource}
}

@article{widrowAdalineSmarterSweet1963,
  title = {Adaline: {{Smarter}} than {{Sweet}}},
  author = {Widrow, Bernard},
  year = {1963},
  journal = {Stanford Today},
  number = {Autumn 1963},
  url = {https://web.archive.org/web/20230606185311/https://www-isl.stanford.edu/~widrow/papers/j1963adalinesmarter.pdf},
  abstract = {Stanford engineers have given this maidenly nickname to their adaptive linear computer. She can forecast the weather, play blackjack, recognize voices, and solve all kinds of problems on her own.},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Widrow_1963_Adaline.pdf}
}

@techreport{widrowAdaptiveAdalineNeuron1960,
  title = {An Adaptive "Adaline" Neuron Using Chemical "Memistors"},
  author = {Widrow, Bernard},
  year = {1960},
  number = {1553-2},
  institution = {{Stanford Electronics Laboratories}},
  abstract = {Prepared under Office of Naval Research Contract Nonr 225(24), NR 373360 Jointly supported by the U.S. Army Signal Corps, the U.S. Air Force, and the U.S. Navy (Office of Naval Research)},
  file = {G:\Other computers\My Laptop\Zotero Files\Widrow_1960_An adaptive adaline neuron using chemical memistors.pdf}
}

@incollection{widrowAncientHistory2023,
  title = {Ancient {{History}}},
  booktitle = {Cybernetics 2.0: {{A General Theory}} of {{Adaptivity}} and {{Homeostasis}} in the {{Brain}} and in the {{Body}}},
  author = {Widrow, Bernard},
  year = {2023},
  series = {Springer {{Series}} on {{Bio-}} and {{Neurosystems}}},
  pages = {277--307},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-98140-2_26},
  url = {https://doi.org/10.1007/978-3-030-98140-2_26},
  urldate = {2023-12-28},
  abstract = {Original photos and drawings from my laboratory in the early to mid-1960s, at the dawn of the field of artificial neural networks, are shown in this chapter. The photos depict apparatus that we built and experiments we had done in developing what today would be called machine learning. Shown are the original ADALINE (adaptive linear neuron), and MADALINE (multiple adaline), a small two-layer network of ADALINEs. Adaptation was done with the LMS (least mean squares) algorithm. An electronically controlled synapse called the memistor (resistor with memory), based on electroplating, was developed. With it an electronic neuron was possible and was demonstrated. Applications were to trainable systems. One of these was the now famous ``broom balancer''. In addition, an IBM1620 computer was used to simulate small networks.},
  isbn = {978-3-030-98140-2},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Widrow_2023_Ancient History.pdf}
}

@book{widrowCyberneticsGeneralTheory2022,
  title = {Cybernetics 2.0: {{A General Theory}} of {{Adaptivity}} and {{Homeostasis}} in the {{Brain}} and in the {{Body}}},
  shorttitle = {Cybernetics 2.0},
  author = {Widrow, Bernard},
  year = {2022},
  volume = {14},
  publisher = {{Springer Nature}},
  url = {https://books.google.com/books?hl=en&lr=&id=X_eVEAAAQBAJ&oi=fnd&pg=PR9&dq=Cybernetics+2.0%0A&ots=MB6pN7dGcd&sig=hTBUvOlnKBRkfYUNL7Ztn1Zm4Mg},
  urldate = {2023-12-28},
  keywords = {#nosource}
}

@article{widrowGeneralizationInformationStorage1962,
  title = {Generalization and Information Storage in Networks of Adaline Neurons},
  author = {Widrow, Bernard},
  year = {1962},
  journal = {Self-organizing systems},
  pages = {435--461},
  publisher = {{Spartan Books}},
  url = {https://cir.nii.ac.jp/crid/1573387449478081920},
  urldate = {2024-01-18},
  keywords = {No DOI found},
  file = {G:\Other computers\My Laptop\Zotero Files\Widrow_1962_Generalization and information storage in networks of adaline neurons.pdf}
}

@misc{widrowOralHistoryBernard1997,
  title = {Oral {{History}}: {{Bernard Widrow}}},
  author = {Widrow, Bernard},
  year = {1997},
  month = mar,
  url = {https://ethw.org/Oral-History:Bernard_Widrow},
  urldate = {2023-12-22},
  collaborator = {Goldstein, Andrew},
  langid = {english},
  file = {C:\Users\lyxkg\Zotero\storage\LWQYZF7C\Oral-HistoryBernard_Widrow.html}
}

@book{widrowQuantizationNoiseRoundoff2008,
  title = {Quantization Noise: Roundoff Error in Digital Computation, Signal Processing, Control and Communications},
  shorttitle = {Quantization Noise},
  author = {Widrow, Bernard and Koll{\'a}r, Istv{\'a}n},
  year = {2008},
  edition = {1. publ},
  publisher = {{Cambridge Univ. Press}},
  address = {{Cambridge}},
  isbn = {978-0-521-88671-0},
  langid = {english},
  keywords = {#nosource}
}

@book{wienerCyberneticsControlCommunication1961,
  title = {Cybernetics, or, Control and Communication in the Animal and the Machine},
  author = {Wiener, Norbert},
  year = {1961},
  edition = {2. ed},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-73009-9},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]  OCLC: 553552880},
  file = {G:\Other computers\My Laptop\Zotero Files\Wiener_1961_Cybernetics, or, control and communication in the animal and the machine.pdf}
}

@book{wienerCyberneticsControlCommunication2019,
  title = {Cybernetics: Or Control and Communication in the Animal and the Machine},
  shorttitle = {Cybernetics},
  author = {Wiener, Norbert},
  editor = {Hill, Doug and Mitter, Sanjoy K.},
  year = {2019},
  edition = {Reissue of the 1961 second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts London, England}},
  isbn = {978-0-262-53784-1},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Wiener_2019_Cybernetics.pdf}
}

@book{wienerGodGolemInc1964,
  title = {God and {{Golem}}, {{Inc}}.: {{A Comment}} on {{Certain Points}} Where {{Cybernetics Impinges}} on {{Religion}}},
  shorttitle = {God and {{Golem}}, {{Inc}}.},
  author = {Wiener, Norbert},
  year = {1964},
  edition = {7th ed. edition},
  publisher = {{MIT Press}},
  address = {{Cambridge}},
  abstract = {The new and rapidly growing field of communication sciences owes as much to Norbert Wiener as to any one man. He coined the word for it{\textemdash}cybernetics. In God \& Golem, Inc., the author concerned himself with major points in cybernetics which are relevant to religious issues.The first point he considers is that of the machine which learns. While learning is a property almost exclusively ascribed to the self-conscious living system, a computer now exists which not only can be programmed to play a game of checkers, but one which can "learn" from its past experience and improve on its own game. For a time, the machine was able to beat its inventor at checkers. "It did win," writes the author, "and it did learn to win; and the method of its learning was no different in principle from that of the human being who learns to play checkers.A second point concerns machines which have the capacity to reproduce themselves. It is our commonly held belief that God made man in his own image. The propagation of the race may also be interpreted as a function in which one living being makes another in its own image. But the author demonstrates that man has made machines which are "very well able to make other machines in their own image," and these machine images are not merely pictorial representations but operative images. Can we then say: God is to Golem as man is to Machines? in Jewish legend, golem is an embryo Adam, shapeless and not fully created, hence a monster, an automation.The third point considered is that of the relation between man and machine. The concern here is ethical. "render unto man the things which are man's and unto the computer the things which are the computer's," warns the author. In this section of the book, Dr. Wiener considers systems involving elements of man and machine. The book is written for the intellectually alert public and does not involve any highly technical knowledge. It is based on lectures given at Yale, at the Soci{\'e}t{\'e} Philosophique de Royaumont, and elsewhere.},
  isbn = {978-0-262-73011-2},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Wiener_1964_God and Golem, Inc.epub}
}

@book{wienerNonlinearProblemsRandom1958,
  title = {Nonlinear {{Problems}} in {{Random Theory}}},
  author = {Wiener, Norbert},
  year = {1958},
  publisher = {{MIT Press}},
  abstract = {A series of lectures on the role of nonlinear processes in physics, mathematics, electrical engineering, physiology, and communication theory. From the preface: For some time I have been interested in a group of phenomena depending upon random processes. One the one hand, I have recorded the random shot effect as a suitable input for testing nonlinear circuits. On the other hand, for some of the work that Professor W. A. Rosenblith and I have been doing concerning the nature of the electroencephalogram, and in particular of the alpha rhythm, it has occurred to me to use the model of a system of random nonlinear oscillators excited by a random input.... At the beginning we had contemplated a series of only four or five lectures. My ideas developed pari passu with the course, and by the end of the term we found ourselves with a set of fifteen lectures. The last few of these were devoted to the application of my ideas to problems in the statistical mechanics of gases. This work is both new and tentative, and I found that I had to supplement my course by the writing over of these with the help of Professer Y. W. Lee.},
  googlebooks = {HQBRAAAAMAAJ},
  isbn = {978-0-262-73012-9},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Wiener_1958_Nonlinear Problems in Random Theory.djvu}
}

@book{wienerNorbertWienerLife2017,
  title = {Norbert {{Wiener--a}} Life in Cybernetics},
  shorttitle = {Norbert {{Wiener--a}} Life in Cybernetics},
  author = {Wiener, Norbert},
  year = {2017},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {Norbert Wiener{\textemdash}A Life in Cybernetics combines for the first time the two volumes of Norbert Wiener's celebrated autobiography. Published at the height of public enthusiasm for cybernetics{\textemdash}when it was taken up by scientists, engineers, science fiction writers, artists, and musicians{\textemdash}Ex-Prodigy (1953) and I Am a Mathematician (1956) received attention from both scholarly and mainstream publications, garnering reviews and publicity in outlets that ranged from the New York Times and New York Post to the Virginia Quarterly Review.},
  isbn = {978-0-262-53544-1},
  lccn = {QA29.W497 A25 2017},
  keywords = {Biography,Mathematicians,United States,Wiener Norbert},
  file = {G:\Other computers\My Laptop\Zotero Files\Wiener_2017_Norbert Wiener--a life in cybernetics.pdf}
}

@article{wignerUnitaryRepresentationsInhomogeneous1939,
  title = {On Unitary Representations of the Inhomogeneous {{Lorentz}} Group},
  author = {Wigner, Eugene},
  year = {1939},
  journal = {Annals of mathematics},
  eprint = {1968551},
  eprinttype = {jstor},
  pages = {149--204},
  publisher = {{JSTOR}},
  doi = {10.2307/1968551},
  url = {https://www.jstor.org/stable/1968551},
  urldate = {2023-12-12},
  file = {G:\Other computers\My Laptop\Zotero Files\Wigner_1939_On unitary representations of the inhomogeneous Lorentz group.pdf}
}

@article{yangIntroductionNeuralData2023,
  title = {An {{Introduction}} to {{Neural Data Compression}}},
  author = {Yang, Yibo and Mandt, Stephan and Theis, Lucas},
  year = {2023},
  month = apr,
  journal = {Foundations and Trends{\textregistered} in Computer Graphics and Vision},
  volume = {15},
  number = {2},
  pages = {113--200},
  publisher = {{Now Publishers, Inc.}},
  issn = {1572-2740, 1572-2759},
  doi = {10.1561/0600000107},
  url = {https://www.nowpublishers.com/article/Details/CGV-107},
  urldate = {2024-02-02},
  abstract = {An Introduction to Neural Data Compression},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Yang et al_2023_An Introduction to Neural Data Compression.pdf}
}

@article{zapffeLastMessiah2004,
  title = {The Last Messiah},
  author = {Zapffe, Peter},
  translator = {Tangenes, Gisle},
  year = {2004},
  journal = {Philosophy Now},
  volume = {45},
  pages = {35--39},
  keywords = {No DOI found},
  annotation = {ZSCC: 0000038},
  file = {G:\Other computers\My Laptop\Zotero Files\Zapffe_Tangenes_2004_The last messiah.pdf}
}

@book{zimmermanRoutledgeHandbookMoral2019,
  title = {The {{Routledge}} Handbook of Moral Epistemology},
  editor = {Zimmerman, Aaron and Jones, Karen and Timmons, Mark},
  year = {2019},
  publisher = {{Routledge}},
  address = {{New York}},
  abstract = {"The Routledge Handbook of Moral Epistemology brings together philosophers, cognitive scientists, developmental and evolutionary psychologists, animal ethologists, intellectual historians and educators to provide the most comprehensive analysis of the prospects for moral knowledge ever assembled in print. The book's thirty chapters feature leading experts describing the nature of moral thought, its evolution, childhood development and neurological realization. Various forms of moral skepticism are addressed along with the historical development of ideals of moral knowledge and their role in law, education, legal policy, and other areas of social life. Highlights include: - Analyses of moral cognition and moral learning by leading cognitive scientists - Accounts of the normative practices of animals by expert animal ethologists - An overview of the evolution of cooperation by preeminent evolutionary psychologists - Sophisticated treatments of moral skepticism, relativism, moral uncertainty, and know-how by renowned philosophers - Scholarly accounts of the development of western moral thinking by eminent intellectual historians - Careful analyses of the role played by conceptions of moral knowledge in political liberation movements, religious institutions, criminal law, secondary education, and professional codes of ethics articulated by cutting-edge social and moral philosophers"--},
  isbn = {978-1-138-81612-1},
  lccn = {BD176 .R68 2019},
  keywords = {Ethics,Handbooks manuals etc,Knowledge Theory of},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {G:\Other computers\My Laptop\Zotero Files\Zimmerman et al_2019_The Routledge handbook of moral epistemology.pdf}
}

@misc{zophSTMoEDesigningStable2022,
  title = {{{ST-MoE}}: {{Designing Stable}} and {{Transferable Sparse Expert Models}}},
  shorttitle = {{{ST-MoE}}},
  author = {Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  year = {2022},
  month = apr,
  number = {arXiv:2202.08906},
  eprint = {2202.08906},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.08906},
  url = {http://arxiv.org/abs/2202.08906},
  urldate = {2024-01-24},
  abstract = {Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Zoph et al_2022_ST-MoE.pdf}
}
