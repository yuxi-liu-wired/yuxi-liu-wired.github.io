@article{bartoljrNanoconnectomicUpperBound2015,
  title = {Nanoconnectomic Upper Bound on the Variability of Synaptic Plasticity},
  author = {Bartol Jr, Thomas M. and Bromer, Cailey and Kinney, Justin and Chirillo, Michael A. and Bourne, Jennifer N. and Harris, Kristen M. and Sejnowski, Terrence J.},
  year = {2015},
  journal = {elife},
  volume = {4},
  pages = {e10778},
  publisher = {{eLife Sciences Publications, Ltd}},
  urldate = {2023-12-06}
}

@book{bowenEconomicGeographyAir2010,
  title = {The Economic Geography of Air Transportation: Space, Time, and the Freedom of the Sky},
  shorttitle = {The Economic Geography of Air Transportation},
  author = {Bowen, John T.},
  year = {2010},
  publisher = {{Routledge}},
  urldate = {2023-12-06}
}

@article{feldmanConnectionistModelsTheir1982,
  title = {Connectionist Models and Their Properties},
  author = {Feldman, Jerome A. and Ballard, Dana H.},
  year = {1982},
  journal = {Cognitive science},
  volume = {6},
  number = {3},
  pages = {205--254},
  publisher = {{Elsevier}},
  doi = {10.1207/s15516709cog0603_1},
  urldate = {2023-12-06}
}

@book{heilbronerWorldlyPhilosophersLives1999,
  title = {The {{Worldly Philosophers}}: {{The Lives}}, {{Times And Ideas Of The Great Economic Thinkers}}, {{Seventh Edition}}},
  shorttitle = {The {{Worldly Philosophers}}},
  author = {Heilbroner, Robert L.},
  year = {1999},
  month = aug,
  edition = {7th Revised edition},
  publisher = {{Touchstone}},
  address = {{New York}},
  isbn = {978-0-684-86214-9},
  langid = {english}
}

@misc{hoffmannTrainingComputeOptimalLarge2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  year = {2022},
  month = mar,
  number = {arXiv:2203.15556},
  eprint = {2203.15556},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-12-06},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\DeadScholar\\Zotero\\storage\\Z83F65EM\\Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf;C\:\\Users\\DeadScholar\\Zotero\\storage\\3WSBMYM7\\2203.html}
}

@book{keynesGeneralTheoryEmployment2018,
  title = {The {{General Theory}} of {{Employment}}, {{Interest}}, and {{Money}}},
  author = {Keynes, John Maynard},
  year = {2018},
  address = {{Springer International Publishing}},
  isbn = {978-3-319-70343-5},
  langid = {english}
}

@incollection{krugmanRicardoDifficultIdea2002,
  title = {Ricardo's Difficult Idea: Why Intellectuals Don't Understand Comparative Advantage},
  shorttitle = {Ricardo's Difficult Idea},
  booktitle = {The Economics and Politics of International Trade},
  author = {Krugman, Paul},
  year = {2002},
  pages = {40--54},
  publisher = {{Routledge}},
  urldate = {2023-12-06},
  keywords = {economics},
  file = {C:\Users\DeadScholar\Zotero\storage\TLLCQE9S\openurl.html}
}

@book{metzingerBeingNoOne2004,
  title = {Being No One: The Self-Model Theory of Subjectivity},
  shorttitle = {Being No One},
  author = {Metzinger, Thomas},
  year = {2004},
  series = {A {{Bradford}} Book},
  edition = {1. paperback ed},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass.}},
  abstract = {According to Thomas Metzinger, no such things as selves exist in the world: nobody ever had or was a self. All that exists are phenomenal selves, as they appear in conscious experience. The phenomenal self, however, is not a thing but an ongoing process; it is the content of a "transparent self-model." In Being No One, Metzinger, a German philosopher, draws strongly on neuroscientific research to present a representationalist and functional analysis of what a consciously experienced first-person perspective actually is. Building a bridge between the humanities and the empirical sciences of the mind, he develops new conceptual toolkits and metaphors; uses case studies of unusual states of mind such as agnosia, neglect, blindsight, and hallucinations; and offers new sets of multilevel constraints for the concept of consciousness. Metzinger's central question is: How exactly does strong, consciously experienced subjectivity emerge out of objective events in the natural world? His epistemic goal is to determine whether conscious experience, in particular the experience of being someone that results from the emergence of a phenomenal self, can be analysed on subpersonal levels of description. He also asks if and how our Cartesian intuitions that subjective experiences as such can never be reductively explained are themselves ultimately rooted in the deeper representational structure of our conscious minds},
  isbn = {978-0-262-63308-6 978-0-262-13417-0},
  langid = {english},
  annotation = {ZSCC: 0001532  OCLC: 254141765}
}

@book{metzingerEgoTunnelScience2009,
  title = {The Ego Tunnel: {{The}} Science of the Mind and the Myth of the Self},
  shorttitle = {The Ego Tunnel},
  author = {Metzinger, Thomas},
  year = {2009},
  publisher = {{Basic Books (AZ)}},
  annotation = {ZSCC: 0000011}
}

@article{moravecFractalBranchingUltradexterous1996,
  title = {Fractal Branching Ultra-Dexterous Robots (Bush Robots)},
  author = {Moravec, Hans and Easudes, J. and Dellaert, F.},
  year = {1996},
  journal = {Technical report, NASA Advanced Concepts Research Project},
  keywords = {\#nosource,⛔ No DOI found},
  annotation = {ZSCC: 0000008}
}

@book{moravecMindChildrenFuture1995,
  title = {Mind Children: The Future of Robot and Human Intelligence},
  shorttitle = {Mind Children},
  author = {Moravec, Hans},
  year = {1995},
  edition = {4. print},
  publisher = {{Harvard Univ. Press}},
  address = {{Cambridge}},
  isbn = {978-0-674-57618-6},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]  OCLC: 245755104}
}

@article{moravecPigsCyberspace1993,
  title = {Pigs in Cyberspace},
  author = {Moravec, Hans},
  year = {1993},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000047}
}

@article{moravecWhenWillComputer1998,
  title = {When Will Computer Hardware Match the Human Brain},
  author = {Moravec, Hans},
  year = {1998},
  journal = {Journal of evolution and technology},
  volume = {1},
  number = {1},
  pages = {10},
  abstract = {This paper describes how the performance of AI machines tends to improve at the same pace that AI researchers get access to faster hardware. The processing power and memory capacity necessary to match general intellectual performance of the human brain are estimated. Based on extrapolation of past trends and on examination of technologies under development, it is predicted that the required hardware will be available in cheap machines in the 2020s.},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000375}
}

@misc{pattersonCarbonEmissionsLarge2021,
  title = {Carbon {{Emissions}} and {{Large Neural Network Training}}},
  author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  year = {2021},
  month = apr,
  number = {arXiv:2104.10350},
  eprint = {2104.10350},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-12-06},
  abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {$<$}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {C:\Users\DeadScholar\Zotero\storage\GAA6GWGQ\2104.html}
}

@book{ricardoPrinciplesPoliticalEconomy1817,
  title = {On {{The Principles}} of {{Political Economy}}, and {{Taxation}}},
  author = {Ricardo, David},
  year = {1817},
  address = {{London}},
  urldate = {2023-01-02},
  copyright = {Public domain in the USA.},
  langid = {english},
  lccn = {EBook-No. 33310},
  keywords = {Economics}
}

@misc{suttonBitterLesson2019,
  title = {The {{Bitter Lesson}}},
  author = {Sutton, Richard},
  year = {2019},
  month = mar,
  urldate = {2021-10-20},
  abstract = {Incomplete Ideas},
  file = {C:\Users\DeadScholar\Zotero\storage\3W5YRKHM\BitterLesson.html}
}

@book{zimmermanRoutledgeHandbookMoral2019,
  title = {The {{Routledge}} Handbook of Moral Epistemology},
  editor = {Zimmerman, Aaron and Jones, Karen and Timmons, Mark},
  year = {2019},
  publisher = {{Routledge}},
  address = {{New York}},
  abstract = {"The Routledge Handbook of Moral Epistemology brings together philosophers, cognitive scientists, developmental and evolutionary psychologists, animal ethologists, intellectual historians and educators to provide the most comprehensive analysis of the prospects for moral knowledge ever assembled in print. The book's thirty chapters feature leading experts describing the nature of moral thought, its evolution, childhood development and neurological realization. Various forms of moral skepticism are addressed along with the historical development of ideals of moral knowledge and their role in law, education, legal policy, and other areas of social life. Highlights include: - Analyses of moral cognition and moral learning by leading cognitive scientists - Accounts of the normative practices of animals by expert animal ethologists - An overview of the evolution of cooperation by preeminent evolutionary psychologists - Sophisticated treatments of moral skepticism, relativism, moral uncertainty, and know-how by renowned philosophers - Scholarly accounts of the development of western moral thinking by eminent intellectual historians - Careful analyses of the role played by conceptions of moral knowledge in political liberation movements, religious institutions, criminal law, secondary education, and professional codes of ethics articulated by cutting-edge social and moral philosophers"--},
  isbn = {978-1-138-81612-1},
  lccn = {BD176 .R68 2019},
  keywords = {Ethics,{Handbooks, manuals, etc},{Knowledge, Theory of}},
  annotation = {ZSCC: NoCitationData[s0]}
}
