[
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "Over the years I have written some notes. Too short for publication, and not exactly fit for blog posting, but still might be useful to someone, so I collect them here."
  },
  {
    "objectID": "notes/index.html#expository-notes",
    "href": "notes/index.html#expository-notes",
    "title": "Notes",
    "section": "Expository notes",
    "text": "Expository notes\nVisually Deriving the Wigner Rotation by Spacetime Diagrams. Obsoleted by my post. This is a term paper I wrote in 2018 for Theoretical Physics. The date in the pdf is the recompilation date, not the time when I actually wrote the paper.\nHyperbolic dynamical systems, chaos, and Smale’s horseshoe: a guided tour. This is the companion paper to a presentation I gave for the course Introduction to Ergodic theory in 2019.\nAn Overview of Information Geometry. This is the term paper for Advanced Differential Geometry. and since I really did not like lectures, I asked to do something to substitute for the mandatory attendance. I was asked to write a term paper, which produced this. Information geometry is a weird thing. The premise is beautiful, but the books are terribly confusing, and what little I have managed to understand seems disappointing. It feels like the whole field is overpromising and underdelivering.\nHandout for honours seminar talk on AIXI. A presentation handout for AIXI. For my undergraduate thesis, I was going to be advised by Marcus Hutter, but he left ANU just before the start of semester, and I had to scramble for another advisor. Still, I found AIXI worth knowing, so for my mandatory short talk, I gave a presentation. I managed to compress the essentials to two pages, perfect for handing out on double-sided printed sheets."
  },
  {
    "objectID": "notes/index.html#undergraduate-thesis",
    "href": "notes/index.html#undergraduate-thesis",
    "title": "Notes",
    "section": "Undergraduate thesis",
    "text": "Undergraduate thesis\nBeyond expectations, but within limits – the theory of coherent risk measures.\nFor a quick summary, see my seminar presentation.\nMy undergraduate thesis written in 2019 at ANU, on the topic of coherent risk measures. The first chapter is a readable introduction to risk measures in general (as in, why we might need to use more than the mean and the variance). The rest of it is very dry and I imagine it is of only interest to specialists. The centerpiece of the thesis is a straightforward proof of the central limit theorem for CVaR, which is a slight generalization of expectation. Like the central limit theorem, this theorem states that the sample CVaR converges to the true CVaR like\n\\[\n\\frac{\\text{sample CVaR}_\\alpha - \\text{true CVaR}_\\alpha}{\\sqrt N} \\xrightarrow{d} \\mathcal N(0, \\sigma^2(\\alpha))\n\\]\nwhere \\(\\sigma^2(\\alpha)\\) has a certain expression. As soon as I have calculated it myself, thinking that I had finally made a new discovery, I found it published before in the literature. Still my expression is simpler than the previous publications, so I believe it is still worth something after all."
  },
  {
    "objectID": "notes/index.html#corrections",
    "href": "notes/index.html#corrections",
    "title": "Notes",
    "section": "Corrections",
    "text": "Corrections\nWhen I was not yet mathematically mature, I used to study textbooks carefully, checking every letter through a brain-filter. I no longer do this, but while I was doing this, I created some erratas. Perhaps those will be of use to some people.\nIt is a rather odd thing that errata are hard to share. I would have thought there ought to be some kind of Wikipedia for errata, where people just post errata for textbooks. The lack of such an Error-pedia seems to require an economic explanation, as it can just use MediaWiki, the same technology powering Wikipedia.\n\nConway, John B. A course in point set Topology. Belin: Springer, 2014.\nWalter, P. “An introduction to ergodic theory (Graduate Texts in Math. 79) Springer-Verlag.” Berlin-Heidelberg-New York (1982).\nHiriart-Urruty, Jean-Baptiste, and Claude Lemaréchal. Fundamentals of convex analysis. Springer Science & Business Media, 2004."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "Yuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks."
  },
  {
    "objectID": "sketches/posts/neural-network-scrapbook/index.html",
    "href": "sketches/posts/neural-network-scrapbook/index.html",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "",
    "text": "Marvin Minsky, on how he gave up on neural networks after the 1950s because he could not afford a few million neurons.\n\nI had the naive idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks. Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. (Bernstein 1981)\n\nPeter Norvig, on how he quickly gave up on neural networks in the 1980s due to lack of compute.\n\nAnd then it finally worked. And I think the biggest difference was the computing power. Definitely there were advances in data. So we could do image net because Fei-Fei Li and others gathered this large database, and that was really important. There are certainly differences in the algorithm, right? We’ve got a slightly different squashing function. Instead of shaped like this, it’s shaped like this. I mean, I don’t know how big a deal that was, but we learned how to do stochastic gradient dissent a little bit better. We figured that dropout gave you a little bit better robustness.\nSo there were small things, but I think probably the biggest was the computing power. And I mean, I certainly remember Geoffrey Hinton came to Berkeley when I was a grad student in 1981, I think, when he talked about these neural nets. And we fellow grad students thought that was so cool. So we said, “Let’s go back into the lab and implement it.\nAnd of course, there was absolutely nothing you could download, so we had to build it all from scratch. And we got it to do exclusive or, and then we got it to do something a little bit more complicated. And it was exciting. And then we gave it the first real problem, and it ran overnight, and it didn’t converge, and we let it run one more day, and it still didn’t converge. And then we gave up, and we went back to our sort of knowledge-based systems approach. But if we had the computing power of today, it probably would have converged after five seconds. (Norvig 2021)\n\n“The last bits are deepest”\n\nWhy Does Pretraining Work?\nEarly on in training, a model learns the crudest levels: that some letters like ‘e’ are more frequent than others like ‘z’, that every 5 characters or so there is a space, and so on. … once a model has learned a good English vocabulary and correct formatting/spelling, what’s next? There’s not much juice left in predicting within-words. The next thing is picking up associations among words. … If the word “Jefferson” is the last word, then “Washington” may not be far away, and it should hedge its bets on predicting that ‘W’ is the next character, and then if it shows up, go all-in on “ashington”. … Now training is hard. Even subtler aspects of language must be modeled, such as keeping pronouns consistent. This is hard in part because the model’s errors are becoming rare, and because the relevant pieces of text are increasingly distant and ‘long-range’. … If we compared two models, one of which didn’t understand gender pronouns at all and guessed ‘he’/‘she’ purely at random, and one which understood them perfectly and always guessed ‘she’, the second model would attain a lower average error of barely &lt;0.02 bits per character! …\nThe implication here is that the final few bits are the most valuable bits, which require the most of what we think of as intelligence. A helpful analogy here might be our actions: for the most part, all humans execute actions equally well. … Where individuals differ is when they start running into the long tail of novel choices, rare choices, choices that take seconds but unfold over a lifetime, choices where we will never get any feedback (like after our death). One only has to make a single bad decision, out of a lifetime of millions of discrete decisions, to wind up in jail or dead. A small absolute average improvement in decision quality, if it is in those decisions, may be far more important than its quantity indicates, and give us some intuition for why those last bits are the hardest/deepest. (Branwen 2020)\n\nEchos of “The last bits are deepest” from a very early paper on using a trigram model to estimate the entropy of English over the Brown corpus (600 million words).\n\nFrom a loftier perspective, we cannot help but notice that linguistically the trigram concept, which is the workhorse of our language model, seems almost moronic. It captures local tactic constraints by sheer force of numbers, but the more well-protected bastions of semantic, pragmatic, and discourse constraint and even morphological and global syntactic constraint remain unscathed, in fact unnoticed. Surely the extensive work on these topics in recent years can be harnessed to predict English better than we have yet predicted it.\nWe see this paper as a gauntlet thrown down before the computational linguistics community. The Brown Corpus is a widely available, standard corpus and the subject of much linguistic research. By predicting the corpus character by character, we obviate the need for a common agreement on a vocabulary. Given a model, the computations required to determine the cross-entropy are within reach for even a modest research budget. We hope by proposing this standard task to unleash a fury of competitive energy that will gradually corral the wild and unruly thing that we know the English language to be. (Brown et al. 1992)"
  },
  {
    "objectID": "sketches/posts/neural-network-scrapbook/index.html#the-scaling-hypothesis",
    "href": "sketches/posts/neural-network-scrapbook/index.html#the-scaling-hypothesis",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "",
    "text": "Marvin Minsky, on how he gave up on neural networks after the 1950s because he could not afford a few million neurons.\n\nI had the naive idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks. Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. (Bernstein 1981)\n\nPeter Norvig, on how he quickly gave up on neural networks in the 1980s due to lack of compute.\n\nAnd then it finally worked. And I think the biggest difference was the computing power. Definitely there were advances in data. So we could do image net because Fei-Fei Li and others gathered this large database, and that was really important. There are certainly differences in the algorithm, right? We’ve got a slightly different squashing function. Instead of shaped like this, it’s shaped like this. I mean, I don’t know how big a deal that was, but we learned how to do stochastic gradient dissent a little bit better. We figured that dropout gave you a little bit better robustness.\nSo there were small things, but I think probably the biggest was the computing power. And I mean, I certainly remember Geoffrey Hinton came to Berkeley when I was a grad student in 1981, I think, when he talked about these neural nets. And we fellow grad students thought that was so cool. So we said, “Let’s go back into the lab and implement it.\nAnd of course, there was absolutely nothing you could download, so we had to build it all from scratch. And we got it to do exclusive or, and then we got it to do something a little bit more complicated. And it was exciting. And then we gave it the first real problem, and it ran overnight, and it didn’t converge, and we let it run one more day, and it still didn’t converge. And then we gave up, and we went back to our sort of knowledge-based systems approach. But if we had the computing power of today, it probably would have converged after five seconds. (Norvig 2021)\n\n“The last bits are deepest”\n\nWhy Does Pretraining Work?\nEarly on in training, a model learns the crudest levels: that some letters like ‘e’ are more frequent than others like ‘z’, that every 5 characters or so there is a space, and so on. … once a model has learned a good English vocabulary and correct formatting/spelling, what’s next? There’s not much juice left in predicting within-words. The next thing is picking up associations among words. … If the word “Jefferson” is the last word, then “Washington” may not be far away, and it should hedge its bets on predicting that ‘W’ is the next character, and then if it shows up, go all-in on “ashington”. … Now training is hard. Even subtler aspects of language must be modeled, such as keeping pronouns consistent. This is hard in part because the model’s errors are becoming rare, and because the relevant pieces of text are increasingly distant and ‘long-range’. … If we compared two models, one of which didn’t understand gender pronouns at all and guessed ‘he’/‘she’ purely at random, and one which understood them perfectly and always guessed ‘she’, the second model would attain a lower average error of barely &lt;0.02 bits per character! …\nThe implication here is that the final few bits are the most valuable bits, which require the most of what we think of as intelligence. A helpful analogy here might be our actions: for the most part, all humans execute actions equally well. … Where individuals differ is when they start running into the long tail of novel choices, rare choices, choices that take seconds but unfold over a lifetime, choices where we will never get any feedback (like after our death). One only has to make a single bad decision, out of a lifetime of millions of discrete decisions, to wind up in jail or dead. A small absolute average improvement in decision quality, if it is in those decisions, may be far more important than its quantity indicates, and give us some intuition for why those last bits are the hardest/deepest. (Branwen 2020)\n\nEchos of “The last bits are deepest” from a very early paper on using a trigram model to estimate the entropy of English over the Brown corpus (600 million words).\n\nFrom a loftier perspective, we cannot help but notice that linguistically the trigram concept, which is the workhorse of our language model, seems almost moronic. It captures local tactic constraints by sheer force of numbers, but the more well-protected bastions of semantic, pragmatic, and discourse constraint and even morphological and global syntactic constraint remain unscathed, in fact unnoticed. Surely the extensive work on these topics in recent years can be harnessed to predict English better than we have yet predicted it.\nWe see this paper as a gauntlet thrown down before the computational linguistics community. The Brown Corpus is a widely available, standard corpus and the subject of much linguistic research. By predicting the corpus character by character, we obviate the need for a common agreement on a vocabulary. Given a model, the computations required to determine the cross-entropy are within reach for even a modest research budget. We hope by proposing this standard task to unleash a fury of competitive energy that will gradually corral the wild and unruly thing that we know the English language to be. (Brown et al. 1992)"
  },
  {
    "objectID": "sketches/posts/neural-network-scrapbook/index.html#neural-networks-want-to-work",
    "href": "sketches/posts/neural-network-scrapbook/index.html#neural-networks-want-to-work",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "Neural networks want to work",
    "text": "Neural networks want to work\nMarvin Minsky’s SNARC (1951). Designed to simulate one mouse escaping a maze, it ended up simulating multiple mice due to design bugs – which were never debugged. Though the machine had only 40 neurons, and its parts failed all the time, the whole network continued to work.\n\nIt turned out that because of an electronic accident in our design we could put two or three rats in the same maze and follow them all. The rats actually interacted with one another. If one of them found a good path, the others would tend to follow it. We sort of quit science for a while to watch the machine. We were amazed that it could have several activities going on at once in its little nervous system. Because of the random wiring, it had a sort of fail-safe characteristic. If one of the neurons wasn’t working, it wouldn’t make much of a difference—and, with nearly three hundred tubes and the thousands of connections we had soldered, there would usually be something wrong somewhere. In those days, even a radio set with twenty tubes tended to fail a lot. I don’t think we ever debugged our machine completely, but that didn’t matter. By having this crazy random design, it was almost sure to work, no matter how you built it. (Bernstein 1981)\n\nBernard Widrow once built a MADALINE I (circa 1962) in a rush to present at a technical meeting. Despite that only 1/4 of its circuits were defective, it still worked at reduced capacity.\n\nWe discovered the inherent ability of adaptive computers to ignore their own defects while we were rushing through construction of a system called Madaline I for presentation at a technical meeting. The machine was finished late the night before the meeting and the next day we showed some very complex pattern discriminations. Later we discovered that about a fourth of the circuitry was defective. Things were connected backward, there were short circuits, and poor solder joints. We were pretty unhappy until it dawned on us that this system has the ability to adapt around its own internal flaws. The capacity of the system is diminished but it does not fail. (Widrow 1963)\n\nAndrej Karpathy, on how neural network program bugs are very hard to find, because bugged neural networks do not fail, merely degrade.\n\n… perhaps you forgot to flip your labels when you left-right flipped the image during data augmentation. Your net can still (shockingly) work pretty well because your network can internally learn to detect flipped images and then it left-right flips its predictions. Or maybe your autoregressive model accidentally takes the thing it’s trying to predict as an input due to an off-by-one bug. Or you tried to clip your gradients but instead clipped the loss, causing the outlier examples to be ignored during training. Or you initialized your weights from a pretrained checkpoint but didn’t use the original mean. Or you just screwed up the settings for regularization strengths, learning rate, its decay rate, model size, etc. Therefore, your misconfigured neural net will throw exceptions only if you’re lucky; Most of the time it will train but silently work a bit worse. (Karpathy 2019)\n\nResearchers at OpenAI (2018) reported that fixing RL bugs is as important as better algorithms.\n\nBig-picture considerations like susceptibility to the noisy-TV problem are important for the choice of a good exploration algorithm. However, we found that getting seemingly-small details right in our simple algorithm made the difference between an agent that never leaves the first room and an agent that can pass the first level. To add stability to the training, we avoided saturation of the features and brought the intrinsic rewards to a predictable range. We also noticed significant improvements in performance of RND every time we discovered and fixed a bug (our favorite one involved accidentally zeroing an array which resulted in extrinsic returns being treated as non-episodic; we realized this was the case only after being puzzled by the extrinsic value function looking suspiciously periodic). Getting such details right was a significant part of achieving high performance even with algorithms conceptually similar to prior work. This is one reason to prefer simpler algorithms where possible. (Burda and Edwards 2018)\n\nAround 2019, Gwern, Shawn Presser, and others, trained \\(512\\times 512\\) image generation models using the BigGAN architecture. However, they used compare_gan, which had a multiply-by-zero bug. Somehow it still worked, but not well enough compared to the original BigGAN.\n\nOur primary goal was to train & release 512px BigGAN models on not just ImageNet but all the other datasets we had like anime datasets. The compare_gan BigGAN implementation turned out to have a subtle +1 gamma bug which stopped us from reaching results comparable to the model; while we beat our heads against the wall trying to figure out why it was working but not well enough (figuring it out far too late, after we had disbanded) … “Neural nets want to work” – even if they start out being effectively multiplied by zero. (Branwen 2022)\n\nPersonal story at the Berkeley CS 285, Deep Reinforcement Learning, 2022 Fall.\nFor Homework 3, we were asked to implement the soft actor-critic algorithm. We would implement the agent, run the agent on the Half Cheetah environment, and submit the trajectories to Gradescope, where an autograder would check the trajectories and see if the agent achieved a final score above 300. For the Half Cheetah, score means the distance it travels per episode, averaged over several episodes.\nI noticed that the algorithm I implemented did learn, but the learning curve looked like a rollercoaster, jumping up and down around the range of 250 – 300. After many fruitless and paranoid programming sessions I managed to pass the autograder by trying enough random seeds and just submitting the best seeds. The professor, Sergey Levine, offered little help, admitting that RL agents are extremely hard to debug.\nOne day after the assignment deadline, the professor announced that there was a critical one-line bug in the starter code: The correct algorithm should train the model with past game frames in a random order, but the given code always give them in the FIFO order. With the fix, the learning curve would smoothly sigmoid to 350.\n\nThe Neural Net Tank Urban Legend\nA large list of examples in The Neural Net Tank Urban Legend · Gwern.net. I have a few more.\nAccording to Sejnowski, Takeo Kanade did work on detecting tanks in images. This is unconfirmed. I have looked for “Artificial Intelligence Vision: Progress and Non-Progress”, but it is not available online. I looked for your doctoral dissertation of 1974, but it contains only facial recognition. I also cannot find anything about detecting tanks in his publication list.\n\nIn his talk “Artificial Intelligence Vision: Progress and Non-Progress,” Takeo Kanade (from Carnegie Mellon) noted that computer memories back in the 1960s were tiny by today’s standards and could hold only one image at a time. For his doctoral dissertation in 1974, Takeo had shown that, though his program could find a tank in one image, it was too difficult for it to do so in other images where the tank was in a different position and the lighting was different. But, by the time his early students graduated, the programs they designed could recognize tanks under more general conditions because computers were more powerful. Today his students’ programs can recognize tanks in any image. The difference is that today we have access to millions of images that sample a wide range of poses and lighting conditions, and computers are millions of times more powerful. (Sejnowski 2018, 256)\n\nThere was not a lot of actual research on tank recognition. (Kanal and Randall 1964) contains some good pictures. The network was a two-layered perceptron network, of type \\(\\mathbb{R}^{N \\times N} \\to \\{0, 1\\}^{32\\times 32} \\to \\{0, 1\\}^{24} \\to \\{0, 1\\}\\). It works as follows:\n\nThe grayscale photo is down-scaled and binarized by convolution with a discrete Laplace filter: \\(\\mathbb{R}^{N \\times N} \\to \\{0, 1\\}^{32\\times 32}\\).\nThe weights for the 24 hidden perceptrons are constructed by linear discriminant analysis: \\(\\{0, 1\\}^{32\\times 32} \\to \\{0, 1\\}^{24}\\)\nThe output perceptron is learned by the perceptron learning rule: \\(\\{0, 1\\}^{24} \\to \\{0, 1\\}\\).\n\n\nFigure 1: Images from (Kanal and Randall 1964).\n\n\n\n\n\n\n(a) Grayscale photos, some containing tanks, and some not.\n\n\n\n\n\n\n\n(b) A picture of a tank after convolution with a discrete Laplace filter.\n\n\n\n\n\n\n\n\n\n(c) The architecture of the network."
  },
  {
    "objectID": "sketches/posts/neural-network-scrapbook/index.html#the-second-neural-network-winter",
    "href": "sketches/posts/neural-network-scrapbook/index.html#the-second-neural-network-winter",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "The second neural network winter",
    "text": "The second neural network winter\nThe first neural network winter started around 1965, when the main research centers pivoted away from neural networks: the Stanford Research Institute group turned to symbolic AI; the Bernard Widrow group turned to using single neurons as adaptive filters; the Frank Rosenblatt group died from lack of funds and then the literal death of Rosenblatt in 1971. It rose again around 1985, when backpropagation and improved compute allowed researchers to train neural networks on the order of \\(10^4\\) parameters and \\(4\\) layers.\nSomething strange happened during the 1990 – 2010 period: the neural network research community silently disappeared again for another 20 years. Unlike the previous case, there was no great mythology or drama about this winter, no Perceptron controversy.\nI would like to find out why.\n\nLukas: So I remember Daphne Koller telling me, maybe 2003, that the kind of state-of-the-art handwriting systems were neural nets, but that it was such an ad hoc kind of system that we shouldn’t focus on it. And I wonder if maybe I should have paid more attention to that and tried harder to make neural nets work for the applications I was doing.\nPeter: Yeah, me too. And certainly Yann LeCun had success with the digit database, and I think that was over-engineered in that they looked at exactly the features they needed for that set of digitizations of those digits. And in fact, I remember researchers talking about, “Well, what change are we going to do for sample number 347?” Right?\nLukas: Oh, really? Okay.\nPeter: There were individual data points that they would perform theories on, so that was definitely over-tuning to the data. And it should have been an indication that was a good approach. It was better than other approaches at the time.\nLukas: I guess so. Although that does sound like damming level of over-fitting the data, I suppose.\nPeter: Right. There was only a couple thousand data points. I forget exactly how many. Maybe it was 10,000. Maybe it was even 100,000, but it wasn’t many. (Norvig 2021)"
  },
  {
    "objectID": "sketches/posts/info-warfare/index.html",
    "href": "sketches/posts/info-warfare/index.html",
    "title": "Information Warfare",
    "section": "",
    "text": "This essay is unfinished, but it is already interesting to read. I plan to grow this substantially as I find more examples and theories."
  },
  {
    "objectID": "sketches/posts/info-warfare/index.html#introduction",
    "href": "sketches/posts/info-warfare/index.html#introduction",
    "title": "Information Warfare",
    "section": "Introduction",
    "text": "Introduction\nThere is a secret battle on the Internet – info-warfare. It is invisible like the battle for stock market arbitrage.\nEvery time you get an incomprehensible error message refusing to allow your account log-in for reasons they refuse to explain, every time a platform said your post contained something against its content policy without explaining which, every time the captcha gets an upgrade that makes it harder for you to solve… You have been hit by a stray bullet from the secret battle.\nIn the novel Blindsight (Watts 2017), after humans made first contact with an alien probe, it attacked after a long conversation. The humans interpreted that this way: The alien probe could not understand some of human speech, and concluded that it was an information object crafted specifically made to confuse it – an act of war.\n\nImagine you’re a scrambler.\nImagine you have intellect but no insight, agendas but no awareness. Your circuitry hums with strategies for survival and persistence, flexible, intelligent, even technological—but no other circuitry monitors it. You can think of anything, yet are conscious of nothing.\nYou can’t imagine such a being, can you? The term being doesn’t even seem to apply, in some fundamental way you can’t quite put your finger on.\nTry.\nImagine that you encounter a signal. It is structured, and dense with information. It meets all the criteria of an intelligent transmission. Evolution and experience offer a variety of paths to follow, branch-points in the flowcharts that handle such input. Sometimes these signals come from conspecifics who have useful information to share, whose lives you’ll defend according to the rules of kin selection. Sometimes they come from competitors or predators or other inimical entities that must be avoided or destroyed; in those cases, the information may prove of significant tactical value. Some signals may even arise from entities which, while not kin, can still serve as allies or symbionts in mutually beneficial pursuits. You can derive appropriate responses for any of these eventualities, and many others.\nYou decode the signals, and stumble:\n\nI had a great time. I really enjoyed him. Even if he cost twice as much as any other hooker in the dome—\nTo fully appreciate Kesey’s Quartet—\nThey hate us for our freedom—\nPay attention, now—\nUnderstand.\n\nThere are no meaningful translations for these terms. They are needlessly recursive. They contain no usable intelligence, yet they are structured intelligently; there is no chance they could have arisen by chance.\nThe only explanation is that something has coded nonsense in a way that poses as a useful message; only after wasting time and effort does the deception becomes apparent. The signal functions to consume the resources of a recipient for zero payoff and reduced fitness. The signal is a virus.\nViruses do not arise from kin, symbionts, or other allies.\nThe signal is an attack."
  },
  {
    "objectID": "sketches/posts/info-warfare/index.html#game-theory-and-evolution",
    "href": "sketches/posts/info-warfare/index.html#game-theory-and-evolution",
    "title": "Information Warfare",
    "section": "Game theory and evolution",
    "text": "Game theory and evolution\nIn game theory, information warfare would be studied as a kind of signaling game (Sobel 2020). We would explain how creatures actually use their symbols by assuming that they are playing a certain kind of game, then solve for the Nash equilibria. If there are only a few Nash equilibria, and those are the ones we observe, then we declare the theory a success.\nWhereas in classical game theory, every agent is basically picking one strategy at a time to maximize its own utility, in evolutionary game theory, there are no agents. There are still individuals that fight or flee, but they no longer choose to. Each individual acts out its strategy until it dies without adapting.1 The population learns and adapts, and increases fitness of the individuals, but if we were to call the population an agent, it is an odd one. It plans by stochastic gradient ascent, and only increases the relative fitness, not absolute – which might not even exist.1 More sophisticated evolutionary game theory would allow individuals that do adapt, learn, and calculate the optimal strategy. However, this quickly becomes intractable, and this level of generality is too powerful for this essay anyway.\nFor example, if the population can contain only three kinds of individuals, in a game of rock-paper-scissors, and only the winners can reproduce with very low mutation rates, then the population serenely turn around the length-3 cycle, chasing its tail, never increasing absolute fitness. Indeed, there is no absolute fitness, like how there is no absolute height in Escher’s staircase.\n\n[The earth] breathes like a great lung; when it exhales, delicate and graceful life teems out of its pores, and all the creatures stretch out their arms to the sun; but when it takes in its breath, a rustle of fragile spirits breaking sweeps through the multitudes, and their corpses lash the ground like showers of hail. (Zapffe 2004)\n\n\nBrains are survival engines, not truth detectors. If self-deception promotes fitness, the brain lies. Stops noticing – irrelevant things. Truth never matters. Only fitness. By now you don’t experience the world as it exists at all. You experience a simulation built from assumptions. Shortcuts. Lies. Whole species is agnosiac by default. (Watts 2017)\n\nConsider the simplest case. Every round of interaction has two individuals. The interaction is symmetric (so there is no issue of “who goes first”). If I play \\(s\\) and you play \\(t\\), then my payoff from the interaction is \\(\\pi(s|t)\\), and your payoff is \\(\\pi(t|s)\\). In this simplified case, we say that \\(s\\) is…\n\na Nash equilibrium iff for any other strategy \\(s' \\neq s\\), we have \\(\\pi(s'|s) \\leq \\pi(s|s)\\);\nan evolutionarily stable strategy (ESS) iff either \\(\\pi(s'|s) &lt; \\pi(s|s)\\) or \\(\\pi(s'|s) = \\pi(s|s)\\) but \\(\\pi(s|s') &gt; \\pi(s'|s')\\);\na strict Nash equilibrium iff \\(\\pi(s'|s) &lt; \\pi(s|s)\\).\n\nIntuitively speaking, if you play the Nash equilibrium, you would be like “Why bother changing my strategy?”. If you play the strict Nash equilibrium, you would be like “I would be actively punished to change my strategy.”. The ESS is an interesting intermediate case where the first deviant might be fine, but the second deviant would be punished. This captures the idea of a stable population. It is not a matter of whether an individual might unilaterally want to change – individuals are not agents and have no desire at all. It is rather a matter of whether one kind of population could neutrally drift into another kind of population, or be actively replaced by an invasion of mutants."
  },
  {
    "objectID": "sketches/posts/info-warfare/index.html#chinese",
    "href": "sketches/posts/info-warfare/index.html#chinese",
    "title": "Information Warfare",
    "section": "Chinese",
    "text": "Chinese\nThe Chinese government has a mature system for information warfare. It has four main components. I do not know how they are usually termed, so I will describe them in my own language.\n\nespionage: This component is a typical one. Most large governments have an intelligence agency whose job is to discover the secrets of foreign governments. Compared to the espionage of developed countries like America, Chinese espionage has a strong focus on industrial and technological espionage.\nstate-level: This component is used to fight wars with other governments. For example, producing disinformation that confuses other national governments and citizens. During peacetime, this component can be used to degrade democracies.\ninternal: This is the largest component in terms of the number of people employed.\nexpat: This component has the main goal of maintaining support from the community of Chinese students and expats who are living abroad.\nforeigner: This component is the smallest and the least important. Its main goal is to create general support for the Chinese government among people who are not Chinese citizens or descendants. Its activities are mainly restricted to creating bland “cultural enrichment” videos.\n\nThis section discusses only the internal component. The foreign component is not different or more sophisticated from typical public relations campaign, and I know too little about espionage to write on that component. The expat component is very similar to the internal component, except that the Chinese government has to depend on local agents such as the Chinese Students and Scholars Association (Bowe 2018), and indirect threats, such as making visas difficult to obtain. In any case, the expat component is merely meant to support the internal component. It is my impression that the Chinese government cares little what the expats think about politics, as long as they do not turn Chinese people living inside China into heretics.\n\nInternal operations\nInternally, there are several types of operations (King, Pan, and Roberts 2017):\n\nDirect content silencing: As all Chinese websites and ISPs are registered with the local government, essentially any content on the Chinese Internet can be silenced at will by the Chinese government. This weapon can be as precise as disallowing replies to a single post for a limited period, or as blunt as deleting entire websites.\nDirect account silencing: As a simple extension of direct content silencing, any account can be silenced at will. This can be as precise as muting a single account for a week, or as blunt as deleting all accounts that has used hashtags from a list.\nThe Great Firewall: The most famous of all. This blocks certain connections between the Chinese Internet and the external Internet.\nNoise injection: This is a blunt instrument. Like a cuttlefish injecting ink, people employed by the Chinese government or the social media websites post “noise” to dilute potentially dangerous information to a low level.\nOpinion guidance: This is a precise and subtle instrument used on particularly important topics.\n\nThe final goal is to maintain the stability of the regime, but it is too vague. For practice, it is translated to the instrumental goal of preventing mass action. “Mass action” means hundreds of citizens gathering and doing something political together. It can be a demonstration, a protest, a sit-in, a political speaking event, etc. It does not have to be pro- or anti-government. Indeed, even pro-government protests have been suppressed recently. The 2012 anti-Japanese protest was the last mass action acquiesced by the government. Since then, anti-Japanese protests have been suppressed, despite anti-Japanese sentiment being officially promoted as a patriotic duty.\nUnlike what one might expect, the most common mass actions in China have nothing to do with democracy. Instead, they typically happen like this: Some people feel wronged by a corrupt low-level official, a bad merchant, or some other local bully. They find each other and walk to the local government, holding banners. Their goal is to be loud enough that a clean high-level official would see and hear them, and bring them justice.\nFor example, a common cause for mass action is when a local bank has failed. The people with bank accounts in it would stand in front of the bank holding banners and demand for the government to refund them. Another common cause is when a real estate company goes bankrupt. The people with half-developed apartments would stand in front of the gates of the estate and demand either a refund, or another development company to come in and continue the development. This has become a particularly acute problem through the 2010s, as land prices rose, and real estate companies depended increasingly on selling future housing units just to be able to fund current developments. This makes it a dangerous balancing act. If the company cannot sell off future housing units quickly, the development of current units would pause, scaring away potential buyers for future units, leading to a death spiral.\n\n\nDirect content silencing\nIt is curious that on the Chinese internet, there is no lockpicking hobby. It is implicitly censored. Searching “lockpicking” or similar words would only pull up news police reports about people caught lockpicking criminally.\n\n\nThe Great Firewall\nIn simplified terms, The Great Firewall eavesdrops on packets sent above the TCP layer, and replaces them with the TCP Reset packet when it detects possibly censorable content, for instance, when the IP address matches one of Google’s many CDN server addresses. This ends the TCP connection between the client and the server. It is most often used to terminate connections between a client inside the Chinese Internet and a server outside the Chinese Internet, although it can also operate in the other direction. It can also perform other packet replacement than just a TCP Reset packet, as in the Great Cannon. (Marczak et al. 2015)\n\n\nNoise injection\nDespite the Great Firewall, some Chinese citizens would bypass it, and they may organize mass action on social media, such as Twitter, that are not controlled by the Chinese government. For this situation, the Chinese government can drown the signal in a flood of noise. A great example happened during the 2022 COVID-19 protests. To dampen the probability of mass action, the Chinese government awakened sleeper Twitter accounts that had been waiting for just such an emergency, and they spammed like mad under the tags relevant to the protest.\n\nNumerous Chinese-language accounts, some dormant for months or years, came to life early Sunday and started spamming the service with links to escort services and other adult offerings alongside city names. The result: For hours, anyone searching for posts from those cities and using the Chinese names for the locations would see pages and pages of useless tweets instead of information about the daring protests as they escalated to include calls for Communist Party leaders to resign.\nTwitter grapples with Chinese spam obscuring news of protests (Menn 2022)\n\n\nSearch for Beijing/Shanghai/other cities in Chinese on Twitter and you’ll mostly see ads for escorts/porn/gambling, drowning out legitimate search results. Data analysis in this thread suggests that there has been a significant uptick in these spam tweets. … They tweet at a high, steady rate throughout the day, suggesting automation. Then I looked at the number of tweets by each account over time. Interestingly, more than 70% of these spam accounts only started tweeting like crazy recently.\nTwitter post by Air-Moving Device, (Air-Moving Device 2022b)\n\nThe same method shows that the great spamming cannon only fired at Beijing and Shanghai, not the other cities.\n\n\n\nFigure from a subsequent post (Air-Moving Device 2022a).\n\n\n\n\nPublic opinion guidance\n間 (Ma) is the Japanese term for negative space, the art of speaking with and listening for what is not there.\nImagine a space of political ideologies, such as the Political Compass. In an Internet without censorship, we can put a dot on each website espousing a position, and end up with a pointillistic landscape of the political Internet. Now imagine an all-powerful censor comes in and erases whatever it does not accept. It would leave behind a blotted painting. The censor did not paint a single point, but its eraser is a paintbrush too! Reading what is not said is the art of Ma.22 Gwern proposed a Chinese Censorship Audit. The idea is to compare the Chinese Wikipedia with a Chinese fork for overt erasures. Those are undeniable results of censorship.\nAny opinion with an extensive following on the Chinese Internet is the negative space of censorship, and any opinion that should have a following online, but not, is the negative space of speech. This gives us four layers of Chinese opinions:\n\nOfficially endorsed opinion, as declared in published documents. This includes items such as Xi Jinping Thought, anti-historical nihilism, etc.\nOfficially allowed opinion, the negative space of censorship.\nOfficially disallowed opinion, the negative space of speech.\nOfficially forbidden opinion, as declared in published documents. This includes Historical Nihilism, Taiwan independence, etc.\n\nThe first and last items, being officially declared, are overt and easy to point to. The middle two items are by nature ambiguous and difficult to study. It is as if they are already camouflaged for the information battlefield. The use of these two negative spaces is an officially endorsed strategy, called “public opinion guidance” (舆论引导).\nWhile on Twitter, Nazism might survive with heavy camouflage, it is not American official approval; yet on Weibo, Nazism survives with minor or no camouflage. This shows that Nazism is officially allowed, even though Fascism is officially forbidden.\nIn 2022, when Shizo Abe was assassinated, the Chinese internet erupted into cheers. Such cheers are only weakly balanced by those who want to respect the dead. While the cheers still remain online one year later (as a brief search of the hashtag #安倍已无生命体征# on Weibo on 2023-12-16 showed), subtle suggestions that a certain somebody else should be dead were swiftly suppressed within a day.\n\nZeng Ying was brutally trolled by Chinese netizens for sobbing while reporting live on Shinzo Abe’s assassination earlier this month. She was forced to apologize for being “unprofessional,” for “showing personal emotion on a public platform” and “hurting everyone’s feelings”.\nShinzo Abe death: Chinese journalist attempted suicide after being cyberbullied for emotional reportage (Muzaffar 2022)\n\n\n\nExternal attacks\nThe Chinese government maintains its stability by targeting the information received by Chinese citizens. To do this, it mainly censors the Internet inside China, and produces noise outside, but occasionally, it directly attacks the outside.\nSince 2015, the Chinese government has employed the Great Cannon multiple times to perform DDoS attacks on websites it wants to censor, such as a certain GitHub page hosting softwares for bypassing the Great Firewall.\nThe Great Cannon is a man-in-the-middle3 DDoS attack. When an external IP address requests for certain JavaScript files hosted on certain servers located inside the Great Firewall4, as the packet crosses the Great Firewall, a program makes a statistical decision. If the decision rule triggers, it replaces the innocuous JavaScript with a malicious JavaScript, which would rapidly send requests to a list of targeted servers. (Marczak et al. 2015)3 mandarin-in-the-middle?4 Typically, those are Baidu infrastructure servers that host commonly used analytics, social, or advertising scripts.\nThe two main parties in Taiwan are the DPP and the KMT. The older party, KMT, used to be pro-unification, with them being the unifier, not the CCP. It still is interested in eventual unification at some future date, probably after mainland China becomes democratic. In contrast, the DPP, being the newer party, is more interested in independence. During the 2000–2024 period, the KMT had 1 president and the DPP had 3, and the CCP had been significantly more hostile against the DPP presidents than against the KMT. Speaking from personal experience, the internal propaganda painted the DPP presidents as paper tigers with villainous faces, who would bring an apocalyptic war to Taiwan for a few votes.\nDuring the lead-up to the 2024 Taiwanese presidential election, the Chinese government has used generative-AI to smear the pro-independence DPP candidate. (Hung and Hung 2022)\n\nThe key message being spread from the pro-China camp is that William Lai, the candidate from the DPP, is a dictator in the wings who will start a war with his reckless pursuit of Taiwanese independence. … Beijing-backed bots routinely flood the social media accounts of leading DPP candidates with pro-China propaganda.\n“China has been actively waging cognitive warfare against Taiwan through disinformation,” Taiwan’s Premier Chen Chien-jen told the media in reference to how Beijing uses a mixture of economic coercion, military bluster and outright falsehoods to intimidate its neighbor. “Upon receiving the disinformation, local collaborators help disseminate and echo the message, in order to destabilize Taiwanese public sentiment and society.” … In December, a YouTube account called “Eat Rice, No War” put out a deepfake video alleging Lai had three mistresses, according to Taiwan’s Ministry of Justice. YouTube subsequently complied with a government request to remove the videos, and the rumor didn’t snowball into a campaign topic.\nThat followed a similar attempt to fake an audio file in which Ko Wen-je, the presidential candidate of the newly founded Taiwan People’s Party, mocked Lai for visiting the U.S. and “doing a job interview.” Taiwanese official investigators concluded that this was most likely a faked recording, and Ko said no such thing. (Lau 2024)\n\n\nAnalyzing more than 10,000 YouTube videos on a few channels since June, Johns Hopkins School of Advanced International Studies researcher Martin Wendiggensen found that “the DPP is almost always described negatively and mentioned in negative contexts (corruption, incompetence, stagnation) while the reverse is true for the KMT.” (Menn et al. 2024)"
  },
  {
    "objectID": "sketches/posts/info-warfare/index.html#corporate",
    "href": "sketches/posts/info-warfare/index.html#corporate",
    "title": "Information Warfare",
    "section": "Corporate",
    "text": "Corporate\n\nOffensive\nAmong corporations, overt use of offensive information is rare now, as states have gradually consolidated its monopoly on violence. Towards customers, overt deception in advertisement is rare, although misdirection is common. Towards other corporations, overtly announcing deceptive information is possible, but not commonly employed. (Why?)\n\n\nPlatform\n\nA platform is when the economic value of everybody that uses it, exceeds the value of the company that creates it. Then it’s a platform.\n— Bill Gates\n\nSocial media are platforms, but there are other platforms as well. For example, ChatGPT is a platform, as people find even its free tier economically valuable.\nPlatforms must also deal with content censorship, but their kind of censorship is different.\nCensorship by private institutions (companies, websites, etc) is very different from official censorship (Chinese, intelligence agency, etc). For private institutions, censorship has the main economic goals:\n\nremove spam. Spam is defined as content that the intended customers do not want to see. This can usually be done by automated filters. See email spam filters.\nRemove things that might harm the public image. That is, removing content that the non-customers do not want to see. This is usually harder to do right since there is a somewhat different economy.\nSpam producers focus on economies of scale. Their goal is massive scaling, like spawning billions of eggs just for a few to survive and make money. Most of the spam emails are patently unbelievable, but since spam is so cheap they can focus on just scaling up the traffic and wait for a few victims. This makes spam filtering easy, and more resembles defending against DDoS than against information war.\n\nI was once blocked for over ten minutes on ChatGPT trying to debug a certain message. It turned out to be the word “dike”. What annoyed me the most is that the error message is completely uninformative. It merely told me that the message violated some policy with no further information. Why is it like that? It has a simple economic reason.\nSuppose I were a spammer. I would simply try another spam message. For ChatGPT to provide any detailed feedback would only help my work. Suppose I were a non-spammer. I would probably try with more patience and found the offending word. I might curse it, but there is nothing I can do other than donating some money to its competitors.\nNow suppose I were a reporter trying to write an incriminating report. I would be looking for the right kind of censorship. If the feedback message contains any detail that could help the bad actors bypass the censorship, I would be shocked and write about how this makes the censorship pointless.\nThe economic equilibrium in the second case is what you get: the company uses simple censoring methods that balance between protecting the public image and driving away customers. The result is usually simple filter lists for rare words. The customers grumble but don’t switch to a different platform, but instead make their own unscalable methods of trickery. All sides talk vapidly about “harm” and “responsibility” and other defensive non-informative information (what I call “bullshit camouflage”).\n\n\nDefensive\nWhen corporations practice defensive information war, they typically practice it in the form of bland speech.\n\n\n\n\n\n\nSpeculative content follows\n\n\n\n\n\n\nI suspect the increasingly opaque error messages used by websites are not due to incompetence, or a mistaken attempt at being user-friendly, but a design choice, a deliberate defense in their escalating information warfare against spammers.\nFor example, I have found that in certain contexts, trying to use OAuth on my Google account would result in the one and only error message:\n\nSorry, something went wrong there. Try again.\n\nWhat is that “something”? Why can’t Google provide any useful error message?\nSome time ago, I attempted to buy a pepper spray on Amazon, and I simply could not get pass the age verification screen. Calling the customer service resulted in twenty minutes of wrangling with the website. It finally turned out that for some reason, the GUI on my end is different from the GUI on their end – presumably just a backend server bug. Whereas on their end, the presumably correct GUI asks for the driver’s license number as the only acceptable form of age verification, on my end I was able to use a passport number, which then the system rejected for not being a driver’s license number.\nWhile this in itself is frustrating but at least reasonable – the Amazon website is vast and sprawling, and I understand enough about websites and programming to sympathize with the programmers who maintain it – I was greatly annoyed that the only error message is just Based on the ID number provided, we are unable to complete your order.\nSometimes, I help others access or solve problems with e-commerce websites from within China, such as Taobao or Tmall. This form of highly nondescript error message is also prevalent inside Chinese e-commerce websites, and as far as I can discern, is not caused by the Chinese-specific issue of censorship. Rather, they typically convey the message “According to our statistical analysis, you are sufficiently suspicious to trigger a statistical decision rule, but we won’t tell you what, because if you were a bad actor we would not want you to know which of the things you did was suspicious.”.\nThe convergence of behavior both inside and outside China suggests that it is not due to Chinese censorship, or lack thereof. It suggests that it is a common issue encountered by e-commerce, which suggests it is mostly intended to combat commercial spam.\nSpammers are always attempting to bypass statistical detection of spammers, and on the other side, e-commerce websites are attempting to create statistical detectors of spammers. It is always a battle on the margin. For the e-commerce websites, the goal is not to stamp out spammers, or to ensure all legitimate users were accepted, but rather balance stamping out spammers, retaining legitimate users, and cost. The calculus of cost governs each aspect. In the equilibrium state, they occasionally ban legitimate users and then handle a fraction of those with manual interview. Each legitimate user grumbles but mostly acquiesces. Similarly, the marginal cost of losing a spam account is smaller than the marginal cost of going through a manual interview, so spammers would just open another account than go through the manual interview.\n\n\nRegulatory\nBanks, like e-commerce websites, often have vague nondescript error messages as well. However, banks are typically less worried about spammers than about money-laundry and government regulations. In this case, it is still defensive information, but not against spammers, but against the possibility of leaking sensitive information.\n\nIn the specific case of “Why did the bank close my account, seemingly for no reason? Why will no one tell me anything about this? Why will no one take responsibility?”, the answer is frequently that the bank is following the law. … [Suspicious Activity Reports] can (and sometimes must!) be filed for innocuous reasons and do not necessarily imply any sort of wrongdoing.\nIf the United States brings its subpoena power to bear against a bank teller and asks them about a SAR, they’re supposed to say nothing. That is the law… to avoid constantly violating this, Compliance at most functioning institutions has long-since decided that SARs will live in their own walled garden of a subsystem … If, for example, a SAR is misfiled because that subsystem doesn’t share the same view of account ownership as another part of the overall system, investigating that problem might require telling the customer that they were investigated, which you cannot do. And because this is insufficiently Kafkaesque, at some financial institutions, you can get a SAR filed for knowing what a SAR is, because “advanced knowledge of anti-moneylaundering procedure” is a characteristic only of financial professionals and terrorists.\nSeeing like a Bank (McKenzie 2023)"
  },
  {
    "objectID": "sketches/posts/info-warfare/index.html#fictional",
    "href": "sketches/posts/info-warfare/index.html#fictional",
    "title": "Information Warfare",
    "section": "Fictional",
    "text": "Fictional\nIn fiction, there have been cases of “cognitohazard”. However, their purported effects are much more overt than the somewhat subtle effects described in the essay. Whereas the information objects described in the essay redirect, confuse, camouflage, and waste the opponents’ time, cognitohazards typically directly destroy the information-processing hardware of the opponents. In general, cognitohazards differ from the other examples, because they operate by breaking the abstraction layer. They are digital operations that causes analog operations that destroy the hardware. On the digital layer, the underlying hardware is assumed to be always there, like air before the discovery of vacuum. Cognitohazards pull out the air of the digital world and exposes the vacuum, destroying digital life in the process.\nThe short story BLIT (Langford 1999) is a common reference point for speculations on cognitohazards, although the namesake of BLIT itself does not break the software-hardware abstraction barrier, and therefore is not a cognitohazard according to our definition. In the story, a human mind is a software running on a brain-hardware, and upon “Gödelian shock inputs”5, the software falls into “vicious circles” from which it cannot recover to a working state, although the hardware seems to remain just fine. Many BLITs were discovered, with the first one resembling a fractal parrot.65 The story does not elaborate, but it is common knowledge among mathematical logicians that anything powerful enough for Peano arithmetics is powerful enough to be hacked. For example, (Dowling 1989) is a one-page proof that shows that “no program can both test its input for the presence of a virus and simultaneously be guaranteed not to spread a virus itself”, by translating this statement into math, then quote Rice’s theorem. The same argument can be used to show that “Gödelian shock inputs” must exist, assuming that human minds are softwares that can process arbitrary formulas in Peano arithmetics.6 \n… so called because its outline, when processed for non-hazardous viewing, is generally considered to resemble that of the bird. A processed (anamorphically elongated) partial image appears in Appendix 3 of this report, page A3-ii. THE STATED PAGE MUST NOT BE VIEWED THROUGH ANY FORM OF CYLINDRICAL LENS. PROLONGED VIEWING IS STRONGLY DISRECOMMENDED. PLEASE READ PAGE A3-i BEFORE PROCEEDING.\n2-6. This first example of the Berryman Logical Image Technique (hence the usual acronym BLIT) evolved from AI work at the Cambridge IV supercomputer facility, now discontinued. V.Berryman and C.M.Turner [3] hypothesized that pattern-recognition programs of sufficient complexity might be vulnerable to “Gödelian shock input” in the form of data incompatible with internal representation. Berryman went further and suggested that the existence of such a potential input was a logical necessity …\n\n\n\n\n\nThe BLIT emerging from AI work at the Cambridge IV supercomputer facility.\n\n\nReal-life analogs of cognitohazards are extremely rare, and their status as “cognitohazards” is controversial. Nevertheless, here are some possible analogs.\nThe McCollough effect shows that visual perception can be modified for up to 3 months with just a few minutes of visual stimulus. There have been several explanations, but none is satisfactory. They however all share the common idea that the visual stimulus itself changes the neurons in the low-level of perception.\nBadly designed digital computers, such as the Zuse Z1 or Commodore PET, can suffer from the “killer poke”.\nOther than cognitohazards, the SCP world contains examples of anti-memetics, which are information objects that are hard to remember and reason about. They are not cognitohazards, but are rather camouflaged information."
  },
  {
    "objectID": "sketches/posts/info-warfare/index.html#argument",
    "href": "sketches/posts/info-warfare/index.html#argument",
    "title": "Information Warfare",
    "section": "Argument",
    "text": "Argument\nAn argument can be thought of as a game of wits and rhetorics, structured like a tennis game. Two sides stand facing each other, and hit the “point of the argument” back and forth. When one side finally “drops the ball”, the other side wins the point. When seen this way, many puzzling aspects of argument games turns clear.\nLet’s start with the simplest problem: Why is it that people have an urge to “have the last say”? To not drop the ball. If one side makes an argument, and the other does not offer a counter-argument, the round is assumed to be over and the first side won. If argument games are used to discover truths, this can get rather strange.\n\nThe algorithmic fairness controversy\nIn 2019, I looked into the controversy over “algorithmic fairness” of the Northpointe COMPAS, which I explained in The Racial Algorithmic Bias Controversy. What was curious about the controversy is that neither side did it the way I would have done it. I would have just written out the simple mathematical model, then argued about which measure to choose. Instead, the actual controversy went like this:\n\nProPublica wrote a long article, putting in a lot of human faces for pathos.\nNorthpointe replied tersely that the COMPAS system has good statistical properties.\nProPublica replied with such detailed statistical arguments that I can only characterize as “statistical spam”.\n\nAnd here, the game stood. Who “won”? From my point of view, both sides missed the point completely and the whole game was moot. In fact I would rank ProPublica much lower than Northpointe for first using pathos, then using statistical spam. However, from a tennis game model of argument, ProPublica won, because Northpointe dropped the ball. Statistical spam is bad for discovering the truth, but a good play, because it is hard to counter-argue statistical spam. To the onlookers, they can use the controversy to have their own argument games, which would go as follows:\n\nA: “Haven’t you heard of the algorithmic bias?”\nB: “Yes, but Northpointe made a counter-argument.”\nA: “Well, ProPublica have shown that the counter-argument was statistically moot.”\nAnd at that point, person B would have to admit defeat, or attempt to actually parsing the statistical spam and then give up out of frustration, or dismiss the ProPublica statistical spam as irrelevant.\n\nIn my view, dismissing outright the ProPublica statistical spam is the right move, but it is going against the human intuition that whoever has the last say in an argument is in the right. People intuitively disdain “dismissing the last move” as childish cheating, much as shouting “That doesn’t count!” in a tennis game.\n\n\nContinental philosophy\n\n[Bertrand Russell] thinks I am muddleheaded; but then I think he is simpleminded.\n— Alfred North Whitehead\n\nContinental philosophy presents another example of information warfare. The very definition of continental philosophy is tricky, especially if you want to respect the wishes of continental philosophers, who simultaneously want to build a coherent movement and want to reject all attempts at pigeonholing as inherently dehumanizing. To keep this section meaningful, we will disrespect their wishes.\nTo begin, consider the infamous passage “God is a Lobster”:\n\nChallenger quoted a sentence he said he came across in a geology textbook. He said we needed to learn it by heart because we would only be in a position to understand it later on: “A surface of stratification is a more compact plane of consistency lying between two layers.” The layers are the strata. They come at least in pairs, one serving as substratum for the other. The surface of stratification is a machinic assemblage distinct from the strata. The assemblage is between two layers, between two strata; on one side it faces the strata (in this direction, the assemblage is an inter stratum), but the other side faces something else, the body without organs or plane of consistency (here, it is a metastratum). In effect, the body without organs is itself the plane of consistency, which becomes compact or thickens at the level of the strata.\nGod is a Lobster, or a double pincer, a double bind. Not only do strata come at least in pairs, but in a different way each stratum is double (it itself has several layers). Each stratum exhibits phenomena constitutive of double articulation. Articulate twice, B-A, BA. This is not at all to say that the strata speak or are language based. Double articulation is so extremely variable that we cannot begin with a general model, only a relatively simple case. The first articulation chooses or deducts, from unstable particle-flows, metastable molecular or quasi-molecular units (substances) upon which it imposes a statistical order of connections and successions (forms). (Deleuze and Guattari 1987, chap. 3)\n\nA mathematician might look at this passage, and, if not immediately dismissing it as word salad, start writing down equations like\n\\[\n\\exists G : G \\in \\text{Lobster}, \\exists P : \\text{isPincers}(G, P), \\exists B, A: P = BAAB, \\dots.\n\\]\nThey would then attempt to define an algebraic structure – call it \\(L\\)-sequences, \\(L\\) for “lobster” – and try to construct examples of \\(L\\)-sequences. Deleuze and Guattari never made such an attempt.\n\nOne of the fundamental tasks of the State is to striate the space over which it reigns, or to utilize smooth spaces as a means of communication in the service of striated space. It is a vital concern of every State not only to vanquish nomadism but to control migrations and, more generally, to establish a zone of rights over an entire “exterior,” over all of the flows traversing the ecumenon. (Deleuze and Guattari 1987, 385)\n\nTorrents of noise flood the world every second. Continental philosophy is not noise, even if they resemble noise. Why do they exist? Why do people write them, and why do people transmit them?\n\nThere are no meaningful translations for these terms. They are needlessly recursive. They contain no usable intelligence, yet they are structured intelligently; there is no chance they could have arisen by chance.\n\nI explain them as information objects with three aspects: the literal text, the actual practical use, and the rhetorical game mechanics.\nThe literal text of continental philosophy is typically made of complex sentences, and filled with allusions to past literature, word-plays, puns, neologisms, and foreign words. To interpret them coherently, you need concentration, patience, and to have read the prerequisite texts. Consequently, such texts allow those few readers who can actually interpret them coherently a lot of bragging rights and all the pleasures of being in a society of esoteric wisdom. One can get a distinct feeling of this by going into the local university’s philosophy department and eavesdrop on their late-night discussions, or going to an online forum of continental philosophy.\nHowever, if they are merely hard to understand, they would not have more followers than modern set theory, which is equally complex and arcane:\n\nTheorem 44: The generic mantle of \\(V\\) is a definable transitive class in \\(V\\), containing all ordinals, invariant under set forcing, and a model of the ZF axioms of set theory.\nProof: The generic mantle is a definable transitive class containing all ordinals and closed under the Gödel operations, because it is the intersection of the generic grounds, each of which has those closure properties in the corresponding extension of \\(V\\). So by Fact 10, in order to show that \\(gM\\) is an inner model, it remains to show merely that it is almost universal, which we do below… (Fuchs, Hamkins, and Reitz 2015)\n\nEven a complete outsider to mathematics could quickly understand naive set theory, with an hour of tutorial on drawing circles and dots and Venn diagrams. Modern set theory, while a vast complication over the simple picture of Venn diagrams, is not different in kind. If the outsider looks at the paper on set-theoretic geology and asks “But what does it have to do with geology?”, I can reply simply, “It is a one-way analogy. Set theorists noticed that they can start with ZF set theory and add the Axiom of Choice to get ZFC set theory, then add ZFC-consistency to get an even bigger set theory, and so on. So they thought ‘What if we went the other way? Going down instead of up?’ So they tried making smaller set theories below ZF set theory, and smaller, and smaller, all the way down the ‘mantle’ and below.”\nThere we go, I have explained set-theoretic geology. Reading it, everyone can see that set-theoretic geology has no relevance to actual geology, anymore than the Conway’s Game of Life has relevance to actual biology.\nIn comparison, continental philosophy is designed to look as if they talk about general practical issues, such as economics, justice, the meaning of life, happiness, God (which is a lobster), and more.\nFinally, continental philosophy occupies a particular niche in the game of rhetorics that allows it to reproduce itself socially.\nSince its language is vague and suggestive, it seems relevant to any field, and so it can just walk into any tennis court uninvited and make an opening move. Since its language is dense and hard to distinguish from noise, the others might just ignore it, allowing it to declare victory by default. If someone responds, then the real fun begins.\n\n\n\n\n\n\n\nobjection\nresponse\n\n\n\n\nThis is irrelevant.\nThis problem is relevant to real people’s real lives.\n\n\nThis philosophical argument is irrelevant to the real problem.\n(Quote someone like the Frankfurt School.)\n\n\nWe do not talk about this topic in this style.\nOpen yourself to the challenges of alterity, lest you impose hegemonic reason.\n\n\nSpeak more simply.\nSpeaking simply is reductionistic. The real world is complex.\n\n\n[points at quote] This is technically wrong.\nYou have misunderstood what the authors are really saying. Engage with the source respectfully instead of quoting them out of context.\n\n\n\nBy repeatedly butting into any fashionable controversy of the day, continental philosophy keeps itself from falling into oblivion like modern set theory. By not losing the rhetorical game, it keeps itself from being debunked like old quantum theory. However, to complete its social reproduction, continental philosophy still has to protect itself from challenges from within.\nEuclid’s Elements was a compilation and cleaning-up of geometry books that came before it. It was of such fine quality that it destroyed the works upon which it was based.7 Back then, books were expensive and required frequent copying by hand. Faced with a choice between copying Euclid’s Elements or the books that it obsoleted, the ancients chose to copy Euclid instead of the previous works that were of historical interest only, historical interest that the ancients could not afford.7 This is a cartoon sketch. The actual textual history of Euclid’s Elements is studied only by historians of mathematics. Working mathematicians typically hold it at a respectful distance and actively try not to not get too close to it, since an accurate view of the history of mathematics is harmful to the actual practice of mathematics (I speak from personal experience), because the real history of mathematics is not a mathematical textbook written for its educational content. History of mathematics is useful only as a carefully sketched cartoon, omitting almost all the real details, and completely violating historical truth sometimes, because some truths are not adaptive.\nTo reproduce itself, continental philosophy must keep itself from being explained or clarified, especially by insiders. To educate new continental philosophers, the old continental philosophers would write simple explainers and summaries, like The Edinburgh Dictionary of Continental Philosophy. However, such reference books risk becoming substitutes for the original source material, starting a slippery slope into clarity. This prevents the old confusions from being cleared up. “textual deference”, and “there is no substitute for reading the masters” (Smith 1991).\nTo throw such textual deference into stark relief, consider how scientists treat their history. Working scientists have only the most cartoonish knowledge of the history of their fields. Classical papers are rarely read, except in fields so immature that they do not have good textbooks yet and the only way to proceed is reading the original papers. In fact, one can measure the maturity of a field by the ignorance of its practitioners about its original sources, and the lack of bibliography in textbooks. A textbook on electrodynamics does not refer to Coulomb or Maxwell’s original books. A course in relativity does not attempt to explicate the original papers by Einstein. The original sources are typically fumbling and barely comprehensible, a target for improvement, then swiftly relegated to mere history.\nHow does continental philosophy protect itself from progress? The easiest method is to simply declare some sources as original and all else derivative, to freeze the great canonical works for all times. However, this is not sustainable, as new philosophers are not content to merely read the great canons, but want their own chance at joining the great canons. Thus we arrive at the tradition of “the Great Conversation”:\n\nWhat binds the authors together in an intellectual community is the great conversation in which they are engaged. In the works that come later in the sequence of years, we find authors listening to what their predecessors have had to say about this idea or that, this topic or that. They not only harken to the thought of their predecessors, they also respond to it by commenting on it in a variety of ways. (Adler 1990, 28)\n\nWith the Great Conversation, continental philosophy reproduces itself. The old master texts remain inscrutable. New inscrutable texts are added to the canon. New texts that are too clear or sensible are denounced as derivative, analytical, reductionistic, or perhaps kitsch, and denied entrance to the canon."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html",
    "href": "docs/posts/1998-robin-hanson/index.html",
    "title": "Is a singularity just around the corner?",
    "section": "",
    "text": "Economic growth is determined by the supply and demand of investment capital; technology determines the demand for capital, while human nature determines the supply. The supply curve has two distinct parts, giving the world economy two distinct modes. In the familiar slow growth mode, rates of return are limited by human discount rates. In the fast growth mode, investment is limited by the world’s wealth. Historical trends suggest that we may transition to the fast mode in roughly another century and a half.\nCan some new technology switch us to the fast mode more quickly than this? Perhaps, but such a technology must greatly raise the rate of return for the world’s expected worst investment project. It must thus be very broadly applicable, improving almost all forms of capital and investment. Furthermore, investment externalities must remain within certain limits."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#abstract",
    "href": "docs/posts/1998-robin-hanson/index.html#abstract",
    "title": "Is a singularity just around the corner?",
    "section": "",
    "text": "Economic growth is determined by the supply and demand of investment capital; technology determines the demand for capital, while human nature determines the supply. The supply curve has two distinct parts, giving the world economy two distinct modes. In the familiar slow growth mode, rates of return are limited by human discount rates. In the fast growth mode, investment is limited by the world’s wealth. Historical trends suggest that we may transition to the fast mode in roughly another century and a half.\nCan some new technology switch us to the fast mode more quickly than this? Perhaps, but such a technology must greatly raise the rate of return for the world’s expected worst investment project. It must thus be very broadly applicable, improving almost all forms of capital and investment. Furthermore, investment externalities must remain within certain limits."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#introduction",
    "href": "docs/posts/1998-robin-hanson/index.html#introduction",
    "title": "Is a singularity just around the corner?",
    "section": "Introduction",
    "text": "Introduction\nMany technological enthusiasts, impressed by the potential of various envisioned technologies, have speculated that technology may soon become so productive as to induce a “singularity”, a period of extremely rapid growth. (A collection of speculations can be found here and here.)\nHow rapid? Over the last few centuries, population has doubled roughly every 70 years, per-capita consumption has doubled roughly every 35 years,and scientific progress has doubled roughly every 15 years. In contrast, computing power1 has doubled roughly every two years for the last half-century. Many have speculated that perhaps economic growth rates will soon match or even greatly exceed current computing-power growth rates.1 Editor’s note: It links to (Moravec 1998).\nWhat would it take, exactly, for a technology to make the economy grow this fast? This paper offers a simple economic analysis intended to illuminate this question. We will find that while very rapid growth is possible in principle, this requires enabling technologies to meet some strong conditions. It is hard to see how any single new technology could do this, though historical trends suggest that the accumulation of all new technologies over the century and a half might."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#the-supply-and-demand-of-investment-capital",
    "href": "docs/posts/1998-robin-hanson/index.html#the-supply-and-demand-of-investment-capital",
    "title": "Is a singularity just around the corner?",
    "section": "The supply and demand of investment capital",
    "text": "The supply and demand of investment capital\nGrowth in consumption over any extended period requires growth in what economists call “capital”, which means any thing which helps make “products” people want. These terms are considered abstractly, so a music concert is a “product”, and the skills of a musician are “human capital”.\nCapital can be used not only to make products, but also to make more capital, and the rate at which this happens is called a “rate of return”. The fraction of capital devoted to making more capital is called the “savings rate”, and the price people pay to rent capital for this purpose is called a “market rate of return.” This is a “real rate of return” if capital is calibrated so that one more unit of capital always produces the same “amount” (or “value” or “utility”) of products people like.\nWhat determines how much people save and the interest rate they get for it? Supply and demand, naturally. Technology supplies a pool of investment projects which are physically possible, and investors demand these projects more or less depending on how productive they are. Or, looking at it in terms of investment capital, investment projects demand capital to be carried out, and investors supply capital to such projects. This view is illustrated in the following figure.\n\nAs is usual in supply and demand graphs, the x-axis is a quantity axis, here amount of capital, and the y-axis is a price axis, here a rate of return. The origin where the axes meet has no capital saved and a zero rate of return.\nSupply curves slope up, so that higher returns are required to induce investors to save larger fractions of their income. Demand lines slope down, so that the more projects are undertaken, the worse their return. Projects can be thought of as lined up in order along the savings axis, with the best projects at the left and the worst projects at the right. At the point where supply and demand meet, the marginal investor is just barely willing to offer capital to get the market return offered by the marginal project.\nWhile the demand lines shown are fictitious, the shape of the supply curve shown is true to a reasonable understanding of what investment supply looks like. This is based both on observing current human preferences, and on an understanding of what sort of preferences should have been selected for during our evolution. Moreover, the dates on the figure show roughly where the economy has been along this curve. (See the technical appendix for details.)\nThe supply curve eventually turns up sharply, shooting off to infinity at a particular bound. At least it does this if the population growth rate stays below twice its current value, and if investments on net benefit rather than harm non-investors (again, see the appendix). The supply curve is relatively flat, however, over a large range, at a rate of return determined mainly by the population growth rate and the degree to which our genes have taught us to discount risk and time. Since capital supply should be relatively insensitive to technology, we can hold this curve fixed in our minds as we consider how technology might change the demand for capital, i.e., the supply of investment projects.\nWhen poor technology creates a low demand for investment capital, as with Demand 1 in the figure, the resulting market rate of return is not very sensitive to demand. In this case improving technology mainly just raises the savings rate. Demand 2 in the figure describes an economy nearing the middle of a transition between the two distinct economic modes, which may describe our world economy in roughly a century.\nWhen technology is good enough to create a high demand for investment capital, as with Demand 3, savings becomes insensitive to demand, and the market rate of return can become sensitive to technology. In the high demand mode, growth rates can in principle be very high. Given demand curves with the right shape, growth rates might increase much faster with time in the high growth mode than they did in the low growth mode. Such rapidly increasing growth rates seems most like the imagined singularity scenarios. With other demand shapes, however, fast growth rates might increase more slowly than slow growth rates did."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#burning-up-excess-return-in-investment-races",
    "href": "docs/posts/1998-robin-hanson/index.html#burning-up-excess-return-in-investment-races",
    "title": "Is a singularity just around the corner?",
    "section": "Burning Up Excess Return in Investment Races",
    "text": "Burning Up Excess Return in Investment Races\nThe reader may have noticed that the figure above suggests that while in the low demand mode the market rate of return stays roughly constant, the average rate of return of the projects actually tried might depend dramatically on the demand. After all, the top of the demand curve could in principle be very far above the low supply curve. So why couldn’t growth rates also be very large in the low demand mode?\nThe problem with this argument is that this figure has neglected to include the time dimension, and a lack of long-term property rights in most investment projects means that returns above the market rate are burned up an a race to be first.\nThere are, in fact, very few long-term property rights regarding the right to undertake investment projects. Think of developing a new kind of car, colonizing the moon, developing specialized CAD software, or making a movie with a certain kind of gimmick. Each such project requires various forms of capital, such as machines or skilled labor. While in the short term only one investor may have the rights to tackle a given project, in the long term many competing investors could have positioned themselves to have this short-term opportunity.\nFor example, Microsoft’s dominant position in PC operating systems now gives it the right to many very attractive investments. But there was once an open race to become the dominant operating system, and competitors then tried harder because of the prospect of later high returns. And when deciding whether to enter this earlier race and how hard to try, investors mainly wondered if they could get a competitive rate of return. Similarly, while one group now has the right to make the next Batman movie sequel, there have long been open contests to create popular movie series, and popular comic strips.\nConsider a typical as-yet-untried investment project, becoming more and more attractive with time as technology improves and the world market grows larger. If there wasn’t much point in attempting such a project very long after other teams tried, then a race to be first should make sure the project is attempted near when investors first expect such attempts to produce a competitive rate of return. This should happen even if the project returns would be much greater if everyone waited longer. The extra value the project would have if everyone waited is burned up in the race to do it first.\nThus most of the return above the market return in our supply and demand figure above should be burned up, leaving the average return at about the market return. Thus in the low demand mode the height of the demand curve is relatively unimportant. If anticipated, a technology which makes a moderate number of investment projects much more productive may have no effect on growth rates or on rates of return.\nThe width of the demand curve, however, does matter in low demand mode. If technology creates many new attractive investment projects, growth rates can rise due to a rise in the savings rate. This happens by changing the rate of return offered on the marginal investment project, the project actually tried which is expected to offer the worst return. To improve the marginal project, a technology needs to have a very broad range of productive applications."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#investment-bottlenecks",
    "href": "docs/posts/1998-robin-hanson/index.html#investment-bottlenecks",
    "title": "Is a singularity just around the corner?",
    "section": "Investment Bottlenecks",
    "text": "Investment Bottlenecks\nA big problem with having a very broad range of applications, however, is the possibility of bottleneck resources. Most production, for consumption or investment, requires a wide variety of forms of capital. These include land, raw materials, energy, various sorts specialized labor and specialized machines, information sources, places to dispose of waste, access to channels of distribution and advertising, legal adjudication, and regulatory approval. A technology that dramatically improves the productivity of one of these typically gives only a small improvement to the final output. This is the concept of diminishing returns in production. If the rate of improvement in any one input consistently lags behind the rest, that input then becomes a bottleneck limiting the growth rate for the entire process.\nFor example, computer hardware may be getting cheap very fast, but one cannot now as quickly reduce the costs of training professionals who understand both computers and application areas well. Thus such training becomes a bottleneck limiting the contribution of computers to the larger economy. Similarly, even having the cost of producing electricity drop to near zero would only have a minor effect on the economy, at least in the short term. Most of the price consumers pay for electricity is to transport it, not to produce it, and new ways of organizing the transportation and production of electricity would have to evolve to take more advantage of the new situation.\nThe notion of bottlenecks applies to timescales in particular. For example, I think the rapid growth in computing speed and communication bandwidth, together with loose notions that most of the economy must be at root computation and communication, have suggested to many that most all economic timescales, including economic doubling times, must soon rise to meet these faster rates. But this ignores the prospect of other physical processes becoming time bottlenecks, and the diminishing returns to raw computation. Note also that in the low growth mode, even changing the total investment time scale by a large factor, which implies changing the capital demand function by a large factor, need not change the market return or growth rate by very much.\nIt seems hard to escape the conclusion that it just takes a lot of time for the world economy to absorb even very broadly applicable technologies like the computer, especially if the criteria of interest is raising the rate of return on the marginal investment project worldwide. Thus it seems unlikely that a single new technology could quickly knock the economy into a high demand mode with very high growth rates. (For similar conclusions, see here and here.)\nEven if no single technology can create fast growth, the cumulative effect of all new technologies over many decades, might, still slowly push the economy up the supply curve into the high demand mode. This requires that such technologies keep creating enough productive investment opportunities to keep up with a very rapidly growing economy, that investments on average benefit non-investors, and that we don’t double population growth rates. This doesn’t seem easy, but it may not be impossible either."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#conclusion",
    "href": "docs/posts/1998-robin-hanson/index.html#conclusion",
    "title": "Is a singularity just around the corner?",
    "section": "Conclusion",
    "text": "Conclusion\nIf we assume that the risk and time preferences of future investors will not vary much from those which evolution has given us, then the supply curve for investment capital should say nearly fixed while technology changes the demand curve. And since a lack of long-term property rights in investment projects creates a race to be first, the two parts of the supply curve create two distinct modes for the economy: a low growth rate mode and a high growth rate mode. Historical trends on growth rates suggest that we are slowly moving up the capital supply curve toward the high growth mode, and may reach it in roughly a century and a half.\nMany have suggested that some special new technology will induce rapid growth rates much more quickly than this. This is possible in principle, but we have identified a number of conditions which such a technology must meet. Investments must on average benefit, rather than harm, non-investors. And most difficult, the special technology must have such broad and attractive enough applications across the world economy that it dramatically raises the returns on the worst investment anyone undertakes. To do this, such a technology must dramatically improve the productivity of almost all forms of capital, not just a few. And this must be true even though, in a race to be first, each investment project is started as soon as it seemed marginally attractive.\nThese conditions do not appear to have been met by any of the very diverse range of technologies that have yet appeared. It is possible that such conditions may be met by some future technology, but a persuasive argument for this case should explain, in standard economic terminology, why we should expect this technology to meet these conditions."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#references",
    "href": "docs/posts/1998-robin-hanson/index.html#references",
    "title": "Is a singularity just around the corner?",
    "section": "References",
    "text": "References\nBarro, R., Sala-I-Martin, X., Economic Growth, McGraw Hill, 1995.\nBlume, L., Easley, D., “Evolution and Market Behavior”, Journal of Economic Theory, 58:9-40, 1992.\nHansson, I., Stuart, C., “Malthusian Selection of Preferences,” American Economic Review, 80(3):529-544, June 1990.\nMaddison, A., Dynamic Forces in Capitalist Development, Oxford Univ. Press, 1991.\nPrice, D., Little science, big science… and beyond, Columbia Univ. Press, 1986.\nSee also Econ Growth Web site."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#sec-technical-appendix",
    "href": "docs/posts/1998-robin-hanson/index.html#sec-technical-appendix",
    "title": "Is a singularity just around the corner?",
    "section": "Technical Appendix",
    "text": "Technical Appendix\nThere are many more complex models of economic growth out there, but the following simple model is complex enough to embody many results relevant to singularity speculations.\nThis model is one of the simplest models of endogenous growth, allowing for real long-term growth within the model and implicitly including all forms of capital improvement, including research and development. The model’s main simplification is that it does not explicitly distinguish between various forms of capital and consumption, aggregating them instead to a single generic form.\nFirst some notation, including standard values where applicable.\n\n\\(n\\) = growth rate of world population, now \\(\\sim 1.4\\% / \\rm{yr}\\).\n\\(g\\) = growth rate of world per-capita consumption, now \\(\\sim 2\\% / \\rm{yr}\\).\n\\(p\\) = typical discount rate, \\(\\sim 3\\% / \\rm{yr}\\) (= factor of \\(2\\) in \\(23\\) years).\n\\(a\\) = typical risk-aversion, \\(\\sim 1\\quad\\) (\\(a = -c u''/u'\\) for utility \\(u(c)\\) of consumption rate \\(c\\)).\n\\(r\\) = real market rate of return (not the risk-less interest rate).\n\\(s\\) = savings, fraction of world capital making more capital.\n\\(A\\) = growth rate of invested capital (corrected for depreciation).\n\\(i\\) = internality, fraction of investment return the investor gets.\n\nWe have five primary equations using these parameters.\n\\[\n\\begin{aligned}\n\\text{Supply of Capital:} \\quad r &= p + ag \\\\\n\\text{Demand for Capital:} \\quad A &= A(s) \\\\\n\\text{Capital Rental:} \\quad r &= iA \\\\\n\\text{Accounting:} \\quad g+n &= sA\n\\end{aligned}\n\\]\nSUPPLY: The supply of capital for investment depends on how fast consumption is growing relative to the typical rate at which people discount the future. If people were risk-neutral, the market return would equal the typical discount rate. But for risk-adverse people who plan to consume more tomorrow than today, stuff today is worth more.\nDEMAND: The demand function \\(A(s)\\) describes the investment projects that current technology makes possible. It says what the expected total (private and external) return on the worst project would be if a fraction \\(s\\) of total income were invested.\nRENTAL: The rental price of capital depends on how fast a unit of capital can produce more capital, corrected for the fact that an investor may not internalize all the costs or benefits of a project. A project may, for example, create improvements in technology that others can copy.\nACCOUNTING: The total growth rate in consumption and capital depends on how productive capital is, and on what fraction of income is saved.\nSome comments:\n\nTypical utility parameter values are predicted from evolutionary selection. When trading resources for a parent now vs. for a child a generation from now, it matters that a child shares only half a parent’s genes. And unit risk-aversion, which is log utility \\(u(c) = log(c)\\), is selected for, at least regarding shocks to all copies of a gene, such as the total market risk in the CAPM.\nThese equations allow for different people to own different amounts of capital, and for different investment projects to have different returns and internality. This is because the supply equation is a very general implication of maximizing expected discounted utility, and the internality parameter \\(i\\) can be considered an average over the projects tried.\nWe have neglected the probably-weak dependence of internality \\(i\\) on the savings rate \\(s\\). Having \\(i&lt;1\\) says that investment projects on net benefit non-investors, while \\(i&gt;1\\) says that non-investors are on net harmed.\nWe’ve set the market return equal to the average return, and so are assuming no long-term property rights in projects. A projects happens at the first time its return becomes competitive. Relaxing this assumption is equivalent to reducing the internality \\(i\\), which then becomes more dependent on savings \\(s\\).\n\\(A(s)\\) summarizes all technical change which creates economic growth. It does not, however, describe changes in the growth rate. neralize While the return to any one not-yet-started project typically rises with time as technology improves, the best projects will be done first, so the \\(A(s)\\) function may rise or fall with time. \\(A(s)\\) also falls if the number of attractive investments did not grow as fast as the economy did.\nThe growth rate \\(g\\) in the accounting equation is really the growth rate of capital, while the rate \\(g\\) in the demand equation is the growth rate of consumption. When savings \\(s\\) is constant, such as with demand \\(A(s)\\) unchanging with time, these are the same thing. We are thus assuming that the growth rate of savings \\(s\\) is much less than the growth rate of consumption \\(g\\).\nWe are using depreciation-corrected parameters \\(A\\) and \\(s\\) in the above equations. To have a depreciation \\(d\\) appear explicitly, change to raw terms \\(A_0, s_0\\), where \\(A_0 = A + d\\) and \\(s_0 = (s A+d)/A_0\\). The accounting and rental equations become \\(n + g = s_0 A_0 - d\\) and \\(r = i (A_0-d)\\). For non-human capital, depreciation is typically \\(\\sim 5\\% / \\rm{yr}\\).\n\nThe first four columns of the following table shows historical estimates for the annual per-capita and population growth rates \\(g, n\\) for four different dates from 1750 to 1995. (Warning: the per-capita growth estimate for 1750 is very crude.) Using the above model and our standard values for preference parameters \\(a, p\\), we then infer the next two columns, an annual rate of return \\(r\\) and a savings fraction \\(s/i\\) for each date. The return estimates seem roughly within reason.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\n# nations in data\nPer-capita growth rate\nPopulation growth rate\nRate of Return\nSavings fraction\nTotal return\nSavings\nRaw total return\nRaw savings\n\n\n\n\n\\(g\\)\n\\(n\\)\n\\(r\\)\n\\(s/i\\)\n\\(A\\)\n\\(s\\)\n\\(A_0\\)\n\\(s_0\\)\n\n\n1995\n100+\n2.0%\n1.4%\n5.0%\n68%\n10%\n34%\n15%\n56%\n\n\n1905\n29\n1.3%\n0.9%\n4.3%\n50%\n9%\n25%\n14%\n53%\n\n\n1825\n7\n0.8%\n0.5%\n3.8%\n34%\n8%\n17%\n13%\n50%\n\n\n1750\n2\n0.0%\n0.4%\n3.0%\n13%\n6%\n7%\n11%\n49%\n\n\n\nSomewhat arbitrarily assuming internality \\(i = 50\\%\\), and using the standard value of depreciation \\(d = 5\\%\\), the rest of the table gives estimates for total return and savings \\(A\\), \\(s\\), and their raw values \\(A_0, s_0\\). A standard estimate of \\(56\\%\\) for current raw total savings roughly fits with the standard values that \\(\\sim 20\\%\\) of raw savings is invested in non-human capital, which gets \\(\\sim 1/3\\) of income. Note that the savings growth rate seems to be growing at \\(\\sim 0.3\\%/\\rm{yr}\\), which is much less than the per-capita growth rate, as we had assumed.\n\nWhat fast growth requires\nAssuming that preference parameters \\(a, p\\) don’t change much, what does this model say about the possibility of an economic “singularity,” that is, very large growth rates \\(g\\) in per-capita consumption? Such growth rates, if they persisted long, would imply vast changes in per-capita consumption in a single human generation.\nThe demand equation says that per-capita consumption growth \\(g\\) can’t get very large unless the interest rate \\(r\\) does, and the rental equation says that the interest rate \\(r\\) can’t get very large unless the total return \\(A\\) does. The accounting equation also says that total consumption growth n+g can’t get very large unless total return \\(A\\) does.\nUsing our equations to eliminate \\(r\\) and \\(g\\), we get a market equation, with capital demand on the left and capital supply on the right,\n\\[\nA(s) = \\frac{p-an}{i - as}.\n\\]\nFor \\(p &lt; an\\) and \\(as &lt; i\\), this gives the functional form shown in the figure in the body of the paper. Thus the relevant savings fraction is actually \\(s/i\\), savings relative to internality, rather than savings relative to total income.\nThe limits to fast growth appear more directly in\n\\[\ng = s \\frac{p - n}{i - as}.\n\\]\nThus the only way to allow very fast growth \\(g \\gg p\\) is for \\(s \\sim i/a\\) without \\(n \\sim p\\). Thus since \\(s&lt; 1\\), we require \\(i &lt; a\\).\nThus an economic singularity, \\(g \\gg p\\), requires:\n\\[\ni &lt; a, \\quad s \\sim i/a, \\quad A(i/a) \\gg p\n\\]\nThat is, for unit (i.e., log) risk-aversion, an economic singularity requires that\n\nInvestment projects on net benefit, rather than hurt, non-investors.\nSavings must be carefully balanced near the internality parameter.\nThe return expected for the worst invested-in project becomes very large.\n\nThe bottom line is that this model does allow for an economic singularity under certain circumstances. The historical increase in the savings fraction has been roughly constant with time for the last two centuries, suggesting a near 100% savings fraction near the year 2150. Our ignorance about the internality parameter is cause for concern. Thus it is possible, though not obvious, that a continuation of historical trends will result in an economic singularity of \\(g \\gg p\\) in roughly a century and a half."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#appendix-peer-commentary",
    "href": "docs/posts/1998-robin-hanson/index.html#appendix-peer-commentary",
    "title": "Is a singularity just around the corner?",
    "section": "Appendix: Peer Commentary",
    "text": "Appendix: Peer Commentary\n\nNick Bostrom:7/7/98\nRobin Hanson’s paper gives an analysis of the conditions for an economic singularity. In a few brief sections, important and non-obvious relations between the key concepts related to explosive economic growth are described. Everybody who is interested in “the singularity” will benefit from understanding what economic theory can say about the issue.\nI have some doubt about the empirical part of the paper. Robin writes: “The historical increase in the savings fraction has been roughly constant with time for the last two centuries, suggesting a near 100% savings fraction near the year 2150.” Extrapolations of this sort are of course very precarious, and Robin does not claim otherwise. Yet, even as extrapolations go, this one seems especially problematic. The historical data determining the model parameters contains only three data points, if we bracket the one for 1750 which Robin says is “very crude”. It would be interesting to know whether there is more data available that fits the curve. And what are the error margins? How robust is the 2150 estimate to variations in the parameter values?\nI’m also wondering about whether there might be an alternative explanations for the increasing savings rates. Rather than saying that people save a greater fraction of their capital because the rate of return on savings has gone up, is it possible that people save more because the risk factor has decreased? A person considering saving some of his money in 1750 might have faced a greater risk that his savings would go up in smoke due to some political upheaval, be expropriated by a monarch, or that he himself would get killed in a plague before he got a chance to harvest the payoff. Even if the average rate of return had been constant between then and now, might one not have conjectured that risk-averse individuals would be less inclined to invest under such circumstances, especially since institutions that could insure against unexpected losses were presumably less developed in those days?\nAn interesting question to consider is: What possible technologies would have the properties that could make for an economic singularity? My own view is that when we get both superintelligence and nanotechnology (and I think one would quickly lead to the other) then that will cause a singularity. This technological combination certainly has the extremely wide range of applicability that Robin lists as a precondition for a singularity. Whether it would benefit or harm non-investors is more difficult to predict, since the social ramifications could well be so dramatic that Robin’s economic model would no longer be valid. For example: It might not be possible to enforce property rights; preferences and living conditions might change so radically that comparisons between the amount of capital in the world before and after the singularity will no longer be meaningful; there might not even be a multitude of competing economical agents after the singularity.\nNick Bostrom\nDepartment of Philosophy, Logic and Scientific Method\nLondon School of Economics\nn.bostrom@lse.ac.uk\nhttp://www.hedweb.com/nickb\n\n\nRobin Hanson replies:7/7/98\n— I thank Nick Bostrom for his thoughtful review.\nI do not dispute Nick’s assessment that my empirical extrapolations seem “especially problematic”. I offered them mainly to give readers some sense of where we have been within the model I describe. I don’t really expect finer data to continue to support a linear relation of the savings fraction with time. Instead, I have recently been working on a simple sum of exponentials model of long term economic growth, using a data series by Brad DeLong (Estimating World GDP, One Million B.C. - Present). A rough draft is available.\nNick wonders whether “people save more because the risk factor has decreased.” This certainly makes sense, and is obviously one of many issues I left out of my simple model. It turns out that there is a long-standing puzzle in finance regarding pricing for risk; investors act as if the risks of strong downturns are much larger than they appear in the recent historical record. So you’d really need historical data on perceived risks to examine this empirically.\nI agree with Nick that superintelligence and nanotechnology are technologies with potentially very wide applicability. But I’m more skeptical about how fast early breakthroughs in either field will lead to more advanced breakthroughs. I’m thus relatively confident that we would retain “a multitude of competing economical agents,” and I’m not convinced that correcting for technical change in comparing capital amounts will get much harder.\nA final note: a dramatic loss in ability to enforce property rights doesn’t actually invalidate the model, though it might dramatically change some parameters.\nRobin Hanson\nhanson@econ.berkeley.edu http://hanson.berkeley.edu/\nRWJF Health Policy Scholar, Sch. of Public Health 510-643-1884\n140 Warren Hall, UC Berkeley, CA 94720-7360 FAX: 510-643-2627\n\n\nKathryn Aegis:7/7/98\nOnce again, Robin Hanson has found a connection between the principles of economics and the goals of transhumanism. It sparks thinking in a new direction and provides a potential avenue of practical application by an entrepreneur or investor.\nA question that raised in my mind as I read Robin’s paper relates to a recent discussion with Kurt Schuler regarding future alternatives to our present day parochial monetary currencies and the explosive growth in investment that could result. Could the digital technologies (encryption, ubiquitous exchange, instant transfer) of alternative monetary regimes represent an example of the very technology that Robin references?\nKathryn Aegis, aegis@igc.apc.org\n\n\nHanson replies:8/7/98\nKathryn Aegis asks if “digital technologies … of alternative monetary regimes represent an example” of technologies which could induce explosive economic growth. My intuition would be that by themselves such technologies are far from sufficient. You might have a better case if you added in lots of new kinds of markets that such digital money might be used in. But most new markets are blocked due to regulatory reasons, not because of poor digital money. I’m not very confident of my intuitions here, however, and could be persuaded to change my mind by someone like Lawrence H. White.\n\n\nBilly Brown:26/2/99\nRobin Hanson paints an interesting picture of the relationships between the factors that underlie economic growth, and I certainly would not argue his conclusions on economic grounds. I would, however, suggest that his results could easily be misinterpreted when one attempts to apply them to the real world.\nThe problem is that in the kind of future many transhumans expect to see, economic growth is a poor proxy for human benefit. Consider, for example, the parallels between electronic computers and molecular manufacturing:\nComputers have been undergoing an exponential improvement in price/performance ratios for some decades now. Enthusiasts like to point out that if cars had improved at the same rate over the last twenty years, the average vehicle would cost a few pennies, travel at supersonic speeds, and be capable of running for years without refueling. Nevertheless, as Hanson points out, the economic effects have been relatively modest. Computers have made some companies rich and others poor, and have on the whole been beneficial, but they have not turned us all into millionaires.\nNow, molecular manufacturing promises to bring about the same kind of change in most manufacturing industries. This implies that most material goods will undergo a period of extremely rapid innovation, with costs collapsing as capabilities rapidly improve. Hanson’s model predicts, probably correctly, that the net effect on economic growth will again be much smaller than we expect. But there is a crucial difference: after a few decades of such progress our car really will cost only a few pennies, and so will all other manufactured goods.\nWhat this means is that economic growth will become much less relevant as a means of measuring human prosperity, at least by our current standards. If prices for most material goods collapse while measures such as GNP show modest growth, the practical result is a vast improvement in the human condition.\nBilly Brown, MCSE+I\nbbrown@conemsco.com\n\n\nHanson replies: 26/2/99\nWhile Billy Brown “would not argue [with my] conclusions on economic grounds,” he cautions against applying them to the real world because “economic growth is a poor proxy for human benefit.” Why? Currently the “economic effects” of rapidly falling computer hardware prices have been “relatively modest.” By analogy, Mr. Brown presumes that with molecular manufacturing (i.e., nanotech) the “prices for most material goods [would] collapse while measures such as GNP show[ed] modest growth.” Since “the practical result [would be] a vast improvement in the human condition,” he concludes “economic growth will become much less relevant as a means of measuring human prosperity.”\nMr. Brown, there is no such thing as a non-economic human benefit. If the participants in some social process perceive a type of benefit, economists consider that benefit type fair game for economic analysis.\nNow published statistics like GNP do neglect many types of human benefit. They would not, however, fail to notice a rapid fall in the price of most material goods such as cars. Such a fall would certainly show up as rapid GDP growth. How fast would car prices fall with nanotech? That is exactly the question at issue here, isn’t it? The arguments of fast growth skeptics are not refuted by the fast growth claims of nanotech optimists, though of course skeptics might be refuted by more detailed economic analyses suggesting fast nanotech growth.\nRobin Hanson, hanson@econ.berkeley.edu\n\n\nBilly Brown:11/3/99\nAfter re-reading Robin Hanson’s paper, exchanging a bit of private e-mail, and thinking about the issue, I’ve come to conclude that our disagreement is a result of one of those definition problems that always crop up when you have people from different fields debating a complex point. It seems to me that what he means by “economic growth” would by definition include any sort of human benefit, which of course invalidates my claim. What I meant by “economic growth” was something more like “the Federal government’s official GNP figures”, which is another matter entirely.\nI must therefore concede that his model is not subject to the sorts of problems I suggested. Anything that has any real effect on human welfare should show up as economic growth, and I don’t see any reason to contest his conclusion about the conditions a technology must meet to cause economic growth. Whether nanotechnology can actually meet those conditions is a complex question best addressed elsewhere.\nSo, that reduces most of my comments to a complaint about the inaccuracy of current measurement methods, which isn’t especially relevant to the paper. That being the case, I think that it is time for me to retire from the field.\nBilly Brown, MCSE+I\nbbrown@conemsco.com\n\n\nHanson replies: 12/3/99\nI’m happy that Mr. Brown and I were able to work out our differences via a private conversation, and I’m sorry that I didn’t take the discussion private from the very start.\nEconomists are well aware of the problems with government statistics. The problem is that allowing government agencies more leeway in including “squishy” harder to measure factors in their estimates also allows more opportunities for corruption in constructing estimates. Privately produced estimates can and do include squishy estimates, but their quality is limited by the fact that much less money goes into producing private estimates."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#appendix-editors-notes",
    "href": "docs/posts/1998-robin-hanson/index.html#appendix-editors-notes",
    "title": "Is a singularity just around the corner?",
    "section": "Appendix: Editor’s notes",
    "text": "Appendix: Editor’s notes\n\nThis markdown document taken mostly verbatim from the original HTML file. I converted ASCII-math into LaTeX math, fixed typos, chased down a few dead links, etc.\nThe original metadata\nJournal of Evolution and Technology June 1998. Vol. 2\n(Received 10 April, 1998)\n \nRobin Hanson\nSchool of Public Health\nUniversity of California, Berkeley\nWarren Hall, CA 94720-7360\nnet: http://hanson.berkeley.edu/\nemail: hanson@econ.berkeley.edu\n\nJournal of Evolution and Technology\nA peer-reviewed electronic journal publishing contemporary research into future science and philosophy.\nISSN 1541-0099"
  },
  {
    "objectID": "docs/posts/1993-vernor-vinge/index.html",
    "href": "docs/posts/1993-vernor-vinge/index.html",
    "title": "The Coming Technological Singularity",
    "section": "",
    "text": "Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended.\nIs such progress avoidable? If not to be avoided, can events be guided so that we may survive?These questions are investigated. Some possible answers (and some further dangers) are presented."
  },
  {
    "objectID": "docs/posts/1993-vernor-vinge/index.html#abstract",
    "href": "docs/posts/1993-vernor-vinge/index.html#abstract",
    "title": "The Coming Technological Singularity",
    "section": "",
    "text": "Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended.\nIs such progress avoidable? If not to be avoided, can events be guided so that we may survive?These questions are investigated. Some possible answers (and some further dangers) are presented."
  },
  {
    "objectID": "docs/posts/1993-vernor-vinge/index.html#what-is-the-singularity",
    "href": "docs/posts/1993-vernor-vinge/index.html#what-is-the-singularity",
    "title": "The Coming Technological Singularity",
    "section": "What is The Singularity?",
    "text": "What is The Singularity?\nThe acceleration of technological progress has been the central feature of this century. I argue in this paper that we are on the edge of change comparable to the rise of human life on Earth. The precise cause of this change is the imminent creation by technology of entities with greater than human intelligence. There are several means by which science may achieve this breakthrough (and this is another reason for having confidence that the event will occur):\n\nThe development of computers that are “awake” and superhumanly intelligent. (To date, most controversy in the area of AI relates to whether we can create human equivalence in a machine. But if the answer is “yes, we can”, then there is little doubt that beings more intelligent can be constructed shortly thereafter.\nLarge computer networks (and their associated users) may “wake up” as a superhumanly intelligent entity.\nComputer/human interfaces may become so intimate that users may reasonably be considered superhumanly intelligent.\nBiological science may find ways to improve upon the natural human intellect.\n\nThe first three possibilities depend in large part on improvements in computer hardware. Progress in computer hardware has followed an amazingly steady curve in the last few decades 1. Based largely on this trend, I believe that the creation of greater than human intelligence will occur during the next thirty years. (Charles Platt 2 has pointed out the AI enthusiasts have been making claims like this for the last thirty years. Just so I’m not guilty of a relative-time ambiguity, let me more specific: I’ll be surprised if this event occurs before 2005 or after 2030.)1 Moravec, Hans, Mind Children, Harvard University Press, 1988.2 Platt, Charles, Private Communication.\nWhat are the consequences of this event? When greater-than-human intelligence drives progress, that progress will be much more rapid. In fact, there seems no reason why progress itself would not involve the creation of still more intelligent entities – on a still-shorter time scale. The best analogy that I see is with the evolutionary past: Animals can adapt to problems and make inventions, but often no faster than natural selection can do its work – the world acts as its own simulator in the case of natural selection. We humans have the ability to internalize the world and conduct “what if’s” in our heads; we can solve many problems thousands of times faster than natural selection. Now, by creating the means to execute those simulations at much higher speeds, we are entering a regime as radically different from our human past as we humans are from the lower animals.\nFrom the human point of view this change will be a throwing away of all the previous rules, perhaps in the blink of an eye, an exponential runaway beyond any hope of control. Developments that before were thought might only happen in “a million years” (if ever) will likely happen in the next century. (In 3, Greg Bear paints a picture of the major changes happening in a matter of hours.)3 Bear, Greg, “Blood Music”, Analog Science Fiction-Science Fact, June, 1983. Expanded into the novel Blood Music, Morrow, 1985\nI think it’s fair to call this event a singularity (“the Singularity” for the purposes of this paper). It is a point where our models must be discarded and a new reality rules. As we move closer and closer to this point, it will loom vaster and vaster over human affairs till the notion becomes a commonplace. Yet when it finally happens it may still be a great surprise and a greater unknown. In the 1950s there were very few who saw it: Stanisław Ulam 4 paraphrased John von Neumann as saying:4 Ulam, S., Tribute to John von Neumann, Bulletin of the American Mathematical Society, vol 64, nr 3, part 2, May, 1958, p1-49.\n\nOne conversation centered on the ever accelerating progress of technology and changes in the mode of human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue.\n\nVon Neumann even uses the term “singularity”, though it appears he is still thinking of normal progress, not the creation of superhuman intellect. (For me, the superhumanity is the essence of the Singularity. Without that we would get a glut of technical riches, never properly absorbed 5.)5 Stent, Gunther S., The Coming of the Golden Age: A View of the End of Progress, The Natural History Press, 1969.\nIn the 1960s there was recognition of some of the implications of superhuman intelligence. I. J. Good wrote 6:6 Good, I. J., “Speculations Concerning the First Ultraintelligent Machine”, in Advances in Computers, vol 6, Franz L. Alt and Morris Rubinoff, eds, pp31-88, 1965, Academic Press.\n\nLet an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. … It is more probable than not that, within the twentieth century, an ultraintelligent machine will be built and that it will be the last invention that man need make.\n\nGood has captured the essence of the runaway, but does not pursue its most disturbing consequences. Any intelligent machine of the sort he describes would not be humankind’s “tool” – any more than humans are the tools of rabbits or robins or chimpanzees. Through the ’60s and ’70s and ’80s, recognition of the cataclysm spread 7 8 9 10. Perhaps it was the science-fiction writers who felt the first concrete impact. After all, the “hard” science-fiction writers are the ones who try to write specific stories about all that technology may do for us. More and more, these writers felt an opaque wall across the future. Once, they could put such fantasies millions of years in the future 11. Now they saw that their most diligent extrapolations resulted in the unknowable … soon. Once, galactic empires might have seemed a Post-Human domain. Now, sadly, even interplanetary ones are.7 Vinge, Vernor, “Bookworm, Run!”, Analog, March 1966, pp8-40. Reprinted in True Names and Other Dangers, Vernor Vinge, Baen Books, 1987.8 Alfvén, Hannes, writing as Olof Johanneson, The End of Man?, Award Books, 1969 earlier published as “The Tale of the Big Computer”, Coward-McCann, translated from a book copyright 1966 Albert Bonniers Forlag AB with English translation copyright 1966 by Victor Gollanz, Ltd.9 Vinge, Vernor, First Word, Omni, January 1983, p10.10 Bear, Greg, “Blood Music”, Analog Science Fiction-Science Fact, June, 1983. Expanded into the novel Blood Music, Morrow, 198511 Stapledon, Olaf, The Starmaker, Berkley Books, 1961 (but from the forward probably written before 1937).\nWhat about the ‘90s and the ’00s and the ’10s, as we slide toward the edge? How will the approach of the Singularity spread across the human world view? For a while yet, the general critics of machine sapience will have good press. After all, till we have hardware as powerful as a human brain it is probably foolish to think we’ll be able to create human equivalent (or greater) intelligence. (There is the far-fetched possibility that we could make a human equivalent out of less powerful hardware, if were willing to give up speed, if we were willing to settle for an artificial being who was literally slow 12. But it’s much more likely that devising the software will be a tricky process, involving lots of false starts and experimentation. If so, then the arrival of self-aware machines will not happen till after the development of hardware that is substantially more powerful than humans’ natural equipment.)12 Vinge, Vernor, “True Names”, Binary Star Number 5, Dell, 1981. Reprinted in True Names and Other Dangers, Vernor Vinge, Baen Books, 1987.\nBut as time passes, we should see more symptoms. The dilemma felt by science fiction writers will be perceived in other creative endeavors.(I have heard thoughtful comic book writers worry about how to have spectacular effects when everything visible can be produced by the technically commonplace.) We will see automation replacing higher and higher level jobs. We have tools right now (symbolic math programs, CAD/CAM) that release us from most low-level drudgery. Or put another way: The work that is truly productive is the domain of a steadily smaller and more elite fraction of humanity. In the coming of the Singularity, we are seeing the predictions of true technological unemployment finally come true.\nAnother symptom of progress toward the Singularity: ideas themselves should spread ever faster, and even the most radical will quickly become commonplace. When I began writing, it seemed very easy to come up with ideas that took decades to percolate into the cultural consciousness; now the lead time seems more like eighteen months. (Of course, this could just be me losing my imagination as I get old, but I see the effect in others too.) Like the shock in a compressible flow, the Singularity moves closer as we accelerate through the critical speed.\nAnd what of the arrival of the Singularity itself? What can be said of its actual appearance? Since it involves an intellectual runaway, it will probably occur faster than any technical revolution seen so far. The precipitating event will likely be unexpected – perhaps even to the researchers involved. (“But all our previous models were catatonic! We were just tweaking some parameters…”) If networking is widespread enough (into ubiquitous embedded systems), it may seem as if our artifacts as a whole had suddenly wakened.\nAnd what happens a month or two (or a day or two) after that? I have only analogies to point to: The rise of humankind. We will be in the Post-Human era. And for all my rampant technological optimism, sometimes I think I’d be more comfortable if I were regarding these transcendental events from one thousand years remove … instead of twenty."
  },
  {
    "objectID": "docs/posts/1993-vernor-vinge/index.html#can-the-singularity-be-avoided",
    "href": "docs/posts/1993-vernor-vinge/index.html#can-the-singularity-be-avoided",
    "title": "The Coming Technological Singularity",
    "section": "Can the Singularity be Avoided?",
    "text": "Can the Singularity be Avoided?\nWell, maybe it won’t happen at all: Sometimes I try to imagine the symptoms that we should expect to see if the Singularity is not to develop. There are the widely respected arguments of Penrose 13 and Searle 14 against the practicality of machine sapience. In August of 1992, Thinking Machines Corporation held a workshop to investigate the question “How We Will Build a Machine that Thinks” 15. As you might guess from the workshop’s title, the participants were not especially supportive of the arguments against machine intelligence. In fact, there was general agreement that minds can exist on nonbiological substrates and that algorithms are of central importance to the existence of minds. However, there was much debate about the raw hardware power that is present in organic brains. A minority felt that the largest 1992 computers were within three orders of magnitude of the power of the human brain. The majority of the participants agreed with Moravec’s estimate 16 that we are ten to forty years away from hardware parity. And yet there was another minority who pointed to 17 18, and conjectured that the computational competence of single neurons may be far higher than generally believed. If so, our present computer hardware might be as much as ten orders of magnitude short of the equipment we carry around in our heads. If this is true (or for that matter, if the Penrose or Searle critique is valid), we might never see a Singularity. Instead, in the early ’00s we would find our hardware performance curves begin to level off – this caused by our inability to automate the complexity of the design work necessary to support the hardware trend curves. We’d end up with some very powerful hardware, but without the ability to push it further. Commercial digital signal processing might be awesome, giving an analog appearance even to digital operations, but nothing would ever “wake up” and there would never be the intellectual runaway which is the essence of the Singularity. It would likely be seen as a golden age … and it would also be an end of progress. This is very like the future predicted by Gunther Stent. In fact, on page 137 of 19, Stent explicitly cites the development of transhuman intelligence as a sufficient condition to break his projections.13 Penrose, R., The Emperor’s New Mind, Oxford University Press, 1989.14 Searle, John R., “Minds, Brains, and Programs”, in The Behavioral and Brain Sciences, v.3, Cambridge University Press, 1980. The essay is reprinted in The Mind’s I, edited by Douglas R. Hofstadter and Daniel C. Dennett, Basic Books, 1981. This reprinting contains an excellent critique of the Searle essay.15 Thearling, Kurt, “How We Will Build a Machine that Thinks”, a workshop at Thinking Machines Corporation. Personal Communication.16 Moravec, Hans, Mind Children, Harvard University Press, 1988.17 Conrad, Michael et al., “Towards an Artificial Brain”, BioSystems, vol23, pp175-218, 1989.18 Rasmussen, S. et al., “Computational Connectionism within Neurons: a Model of Cytoskeletal Automata Subserving Neural Networks”, in Emergent Computation, Stephanie Forrest, ed., p428-449, MIT Press, 1991.19 Stent, Gunther S., The Coming of the Golden Age: A View of the End of Progress, The Natural History Press, 1969.\nBut if the technological Singularity can happen, it will. Even if all the governments of the world were to understand the “threat” and be in deadly fear of it, progress toward the goal would continue. In fiction, there have been stories of laws passed forbidding the construction of “a machine in the form of the mind of man” 20.In fact, the competitive advantage – economic, military, even artistic – of every advance in automation is so compelling that passing laws, or having customs, that forbid such things merely assures that someone else will get them first.20 Herbert, Frank, Dune, Berkley Books, 1985. However, this novel was serialized in Analog Science Fiction-Science Fact in the 1960s.\nEric Drexler 21 has provided spectacular insight about how far technical improvement may go. He agrees that superhuman intelligences will be available in the near future – and that such entities pose a threat to the human status quo. But Drexler argues that we can embed such transhuman devices in rules or physical confinement such that their results can be examined and used safely. This is I. J. Good’s ultraintelligent machine, with a dose of caution. I argue that confinement is intrinsically impractical. For the case of physical confinement: Imagine yourself confined to your house with only limited data access to the outside, to your masters. If those masters thought at a rate – say – one million times slower than you, there is little doubt that over a period of years (your time) you could come up with “helpful advice” that would incidentally set you free. (I call this “fast thinking” form of superintelligence “weak superhumanity”. Such a “weakly superhuman” entity would probably burn out in a few weeks of outside time. “Strong superhumanity” would be more than cranking up the clock speed on a human-equivalent mind. It’s hard to say precisely what “strong superhumanity” would be like, but the difference appears to be profound. Imagine running a dog mind at very high speed. Would a thousand years of doggy living add up to any human insight? (Now if the dog mind were cleverly rewired and then run at high speed, we might see something different….) Most speculations about superintelligence seem to be based on the weakly superhuman model. I believe that our best guesses about the post-Singularity world can be obtained by thinking on the nature of strong superhumanity. I will return to this point later in the paper.)21 Drexler, K. Eric, Engines of Creation, Anchor Press/Doubleday, 1986.\nThe other approach to Drexlerian confinement is to build rules into the mind of the created superhuman entity (Asimov’s Laws). I think that performance rules strict enough to be safe would also produce a device whose ability was clearly inferior to the unfettered versions (and so human competition would favor the development of the those more dangerous models).Still, the Asimov dream is a wonderful one: Imagine a willing slave, who has 1000 times your capabilities in every way. Imagine a creature who could satisfy your every safe wish (whatever that means) and still have 99.9% of its time free for other activities. There would be a new universe we never really understood, but filled with benevolent gods (though one of my wishes might be to become one of them).\nIf the Singularity can not be prevented or confined, just how bad could the Post-Human era be? Well … pretty bad. The physical extinction of the human race is one possibility. (Or as Eric Drexler put it of nanotechnology: Given all that such technology can do, perhaps governments would simply decide that they no longer need citizens!). Yet physical extinction may not be the scariest possibility. Again, analogies: Think of the different ways we relate to animals. Some of the crude physical abuses are implausible, yet…. In a Post-Human world there would still be plenty of niches where human equivalent automation would be desirable: embedded systems in autonomous devices, self-aware daemons in the lower functioning of larger sentients. (A strongly superhuman intelligence would likely be a Society of Mind 22 with some very competent components.) Some of these human equivalents might be used for nothing more than digital signal processing. They would be more like whales than humans. Others might be very human-like, yet with a one-sidedness, a dedication that would put them in a mental hospital in our era. Though none of these creatures might be flesh-and-blood humans, they might be the closest things in the new environment to what we call human now. (I. J. Good had something to say about this, though at this late date the advice may be moot: Good 23 proposed a “Meta-Golden Rule”, which might be paraphrased as “Treat your inferiors as you would be treated by your superiors.”It’s a wonderful, paradoxical idea (and most of my friends don’t believe it) since the game-theoretic payoff is so hard to articulate. Yet if we were able to follow it, in some sense that might say something about the plausibility of such kindness in this universe.)22 Minsky, Marvin, Society of Mind, Simon and Schuster, 1985.23 Good, I. J., [Help! I can’t find the source of Good’s Meta-Golden Rule, though I have the clear recollection of hearing about it sometime in the 1960s. Through the help of the net, I have found pointers to a number of related items. G. Harry Stine and Andrew Haley have written about metalaw as it might relate to extraterrestrials: G. Harry Stine, “How to Get along with Extraterrestrials … or Your Neighbor”, Analog Science Fact- Science Fiction, February, 1980, p39-47.]\nI have argued above that we cannot prevent the Singularity, that its coming is an inevitable consequence of the humans’ natural competitiveness and the possibilities inherent in technology. And yet … we are the initiators. Even the largest avalanche is triggered by small things. We have the freedom to establish initial conditions, make things happen in ways that are less inimical than others. Of course (as with starting avalanches), it may not be clear what the right guiding nudge really is:"
  },
  {
    "objectID": "docs/posts/1993-vernor-vinge/index.html#other-paths-to-the-singularity-intelligence-amplification",
    "href": "docs/posts/1993-vernor-vinge/index.html#other-paths-to-the-singularity-intelligence-amplification",
    "title": "The Coming Technological Singularity",
    "section": "Other Paths to the Singularity: Intelligence Amplification",
    "text": "Other Paths to the Singularity: Intelligence Amplification\nWhen people speak of creating superhumanly intelligent beings, they are usually imagining an AI project. But as I noted at the beginning of this paper, there are other paths to superhumanity. Computer networks and human-computer interfaces seem more mundane than AI, and yet they could lead to the Singularity. I call this contrasting approach Intelligence Amplification (IA). IA is something that is proceeding very naturally, in most cases not even recognized by its developers for what it is. But every time our ability to access information and to communicate it to others is improved, in some sense we have achieved an increase over natural intelligence. Even now, the team of a PhD human and good computer workstation (even an off-net workstation!) could probably max any written intelligence test in existence.\nAnd it’s very likely that IA is a much easier road to the achievement of superhumanity than pure AI. In humans, the hardest development problems have already been solved. Building up from within ourselves ought to be easier than figuring out first what we really are and then building machines that are all of that. And there is at least conjectural precedent for this approach. Cairns-Smith 24 has speculated that biological life may have begun as an adjunct to still more primitive life based on crystalline growth. Lynn Margulis 25 has made strong arguments for the view that mutualism is the great driving force in evolution.24 Cairns-Smith, A. G., Seven Clues to the Origin of Life, Cambridge University Press, 1985.25 Margulis, Lynn and Dorion Sagan, Microcosmos, Four Billion Years of Evolution from Our Microbial Ancestors, Summit Books, 1986.\nNote that I am not proposing that AI research be ignored or less funded. What goes on with AI will often have applications in IA, and vice versa. I am suggesting that we recognize that in network and interface research there is something as profound (and potential wild) as Artificial Intelligence. With that insight, we may see projects that are not as directly applicable as conventional interface and network design work, but which serve to advance us toward the Singularity along the IA path.\nHere are some possible projects that take on special significance, given the IA point of view:\n\nHuman/computer team automation: Take problems that are normally considered for purely machine solution (like hill-climbing problems), and design programs and interfaces that take a advantage of humans’ intuition and available computer hardware. Considering all the bizarreness of higher dimensional hill-climbing problems (and the neat algorithms that have been devised for their solution), there could be some very interesting displays and control tools provided to the human team member.\nDevelop human/computer symbiosis in art: Combine the graphic generation capability of modern machines and the esthetic sensibility of humans. Of course, there has been an enormous amount of research in designing computer aids for artists, as labor saving tools. I’m suggesting that we explicitly aim for a greater merging of competence, that we explicitly recognize the cooperative approach that is possible. Karl Sims 26 has done wonderful work in this direction.\nAllow human/computer teams at chess tournaments. We already have programs that can play better than almost all humans. But how much work has been done on how this power could be used by a human, to get something even better? If such teams were allowed in at least some chess tournaments, it could have the positive effect on IA research that allowing computers in tournaments had for the corresponding niche in AI.\nDevelop interfaces that allow computer and network access without requiring the human to be tied to one spot, sitting in front of a computer. (This is an aspect of IA that fits so well with known economic advantages that lots of effort is already being spent on it.)\nDevelop more symmetrical decision support systems. A popular research/product area in recent years has been decision support systems. This is a form of IA, but may be too focussed on systems that are oracular. As much as the program giving the user information, there must be the idea of the user giving the program guidance.\nUse local area nets to make human teams that really work (ie, are more effective than their component members). This is generally the area of “groupware”, already a very popular commercial pursuit. The change in viewpoint here would be to regard the group activity as a combination organism. In one sense, this suggestion might be regarded as the goal of inventing a “Rules of Order” for such combination operations. For instance, group focus might be more easily maintained than in classical meetings. Expertise of individual human members could be isolated from ego issues such that the contribution of different members is focussed on the team project. And of course shared data bases could be used much more conveniently than in conventional committee operations. (Note that this suggestion is aimed at team operations rather than political meetings. In a political setting, the automation described above would simply enforce the power of the persons making the rules!)\nExploit the worldwide Internet as a combination human/machine tool. Of all the items on the list, progress in this is proceeding the fastest and may run us into the Singularity before anything else. The power and influence of even the present-day Internet is vastly underestimated. For instance, I think our contemporary computer systems would break under the weight of their own complexity if it weren’t for the edge that the USENET “group mind” gives the system administration and support people!) The very anarchy of the worldwide net development is evidence of its potential. As connectivity and bandwidth and archive size and computer speed all increase, we are seeing something like Lynn Margulis’ 27 vision of the biosphere as data processor recapitulated, but at a million times greater speed and with millions of humanly intelligent agents (ourselves).\n\n26 Sims, Karl, “Interactive Evolution of Dynamical Systems”, Thinking Machines Corporation, Technical Report Series, published in Toward a Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life, Paris, MIT Press, December 1991.27 Margulis, Lynn and Dorion Sagan, Microcosmos, Four Billion Years of Evolution from Our Microbial Ancestors, Summit Books, 1986.The above examples illustrate research that can be done within the context of contemporary computer science departments. There are other paradigms. For example, much of the work in Artificial Intelligence and neural nets would benefit from a closer connection with biological life. Instead of simply trying to model and understand biological life with computers, research could be directed toward the creation of composite systems that rely on biological life for guidance or for the providing features we don’t understand well enough yet to implement in hardware. A long-time dream of science-fiction has been direct brain to computer interfaces 28 29. In fact, there is concrete work that can be done (and has been done) in this area:28 Anderson, Poul, “Kings Who Die”, If, March 1962, p8-36. Reprinted in Seven Conquests, Poul Anderson, MacMillan Co., 1969.29 Vinge, Vernor, “Bookworm, Run!”, Analog, March 1966, pp8-40. Reprinted in True Names and Other Dangers, Vernor Vinge, Baen Books, 1987.\n\nLimb prosthetics is a topic of direct commercial applicability. Nerve to silicon transducers can be made 30.This is an exciting, near-term step toward direct communication.\nSimilar direct links into brains may be feasible, if the bit rate is low: given human learning flexibility, the actual brain neuron targets might not have to be precisely selected. Even 100 bits per second would be of great use to stroke victims who would otherwise be confined to menu-driven interfaces.\nPlugging in to the optic trunk has the potential for bandwidths of 1 Megabit/second or so. But for this, we need to know the fine-scale architecture of vision, and we need to place an enormous web of electrodes with exquisite precision. If we want our high bandwidth connection to be in addition to what paths are already present in the brain, the problem becomes vastly more intractable. Just sticking a grid of high-bandwidth receivers into a brain certainly won’t do it. But suppose that the high-bandwidth grid were present while the brain structure was actually setting up, as the embryo develops. That suggests:\nAnimal embryo experiments. I wouldn’t expect any IA success in the first years of such research, but giving developing brains access to complex simulated neural structures might be very interesting to the people who study how the embryonic brain develops. In the long run, such experiments might produce animals with additional sense paths and interesting intellectual abilities.\n\n30 Kovacs, G. T. A. et al., “Regeneration Microelectrode Array for Peripheral Nerve Recording and Stimulation”, IEEE Transactions on Biomedical Engineering, v 39, n 9, pp 893-902.Originally, I had hoped that this discussion of IA would yield some clearly safer approaches to the Singularity. (After all, IA allows our participation in a kind of transcendence.) Alas, looking back over these IA proposals, about all I am sure of is that they should be considered, that they may give us more options. But as for safety … well, some of the suggestions are a little scarey on their face. One of my informal reviewers pointed out that IA for individual humans creates a rather sinister elite. We humans have millions of years of evolutionary baggage that makes us regard competition in a deadly light. Much of that deadliness may not be necessary in today’s world, one where losers take on the winners’ tricks and are coopted into the winners’ enterprises. A creature that was built de novo might possibly be a much more benign entity than one with a kernel based on fang and talon. And even the egalitarian view of an Internet that wakes up along with all mankind can be viewed as a nightmare 31.31 Swanwick Michael, Vacuum Flowers, serialized in Isaac Asimov’s Science Fiction Magazine, December(?) 1986 - February 1987. Republished by Ace Books, 1988.\nThe problem is not that the Singularity represents simply the passing of humankind from center stange, but that it contradicts some of our most deeply held notions of being. I think a closer look at the notion of strong superhumanity can show why that is."
  },
  {
    "objectID": "docs/posts/1993-vernor-vinge/index.html#strong-superhumanity-and-the-best-we-can-ask-for",
    "href": "docs/posts/1993-vernor-vinge/index.html#strong-superhumanity-and-the-best-we-can-ask-for",
    "title": "The Coming Technological Singularity",
    "section": "Strong Superhumanity and the Best We Can Ask for",
    "text": "Strong Superhumanity and the Best We Can Ask for\nSuppose we could tailor the Singularity. Suppose we could attain our most extravagant hopes. What then would we ask for: That humans themselves would become their own successors, that whatever injustice occurs would be tempered by our knowledge of our roots. For those who remained unaltered, the goal would be benign treatment (perhaps even giving the stay-behinds the appearance of being masters of godlike slaves).It could be a golden age that also involved progress (overleaping Stent’s barrier). Immortality (or at least a lifetime as long as we can make the universe survive 32 33) would be achievable.32 Dyson, Freeman, “Physics and Biology in an Open Universe”, Review of Modern Physics, vol 51, pp447-460, 1979.33 Barrow, John D. and Frank J. Tipler, The Anthropic Cosmological Principle, Oxford University Press, 1986.\nBut in this brightest and kindest world, the philosophical problems themselves become intimidating. A mind that stays at the same capacity cannot live forever; after a few thousand years it would look more like a repeating tape loop than a person. (The most chilling picture I have seen of this is in 34.) To live indefinitely long, the mind itself must grow … and when it becomes great enough, and looks back … what fellow-feeling can it have with the soul that it was originally? Certainly the later being would be everything the original was, but so much vastly more. And so even for the individual, the Cairns-Smith (or Lynn Margulis) notion of new life growing incrementally out of the old must still be valid.34 Niven, Larry, “The Ethics of Madness”, If, April 1967, pp82-108. Reprinted in Neutron Star, Larry Niven, Ballantine Books, 1968.\nThis “problem” about immortality comes up in much more direct ways. The notion of ego and self-awareness has been the bedrock of the hardheaded rationalism of the last few centuries. Yet now the notion of self-awareness is under attack from the Artificial Intelligence people (“self-awareness and other delusions”). Intelligence Amplification undercuts the importance of ego from another direction. The post-Singularity world will involve extremely high-bandwidth networking. A central feature of strongly superhuman entities will likely be their ability to communicate at variable bandwidths, including ones far higher than speech or written messages. What happens when pieces of ego can be copied and merged, when the size of a self-awareness can grow or shrink to fit the nature of the problems under consideration?These are essential features of strong superhumanity and the Singularity. Thinking about them, one begins to feel how essentially strange and different the Post-Human era will be – no matter how cleverly and benignly it is brought to be.\nFrom one angle, the vision fits many of our happiest dreams: a place unending, where we can truly know one another and understand the deepest mysteries. From another angle, it’s a lot like the worst case scenario I imagined earlier in this paper.\nWhich is the valid viewpoint? In fact, I think the new era is simply too different to fit into the classical frame of good and evil. That frame is based on the idea of isolated, immutable minds connected by tenuous, low-bandwidth links. But the post-Singularity world does fit with the larger tradition of change and cooperation that started long ago (perhaps even before the rise of biological life). I think there are notions of ethics that would apply in such an era. Research into IA and high-bandwidth communications should improve this understanding. I see just the glimmerings of this now, in Good’s Meta-Golden Rule, perhaps in rules for distinguishing self from others on the basis of bandwidth of connection. And while mind and self will be vastly more labile than in the past, much of what we value (knowledge, memory, thought) need never be lost. I think Freeman Dyson has it right when he says 35: “God is what mind becomes when it has passed beyond the scale of our comprehension.”35 Dyson, Freeman, Infinite in All Directions, Harper && Row, 1988."
  },
  {
    "objectID": "docs/posts/1993-vernor-vinge/index.html#appendix-metadata",
    "href": "docs/posts/1993-vernor-vinge/index.html#appendix-metadata",
    "title": "The Coming Technological Singularity",
    "section": "Appendix: Metadata",
    "text": "Appendix: Metadata\nModernized from The Coming Technological Singularity on the original author’s website. The original header for the is as follows:\nVernor Vinge\n\nDepartment of Mathematical Sciences San Diego State University\n\n(c) 1993 by Vernor Vinge (Verbatim copying/translation and distribution of this entire article is permitted in any medium, provided this notice is preserved.)\n\n[I wish to thank John Carroll of San Diego State University and Howard Davidson of Sun Microsystems for discussing the draft version of this paper with me.]\n\nThis article was for the VISION-21 Symposium sponsored by NASA Lewis Research Center and the Ohio Aerospace Institute, March 30-31, 1993. It is also retrievable from the NASA technical reports server as part of NASA CP-10129. A slightly changed version appeared in the Winter 1993 issue of _Whole Earth Review_.\nVISION-21 Symposium was a conference held in March 1993, described as:\n\nThe symposium Vision-21: Interdisciplinary Science and Engineering in the Era of Cyberspace was held at the NASA Lewis Research Center on March 30-31, 1993. The purpose of the symposium was to simulate interdisciplinary thinking in the sciences and technologies which will be required for exploration and development of space over the next thousand years. The keynote speakers were Hans Moravec, Vernor Vinge, Carol Stoker, and Myron Krueger. The proceedings consist of transcripts of the invited talks and the panel discussion by the invited speakers, summaries of workshop sessions, and contributed papers by the attendees. (Vision 21: Interdisciplinary Science and Engineering in the Era of Cyberspace 1993)"
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html",
    "href": "essays/posts/wigner-rotation/index.html",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "",
    "text": "In the eyes of a geometer, special relativity is not mysterious, but it is simply another geometry that resembles our familiar Euclidean geometry of the plane, and the spherical geometry of the earth. Just like how Euclid compressed the geometry of the plane into around 20 axioms, we can compress the geometry of spacetime into the following axioms:\n\nSpacetime is a smooth structure, shaped like1 the real vector space \\(\\mathbb{R}^{n+1}\\), where \\(n\\) is the number of spatial dimensions, and \\(1\\) is the number of temporal dimensions.\nSpacetime is symmetric under rotation, translation, and constant velocity motion.\nThere exists a fundamental speed, a “conversion factor” between the spatial and temporal dimensions, which we call \\(c\\).\nThere are many ways to map spacetime into \\(\\mathbb{R}^{n+1}\\), but only some of them are physically meaningful. Those are called the (inertial) coordinate systems.\nYou can go from any one coordinate system to any other by a coordinate transformation.\nTransformations can be combined one after another, and they can be reversed. That is, they make up a group, named the symmetry group of spacetime.\nThe symmetry group is smooth and connected, meaning that starting with the “do-nothing” identity transformation, you can combine it with tiny transformations, and end up at any other transformation.\n\n1 In jargon, it is isomorphic as a smooth manifold to \\(\\mathbb{R}^{n+1}\\), or diffeomorphic to \\(\\mathbb{R}^{n+1}\\).Do not worry if some terms were used without a precise definition. You can start with your best intuitive guess, and sculpt them as we go.\nWe will follow the spirit of Erlangen program, which states clearly what our target is (Klein 1893):\n\nGiven a manifoldness [smooth space] and a group of transformations of the same; to develop theory of invariants relating to that group. This is the general problem, and it comprehends not alone ordinary geometry, but also and in particular the more recent geometrical theories which we propose to discuss, and the different methods of treating manifoldnesses of \\(n\\) dimensions.\n\nConsidering that Felix Klein first stated it in 1872, decades before special relativity, this is prescient.\nApplied to special relativity, our target is to start with the axioms, find out something about what kinds of symmetries there can be, and what kinds of objects are unchanged by symmetries. In other words, we will study the geometry of spacetime.\n\n\n\nSo far, while we were talking about “spacetime”, we were actually talking about mathematical objects. There is no necessary connection between the mathematical objects and whatever we observe in the real world. So why do we call those mathematical objects with physically suggestive names like “spacetime”?\nWell, special relativity is really a mathematical object. It is connected to the physical world like all objects in mathematical physics: by experiments. Experimental data is physical, and by studying the mathematical-object special relativity, we found that it can generate the experimental data with a minimal amount of “glue” between the physical world and the mathematical world. The “glue” looks like:\n\nPhysical events in physical spacetime are mathematical points in mathematical spacetime.\nPhysical light moves at the mathematical speed of \\(c\\)\nPhysical clocks tick at a mathematical constant rate.\nIf you move a physical clock from physical-here to physical-there, it is still ticking at the same mathematical constant rate.\n…\n\nEinstein’s The Meaning of Relativity* (Albert Einstein 1922) details physical and mathematical correspondences, such as clock synchronization and the movement of meter sticks.\n\nAs far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#relativity-in-general",
    "href": "essays/posts/wigner-rotation/index.html#relativity-in-general",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "",
    "text": "In the eyes of a geometer, special relativity is not mysterious, but it is simply another geometry that resembles our familiar Euclidean geometry of the plane, and the spherical geometry of the earth. Just like how Euclid compressed the geometry of the plane into around 20 axioms, we can compress the geometry of spacetime into the following axioms:\n\nSpacetime is a smooth structure, shaped like1 the real vector space \\(\\mathbb{R}^{n+1}\\), where \\(n\\) is the number of spatial dimensions, and \\(1\\) is the number of temporal dimensions.\nSpacetime is symmetric under rotation, translation, and constant velocity motion.\nThere exists a fundamental speed, a “conversion factor” between the spatial and temporal dimensions, which we call \\(c\\).\nThere are many ways to map spacetime into \\(\\mathbb{R}^{n+1}\\), but only some of them are physically meaningful. Those are called the (inertial) coordinate systems.\nYou can go from any one coordinate system to any other by a coordinate transformation.\nTransformations can be combined one after another, and they can be reversed. That is, they make up a group, named the symmetry group of spacetime.\nThe symmetry group is smooth and connected, meaning that starting with the “do-nothing” identity transformation, you can combine it with tiny transformations, and end up at any other transformation.\n\n1 In jargon, it is isomorphic as a smooth manifold to \\(\\mathbb{R}^{n+1}\\), or diffeomorphic to \\(\\mathbb{R}^{n+1}\\).Do not worry if some terms were used without a precise definition. You can start with your best intuitive guess, and sculpt them as we go.\nWe will follow the spirit of Erlangen program, which states clearly what our target is (Klein 1893):\n\nGiven a manifoldness [smooth space] and a group of transformations of the same; to develop theory of invariants relating to that group. This is the general problem, and it comprehends not alone ordinary geometry, but also and in particular the more recent geometrical theories which we propose to discuss, and the different methods of treating manifoldnesses of \\(n\\) dimensions.\n\nConsidering that Felix Klein first stated it in 1872, decades before special relativity, this is prescient.\nApplied to special relativity, our target is to start with the axioms, find out something about what kinds of symmetries there can be, and what kinds of objects are unchanged by symmetries. In other words, we will study the geometry of spacetime.\n\n\n\nSo far, while we were talking about “spacetime”, we were actually talking about mathematical objects. There is no necessary connection between the mathematical objects and whatever we observe in the real world. So why do we call those mathematical objects with physically suggestive names like “spacetime”?\nWell, special relativity is really a mathematical object. It is connected to the physical world like all objects in mathematical physics: by experiments. Experimental data is physical, and by studying the mathematical-object special relativity, we found that it can generate the experimental data with a minimal amount of “glue” between the physical world and the mathematical world. The “glue” looks like:\n\nPhysical events in physical spacetime are mathematical points in mathematical spacetime.\nPhysical light moves at the mathematical speed of \\(c\\)\nPhysical clocks tick at a mathematical constant rate.\nIf you move a physical clock from physical-here to physical-there, it is still ticking at the same mathematical constant rate.\n…\n\nEinstein’s The Meaning of Relativity* (Albert Einstein 1922) details physical and mathematical correspondences, such as clock synchronization and the movement of meter sticks.\n\nAs far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#relativity-in-11-dimensions",
    "href": "essays/posts/wigner-rotation/index.html#relativity-in-11-dimensions",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Relativity in 1+1 dimensions",
    "text": "Relativity in 1+1 dimensions\n\nMaking one coordinate system\nTo begin, let’s consider relativity in 1+1 dimensions. We will start with a “default” coordinate system (“the rest frame”).\nIn the beginning was just a point in spacetime. We call this the origin, and write its coordinates as \\(x=0, t=0\\). The world was yet without form. To give it form, we build a meter stick and an infinite number of light bulbs.\nWe put a light bulb at the origin. Because the bulb does not “stand still”, the light bulb traces out a path in spacetime. We call this path the \\(t\\)-axis at \\(x=0\\). Because the bulb does not stand still, we cannot properly say it is “at the origin”, but rather “passing the origin”.\n\nany real body must have extension in four directions: it must have Length, Breadth, Thickness, and—Duration. But through a natural infirmity of the flesh, which I will explain to you in a moment, we incline to overlook this fact. There are really four dimensions, three which we call the three planes of Space, and a fourth, Time. There is, however, a tendency to draw an unreal distinction between the former three dimensions and the latter, because it happens that our consciousness moves intermittently in one direction along the latter from the beginning to the end of our lives.\n— The Time Traveller, The Time Machine, Chapter 1\n\nWe put another light bulb to the left of the light bulb passing the origin, separated by one meter stick. This light bulb traces out a line at \\(x=1\\). And so on. Thus, we obtain an infinite number of lines: \\(\\dots, x = -1, x = 0, x = +1, \\dots\\). By subdividing the meter stick, we obtain one line \\(x = r\\) for each real number \\(r\\).\nNow we have measured space, we will measure time. We remove all light bulbs except two: one at \\(x=0\\), one at \\(x=1\\). At origin, the \\(x=0\\) light bulb flashes, and then the light bulb at \\(x=1\\) flashes when it receives the flash. When the light bulb at \\(x=0\\) receives the echo, this is the echo event – and we write it down as \\(x=0, t=2\\). Let spacetime reverberate with shining echoes, and in this way, every point in spacetime receives a unique coordinate \\((x, t)\\).\nNotice that in this construction, the fundamental speed \\(c\\) is equal to one, and if we draw space and time as two perpendicular directions on a graph-paper, then the trajectory of every light is a 45-degree straight line on the graph-paper. These are not fundamental aspects of theory, but are convenient outcomes given by how we constructed the coordinate system.\nAbstractly speaking, a coordinate system is a function that maps a point in spacetime to two real numbers, like \\((x, t): \\mathcal{M} \\to \\mathbb{R}^{1+1}\\). Our coordinate system constructed so far means that:\n\nThe origin-point \\(p_O\\) has coordinates \\(x(p_O) = 0, t(p_O) = 0\\).\nIf a beam of light is traveling to the right, and passes a certain point \\(p\\), then the set of all points on the beam of light is of form \\(\\{q : x(q) - x(p) = t(q) - t(p)\\}\\).\nAnd so on.\n\nSuch explicit distinction between the spacetime itself and the coordinate system is uncommon in physics, and once you are used to it, you should throw it into your subconscious like a muscle memory. However, it is good to keep it in mind for now.\n\n\nThe first moving frame\nThe integrity of the coordinate system is based on the meter stick. If two light bulbs are moving relative to each other, then they cannot be always connected by a meter stick – one of them would bump into the meter stick, or move away from it. In other words, the entire system is static.\nNow, we introduce another frame, moving at a velocity \\(v\\) relative to the first. While there is no fundamental reason to privilege one over the other, we call one the “rest frame” and the other the “moving frame” for convenience.\nThe moving frame constructs its own coordinate system \\((x', t')\\). What must it look like?\nSince the line \\(x' = 0\\) is constructed by the trajectory of the light bulb that passes the origin, we know that the \\(x' = 0\\) line is \\(\\{p : x(p) = v t(p)\\}\\). However, when it comes to the \\(x' = 1\\) line, we have a problem: How long should be the meter stick in the moving frame be? What does it even mean to compare a meter stick in a moving frame with a meter stick in the resting frame?\nThis difficulty is not pedantic. In the resting frame, checking that two meter sticks are equal means taking one, accelerating it then decelerating it, until it overlaps exactly with the other. This does not apply if they are in relative motion.\nThe solution is another “gluing”. Like how we “glued” the mathematical world with the physical world, we also glue one frame with another frame. This is connection, a central idea in differential geometry. However we will just do this intuitively, since in special relativity, connections are done in the most straightforward way.\nTo connect two frames, we divide their difference into tiny steps, and then do those tiny steps one after another. So, if we know how to connect two frames when \\(v\\) is an infinitesimal, then we know how to connect two arbitrary frames by taking an integral.\nSo for now, let \\(v\\) be an infinitesimal. What should be the meter stick in the moving frame? Its meter stick is whatever is necessary so that light speed is \\(1\\) in the moving frame. So, if we know how to tick the moving clock, we know how to build the moving meter stick.\nIn the resting frame, the clock passing the origin makes a tick at every point:\n\\[\\dots, (0, -1), (0, 0), (0, +1), \\dots\\]\nIn the moving frame, the clock passing the origin makes a tick at every point:\n\\[\\dots, (-v \\; ?, -?), (0, 0), (v \\; ?, ?), \\dots\\]\nThe naive choice is \\(? = 1\\). With this seemingly trivial step, we have in fact completely defined special relativity – all else is derivation.\n\n\nWhy \\(? = 1\\)\nBy the smoothness axiom, if \\(v\\) is infinitesimal, the coordinate transformation between the rest frame and the moving frame should be infinitesimally close to the identity.\nSince special relativity does not distinguish left from right, if we were to make the moving frame move at velocity \\(-v\\), then the coordinate transformation should send \\((0, 1)\\) to \\((-\\delta x, 1 + \\delta t)\\) by mirror symmetry.\nNow, if we compose two boosts, first with \\(v\\) then with \\(-v\\), then we would send \\((0, 1)\\) to \\((0, 1+ 2\\delta t)\\). But we really should get back to \\((0, 1)\\) since we are back to the resting frame again. Therefore \\(\\delta t = 0\\).\n\n\n\nThree infinitesimal boosts\n\n\n\n\nThe Lorentz transformations\nIn the resting frame, we constructed the meter stick by shooting out one pulse of light, then listening for the echo that arrives two ticks later. The point at which the echo is reflected is one meter stick away. Equivalently, we can shoot a forward-pointing light cone at \\((0, -1)\\), and shoot a backward-pointing light cone at \\((0, +1)\\). Their intersections are the two ends of two meter sticks: \\((-1, 0), (+1, 0)\\).\nIn the boosted frame, the clock passing the origin ticks at \\(\\dots, (-v, -1), (0, 0), (v, 1), \\dots\\). Therefore, we can construct its meter stick in the same way, and we would find that the two light cones intersect at \\((-1, -v), (+1, +v)\\). This gives us the coordinate transformation:\n\\[\\forall p \\in \\mathcal{M}, \\quad\n\\begin{bmatrix} x(p) \\\\ t(p) \\end{bmatrix} =\n\\begin{bmatrix} 1 & v \\\\ v & 1 \\end{bmatrix}\n\\begin{bmatrix} x'(p) \\\\ t'(p) \\end{bmatrix}\n\\]\nHere, we find that we need to distinguish between “active” and “passive” transformations. Everything we have said so far is “passive”. We assume that there is an underlying spacetime \\(\\mathcal{M}\\), and we are to construct coordinate systems over it. The coordinate transformation tells us how to turn the coordinates of the same point from one coordinate system to another. However, while this is often useful, it would make everything we are going to say next very awkward.\nTherefore, we immediately change our point of view: we are going to study “active” transformations from now on. While passive transformation of a clock means that we have just one clock and measure its ticks in two coordinate systems, active transformation of a clock means that we actually pick up the clock at rest frame, accelerate it on a rocket, and then let it glide at constant velocity at the moving frame, then study what the clock is doing in the rest frame.\nSince an active transformation is the opposite of a passive transform, we have the active Lorentz transformation formula:\n\\[\n\\begin{bmatrix} x' \\\\ t' \\end{bmatrix} =\n\\begin{bmatrix} 1 & v \\\\ v & 1 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ t \\end{bmatrix}\n\\tag{1}\\]\nThis formula states that if we “pick up” the spacetime event at \\((x, t)\\) in the rest frame, and accelerate it infinitesimally to speed \\(v\\), then the spacetime event ends up at \\((x', t')\\) in the rest frame.\n\n\nIntegrating the Lorentz boosts\nSince \\(v\\) is infinitesimal, to find the Lorentz transformation for a non-infinitesimal \\(v\\) (say, \\(v = 10^{-100}\\), which, while small, is really big for an infinitesimal), we need to integrate over it.\nBefore we do so, we should play with the discrete version: what happens if we repeatedly apply the matrix \\(\\begin{bmatrix} 1 & v \\\\ v & 1 \\end{bmatrix}\\) on the vectors \\((0, 1), (1, 0)\\)?\n\n\n\n  \n  Rapidity = 0.05\n\n\n  \n  Number of boosts = 1\n\n  \n\nIn the diagram, \\(v\\) is “rapidity”2. We apply the Lorentz boost with the given \\(v\\) repeatedly forwards and backwards. The resulting image is beautiful and suggestive: it looks like the unit hyperbolas!2 The reason for calling it “rapidity”, instead of “velocity” would be soon clear.\n\n    \n\nHaving guessed the answer, we proceed with the explicit integral to confirm our guess.\n\\[\n  \\begin{aligned}\n  \\begin{bmatrix} 1 & dv \\\\ dv & 1 \\end{bmatrix}^{\\frac{v}{dv}} &= \\left(I + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}\\right)^{\\frac{v}{dv}}\\\\\n  &= \\exp\\left(\\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} v\\right) \\\\\n  &= I + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} v + \\frac{1}{2!}v^2\\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}^2 + \\frac{1}{3!}v^3\\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}^3 + \\cdots \\\\\n  &= I(1 + v^2/2! + v^4/4! + \\cdots) + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}(v + v^3/3! + v^5/5! + \\cdots) \\\\\n  &= I\\cosh(v) + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\sinh(v)\n  \\end{aligned}\n\\tag{2}\\]\nAt this point, we should pause to take a careful look at \\(v\\). While it is the same as velocity when \\(v\\) is infinitesimal, it is not the same as velocity otherwise, because velocity is bounded between \\(-1\\) and \\(+1\\), whereas this \\(v\\) quantity is free to vary over all real values. This is why we call it “rapidity” and relabel it as \\(w\\), since it is like velocity, but different.\nThe velocity \\(v\\) for a given rapidity \\(w\\) is determined by transforming the \\(t\\)-axis using the Lorentz transformation matrix \\(\\begin{bmatrix} \\cosh(w) & \\sinh(w) \\\\ \\sinh(w) & \\cosh(w) \\end{bmatrix}\\). The \\(t\\)-axis \\[\\{(0, s) : s \\in \\mathbb{R}\\}\\] is boosted to the line \\[\\{(\\sinh(w) s, \\cosh(w) s) : s \\in \\mathbb{R}\\}\\] Therefore, the velocity of this boosted frame is \\[w = \\sinh(w) / \\cosh(w)  = \\tanh(w).\\]\nNow you can play with the diagram below to grow a new intuitive sense of how it all ties together. After you have grown this new intuition, you can proceed.\n\n    \n    \n\n\n\nA “spacetime square” would transform like:\n\n  \n\n\n\n3-velocity, 4-velocity, and inner product in spacetime\nIn the diagram above, we have used the terms 3-velocity and 4-velocity. To explain those terms, we briefly go back to 3+1 dimensions.\nWhen we say “velocity” informally, we mean a vector in \\(\\mathbb{R}^3\\). But how is velocity really defined? It is displacement divided by time. In relativity, when we say “3-velocity”, there are two possible representations: we can represent it as \\((v_x, v_y, v_z)\\), or we can represent it as \\((v_x, v_y, v_z, 1)\\). The first is close to how the rest of the world use “velocity”, but the second is close to how special relativity want us to use the word “velocity”. The second representation can be interpreted as follows: “The 3-velocity of an object is the spacetime displacement of the object after one tick of my clock.”.\nNeither is satisfactory, however, because 3-velocity behaves badly under Lorentz transformations. The first representation \\((v_x, v_y, v_z)\\) does not have the time-coordinate, so it can’t even be multiplied with the Lorentz transformation matrix. The second representation \\((v_x, v_y, v_z, 1)\\) would, after a Lorentz transformation, have its time-coordinate \\(\\neq 1\\).\nBoth problems are elegantly resolved if we use the 4-velocity, which means we have to divide 3-velocity by its norm… but what norm? Why, think back to the Erlangen program:\n\nGiven a manifoldness [smooth space] and a group of transformations of the same; to develop theory of invariants relating to that group.\n\nIn plane geometry, we know that although \\(x, y, x', y'\\) are geometrically meaningless, the norm-squared \\((x-x')^2 + (y-y')^2\\) is meaningful, because that quantity is not changed (invariant) if you apply the rotation matrix \\(\\begin{bmatrix}\\cos\\theta & \\sin\\theta \\\\ -\\sin\\theta & \\cos\\theta \\end{bmatrix}\\) to it. What would be an invariant for spacetime?\nLooking at the picture of the Lorentz transformation in 1+1 dimensions, we can see that the intersection of the two light cones appears to have a constant area. In particular, the upper rectangle  has constant area. That is, the quantity \\((x-t)(x+t)\\) is preserved.\n\n    \n\nThus we define the Minkowski norm-squared: \\(\\|(x, t)\\|^2 := x^2 - t^2\\). Note that this is not an actual square because it can be negative. However it is preserved under Lorentz transformations.\nOnce we have a norm-squared, we can extend it to an inner product::\n\\[\\braket{v, w} := \\frac{\\|v+w\\|^2 - \\|v-w\\|^2}{4} = v_x w_x - v_t w_t\\]\nwhich is still an invariant under Lorentz transformations, and thus physically meaningful. This extends in general to n+1 dimensions. For example, in 3+1 dimensions, the inner product is \\(\\sum_{i = x, y, z} v_i w_i - v_t w_t\\).\nWith this, the 4-velocity is at hand! We just need to normalize \\((v_x, v_y, v_z, 1)\\), taking care to remove the negative sign:\n\\[\n(v_x, v_y, v_z, 1)/\\sqrt{-\\|(v_x, v_y, v_z, 1)\\|^2} = \\frac{(v_x, v_y, v_z, 1)}{\\sqrt{1-(v_x^2 + v_y^2 + v_z^2)}}\n\\]\nAs an example of the power of this geometric algebra, we rederive the Lorentz transformation with infinitesimal boost. In the rest frame, the unit clock-tick is the vector \\((0, 1)\\), and the two unit meter sticks are the vectors \\((-1, 0), (1, 0)\\). They are uniquely defined by the two geometric properties:\n\\[\n\\braket{s, (0, 1)} = 0; \\braket{s, s} = 1\n\\]\nTherefore, in the boosted frame, the boosted meter sticks are the two solutions to\n\\[\n\\braket{s', (v, 1)} = 0; \\braket{s', s'} = 1\n\\]\nwhich are \\(s' = (-1, v), (1, v)\\). By continuity, \\((1, 0)\\) cannot have been boosted to \\((-1, v)\\), so it must be boosted to \\((1, v)\\). This gives us Equation 1."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#relativity-in-21-dimensions",
    "href": "essays/posts/wigner-rotation/index.html#relativity-in-21-dimensions",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Relativity in 2+1 dimensions",
    "text": "Relativity in 2+1 dimensions\n\nInfinitesimal Lorentz transformations\nIn 1+1 dimensions, we have only two spatial directions: left and right. In 2+1 dimensions, there is a whole circle of directions. We call this circle the “space circle”, because the alternative name “hula hoop for spacetime” sounds too silly.\nJust like in 1+1 dimensions, the space circle is constructed by shooting a forward light cone at \\((0, 0, -1)\\) and a backward light cone at \\((0, 0, +1)\\). Their intersection is the space circle. A space circle plus a ticking clock together allows us to define a coordinate system with three unit vectors \\((1, 0, 0), (0, 1, 0), (0, 0, 1)\\). Both the clock and the space circle can be boosted. We will pick the simplest possible way to transport the ticking clock and the space circle, and experiments with Thomas precession would show that this is the right way.\n\n    \n\nSuppose we perform an infinitesimal boost by \\((v_x, 0)\\), then, by the same picture as in the 1+1 dimension case, we know that the clock-tick \\((0, 0, 1)\\) is boosted to \\((v_x, 0, 1)\\), and the space circle is boosted to the ellipse with long semiaxis \\((1, 0, v_x)\\) and short semiaxis \\((0, 1, 0)\\). Where should each point on the space circle go to? Since the space circle is rigid, if we know where one point must go to, we know where all points must go to.\nThe semiaxis \\((0, 1, 0)\\) is both on the original space circle and the boosted space circle. It stands to reason that the simplest pick would preserve it after the infinitesimal boost. That is, we should not “twist” the space circle under boosting. With this choice, we are forced to pick \\((1, 0, v_x)\\) as the boosted \\((1, 0, 0)\\), since it is the unique unit vector that is perpendicular to \\((0, 1, 0), (v_x, 0, 1)\\), and infinitesimally close to \\((1, 0, 0)\\).\nAlternatively, we can think of Lorentz transformation in 2+1 dimensions as the same with Lorentz transformation in 1+1 dimensions, but with an extra dimension. Therefore, we can minimally modify Equation 1 to:\n\\[\n\\begin{bmatrix} x' \\\\ y' \\\\ t' \\end{bmatrix} =\n\\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & 0 \\\\ v_x & 0 & 1 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\\\ t \\end{bmatrix}\n\\]\nthe same result as our previous argument.\nThis extends to the case of an infinitesimal boost \\((v_x, v_y)\\):\n\\[\n\\begin{bmatrix} x' \\\\ y' \\\\ t' \\end{bmatrix} =\n\\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & v_y \\\\ v_x & v_y & 1 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\\\ t \\end{bmatrix}\n\\tag{3}\\]\n\n\n\n\n\n\nFour ways to derive it\n\n\n\n\n\n\nJust guess it.\nRotate the coordinate system so that the boost is in the \\(x\\)-direction, boost using the previous result, then rotate the coordinate system back. To be pedantic: We are performing one passive, then one active, then one passive transformation.\nTake the previous derivation, and modify it by strategically inserting \\(v_y\\) at places.\nSince two infinitesimal boosts do not interact except at the second-order infinitesimal level, and \\(O(v^2) \\ll O(v) \\ll 1\\), we are free to discard the second-order infinitesimal. Therefore, we can just multiply the two matrices together to get the full thing:\n\n\\[\\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & 0 \\\\ v_x & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & v_y \\\\ 0 & v_y & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & v_y \\\\ v_x & v_y & 1 \\end{bmatrix} + O(v^2)\\]\n\n\n\n\n\nFermi–Walker transport\nEquation 3 is correct, but it is still not geometric. We must convert it with only quantities that are invariant under Lorentz transformations – that is, the inner product.\nDenote the 4-velocity vector \\((0, 0, 1)\\) by the letter \\(u\\), and the infinitesimal boost \\((v_x, v_y, 0)\\) by \\(\\delta u\\). Then, an arbitrary vector \\(e\\) is boosted to \\(e + \\delta e\\), Where\n\\[\n\\delta e =\n\\begin{bmatrix} 0 & 0 & v_x \\\\ 0 & 0 & v_y \\\\ v_x & v_y & 0 \\end{bmatrix}\n\\begin{bmatrix} e_x \\\\ e_y \\\\ e_t \\end{bmatrix} = e_t (v_x, v_y, 0) + (e_x v_x + e_y v_y) (0, 0, 1)\n\\]\nThis gives us the Fermi–Walker transport equation:\n\\[\n\\delta e = - \\braket{e, u} \\delta u + \\braket{e, \\delta u}u\n\\tag{4}\\]\nIf you want a more amusing mental image, here it is: Consider a spaceship that looks like a spherical cow porcupine in a vacuum. Every porcupine spine is a rocket engine. The spaceship can boost in any direction, but it does not rotate. Thanks to Wigner rotation, it can rotate anyway. Now, the spaceship is performing some complicated manuever in spacetime. If we allow it to carry around a general vector pointing in an arbitrary direction \\(e\\), the question becomes: as the vector is boosted alongside the spaceship, how does the vector change?\nConsider a problem in general relativity, where we have a vector field over spacetime, and an accelerating observer. To calculate how quickly the vector field is changing relative to the observer, we must account for:\n\nacceleration of the observer, using the Fermi–Walker transport equation;\nthe curvature of spacetime itself, so that comparing one vector with another requires us to perform parallel transport in spacetime (with the “covariant derivative”).\n\n\n\nGeneral Lorentz transformations\nThe Lorentz transformation in 2+1 dimensions can be derived similarly to how Equation 2 was derived. Let the rapidity vector \\(\\vec{w}\\) be equal to \\(w (n_x, n_y)\\), where \\((n_x, n_y)\\) is a unit vector (the direction of rapidity). Then the Lorentz transformation is found by performing another matrix exponentiation:\n\\[\\exp\\left(\\begin{bmatrix} 0 & 0 & n_x \\\\ 0 & 0 & n_y \\\\ n_x & n_y & 0 \\end{bmatrix} w\\right)\\]\nTo be more succinct, we define the matrices\n\\[K_x := \\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 1 & 0 & 0 \\end{bmatrix}, \\quad K_y := \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}\\]\nthen the Lorentz transformation is just \\(e^{\\vec{w} \\cdot \\vec{K}} = e^{w \\vec{n} \\cdot \\vec{K}}\\).\nNow, since \\((\\vec{n} \\cdot \\vec{K})^3 = \\vec{n} \\cdot \\vec{K}\\), we have the Lorentz transformation equation\n\\[\nI + \\sinh(w) \\vec{n} \\cdot \\vec{K} + (\\cosh(w) - 1) (\\vec{n} \\cdot \\vec{K})^2\n\\tag{5}\\]\nThis equation extends naturally to relativity in n+1 dimensions, but we will not need it.\n\n\n\n\n\n\nDeriving \\((\\vec{n} \\cdot \\vec{K})^3 = \\vec{n} \\cdot \\vec{K}\\)\n\n\n\n\n\nLet \\(t\\) stand for the index of the time-coordinate, and let \\(i, j, k\\) be the indices of the space-coordinates. Then we have \\[(\\vec{n} \\cdot \\vec{K})^3 = n_in_jn_k(e_{it} + e_{ti})(e_{jt} + e_{tj})(e_{kt} + e_{tk})\\]\nwhere \\(e_{mn}\\) means the matrix with entry \\((m, n)\\) being one and all other entries being zero. We use Einstein summation convention, so repeated indices means summing over it (except \\(t\\), which is not an index to sum over).\nSince \\(e_{mn}e_{kl} = \\delta_{nk} e_{ml}\\), and \\(i \\neq t, j \\neq t, k \\neq t\\), the above multiplication expands to 8 terms, but only two are nonzero:\n\\[= n_in_jn_k(e_{it}\\delta_{jk} + \\delta_{ij} e_{tk}) = \\underbrace{\\|\\vec{n}\\|^2}_{= 1} (n_i e_{it} + n_k e_{tk}) = n_i (e_{it} + e_{ti}) = \\vec{n} \\cdot \\vec{K}\\]"
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#wigner-rotation",
    "href": "essays/posts/wigner-rotation/index.html#wigner-rotation",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Wigner rotation",
    "text": "Wigner rotation\nWe are ready to derive a rarely discussed effect in special relativity: the Wigner rotation. Although it is as fundamental as time dilation and space contraction, it often goes unmentioned in undergraduate textbooks I have encountered.\n\nTheorem 1 (Wigner rotation) When three boosts are made in a cycle, such as \\(p_1 \\to p_2 \\to p_3 \\to p_1\\), the result is a rotation. The angle of rotation is equal to the hyperbolic area of the triangle \\(p_1 p_2 p_3\\), but in the opposite direction. Furthermore, the hyperbolic area is equal to the angle defect of the triangle.\n\nThe phrase “in the opposite direction” means that, if, looking from the \\(+t\\)-direction down at the origin, you see the three 3-velocities make a counterclockwise cycle in the disk \\(\\{(v_x, v_y, 1) : v_x^2 + v_y^2 = 1\\}\\), then you would see that the Wigner rotation angle is in the clockwise direction, and vice versa. The phrase “angle defect” means \\(\\pi - (\\angle{p_1 p_2 p_3} + \\angle{p_2 p_3 p_1} + \\angle{p_3 p_1 p_2})\\).\n\nStep-by-step demonstration of the Wigner rotation\nConsider the simplest type of three-boost cycle as shown in the figure below. We start at the rest frame, boost to the frame with a 3-velocity of \\((v_x, 0)\\), then boost to the frame with a 3-velocity of \\((v_x, v_x d\\varphi)\\), and finally boost back to the rest frame.\n\n    \n\nTracing out the trajectory of every point on the space circle gives the following sequence:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen projected to the \\(xy\\)-plane, the sequence looks like:\n\n    \n\nTherefore, after three pure boosts, we end up with a rotation by \\((\\gamma - 1)d \\varphi\\) radians in the opposite direction. Here, \\(\\gamma\\) is the long semiradius of the boosted space circle, after projecting it down to the \\(xy\\) plane. This is the “Lorentz factor” often used in special relativity:\n\\[\\gamma = \\frac{1}{\\sqrt{1-v^2}}\\]\n\n\n\n\n\n\nDeriving the Lorentz factor\n\n\n\n\n\nIn the rest frame, the space circle is spanned by \\((1, 0, 0)\\) and \\((0, 1, 0)\\). After boosting by the 3-velocity \\((v, 0)\\), it is boosted to an ellipse with semiaxes \\((\\cosh(w), 0, \\sinh(w)), (0, 1, 0)\\), where \\(w = \\arctanh(v)\\) is its rapidity. Thus, its projection to the \\(xy\\) plane is an ellipse with semiaxes \\((\\cosh(w), 0), (0, 1)\\).\n\\[\\gamma = \\cosh(w) = \\cosh(\\arctanh(v)) = \\frac{1}{\\sqrt{1-v^2}}\\]\n\n\n\nPerforming boosts around a full cycle of 3-velocities and returning to our starting 3-velocity is equivalent to performing an infinite number of infinitesimal triangle-circuit boosts:\n\n    \n\nTherefore, a full cycle of boosts rotates our space circle by a full \\((\\gamma - 1)2\\pi\\) radians in the opposite direction.\n\n\nInteractive app for the Wigner rotation\nWith this understanding, you can now fully appreciate the following interactive app demonstrating the Wigner rotation.\nThe left picture represents the \\(xy\\) plane, and the right picture represents the \\(xyt\\) spacetime.\nYou can drag the black dot on the left, which represents the 3-velocity. The 3-velocity is restricted to have maximal norm \\(0.8\\), to keep the app numerically stable. The blue ellipse is the projection of the space circle to the \\(xy\\) plane. A fixed point on the space circle is distinguished by a big red dot, so that you can see how the space circle Wigner-rotates as you drag the 3-velocity around.\nIn the right picture, the forward and backward light cones intersect at the space circle. You can drag and scroll to adjust the camera.\n\n    \n    \n\n\n\nHere is a “tourist’s guide to the visualization”.\nThe three boosts in animated form:\n\n    \n\nBecause \\(\\gamma - 1 = \\frac{2}{3}\\), if you drag the 3-velocity for 3 full cycles at maximal velocity, the space circle would complete -2 full cycles.\n\n    \n\nSuppose a particle is moving close to the speed of light and emitting photons at equal angular density in all directions in its own frame, then in the rest frame the photons are bunched in front of the particle. This is the relativistic beaming effect, or the headlight effect.\n\n    \n\n\n\nThomas precession\nThe Wigner rotation formula is equivalent to the following formula, often called the Thomas precession formula:\n\\[\n\\vec{\\omega} = \\frac{\\gamma^2}{\\gamma + 1} \\vec{a} \\times \\vec{v}\n\\tag{6}\\]\nExperiments with particles moving in cyclotrons have verified this, thus justifying our guess that we should transport space circles without twisting.\n\n\n\n\n\n\nDeriving the Thomas precession formula\n\n\n\n\n\nConsider a particle moving counterclockwise at a constant speed \\(v\\) in a circle of radius \\(R\\). Its acceleration is \\(a = v^2/R\\), and after completing one cycle in time \\(T = 2\\pi R/v\\), it has Wigner-rotated by \\(-(\\gamma - 1)2\\pi\\) radians. Simplifying, we have\n\\[\\omega = \\frac{-(\\gamma - 1)2\\pi}{T} = \\cdots = -\\frac{\\gamma^2}{\\gamma + 1} av\\]\nBy the right hand rule of cross products, we can get all the directions correct with \\(\\vec{\\omega} = \\frac{\\gamma^2}{\\gamma + 1} \\vec{a} \\times \\vec{v}\\).\nFor a particle undergoing a generic acceleration, the 3-acceleration decomposes into a component parallel to the 3-velocity, and a component perpendicular to it. The parallel component does not change Wigner rotation, therefore the Thomas precession equation is still true.\n\n\n\nHowever, notice that the formula is made of 3-vectors, and we know that 3-vectors are meaningless in special relativity. This suggests that something is off here. If the rest of this section does not make much sense, first read the section on Foucault pendulum, then return here.\nIf the Earth were flat, then we could stand at a single point, and draw two perpendicular arrows. We say, “this arrow is pointing at \\(+x\\)”, and “that arrow is pointing at \\(+y\\)”. Then we create an infinite number of missionaries. Each would pick up the two arrows and parallel-transport the arrows to their given station on the Earth. In this way, we would provide a unique direction-system for every point on the Earth.\nHowever, because the Earth is a sphere, this does not work. Suppose we stand at the North Pole, and we do the same. Then all the missionaries would meet at the South Pole and start arguing about who has the right one, because they would all disagree. This problem happens because the Earth is curved, and transporting a vector around a cycle would rotate it.\nWe might shrug and say, “Well, nobody lives in the South Pole, so we will just tell our missionaries to avoid it.”. Great idea, except then they found that Emperor of China has also sent out his own missionaries. We shrug and say, “Well, we’ll just give the Emperor of China a call and ask him what angle he picked. Then we will rotate our map until all our missionaries agreed with his.”. After some fruitless fiddling with our maps, it has dawned to us that this plan is doomed. Why?\nImagine if the North Pole is at point \\(N\\), and the Emperor of China is at point \\(C\\). If somehow, we could pick our \\(xy\\) directions so that all our missionaries agree with his, then we can send one missionary around a three-part journey around the world: \\(N \\to C \\to K \\to N\\), where \\(K\\) is a point off the great circle passing \\(N, C\\). Along the path \\(N \\to C\\), the missionary is leaving the North Pole on a straight arc away from the North Pole, so the \\(xy\\) chart he is carrying agrees with the missionaries he is passing by. Similarly for the other two parts. But when the missionary has returned to the North Pole, he must find his \\(xy\\) chart rotated, contradiction!\nThere is no way to resolve this disagreement other than forcing one side to give up their coordinate system. Perhaps we will have to call up a crusade to enforce our coordinate system.\n\nThe introduction of numbers as coordinates is an act of violence.\n— Hermann Weyl\n\nSimilarly, in special relativity, we can stand at the “North Pole” (rest frame), construct the \\(xyz\\) axes, then send out missionaries on rockets to provide directions for every inertial frame. The good news is that no two missionaries can meet each other, so there is no argument. The bad news is that we still have the Wigner rotation problem, so if some aliens are executing the same project, but starting at a different coordinate frame (perhaps because their galaxy is moving relative to ours), then it is impossible for our missionaries to agree in every frame.\nSo, we are forced to only use one rest frame. It is in this context that the Thomas precession formula works.\nConsider an object, boosted from frame \\(p_1\\) to frame \\(p_2\\), with their 3-velocities in the rest frame being \\(\\vec{v}\\) and \\(\\vec{v} + \\delta \\vec{v}\\). To use the Thomas precession formula, we must first – in imagination – boost it back to the rest frame, and take a snapshot of its orientation against our \\(xyz\\) axes at origin. Then we boost it back to frame \\(p_1\\), boost it to \\(p_2\\), and boost it – in imagination – back to the rest frame again. By using the Wigner rotation formula in the rest frame, it has rotated by\n\\[\n\\frac{\\gamma^2}{\\gamma + 1} \\delta\\vec{v} \\times \\vec{v}\n\\]\nTo emphasize again: the Thomas precession formula only works in one frame. It is neither Lorentz invariant nor geometrically meaningful. It cannot be geometrically meaningful, because it is made of 3-vectors \\(\\vec{v}, \\vec{a}, \\vec{\\omega}\\), and 3-vectors are geometrically meaningless in special relativity.33 Or as I like to say, 3-vectors are inherently violent in special relativity, and anything involving anything inherently violent is also inherently violent."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#relativity-in-31-dimensions",
    "href": "essays/posts/wigner-rotation/index.html#relativity-in-31-dimensions",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Relativity in 3+1 dimensions",
    "text": "Relativity in 3+1 dimensions\nWe began our study of special relativity in 1+1 dimensions and discovered the Lorentz transformations, then proceeded to 2+1 dimensions and discovered Wigner rotation. In 3+1 dimensions, Wigner rotation still occurs, but there is something new: rotations are no longer describable by a single number.\nThis is not a place to get into the details; suffice to say that the space of rotations in \\(\\mathbb{R}^3\\) is much harder to understand than the space of rotations in \\(\\mathbb{R}^2\\).4 We must be more careful here.4 The space of rotations in \\(\\mathbb{R}^3\\) is \\(SO(3)\\) and that in \\(\\mathbb{R}^2\\) is \\(SO(2)\\). While \\(SO(2)\\) is just the circle and can be easily represented by a real number or an angle, \\(SO(3)\\) is the projective 3-space \\(\\mathbb{PR}^3\\) and best represented by quaternions.\nSuppose we have four frames: \\(0, 1, 2, 3\\). We start with a tripod in frame \\(0\\), then boost it successively to frames \\(1, 2, 3, and 0\\), how much does it rotate? To discover this, we need to compose the Wigner rotation around the cycle \\(0120\\) with the Wigner rotation around the cycle \\(0230\\). The full rotation is composed of two rotations, first rotation in the plane of the triangle \\(012\\), and the second one in the plane of the triangle \\(023\\).\nThe Wigner rotation coming from an arbitrary sequence of boosts can be calculated easily with the Thomas precession formula, if we are willing to use violence.5 Without violence, we have to do spherical trigonometry, much harder than adding 2-dimensional rotation angles. My intuition is that it should most naturally involve the principal \\(G\\)-connections, where \\(G = SO(n, 1)\\) is the group of Lorentz transformations.5 Since this usage of “violence” comes directly from Hermann Weyl, perhaps we can call it Weylence?"
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#velocity-space-is-hyperbolic-geometry",
    "href": "essays/posts/wigner-rotation/index.html#velocity-space-is-hyperbolic-geometry",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Velocity space is hyperbolic geometry",
    "text": "Velocity space is hyperbolic geometry\nIt is time to fulfill the promise in the title, that hyperbolic geometry is involved.\nOur first hint that hyperbolic geometry is relevant is that we have encountered \\(\\cosh\\) and \\(\\sinh\\) – the hyperbolic trigonometric functions. With this hint, a quick check shows us that we are indeed dealing with hyperbolic geometry. This is easy to do in 2+1 dimensions, but it generalizes readily to n+1 dimensions after slightly cluttering the notation.\n\nDeriving the hyperbolic metric\nSince \\(\\|(0, 0, 1)\\|^2 =-1\\), under arbitrary Lorentz transformations, \\((0, 0, 1)\\) can, under arbitrary Lorentz transformations, reach any point on the upper half of the hyperbola \\(x^2 + y^2 - t^2 = -1\\). It stands to reason that the upper hyperbola, which is equivalent to a plane as a smooth manifold, should have some kind of planar geometry. We need only discover its metric. The most direct path goes through the disk of 3-velocities, which gives us the metric\n\\[\nds^2 = \\frac{dv_\\theta^2}{1-v_r^2} + \\frac{dv_r^2}{(1-v_r^2)^2}\n\\tag{7}\\]\n\n\n\n\n\n\nDeriving the metric of the 3-velocity disk\n\n\n\n\n\nThe disk of 3-velocities \\(\\{(v_x, v_y, 1) : v_x^2 + v_y^2 &lt; 1\\}\\) is not preserved under Lorentz transformations, but we can project it back to the circular disk. This means that we can take the metric at origin\n\\[ds^2 = dv_x^y + dv_y^2\\]\nand perform a Lorentz transformation, followed by a projection, to move the origin to any other point on the disk. Multiplying the metric at the origin with the gradient-matrix gives the metric at that other point.\nWe exploit the rotational symmetry of the disk using polar coordinates. Express any point on the disk as \\((r, \\theta)\\). Its local metric must be of form \\(ds^2 = f(r) dr^2 + g(r) d\\theta ^2 + h(r) drd\\theta\\) for some functions \\(f, g, h\\). Further, by reflection symmetry of \\(\\theta \\leftrightarrow -\\theta\\), we have \\(h(r) = 0\\).\nTo find \\(f(r)\\), we construct an infinitesimal segment \\((r, 0) \\to (r+dr, 0)\\) and calculate with the velocity-addition formula. Similarly, we find \\(g(r)\\) using the segment \\((r, 0) \\to (r, d\\theta)\\).\n\n\n\nThis is the Beltrami–Klein metric, and so we have discovered that this is exactly the Beltrami–Klein model of hyperbolic geometry. We can then project this metric onto the hyperboloid \\(x^2 + y^2 - t^2 = -1\\) to obtain its metric\n\\[\nds^2 = dx^2 + dy^2 - dt^2 = \\| (dx, dy, dt) \\|^2\n\\tag{8}\\]\nThis, in hindsight, is obvious: We have already known that the Lorentz transformation preserves the Minkowski norm-squared. But such is the journey of discovery: the first pass is rarely the most elegant. It is probably better to write down the first pass to show how to discover things, then write a second pass to show how to tidy things. Better this than what Gauss did:\n\n[Gauss] makes his mathematics like a fox, wiping out the traces in the sand with his tail.\n— unnamed German student, as reported by Abel\n\n\n\nInterpreting the hyperbolic geometry\nNow we can connect concepts between hyperbolic geometry and special relativity.\nWhen \\(v\\) is small, the Wigner rotation for a full cycle, \\((\\gamma - 1)2\\pi\\), is \\(\\pi v^2 + O(v^4)\\), coinciding with the area enclosed by the cycle. Therefore, since both area and Wigner rotation are additive, we have proven Theorem 1.66 This is not a handwaved proof, but fully rigorous. The Wigner rotation of a full cycle is \\(\\pi v^2 + O(v^4)\\), and the area enclosed by the cycle is also \\(\\pi v^2 + O(v^4)\\). Therefore, we can integrate over an arbitrary area, to find that the Wigner rotation angle is equal to the area enclosed, up to an infinitesimal term of \\(O(v^2) \\to 0\\).\nIn rocketry, if we trace out the trajectory of the rocket’s velocity on a graph paper, we obtain a hodograph. Similarly, in relativistic rocketry, a path in the velocity space is a hodograph. The length of a hodograph is the total delta-v of the rocket.\nA straight line in hyperbolic space is the hodograph of the most fuel-efficient control-trajectory for a rocket to get from one velocity to another.\nIf a rocket explodes with spherical symmetry, the velocities of its debris will lie on a hyperbolic sphere centered at the rocket’s original velocity."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#two-parables",
    "href": "essays/posts/wigner-rotation/index.html#two-parables",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Two parables",
    "text": "Two parables\n\nThe star worshippers\nAn alien race, who worshipped Rigel, Deneb, and Betelgeuse, prayed in the direction of these three stars. In order to pray in the correct direction even without seeing them, they constructed the Sacred Tripods, which are steel tripods that are oriented such that each leg points at one of the Sacred Stars. These Sacred Tripods, placed in public spaces, were required to point precisely at the Sacred Stars.\nThen a Cosmic Dark Age began and the stars winked out of existence, including the Sacred Stars, but the aliens did not lose their faith. Instead, they intensified their prayers in hopes of resurrecting the Sacred Stars.\nIn a region of space, there were two space stations A and B, at rest with each other. The Sacred Tripod on B was unstable, and required yearly calibration against the more stable Sacred Tripod at A. So station A would align its spare Sacred Tripod to its own one, put it on a gimbal-mount, and send it by rocket to station B. B would align its Tripod with the one sent, then let the Tripod go back.\nAs years passed, the aliens on station B grew confident that their Tripod did not drift more than \\(1''\\) per year. One day, an asteroid field blocked the straight path, forcing the next shipment to make a big detour around the asteroid field. When it arrived, to their astonishment, it was found that the Tripod on B was clearly misaligned with the Tripod from A. What could be the cause of this misalignment?\n\n\nThe day the Earth stood still\nOne day, the Earth stopped spinning. This annoyed the visitors to the science museum, who wanted to see Foucault’s pendulum rotating. To satisfy those visitors, the museum keeper put the pendulum on the top of a bus, and loaded the visitors on the bus, then they started driving around the Earth at the same speed as how the Earth used to rotate at that latitude, so that after one day, they returned to the museum. The visitors looked up at the pendulum and were satisfied to see that it indeed has turned.\nTo find out how much the pendulum has turned, we divide up the circular trajectory into tiny segments \\(p_0, p_1, \\dots, p_n\\), then draw tiny triangles \\(p_0 p_1 N, p_1 p_2 N, \\dots, p_{n-1} p_n N\\), with \\(N\\) being the North Pole. We imagine that, instead of driving around the Earth in a circle, we drive around the triangle \\(N \\to p_0 \\to p_1 \\to N\\), then the triangle \\(N \\to p_1 \\to p_2 \\to N\\), etc.\nWhen we drive around \\(N \\to p_0 \\to p_1 \\to N\\), the bus makes three turns, and each time the pendulum turns by an opposite amount relative to the bus. After three turns, the bus has turned the same angle as the sum of three external angles of the triangle. By spherical trigonometry, we know that this is equal to \\(2\\pi - 4\\pi\\frac{\\text{area of triangle $p_0 p_1 N$}}{\\text{surface area of the Earth}}\\). Thus, the pendulum has turned relative to the bus by \\(-2\\pi + 4\\pi\\frac{\\text{area of triangle $p_0 p_1 N$}}{\\text{surface area of the Earth}}\\). The \\(-2\\pi\\) part has no effect, as it is a full cycle, leaving us with \\(\\frac{\\text{area of triangle $p_0 p_1 N$}}{\\text{surface area of the Earth}}\\).\nTherefore, by adding them up, we find that after driving around an arbitrary cycle on the Earth, the pendulum would have turned by\n\\[4\\pi\\frac{\\text{area of the cycle}}{\\text{surface area of the Earth}}\\]\nWe can turn this insight into a formula for Fermi–Walker transport on the Earth.\nFirst, we scale the Earth so that its radius is one. Now set up the coordinate system \\(\\phi, \\theta\\), where \\(\\phi\\) is the latitude (zero at equator, \\(\\pi/2\\) at the North Pole) and \\(\\theta\\) the longitude. At each point on the Earth (except the two poles) we set up a local frame with two unit vectors \\(\\hat\\phi, \\hat\\theta\\). One very important fact is that this local frame is not parallel-transported7. If you try to drive in a small circle around the North Pole, while always pointing in the direction of \\(\\hat\\theta\\), you would feel a strong centrifugal force pushing against your steering wheel. Centrifugal forces and all other “inertial forces” are nature’s way of telling you that you are not being parallel-transported.7 Indeed, there is no way to cover the space with parallel-transported local frames, because the space is curved, and therefore if you try to transport a local frame in a cycle back to its starting point, it would have rotated against itself. To be a curved space is equivalent to have no system of parallel-transported local frame, and curvature measures the amount of rotation-against-itself that happens when you transport a local frame in an infinitesimal circle.\nConsider two infinitesimally close points on the Earth, \\(p_1 = (\\phi, \\theta)\\) and \\(p_2 = (\\phi + \\delta \\phi, \\theta + \\delta\\phi)\\). Since the surface area of a spherical cap around latitude \\(\\theta\\) is \\(\\frac{1 -\\sin\\theta}{2}\\) that of the whole sphere8, the area of a thin spherical triangle with vertices \\(p_1, p_2, N\\) is8 This is immediate from Archimedes’ hat-box theorem, which is the basis of the Lambert cylindrical equal-area projection.\n\\[4\\pi\\frac{1 -\\sin\\theta}{2} \\times \\frac{\\delta\\phi}{2\\pi} = (1-\\sin\\theta)\\delta\\phi\\]\nThus, if we parallel transport a vector clockwise in the order \\(p_1 \\to N \\to p_2 \\to p_1\\), then the vector would turn counterclockwise by \\(-(1 -\\sin\\theta) \\delta\\phi\\).\nNow, consider two ways to move the pendulum from \\(p_1\\) to \\(p_2\\). We can transport it directly, or detour through the North Pole. If we detour through the North Pole like \\(p_1 \\to N \\to p_2\\), then by the same argument as the “polar bear puzzle”9, you see that the pendulum has rotated counterclockwise relative to the local frame by \\(-\\delta\\phi\\). Then we complete its journey with \\(p_2 \\to p_1\\), to create an absolute10 full \\(-(1 -\\sin\\theta) \\delta\\phi\\) rotation. Therefore, moving it \\(p_2 \\to p_1\\) has created a rotation relative to the local frame by \\(\\sin\\theta \\delta\\phi\\). Since we are actually moving it \\(p_1 \\to p_2\\), we reverse the sign, and obtain our Fermi–Walker transport equation (the Earthbound version):9 You walk 1 km south, 1 km east, and 1 km north, and ended up at the same point. You see a bear. Why is the bear white?10 Relative rotation means that we are measuring the orientation of the vector relative to the local frame. However, since the frames themselves are not parallel-transports of each other, relative rotation is arbitrary and not a fact of geometry, but a fact of convenience. If you move the vector back to its starting point, however, there is absolutely no dispute about how much it has rotated, and it does not depend on any system of local frames. You just have to compare the vector against itself.\n\\[\n\\delta(\\text{vector angle}) = -\\sin\\theta \\delta\\phi\n\\tag{9}\\]\nApplied to the Foucault pendulum problem, we find that it rotates clockwise by \\(2\\pi \\sin\\theta\\) every day, and it takes \\(\\frac{1}{\\sin\\theta}\\) days11 to make a full rotation. At the Paris Observatory, the original place where Foucault made his experiment in 1851, we have11 To be precise, this is a sidereal day, the time it takes for the Earth to rotate one cycle relative to distant stars. It is shorter than a solar day.\n\\[\\theta = \\mathrm{48^\\circ 52' N}, \\quad \\text{period} = \\frac{\\mathrm{23h56'}}{\\sin \\theta} \\approx \\mathrm{31\\,h\\,50\\,min}\\]\nIf a museum visitor can stay for 5 minutes at the pendulum, then they would see the pendulum complete \\(\\approx 1/382\\) of a cycle. Typical museums would put up about 400 wooden blocks in a cycle, to be knocked down by the pendulum. This allows each visitor to see at least one block being knocked down."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#history-notes",
    "href": "essays/posts/wigner-rotation/index.html#history-notes",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "History notes",
    "text": "History notes\n\nThe history of the phenomenon\nThe history of the Wigner rotation is winding and complicated. Here is a brief summary. The details are given in (Walter 1999).\n\n1905: Einstein published special relativity. (A. Einstein 1905)\n1908: Minkowski published the spacetime interpretation of special relativity. (Minkowski 1908)\n1910: Vladimir Varićak (Varicak 1910) and E. T. Whittaker (Whittaker 1910, 441) both introduced the hyperbolic functions and the rapidity parameter.\n1911: Alfred Robb named the rapidity parameter, and found that velocities compose in hyperbolic space. (Robb 1911).\n1913: Émile Borel discovered the Wigner rotation as a part of his general study of special relativity by hyperbolic geometry. (Borel 1913)\n1921: Fermi discovered the Fermi–Walker transport. (Fermi 1921)\n1926: Llewellyn Thomas rediscovered the Thomas precession, a special case of the Wigner rotation. (Thomas 1926)\n1932: Walker rediscovered the Fermi–Walker transport. (Walker 1932)\n1939: Wigner rediscovered the Wigner rotation. (Wigner 1939)\n\nPedagogical attempts to present the Foucault pendulum and the Wigner rotation have been numerous, such as (Criado and Alamo 2009), but perhaps due to the decade-long latency in academic publishing, none has yet made it into the usual undergraduate textbooks. Tevian Dray has written an entire textbook (Dray 2021) treating both special and general relativity in the same geometric style. For a summary paper of the textbook, see (Dray 2017).\n\n\nAmusing quotes from (Walter 1999)\n\n[After 1907] Minkowski never again referred to a manifold as both four-dimensional and non-Euclidean. Along with the problematic label, the geometric interpretation of velocity vectors likewise vanishes from view in Minkowski’s subsequent writings. Felix Klein, for one, regretted the change; in his opinion, Minkowski later hid from view his “innermost mathematical, especially invariant-theoretical thoughts” on theory of relativity (Klein 1927, 75).\nPlanck lavished praise on Einstein for his modification of the concept of time:\n\nIt need scarcely be emphasized that this new view of the concept of time makes the most serious demands upon the capacity of abstraction and the imaginative power of the physicist. It surpasses in boldness everything achieved so far in speculative investigations of nature, and even in philosophical theories of knowledge: nonEuclidean geometry is child’s play in comparison.30 (Planck 1910a, 117)\n\nUnder the new space-time view, Minkowski announced, “Three-dimensional geometry becomes a chapter of four-dimensional physics.” In the same triumphant spirit, Minkowski suggested that his new four-dimensional understanding of the laws of physics deserved its own label. The “Principle of the Hyperbolic World” that he had tried on Hurwitz was shelved in favor of the more ecumenical “Postulate of the Absolute World” (Minkowski 1909, 82). Although Minkowski explained this to mean that only the four-dimensional world in space and time is given by phenomena (Minkowski 1909, 82), one suspects an inside joke with Hurwitz, since in the German mathematical community, hyperbolic geometry was sometimes referred to as absolute geometry.\nEven the watered-down version of the space-time theory presented in Minkowski’s Cologne lecture repelled some physicists. For instance, Willy Wien’s cousin Max (1866-1938), a physicist at Danzig Polytechnic, confided to his friend Arnold Sommerfeld that reading Minkowski gave him vertigo:\n\nSommer[feld] maintains that [Minkowski’s] speech in Cologne was simply grand; when reading it, however, I always get a slight brain-shiver, now (that) space and time appear conglomerated together in a gray, miserable chaos.36 (Max Wien to Arnold Sommerfeld, February 16, 1909, Benz 1975, 71)\n\n\n\n\nThe history of this document\nDuring high school, I was in the physics Olympiad team. One afternoon, I got into an argument with someone about what happens if you take a square \\([0, 1] \\times [0, 1]\\), boost it by \\((v, 0)\\), and then boost it by \\((0, v')\\) within the boosted frame. By the velocity addition formula, the square would move at 3-velocity\n\\[\n(v, \\frac{v'\\sqrt{1-v^2}}{1+vv'})\n\\]\nIf \\(v' = \\frac{v}{\\sqrt{1-v^2}-v^2}\\), then the square would be moving at \\((v, v)\\).\nThen, the paradox. He argued that, by length contraction along the diagonal, the square should look like a diamond:\n\n  \n\n\n\nI objected that in the boosted frame, the square looks like a rectangle moving upwards. Decompose the rectangle into a bundle of line-segments, all parallel to the \\(y'\\)-axis. Now, each line-segment is moving in the \\(y'\\)-direction, and \\(y'\\) is parallel to \\(y\\) (since the perpendicular direction is preserved under boosting), we know that each moving line-segment would still be a line-segment parallel to the \\(y\\)-axis in the resting frame, still moving in the \\(y\\)-direction – just slower. Therefore, in the resting frame, the whole square would look like a parallelogram, with two sides parallel to the \\(y\\)-axis, and the other two sides oblique to the \\(x\\)-axis.\n\n    \n        \n        \n    \n    \n\n\nAfter a brief shouting match, we figured out that I was right, but also that we met something no teacher has taught us before: you can create a rotation in special relativity by pure boosting.\nDuring my third undergraduate year, I took a course in theoretical physics, which required a term paper. I first tried to write one on the Ostrogradsky instability, but could not understand it, so I quickly switched to finally solving the rotation effect in special relativity.\nSuffice to say that, after some hours walking and staring at the night sky, I figured out that it is nothing else than hyperbolic geometry, and nothing more paradoxical than the fact that the external angle of a hyperbolic triangle is equal to \\(2\\pi + (\\text{area of the triangle})\\), which I remember from hyperbolic geometry.\nThis is the old poler-bear puzzle again. If you walk from the North Pole to the equator, then walk \\(1/4\\) of the way around the Earth, and finally walk back to the North Pole, you will have traversed a triangle with an external angle of \\(\\frac 32 \\pi\\). In general, the external angle of a triangle on the unit sphere is \\(2\\pi - (\\text{area of the triangle})\\), the perfect opposite to the case in hyperbolic geometry.\nHalf-mad, I ran home and smashed into the search engine all the keywords I knew must be there: “special relativity rotation hyperbolic triangle Foucault pendulum”. My disappointment was swift and certain: This had been repeatedly discovered over the past hundred years.\nBitterly, I searched every undergraduate physics textbook in the school library. None included it. The standard textbook (Goldstein, Poole, and Safko 2008, sec. 7.3) went on a three-page long computation and concluded that, indeed, we obtain a rotation matrix:\n\nThe spatial rotation resulting from the successive application of two nonparallel Lorentz transformations has been declared every bit as paradoxical as the more frequently discussed apparent violations of common sense, such as the so-calIed “twin paradox”. But the present apparent paradox has important applications, especially in atomic physics, and therefore has been abundantly verified experimentally.\n\nFrom that moment on I swore that I would finally present it in a way so simple and direct that it will never be forgotten. I had found the way, and it remained to publicize to the world.\nThere was little use in writing another paper about it, as despite the many papers over the century, somehow this effect did not end up in the physics textbooks. I considered writing a blog post about it, but mere words and figures seemed insufficient. I tried promoting this idea to various physics popularizers on YouTube, but none picked it up. What I needed was interactive animation, but JavaScript defeated me, so I kept it in the backlog until now. With ChatGPT I could finally write JavaScript without losing all my sanity, and so, here it is."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#some-bonus-content",
    "href": "essays/posts/wigner-rotation/index.html#some-bonus-content",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Some bonus content",
    "text": "Some bonus content\n\nFinding the Wigner rotation in one line\nIf you know Lie algebra, then the Wigner rotation is immediate: calculate the commutator.\n\\[\n[K_x, K_y] := K_x K_y - K_y K_x = \\begin{bmatrix} 0 & 1 & 0 \\\\ -1 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\n\\]\nThis is the generator of rotation in the \\(xy\\) plane.\n\n\nGo anywhere with just five boosts\nGiven a rocket that can boost in any direction, but not rotate, you can still make it rotate by Wigner rotation. If you must perform an arbitrary Lorentz transformation, how many boosts do you need?\nIn general, if you have a Lorentz-transformed rocket, you can apply a carefully aimed boost so that it comes to rest, then apply Wigner rotation to return it back to the starting position.\nWith a cycle of three boosts, the Wigner rotation angle is equal to the angle defect of the hyperbolic triangle, which can take any value in \\((0, \\pi)\\) (you can’t have one with zero inner angle sum, but you can get arbitrarily close). With a cycle of four boosts, we can cover the \\(\\pi\\) case as well, since a hyperbolic square can have any angle defect in \\((0, 2\\pi)\\).\nIn summary: almost any Lorentz transformation can be obtained in four boosts, except those requiring a 180-degree rotation, which need five boosts.\nIn fact, the result can be improved by one boost: every Lorentz transformation can be done in three boosts, except “180-degree screws”, which require four. The solution given in (Lightman et al. 1975, 153–58) uses matrix algebra in \\(\\mathbb{C}^{2\\times 2}\\), which is too algebraic for me12. However, I have never found a satisfactory geometric method to demonstrate this, despite thinking on and off about it several times over the years.12 \nIn these days the angel of topology and the devil of abstract algebra fight for the soul of each individual mathematical domain.\n— Hermann Weyl\n\n\n\nProblem 1.28. What is the least number of pure boosts which generate an arbitrary Lorentz transformation? Note: This is a difficult problem! (Lightman et al. 1975)"
  },
  {
    "objectID": "essays/posts/scaling-law-by-data-manifold/index.html",
    "href": "essays/posts/scaling-law-by-data-manifold/index.html",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "",
    "text": "This is a theory of neural scaling law, proposed by (Bahri et al. 2021; Sharma and Kaplan 2022)\nAccording to this theory, a neural network, when trained to convergence, allocates its \\(N\\) parameters in two parts: * A fixed number of parameters that map the data to an intrinsic data manifold of dim \\(d\\). * All other parameters that handle pieces of this manifold. Loss \\(\\propto\\) the volume of each manifold piece.\nThey argued that the loss function should scale as \\(L \\propto N^{-4/d}\\) for cross-entropy and mean-square losses."
  },
  {
    "objectID": "essays/posts/scaling-law-by-data-manifold/index.html#the-manifold-hypothesis",
    "href": "essays/posts/scaling-law-by-data-manifold/index.html#the-manifold-hypothesis",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "The manifold hypothesis",
    "text": "The manifold hypothesis\nConsider the space of all MNIST images. Each MNIST image is a 28x28 grayscale, so the total space is \\(\\mathbb R^{28\\times 28} = \\mathbb R^{784}\\).\nHowever, as you may have seen in experiments with the VAE, most of the MNIST dataset “collapses” onto a much smaller subset of \\(\\mathbb R^{784}\\). This is the “(intrinsic) data manifold”, with a dimension much smaller than \\(784\\). Indeed, the very fact that the intrinsic dimension is small allows us to have meaningful “2D slices” of the dataset:\n\n\n\nFigure from Convolutional Variational Autoencoder - Tensorflow Core Tutorials. More images can be found in Visualizing the Variational Autoencoder, and can be regenerated in about 10 minutes with a Google Colab GPU.\n\n\nReal data can be expensive, though, which is why we often use “toy” datasets with known dimensions, generated by a known random process. For example, the following is the “Swiss roll” dataset. It is generated by first populating a 2D square \\([0, 1]^2\\), then use a function \\(f: \\mathbb R^2 \\to \\mathbb R^3\\) to “roll up” the square into 3D space.\n\n\n\nThe Isomap algorithm (Tenenbaum, Silva, and Langford 2000), popular for constructing data manifolds. Figure from Wikipedia."
  },
  {
    "objectID": "essays/posts/scaling-law-by-data-manifold/index.html#deriving-the-scaling-law",
    "href": "essays/posts/scaling-law-by-data-manifold/index.html#deriving-the-scaling-law",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Deriving the scaling law",
    "text": "Deriving the scaling law\nWe can get a feel for where the number \\(4/d\\) came from by studying a simpler model.\n\nPrototype case\nConsider a problem of regression. We have to learn the true function on the \\(n\\)-dimensional cube: \\(f: [0, 1]^d \\to \\mathbb{R}\\). Assume it is Lipschitz continuous, that is, its first derivative is upper bounded by \\(\\lambda\\). In particular, this means \\(|f(x) - f(y)| \\leq \\lambda |x-y|\\) for all \\(x, y\\) in the domain.\nWe approximate the true function with a piecewise-constant function \\(\\hat f: [0, 1]^d \\to \\mathbb{R}\\), meaning that its graph looks like a staircase. We divide the cube into \\(N\\) equal smaller cubic pieces, and define \\(\\hat f\\) to be equal to the value of \\(f\\) at the center of each cubic piece.\n\nTheorem 1 When the loss is mean square error, it scales like \\(L = \\Theta(N^{-2/d})\\).\n\n\nProof. With \\(N\\) parameters, we can divide the \\([0, 1]^d\\) cube into \\(N\\) equal parts, therefore, each cube has side length \\(N^{-1/d}\\). Therefore, the distance between the center of each cube and the point farthest from the center is also \\(\\Theta(N^{-1/d})\\).\nNow, since the function has bounded first derivative, we know that the difference between the function value at the center of the cube and the function value at the farthest point is bounded by \\(\\lambda \\cdot \\Theta(N^{-1/d}) = \\Theta(N^{-1/d})\\). Therefore, the mean square loss on each individual little cube is bounded by \\(\\Theta(N^{-2/d})\\).\nAnd since the overall mean square loss is the average of the loss on each individual cube, the total loss is also bounded by \\(\\Theta(N^{-2/d})\\).\n\n\n\nGeneralization\nMore generally, if \\(f\\) has bounded second-derivative, and we use a piecewise-linear \\(\\hat f\\) function approximator, then the mean square loss scales like \\(\\Theta(N^{-4/d})\\). By piece-wise linear, we mean that the domain of \\(\\hat f\\) is divided into little cubes, and it is linear on each little cube.\nIndeed, this generalizes in the obvious way:\n\nTheorem 2 If the loss is mean \\(p\\)-th power loss, \\(f\\) has bounded \\(k+1\\)-th order derivatives, and \\(\\hat f\\) is composed of piece-wise \\(k\\)-degree polynomials, then the loss scales like \\(\\Theta(N^{-p(k+1)/d})\\).\nSince the KL-divergence is approximately MSE loss when the predictor is close to correct, the loss scales like \\(\\Theta(N^{-2(k+1)/d})\\) in this case.\n\n\nProof. We prove another case where the loss is still mean square error, but \\(f\\) has bounded \\(2\\)-th order derivatives.\nBy Taylor expansion, if we use the first-order Taylor expansion to approximate \\(f\\) at the center of each cube, then the error is bounded by \\(\\Theta(N^{-2/d})\\). And since the mean square loss is the average of the square of the error, the total mean squared loss is bounded by \\(\\Theta(N^{-4/d})\\) on each little cube.\nAnd since the overall mean square loss is the average of the loss on each individual cube, the total loss is also bounded by \\(\\Theta(N^{-4/d})\\).\nFor the general case, take the Taylor expansion to the \\(k\\)-th order at the center of each little cube.\n\n\n\nScaling of nearest neighbor rule\nWhat is the worst possible scaling? It would be when \\(k=0\\) and \\(p=1\\), giving us \\(L = \\Theta(N^{-1/d})\\). What does this mean? To have \\(k=0\\) means that we use piecewise-constant fitting function \\(\\hat f\\). To have \\(p=1\\) means that we are using the L1-loss. This is essentially piecewise constant, median regression.\nUnder mild assumptions, the nearest neighbor rule for classification has the same form of scaling, where the loss is not L1-loss, but 0-1 loss.\nPeople have found sharper scaling laws under assuming nicer datasets, using complicated functional analysis tools scaling laws. A dataset can be “nice” in several ways. One way is to have few outliers: it should have a thin tail, looking more like a box, rather than a gently rising hill. Another way is to have smoothly varying boundaries: its boundary should not look “bumpy”. See (Yang and Zhang 2023) for a brief review and further citations to the literature."
  },
  {
    "objectID": "essays/posts/scaling-law-by-data-manifold/index.html#experiments",
    "href": "essays/posts/scaling-law-by-data-manifold/index.html#experiments",
    "title": "Neural scaling law by data manifold dimensions",
    "section": "Experiments",
    "text": "Experiments\nAccording to the theory, if the data manifold has dimension \\(d\\), then as we scale up a neural network with \\(N\\) parameters, the MSE loss of a fully-trained network would scale like \\(L \\sim N^{-\\alpha}\\), where \\(\\alpha \\approx 4/d\\). We test this in two ways, once with synthetic datasets, where we know that the data manifold has the desired number of dimensions, and once with the CIFAR-10 dataset, where we do not have the dimension of the data manifold, and must estimate it.\nAll code for generating the dataset, and for analyzing the dataset, are in a GitHub repo: yuxi-liu-wired/scaling-law-by-data-manifold.\n\nSynthetic data manifolds\nSince Consider the simplest data manifold: \\(\\mathbb R^d\\), affine-transformed, then embedded in \\(\\mathbb R^n\\), with \\(n &gt; d\\).\nTo synthesize such a data manifold, we randomly initialize a teacher network, so-called because it implements the function that a student network will fit to by supervised training. Each teacher network is constructed thus:\n\nThe number of neurons in each layer are: \\([d, 9, 600, 600, 1]\\)\nIt has 0 bias.\nThe weights between layers are sampled from \\(\\mathcal N(0, m^{-1/2})\\) , where \\(m\\) is the input size of the layer (a form of He initialization).\nThe activation function at the second (with 9 neurons) and last layers are identity. All other activation functions are ReLU.\n\nOnce we have constructed a teacher network, we use it to generate a dataset \\(\\{(x_i, y_i)\\}_i\\) in this way:\n\nGenerate random gaussian vectors \\(\\{t_i\\}_i\\) in \\(\\mathbb R^d\\), with mean \\(0\\) and std \\(I_{d\\times d}\\).\nFor each \\(t \\in \\{t_i\\}_i\\), push \\(t\\) through the teacher network.\nLet \\(x \\in \\mathbb R^9\\) be the teacher network activation at the second layer, with 9 neurons.\nLet \\(y \\in \\mathbb R\\) be the teacher network output.\n\nFirst, we define the “student” neural network architecture:\n\nThe number of neurons in each layer are: \\([9, n, n, 1]\\).\nThe biases are initialized to 0.\nThe weights between layers are sampled from \\(\\mathcal N(0, m^{-1/2})\\), where \\(m\\) is the input size of the layer (a form of He initialization).\nAll activation functions are ReLU.\n\n\n\n\nAn example teacher network architecture with \\([9, 5, 5, 1]\\) neurons.\n\n\nThe parameter count is\n\\[\nN = \\underbrace{(n+n+1)}_{\\text{first layer}} + \\underbrace{(9n + n^2 + n)}_{\\text{second layer}}\n\\]\nWith these settings, I ran the experiment many times, for \\(N\\) ranging from \\(500\\) to \\(10000\\), and \\(d\\) from \\(2\\) to \\(18\\). The results do not look as clean as given in the paper, despite that I have tried my best to match the experimental design as specified in the paper.\n\n\n\nExperimental data for various synthetic dataset dimensions and student network sizes.\n\n\n\n\nCIFAR-10\nThe CIFAR-10 dataset is a popular benchmark, consisting of 32-by-32 RGB images in 10 different image classes, with 6,000 images per class. While the images live in a space of dimension \\(32^2 \\times 3 = 3072\\), (Sharma and Kaplan 2022) reports that the CIFAR-10 images lies in a data manifold with dimension of only around 16–18.\nTo fit the dataset, I trained a family of convolutional networks with 3 convolution layers and 2 fully connected layers on CIFAR-10. In order to run a controlled experiment, I varied as few parameters as possible, with the following designs:\n\nThe network architecture is fixed, and the network parameter count is changed by changing a single number: the number of channels in the convolutional layers.\nThe experiment is run with 20 different network sizes, from 5408 to 115114.\nEach training run lasts 50 epochs, with batch size 128.\nThe optimizer is AdamW with lr=5e-4.\n\nWith these settings, I generated all the data and logged them into TensorBoard log files, then cleaned them up for quantile regression. Plotting in log-log scale, with the x-axis being the model parameter count, and the y-axis being the cross-entropy loss, we would get a downward sloping line. Our hope is that the line should have a slope of close to \\(-4/d\\), where \\(d \\approx 17\\).\nThis is exactly what I have found. Not only is it true for cross-entropy loss, it is also true for classification accuracy (0-1 loss), except the slope is \\(+4/d\\).\n\n\n\nExperimental data for the train/validation splits of CIFAR-10, and with two different criteria: cross entropy loss and accuracy. We see that in all 4 cases, the scaling exponent is close to the theoretical prediction."
  },
  {
    "objectID": "essays/posts/perplexity-turing-test/index.html",
    "href": "essays/posts/perplexity-turing-test/index.html",
    "title": "Predicting AGI by the Turing Test",
    "section": "",
    "text": "This essay explains the Direct Approach proposed by (Barnett and Besiroglu 2023a).1 I encourage you to play with the Direct Approach Interactive Model to explore an interactive simulation using the approach.1 The thing is released in a scattered way, typical for an internet-native publication. There is the report (Barnett and Besiroglu 2023a), in the form of a paper – clearly meant to be cited, despite being hard to read. There is the website (Barnett and Besiroglu 2023b), in the form of a blog post – clearly meant to be read, despite not being upper-class enough to be cited in journal papers. Finally there is the interactive model which looks like an optional add-on to the blog post.\n\nThe Direct Approach framework bounds the compute requirements for transformative AI by extrapolating neural scaling laws. We combine those estimates with simple models of future progress in algorithms, investment, and compute costs to produce a user-adjustable forecast over the date at which TAI will be achieved. (Barnett and Besiroglu 2023b)\n\nFrom the POV of the judge, a Turing test is a sequential test for two statistical hypotheses – “is human” and “is machine”. Under reasonable assumptions, halving the (reducible part of) log-perplexity loss of the language model would double the time it can survive in a Turing test.\nWe can think of the peer-review of scientific papers as a Turing test, and say that AGI has arrived when we have AI scientists that can pass the papers peer-review. This allows us to calculate the log-perplexity loss of the first AGI. If we assume it is just a scaled-up GPT, then assuming the Chinchilla scaling law, it would cost about 200 years of global GDP. This makes it virtually certain that the first AGI will not be a scaled-up GPT."
  },
  {
    "objectID": "essays/posts/perplexity-turing-test/index.html#abstract",
    "href": "essays/posts/perplexity-turing-test/index.html#abstract",
    "title": "Predicting AGI by the Turing Test",
    "section": "",
    "text": "This essay explains the Direct Approach proposed by (Barnett and Besiroglu 2023a).1 I encourage you to play with the Direct Approach Interactive Model to explore an interactive simulation using the approach.1 The thing is released in a scattered way, typical for an internet-native publication. There is the report (Barnett and Besiroglu 2023a), in the form of a paper – clearly meant to be cited, despite being hard to read. There is the website (Barnett and Besiroglu 2023b), in the form of a blog post – clearly meant to be read, despite not being upper-class enough to be cited in journal papers. Finally there is the interactive model which looks like an optional add-on to the blog post.\n\nThe Direct Approach framework bounds the compute requirements for transformative AI by extrapolating neural scaling laws. We combine those estimates with simple models of future progress in algorithms, investment, and compute costs to produce a user-adjustable forecast over the date at which TAI will be achieved. (Barnett and Besiroglu 2023b)\n\nFrom the POV of the judge, a Turing test is a sequential test for two statistical hypotheses – “is human” and “is machine”. Under reasonable assumptions, halving the (reducible part of) log-perplexity loss of the language model would double the time it can survive in a Turing test.\nWe can think of the peer-review of scientific papers as a Turing test, and say that AGI has arrived when we have AI scientists that can pass the papers peer-review. This allows us to calculate the log-perplexity loss of the first AGI. If we assume it is just a scaled-up GPT, then assuming the Chinchilla scaling law, it would cost about 200 years of global GDP. This makes it virtually certain that the first AGI will not be a scaled-up GPT."
  },
  {
    "objectID": "essays/posts/perplexity-turing-test/index.html#turing-test-as-statistical-hypothesis-test",
    "href": "essays/posts/perplexity-turing-test/index.html#turing-test-as-statistical-hypothesis-test",
    "title": "Predicting AGI by the Turing Test",
    "section": "Turing test as statistical hypothesis test",
    "text": "Turing test as statistical hypothesis test\n\nTuring test\nIn the Turing test, there are three players: one judge and two players. The first player is a human, and the second is a machine. The judge asks each player text questions and receives text answers. The judge must decide who is the human.\nWe consider a simplified Turing test. In this test, the judge does not ask, and simply receives one stream of text \\(X_{1:\\infty}\\). The judge must decide whether the stream is produced by the human or the machine, and do so quickly.\nCast in the language of statistical hypothesis testing, we have two hypotheses:\n\n\\(H_0\\): “the stream is produced by the human”;\n\\(H_1\\): “the stream is produced by the machine”.\n\nThe judge would read from the stream \\(X_{1:\\infty}\\), o-n-e- -t-o-k-e-n at a time, and at each token, decide whether to take another one, or announce its judgment: \\(H_0\\) or \\(H_1\\).\nAs the organizers of the Turing test, we would start the test by flipping a fair coin to decide whether to use the human or the machine. Therefore, \\(Pr(H_0) = Pr(H_1)\\), and by Bayes, the posterior log-probability ratio is\n\\[\n\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)} = \\ln\\frac{Pr(H_0|X_{1:n})}{Pr(H_1|X_{1:n})}\n\\]\nThis allows us to use the sequential probability ratio test (SPRT). The judge would decide on two decision boundaries, and calculate \\(\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\) at each token. It would stop and announce the decision as soon as the quantity exceeds one of the boundaries.\nFor example, suppose the judge wants to decide when the odds ratio is 10 to 1, then it would set the decision boundaries to be \\([-\\ln 10, + \\ln 10]\\). If \\(\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\) goes above \\(+\\ln 10\\) when \\(n = 60\\), then the judge would announce “\\(H_0\\)” at that point.\nThe \\(\\ln 10\\) is a good rule of thumb, which we will use for the remainder of the essay.\n\n\nSequential hypothesis testing\nConsider the following simple equation:\n\\[\n\\underbrace{\\frac 1n \\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[ \\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\right]}_{\\text{$\\frac 1n D_{KL}(Pr(\\cdot | H_0)\\| Pr(\\cdot | H_1))$}} = \\underbrace{\\frac 1n\n\\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[\\ln\\frac{1}{Pr(X_{1:n}|H_1)}\\right]}_{\\text{negative log-likelihood loss per token}} - \\underbrace{\\frac 1n  \\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[\\frac{1}{\\ln Pr(X_{1:n}|H_0)}\\right]}_{\\text{entropy rate of the human itself}}\n\\tag{1}\\]\nThe first term is the KL-divergence per token between the machine and the human. Roughly speaking, it is how different they are, per token emitted. It is an information-theoretic quantity.\nThe second term is negative log-likelihood loss per token. This is what language models are trained to minimize. We write it as \\(L\\).\nThe third term is the entropy rate of the human. It is how random the human is. We write it as \\(L_\\infty\\), because it is the theoretical minimal loss that the language model can reach.\nIf the machine is a perfect replica of the human, then the second term is zero, and the first term equals the third term.\nAssuming that the human is an ergodic speakers of English,2 we can sample an infinite stream \\(X_{1:\\infty}\\) from the human, then call up the Shannon–McMillan–Breiman theorem and find that2 In short, an ergodic speaker is someone who has only one speech. If you hear it speak once for a very long time, then hear it speak again for a very long time, then you can take the first and shift it around, so that it looks like the second over a very long sub-segment. Ergodic speakers allow you to take the average over a single very long speech, and be assured that it is close to the average over all possible speeches.\nIn long, see the appendix on ergodic theory.\n\\[\n\\frac 1n \\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)} \\to L - L_\\infty\n\\]\nOn the other hand, if the machine is also an ergodic speaker of English, then we can sample an infinite stream \\(X_{1:\\infty}\\) from the machine, then call up the SMB theorem and find that\n\\[\n\\frac 1n \\ln\\frac{Pr(X_{1:n}|H_1)}{Pr(X_{1:n}|H_0)} \\to L' - L_\\infty'\n\\]\nwhere unfortunately, we have the odd \\(L'\\) and \\(L_\\infty'\\), defined by\n\\[\nL' := \\lim_n \\frac 1n\n\\mathbb{E}_{X \\sim Pr(\\cdot | H_1)}\\left[\\ln\\frac{1}{Pr(X_{1:n}|H_0)}\\right], \\quad L_\\infty' := \\lim_n \\frac 1n\n\\mathbb{E}_{X \\sim Pr(\\cdot | H_1)}\\left[\\ln\\frac{1}{Pr(X_{1:n}|H_1)}\\right]\n\\]\nWe can interpret them as the loss of the human at imitating the machine, and the entropy rate of the machine itself. When the machine is close enough to the human, we can take the approximation \\(L' \\approx L, L_\\infty' \\approx L_\\infty\\).\nNow, define the log-ratio at step \\(n\\) to be \\(r_n := \\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\). During a Turing test, the judge calculates\n\\[\n\\begin{aligned}\nr_0 &= 1 \\\\\nr_1 &= r_0 + \\frac{Pr(X_{1:1}|H_0)}{Pr(X_{1:1}|H_1)} \\\\\nr_2 &= r_1 + \\frac{Pr(X_{1:2}|X_{1:1}, H_0)}{Pr(X_{1:2}|X_{1:1}, H_1)} \\\\\n&\\cdots\n\\end{aligned}\n\\]\nSo, imagine that such a perfect judge is going through a Turing test, upon receiving “my cat is technically”, and we are listening on its thoughts:\n\n“If it were a human, then it would start with ‘my’ with probability \\(0.01\\). If it were a machine, then \\(0.05\\). Therefore, the odds ratio is 2 to 1.”\n“If it were a human, then it would follow ‘my’ with ‘cat’ with probability \\(0.01\\). If it were a machine, then \\(0.033\\). Therefore, the odds ratio is 3 to 1.”\n“If it were a human, then it would follow ‘is’ with ‘my cat’ with probability… I do not know. However, I do know that the odds ratio is 2 to 1. Now that the total odds ratio is 12 to 1, I can decide: \\(H_0\\).”\n\nWe see that the judge does not have to know the probabilities \\(Pr(X_{1:n}|H_0)\\) and \\(Pr(X_{1:n}|H_1)\\), only their ratio. This might be a minor point, but this idea of likelihood ratio is quite important. It is like “I don’t know how often you say ‘cat’ but I know that you say it twice as often than I do!”.\nLet \\(T^*\\) be the time it takes for the judge to decide.\n\\[T^* \\approx \\frac{\\ln 10}{L - L_\\infty}\\]\nIntuitively, each token on average moves the log-probability-ratio away from 0 by another \\((L-L_\\infty)\\). Decision is triggered when it finally moves out of the interval \\([-\\ln 10, +\\ln 10]\\).\nWe are not able to simply look at a few tokens, draw a straight line, and call it a day, because the trajectory of log-probability-ratio is much closer to a random walk with drift. Subjectively, if you were a judge and watching the log-probability-ratio moving, you’d see ups and downs, keeping you in suspense, until it finally crosses the decision boundaries.\n\n\nSlowdown factor\nTo perform the SPRT as described, the judge must know intimately the difference between a human and a machine. Can the judge do that? Can anyone know, with certainty, that I would start my speech with “Forty cats …” with a probability that is exactly 32.42 times that of GPT-3?\nAs a crude approximation, we can model real-world judges as slowed-down version of the perfect judge. We can imagine that at each step, instead of updating the log-ratio by\n\\[\n\\ln r_{n+1} \\leftarrow \\ln r_n + \\ln \\frac{Pr(X_{n+1}|H_0, X_{1:n})}{Pr(X_{n+1}|H_1, X_{1:n})}\n\\]\nwe update it by\n\\[\n\\ln r_{n+1} \\leftarrow \\ln r_n + \\frac 1s \\ln \\frac{Pr(X_{n+1}|H_0, X_{1:n})}{Pr(X_{n+1}|H_1, X_{1:n})}\n\\]\nwhere \\(s &gt; 1\\) is the slowdown factor. This implies that if it takes \\(\\sim T\\) tokens for the perfect judge to reach a likelihood ratio of \\(r\\), it would take \\(\\sim sT\\) tokens for a human judge.\n\n\nMeasuring the slowdown factor\nThe slowdown factor \\(s\\) is unknown.\n\nInformed by an internal poll, we enforce a lognormal distribution with a median of 53.1, a 15th percentile estimate of 9.84, and an 85th percentile of 290. (Atkinson 2023)\n\nThe original paper (Barnett and Besiroglu 2023a) contains no estimate of \\(s\\). They did propose to measure it experimentally by running the Turing test with a human judge and two language models. One model \\(H_0\\) “perfectly imitates humans” by simply sampling a random text segment from a corpus, and the other model \\(H_1\\) is a trained language model, finetuned to imitate the same corpus. They claimed that for any piece of text \\(X_{1:n}\\), they can calculate the log-ratio \\(\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\), but I found it difficult: Suppose \\(X_{1:n} = \\text{ technically fork}\\), which is unlikely but possible, yet the phrase never appears in the corpus, what should be \\(Pr(X_{1:n}|H_0)\\)? We can use one of the many smoothing tricks (Jurafsky and Martin 2023, chap. 3), but this gets complicated.\n\nMy ideas\nWhat I think would work well is if both \\(H_0\\)and \\(H_1\\) are language models, perhaps even the same model with different sampling temperatures, then the human judge only has to distinguish the two models.\nPerhaps we can make this into a gambling game. The human subject would be presented with two long outputs from two hidden Markov models. Then the subject becomes the judge of a Turing test: “Are you seeing the output from machine 0 or machine 1?”. At each step, the subject can either pay a few cents of fake money to see another character, or stop and make a bet with the entire bankroll: “I bet 70% of my bankroll on machine 0 and the rest on machine 1!”. Both bets have payoff odds of \\(2:1\\). I believe that if the cost of seeing another character is just right, the subject would be nudged to make a decision at exactly \\(10:1\\) posterior odds ratio on the two hypotheses “machine 0” and “machine 1”.\nA diffusion guessing game: The user uploads a lot of MIDI musics. The program plays back note by note, but adds increasingly severe distortions (add a gaussian or Poisson noise to each note). The user guesses which music it is. The more noise there is, the longer it should take the user to guess. Compare the length with the Bayesian optimal predictor.\n\n\nHuman-or-not\nThere was one large-scale attempt at the Turing test in early 2023, in a game called “Human or Not?” (Jannai et al. 2023). Human participants took 2-minute conversations, and at the end, had to decide whether they were talking to a human or a bot.33 There was no mention of whether the bots had to decide the same question.\n\nThe conversations have a “ping-pong” structure that prevents players from sending two consecutive messages without a response, in order to ensure a balanced and dynamic exchange. Each message, limited to a maximum of 100 characters, has to be composed and sent within a 20-second window, and the chat ends after 2 minutes, usually consisting of 4-5 messages from each side. This ensures that players don’t have to wait for too long, so they can remain engaged with the game and a constant suspense is kept. Once the conversation is over, players are prompted to guess whether their conversational partner was a fellow human or an AI bot. (Jannai et al. 2023)\n\nI counted that during a typical message, each side sends \\([20, 40]\\) English words in total, or \\([30, 50]\\) tokens. In \\([60\\%, 70\\%]\\) of trials, the human participant judged correctly. This suggests that the log-ratio achieved after \\([30, 50]\\) tokens is around the range of \\([\\pm \\ln 6/4, \\pm \\ln 7/3]\\). In other words, the average log-ratio per token is\n\\[\n\\frac{[\\ln 6/4, \\ln 7/3]}{[30, 50]} \\in [0.01, 0.03] \\;\\rm{ nat/token}\n\\]\nThey used several different AI, ranging between Jurassic-2, GPT-4, and Cohere. None of them have published their training compute or loss curves. The only good estimate is for GPT-4, which has training cost \\(C = 2\\times 10^{25}\\rm{FLOP}\\).\nAssuming that Chinchilla scaling holds, average log-ratio per token that an ideal judge should achieve is \\(L - L_\\infty = \\frac{1070}{C^{0.154}} = 0.14 \\;\\rm{ nat/token}\\). Therefore,\n\\[s \\in [5, 14]\\]\nI did not expect the estimate to be nearly symmetric around \\(10\\)."
  },
  {
    "objectID": "essays/posts/perplexity-turing-test/index.html#entropy-of-natural-languages",
    "href": "essays/posts/perplexity-turing-test/index.html#entropy-of-natural-languages",
    "title": "Predicting AGI by the Turing Test",
    "section": "Entropy of natural languages",
    "text": "Entropy of natural languages\nIn Equation 1, we argued that \\(L_\\infty\\) should be interpreted as the entropy rate of the source, usually human-generated English. Unfortunately, unlike that of coin flips or Markov chains, the entropy rate of English cannot be calculated, only estimated. Fortunately, it can be estimated in several ways, and we can check their agreement.\nSince tokenizers are temporary, but English is permanent, we convert all units to \\(\\;\\rm{bit/character}\\) for easy comparison.\n\nChinchilla scaling\nIn the Chinchilla scaling law paper, the authors trained many language models with various sizes from a single architecture family, and fitted a statistical law to the data, giving \\(L_\\infty = 1.69 \\;\\rm{ nat/token}\\) (without error bars, unfortunately) (Hoffmann et al. 2022, 25).\nTo find the effective \\(\\;\\rm{bit/character}\\) for the Chinchilla scaling law, we need to convert \\(\\rm{nat}\\) to \\(\\rm{bit}\\), and \\(\\rm{token}\\) to \\(\\rm{character}\\). The first is easy: \\(1 \\;\\mathrm{bit} = \\ln(2)\\;\\mathrm{nat}\\). The second can be estimated by running a tokenizer over a large natural English corpus. I have estimated this by running the GPT-2 tokenizer on the WikiText-2 corpus, and found that on average,\n\\[\n1 \\;\\rm{token} = 4.5 \\;\\rm{character} = 0.85 \\;\\rm{word}\n\\]\nThus, \\(L_\\infty \\approx \\frac{1.69}{4.5\\times \\ln 2} = 0.54 \\;\\rm{bit/character}\\).\n\n\nTuring-completeness\nUnfortunately, to lower-bound entropy rates, we typically have to make strong assumptions, because in general, lower-bounding entropy is not computable. For example, the entropy of \\(982148086513282306647093844\\dots\\) appears to be \\(\\ln 10\\), but it is in fact zero, because those are the digits of \\(10^{100} \\pi\\) starting at the radix point. In general, this means we have all the hairy problems of measuring the Kolmogorov complexity.\nThere is also another problem: Probabilistically speaking, there is no such thing as the entropy rate of a single sequence. We can convert a single sequence into a stochastic process, by, for example, sampling a random positive integer \\(n\\), then start the sequence at the \\(n\\)-th place. The problem is that we really have two versions of the same word “entropy”, one is by minimal description length, and another is by probability. The two versions are connected by Shannon coding theorem, but they are not the same.\nThis is similar to the Halting problem. It’s possible in general to prove that a machine halts: just run it. It’s not possible in general to prove that it does not halt. Similarly, there is a program \\(P\\), such that given any input \\(x\\), it will output a sequence of numbers \\(P(x)_1, P(x)_2, \\dots\\) that can converge to the Kolmogorov complexity from above: \\(P(x)_1 &gt; P(x)_2 &gt; \\cdots, \\lim_n P(x)_n = K(x)\\). The program just runs every possible Turing machine in parallel.\nThere are two catches.\nOne, it is impossible to know in general whether \\(P(x)_n\\) is the last time it will output, or whether if we wait a few more eternities, we will see another output \\(P(x)_{n+1}\\).\nTwo, it is also impossible to have a program \\(P'\\) that approaches it from below: \\(P(x)_1 &lt; P(x)_2 &lt; \\cdots, \\lim_n P(x)_n = K(x)\\). Otherwise, we would have a machine that can calculate the Busy Beaver function.\n(Feutrill and Roughan 2021) reviews the entropy rate formulas for several commonly used models. An ergodic Markov chain with stationary distribution \\(\\pi_i\\) and transition probabilities \\(p_{ij}\\) has entropy rate \\(-\\sum_{ij}\\pi_i p_{ij}\\ln p_{ij}\\). For the hidden Markov model, there is no known closed-form formula in the transition probabilities, though there are upper and lower bounds (T. M. Cover and Thomas 2006, Theorem 4.5.1).\n\n\nGuessing game\nThe earliest attempt to measure the entropy rate of English is by Shannon himself (Shannon 1951): \\([0.6, 1.3] \\;\\rm{bit/character}\\). He obtained the estimate by presenting human subjects \\(n-1\\) characters from a text, and ask them to guess the next character repeatedly, until they got it right. In this case, the optimal strategy is to construct the \\(n\\)-gram table, and pick the argmax character for the given \\((n-1)\\)-gram, then the arg-next-max, and so on.\nLet \\(N\\) be the total number of characters allowed – Shannon’s experiment used \\(N = 27\\), with 26 lowercase letters and one white space. Let \\(p_k\\) be the frequency that the subject makes exactly \\(k\\) guesses – including the correct guess, so that \\(\\sum_{k=1}^N p_k = 1\\). By convention, \\(p_{N+1} := 0\\). Shannon derived both an upper and a lower bound for the entropy per character:\n\\[\n\\sum_{k=1}^N k(p_k - p_{k+1}) \\ln k \\leq H \\leq - \\sum_{k=1}^N p_k \\ln p_k\n\\]\nThe upper bound is proved by Shannon’s source coding theorem. Taking a human subject, copy it, then they can be used as an encoder-decoder pair.4 The lower bound is not only tricky to prove, but also wrong in general. It is only correct when the human subject is the optimal \\(N\\)-gram predictor, and when the language is exactly generated by an \\(N\\)-gram model.54 It still works even if the humans are pseudorandom. We just have to whisper the same RNG seed into both humans’ ears, and then they would behave in the same pseudorandom way.5 The simplest counterexample: Suppose the source is binary, and satisfies \\(X_{n+1} = X_{n} + 1 \\mod 2\\), so it has zero entropy. Nevertheless, the human intentionally guesses wrong the first time. Therefore, we have \\(p_2 = 1\\), and we have violated the lower bound by \\(2\\ln 2 &gt; 0\\).\nThis source can be made ergodic by adding an \\(\\epsilon\\) amount of coin-flip noise: \\(X_{n+1} = X_{n} + 1 \\mod 2\\) with probability \\(1-\\epsilon\\). This would still give us \\(2\\ln 2 + O(\\epsilon) &gt; O(\\epsilon \\ln \\epsilon)\\).\n(Burton and Licklider 1955) uses the same method as Shannon, but with longer contexts and texts from more books (Shannon sampled all passages from just one book). They found that English has “redundancy” in \\([0.6, 0.8]\\), meaning that English has entropy rate \\(\\ln 27 \\times (1- [0.6, 0.8]) = [0.7, 1.3] \\;\\rm{bit/character}\\).\nOver the years, others devised other methods to estimate this entropy. For example, (T. Cover and King 1978) used a gambling game estimation, in the style of the Kelly criterion. Subjects were required to divide their entire bankroll into 27 differently-sized bets over 27 possibilities (26 letters and 1 whitespace). The right bet pays back 27-fold, and the other bets are lost. Let \\(S_n\\) be the size of bankroll after \\(n\\) rounds of betting, then\n\\[\nH \\leq \\ln 27 - \\limsup_n \\frac 1n \\ln S_n\n\\]\nThey found that \\(H \\leq 1.3 \\;\\rm{bit/character}\\).\nThe guesser does not have to be a human. It can very well be a language model. (Brown et al. 1992) made a simple trigram model over the Brown corpus (600 million words), and found that it gives \\(H \\leq 1.75 \\;\\rm{bit/character}\\). (Behr Jr et al. 2002) used a model that combines multiple n-gram models, giving \\(H \\leq 1.46 \\;\\rm{bit/character}\\).\n\n\nLossless compression\nAnother way to estimate is by lossless compression of a large corpus, since the entropy rate is the lower bound on compression rate. In more detail, if you have a source of information emitting symbols, and its symbol stream has an entropy rate of \\(x \\;\\mathrm{bit/symbol}\\), then it takes at least \\(\\sim xl\\) bits to encode a long segment with \\(l\\) symbols. Furthermore, this lower bound is approachable using the entropy encoding.\nThe Hutter prize is a competition for compressing a \\(10^9\\)-byte corpus from the English Wikipedia (enwik9). For the size of the finished product, both the algorithm and the compressed data must be counted. In particular, if a neural network is used, then the size of the neural network weights must be counted as well.\nThe enwik9 dataset is in XML format, and thus contains a lot of non-English content like &lt;timestamp&gt;2005-12-27T18:46:47Z&lt;/timestamp&gt;. It has \\(10^9\\) bytes. It is tricky to decide how to clean it up to remove all the XML formatting. As a simple estimate, we simply counted its characters:\n\\[\n997,520,891 \\text{ characters} = 1,000,000,000 \\text{ bytes}\n\\]\nTherefore, the entropy rate is\n\\[\n\\frac{8\\times 10^8 / 997,520,891}{\\text{compression ratio}} = \\frac{8.02}{\\text{compression ratio}}\\;\\rm{bit/character}\n\\tag{2}\\]\nThe standard zip algorithm can compress it down to about 300 Mb in size, a compression ratio of \\(\\sim 3\\times\\). Over the years, the progress has been slow but somewhat steady. The current winning entry (Kaido Orav, 2024) has a compression ratio of \\(8.88\\times\\). If we extrapolate the prize-winning entries over the years, it seems that the best possible compression ratio is \\(\\sim 10\\times\\).\nSimilar to the Hutter prize, the Large Text Compression Benchmark also asks for compressing the enwik9 dataset. However, there is no limit to the algorithm runtime or size, so the compression ratio for this benchmark is always higher. Currently (2024-01-19), the maximal compression rate reached is \\(9.35\\times\\) with nncp v3.2, which uses a small Transformer model.\n(Grassberger 2002) used a substitutional compression algorithm with increasingly large codebooks. When the codebook had 6000 codes, the algorithm gave \\(h \\leq 1.82 \\;\\rm{bit/character}\\). By extrapolating the {codebook size}-{entropy rate} curve to an infinitely large codebook, they estimated that English has entropy rate \\(0.7 \\pm 0.2 \\;\\rm{bit/character}\\).\n\n\nSummary\n\n\n\nestimate\nmethod\nraw number\neffective entropy rate (bit/char)\n\n\n\n\n(Grassberger 2002)\ncompression, extrapolation\n\\(0.7 \\pm 0.2 \\;\\rm{bit/character}\\)\n\\(\\sim[0.5, 0.9]\\)\n\n\nHutter prize (Kaido Orav, 2024)\ncompression\ncompression ratio \\(\\geq 8.88\\)\n\\(\\leq 0.90\\)\n\n\nHutter prize extrapolated\ncompression, extrapolation\ncompression ratio \\(\\sim 10\\)\n\\(\\sim 0.80\\)\n\n\nLarge Text Compression Benchmark (nncp v3.2, 2023)\ncompression\ncompression ratio \\(\\geq 9.35\\)\n\\(\\leq 0.86\\)\n\n\n(Shannon 1951)\nguessing game\n\\(\\in [0.6, 1.3] \\;\\rm{bit/character}\\)\n\\(\\in [0.6, 1.3]\\)\n\n\n(Burton and Licklider 1955)\nguessing game\n\\(\\in [0.6, 1.3] \\;\\rm{bit/character}\\)\n\\(\\in [0.7, 1.3]\\)\n\n\n(T. Cover and King 1978)\nguessing game\n\\(\\leq 1.3 \\;\\rm{bit/character}\\)\n\\(\\leq 1.3\\)\n\n\n(Brown et al. 1992)\n3-gram language model\n\\(\\leq 1.75 \\;\\rm{bit/character}\\)\n\\(\\leq 1.75\\)\n\n\n(Behr Jr et al. 2002)\nn-gram language model\n\\(\\leq 1.46 \\;\\rm{bit/character}\\)\n\\(\\leq 1.46\\)\n\n\n(Hoffmann et al. 2022)\nTransformer language model, extrapolation\n\\(L_\\infty = 1.69 \\;\\rm{nat/token}\\)\n\\(\\sim 0.54\\)\n\n\n\nNotably, the above table has mostly upper bounds, and only one dubious lower bound (by Shannon) from 1951. Perhaps lower bounds can be established by using randomness extractors on a large corpus, and checking that the output from the extractor passes pseudorandomness tests.\nMost of the data seems to be centered around 0.8 bpc. The one outlier is the Chinchilla scaling law estimate: 0.54 bpc. I have found that a lot hinges on the exact tokenizer-dataset fit, which is why tokenization is so annoying and I wish people would try to do away with it, or at least report bit-per-character in addition to nat-per-token."
  },
  {
    "objectID": "essays/posts/perplexity-turing-test/index.html#sec-forecasting-agi",
    "href": "essays/posts/perplexity-turing-test/index.html#sec-forecasting-agi",
    "title": "Predicting AGI by the Turing Test",
    "section": "Forecasting AGI",
    "text": "Forecasting AGI\nAccording to the Chinchilla scaling law (Hoffmann et al. 2022), if we have a fixed amount of computing budget \\(C\\), by choosing the model and dataset size correctly, the minimal reducible loss achievable is\n\\[\nL - L_\\infty = \\frac{1070}{(C/\\;\\rm{FLOP})^{0.154}} \\;\\rm{nat/token}\n\\tag{3}\\]\nAssuming a slowdown factor \\(s\\), that the judge decides when the odds ratio is \\(r:1\\), and the Chinchilla scaling law, we have a direct method to predict how long a language model can survive in a Turing test, according to the cost of training compute \\(C\\):\n\\[T^* \\sim \\frac{s\\ln r}{1070}(C/\\;\\rm{FLOP})^{0.154} \\;\\rm{token}\\]\nThis gives, as a rule of thumb, \\(100\\times\\) compute means \\(2 \\times\\) length of survival in a Turing test.\nFor example, assuming a slowdown factor of \\(s=10\\), and that the judge decides when the odds ratio is \\(10:1\\), for a language model to survive for 1000 tokens, it needs\n\\[L - L_\\infty \\leq 10 \\times \\ln 10 / 1000 = 0.023 \\;\\rm{nat/token}\\]\nIf GPT-4 costs \\(2\\times 10^{25} \\;\\rm{FLOP}\\) in compute, and \\(1 \\;\\rm{word} \\approx 1.2 \\;\\rm{token}\\), then\n\\[T^* \\approx 170 \\text{ tokens} \\approx 150 \\text{ words}\\]\nmeaning it has a good chance of passing the Turing test if limited to only 150 words. For context, the Attention is All You Need paper has an abstract that’s 200 tokens long.\nA typical scientific paper is about 4000 words long, which is \\(27\\times\\) that of 150 words, so it would need \\(27^{1/0.153} = (2\\times 10^9)\\times\\) that of compute. Assuming that GPT-4 cost 10 million USD to train, this hypothetical AI would cost \\(2\\times 10^{16}\\) USD, or 200 years of global GDP2023.\nThis implies that the first AGI will not be a scaled-up GPT – autoregressive transformer generatively pretrained on a lightly filtered text dataset. It has to include something else, perhaps multimodal data, high-quality data, better architecture, etc. Even if we were to attempt to merely scale it up, turning earth into a GPT-factory,6 with even 50% of global GDP devoted,7 and with 2% growth rate forever, it would still take 110 years,8 arriving at year 2133. Whole brain emulation would likely take less time.96 Consider this anecdote from Edward Teller:\n\nThe possibilities of developing an atomic weapon and the desirability of doing it secretly were discussed at a Princeton University conference in which I participated in March 1939… Bohr said this rare variety could not be separated from common uranium except by turning the country into a gigantic factory. Bohr was worried that this could be done and that an atomic bomb could be developed–but he hoped that neither could be accomplished. Years later, when Bohr came to Los Alamos, I was prepared to say, “You see…” But before I could open my mouth, he said: “You see, I told you it couldn’t be done without turning the whole country into a factory. You have done just that.” (Teller and Brown 1975)\n\n7 Only in a life-or-death situation does 50% of GDP get devoted to one purpose. For example, that is about the level of GDP devoted to war production during WWII in the major combatant countries. The USA spent 4 trillion USD2011 over 6 years out of an annual GDP of 1.3 trillion USD2011.8 Solve for \\(x\\) in \\(200 = \\sum_{k=0}^x 0.5 \\times 1.02^k\\).9"
  },
  {
    "objectID": "essays/posts/perplexity-turing-test/index.html#sec-ergodic-theory",
    "href": "essays/posts/perplexity-turing-test/index.html#sec-ergodic-theory",
    "title": "Predicting AGI by the Turing Test",
    "section": "Appendix: Ergodic theory",
    "text": "Appendix: Ergodic theory\nSince we used ergodic theory during the essay, we should quickly explain what it is about. This section is foundational, but the full complexity is not necessary.\n\nMeasure-theoretic POV\nI know, you know too, nobody really likes measure theory any more than pianists like practicing scales hundreds of times. Still, it is at the right level of abstraction for many theories, including probability.\nWe omit all mentions of “almost-everywhere”, “except on a set of measure zero”, and similar annoying phrases. As long as you never make a union of uncountable many subsets, you will not be hurt by this omission.\nA probability space is a measurable space with a measure of \\(1\\). We write it as \\((\\Omega, \\mathcal B, Pr)\\), where \\(\\mathcal B\\) is the sigma-algebra of measurable sets, and \\(Pr\\) is the probability measure. We also write \\(\\mu\\) for the measure.1010 Pronounced “mu” – it is a pun because both “mu” and “measure” starts with “m”.\nWe consider a single measurable function \\(T : \\Omega \\to \\Omega\\), and call it the shift map.\nWe demand that \\(T\\) must preserve measure. That is, \\(\\forall S \\in \\mathcal B\\), we have \\(Pr(T^{-1}(S)) = Pr(S)\\).\nA subset is measurable iff it is an element of \\(\\mathcal B\\). A measurable set is also called an event.\nA subset \\(S \\in \\mathcal B\\) is \\(T\\)-invariant iff \\(T^{-1}(S) = S\\) almost everywhere.11 Let \\(\\mathcal I\\) be the set of all \\(T\\)-invariant subsets:11 That is, except on a subset of measure zero: \\(Pr(T^{-1}(S) - S) = 0\\) and \\(Pr(S - T^{-1}(S)) = 0\\). This is the last time we will measure this.\n\\[\n\\mathcal I := \\{S \\in \\mathcal B : T^{-1}(S) = S\\}\n\\]\nNow, obviously any set of measure zero or one are \\(T\\)-invariant. We say that those are trivially \\(T\\)-invariant. We say that \\(T\\) is ergodic iff \\(\\mathcal I\\) has only such trivial subsets. In other words, \\(T\\) is ergodic iff it cannot be factored into two nontrivial chunks:\n\\[\nS, S' \\text{ partitions } \\Omega,\\quad \\text{such that } T^{-1}(S) = S ,\\; T^{-1}(S') = S',\\; Pr(S) &gt; 0 ,\\; Pr(S') &gt; 0\n\\]\nWe usually ask \\(T\\) to also be ergodic, though sometimes we don’t need that.\nErgodic maps have many very good properties. We will use the following one. For the theorem, you can picture it as the real space \\(\\mathbb{R}^n\\) with the gaussian probability distribution, but in fact, it applies for just about everything we would care about, such as the space of English texts, queuing jobs, random walks, etc.1212 Except pathological examples constructed by logicians who have nothing better to do than to care about the continuum hypothesis, large cardinals, and the arithmetic hierarchy. Those who desire the rigor-mortis of logic, let them have it.\n\nTheorem 1 (Dense orbits) If the state space is a topological space with a countable basis, and any nonempty open set has positive measure, then almost any \\(X\\in\\Omega\\) has a dense orbit.\n\n\nProof. Let \\(U\\) be a nonempty open set.\n\\(\\Omega - \\cup_{i \\geq 0} T^{-i}U\\) is \\(T\\)-invariant, and since it excludes \\(U\\), it does not have the full measure. Since \\(T\\) is ergodic, the set actually has zero measure.\nNow, \\(\\cup(\\Omega - \\cup T^{-i}U)\\) is a union of countably many zero-measure sets, so it still has zero measure. By expanding the definition, this is the set of all points with non-dense orbit.\n\nFinally, there is a common theme in ergodic theory. There are rigorous versions of it, but instead of going for rigor, the spirit is more important:\n\nTheorem 2 (ergodic decomposition) Any interesting map is a partition/sum/integral of ergodic maps.\n\nFor example, the shear map on the unit square \\([0, 1]^2\\) defined by\n\\[\n(x, y) \\mapsto (x, x+y \\mod 1)\n\\]\ncan be thought of as an integral over rotations: For each \\(x \\in [0, 1]\\), we have \\(T_x : y \\mapsto x+y\\mod 1\\). For almost all \\(x\\in [0, 1]\\), we have \\(T_x\\) an irrational rotation, thus ergodic.\n\n\nSequence POV\nWe must interpret the language of measure theory, which is dead like chalk dust, back into the language of sequence predictions, which is alive like reinforced concrete.\nEach point in the state space \\(X\\in \\Omega\\) is a text: a stream of tokens infinite both forwards and backwards. The state space \\(\\Omega\\) is the all possible texts \\((X_n)_n\\). We assume that all tokens come from the same finite-size alphabet, for example, the 128 ASCII symbols.\nThe shift map on the state space \\(T : \\Omega \\to \\Omega\\) is defined by moving the origin to the right by one:\n\\[\nT(\\dots, X_{-1}, X_0, X_1, \\dots) := (\\dots, X_0, X_1, X_2, \\dots)\n\\]\nThe shift map is measure-preserving, meaning that the process is stationary: We could have started reading at any point, and we would still expect to see the same kind of probability distribution. It would not be like “Sorry, the word ‘cat’ appears with zero probability when \\(n \\geq 1000\\).”. It would be like “No matter where we start reading, we should expect to the first three tokens to be ‘cat’ with probability \\(10^{-4}\\).”.\nRepeatedly applying the shift map \\(T\\) is just reading through the stream, one token at a time:\n\\[\n\\text{...Lorem ipsum ...} \\mapsto \\text{...orem ipsum d...} \\mapsto \\text{...rem ipsum do...} \\mapsto \\cdots\n\\]\nA periodic point of \\(T\\) is a text that repeats itself like a broken record. For example, \\(X := \\text{... and and and ...}\\) satisfies \\(T^4X = X\\).\nA \\(T\\)-invariant set \\(S\\subset \\Omega\\) is a set of texts, such that if we take any text \\(X\\) from \\(S\\), and jump either forwards or backwards for an arbitrary amount, we get another set in \\(S\\). In other words, \\(S\\) is a set of token streams where there is no origin: you can start reading from any token.\nA probability distribution over \\(\\Omega\\) describes the probability of observing various kinds of text streams.\nIf we can partition \\(\\Omega\\) into two subsets \\(P, Q\\), with probabilities \\(\\epsilon &gt; 0, 1-\\epsilon &gt; 0\\), then it means that any text from \\(P\\) is different from any text from \\(Q\\), after any shift. It is as if there are two languages, and each text can be exclusively written in one language only.\nWe wish to consider only texts created by some imaginary “universal English speaker”. In particular, we do not want it to get stuck in one sub-language of English, then never escape from it. That is, we assume the universal speaker is ergodic.\nNow imagine that we randomly sample two pieces of text generated by the universal speaker, and we shift the first text around to match it against the second. By Theorem 1, the orbit of the first text is dense in the space of all possible English texts spoken by the universal speaker. We can gamify this situation thus:\n\nProver: “I take one piece of text \\(x\\), then another piece \\(x'\\).”.\nChallenger: “I challenge you to find a stretch of text from \\(x\\) that matches the \\(-1000:1000\\) stretch in \\(x'\\).”.\nProver asks a team of immortal monkeys to do the task. A million years later: “At \\(49134819\\).”.\nChallenger verifies that \\(T^{49134819}(x)_{-1000:1000} = (x')_{-1000:1000}\\).\n\n\n\nShannon–McMillan–Breiman\nIf someone has created an infinite sequence of coin flips \\(X_{-\\infty:+\\infty}\\), then revealed it to us one by one, then each reveal would give us \\(1 \\;\\rm{bit} = \\ln 2 \\;\\rm{nat}\\). The long-term average obtained per reveal is still \\(\\ln 2 \\;\\rm{nat}\\), a rather boring situation.\nHow do we measure the entropy of an English speaker? It speaks token by token, and we have to measure the average information we obtain per token. The problem is that there are two senses of “average”. It could be the time-average: we listen to the speaker speak for a very long time, and calculate the entropy in the speech. It could be the ensemble-average: we listen to the speaker speak for a very long time, then do it again, then again, etc, then average together the time-averages.\nIf the speaker is ergodic, then the speaker essentially has just one speech, and any two samples of its speech are just translations of each other. Consequently, it is intuitively clear that with probability 1, the time-average of the entropy of one speech equals the ensemble-average of the entropy of all speeches. Intuitively, with probability 1,\n\\[\n\\frac{1}{n} \\ln Pr(X_{1:n}) \\to \\mathbb{E}\\left[\\frac{1}{n} \\ln Pr(X_{1:n})\\right]\n\\]\nFor non-ergodic speakers. We simply decompose the speaker into an ensemble of ergodic speakers, then apply the SMB theorem to each one. It is like the strong law of large numbers. Intuitively, with probability 1,\n\\[\n\\frac{1}{n} \\ln Pr(X_{1:n}| X \\text{ is type }i)\\to \\mathbb{E}\\left[\\frac{1}{n} \\ln Pr(X_{1:n}) | X \\text{ is type }i\\right]\n\\]\nThis is the Shannon–McMillan–Breiman theorem.\nIn textbooks and Wikipedia, the SMB theorem is stated rigorously, but you have already understood the idea of SMB, and the rigorous versions are simply paraphrases of the idea."
  },
  {
    "objectID": "essays/posts/no-nonsense-algorithmic-bias/index.html",
    "href": "essays/posts/no-nonsense-algorithmic-bias/index.html",
    "title": "The Racial Algorithmic Bias Controversy",
    "section": "",
    "text": "Originally published as No nonsense version of the “racial algorithm bias” — LessWrong.\nIn discussions of algorithm bias, the COMPAS incident of 2016 has been too often quoted out of context. This post gives the facts, and the interpretation, as quickly as possible. See this for details."
  },
  {
    "objectID": "essays/posts/no-nonsense-algorithmic-bias/index.html#the-fight",
    "href": "essays/posts/no-nonsense-algorithmic-bias/index.html#the-fight",
    "title": "The Racial Algorithmic Bias Controversy",
    "section": "The fight",
    "text": "The fight\nThe COMPAS system is a statistical decision algorithm trained on past statistical data on American convicts. It takes as inputs features about the convict and outputs a “risk score” that indicates how likely the convict would reoffend if released.\nIn 2016, ProPublica organization claimed that COMPAS is clearly unfair for blacks in one way. Northpointe replied that it is approximately fair in another way. ProPublica rebukes with many statistical details that I didn’t read.\nThe basic paradox at the heart of the contention is very simple and is not a simple “machines are biased because it learns from history and history is biased”. It’s just that there are many kinds of fairness, each may sound reasonable, but they are not compatible in realistic circumstances. Northpointe chose one and ProPublica chose another."
  },
  {
    "objectID": "essays/posts/no-nonsense-algorithmic-bias/index.html#the-math",
    "href": "essays/posts/no-nonsense-algorithmic-bias/index.html#the-math",
    "title": "The Racial Algorithmic Bias Controversy",
    "section": "The math",
    "text": "The math\nThe actual COMPAS gives a risk score from 1-10, but the phenomenon is clearer when there are only two possibilities instead of 10.\nConsider the toy example where we have a decider (COMPAS, a jury, or a judge) judging whether a group of convicts would reoffend or not. How well the decider is doing can be measured in at least three ways:\n\nFalse negative rate = (false negative)/(actual positive)\nFalse positive rate = (false positive)/(actual negative)\nCalibration = (true positive)/(test positive)\n\nA good decider should have false negative rate close to 0, false positive rate close to 0, and calibration close to 1.\nVisually, we can draw a “square” with four blocks:\n\n\n\na square with 4 blocks\n\n\n\nfalse negative rate = the “height” of the false negative block,\nfalse positive rate = the “height” of the false positive block,\ncalibration = (true positive block)/(total area of the yellow blocks)\n\nNow consider black convicts and white convicts. Now we have two squares. Since they have different reoffend rates for some reason, the central vertical line of the two squares are different.\n\n\n\ntwo squares, one for White, one for Black\n\n\nThe decider tries to be fair by making sure that the false negative rate and false positive rates are the same in both squares, but then it will be forced to make the calibration in the Whites lower than the calibration in the Blacks.\nThen suppose the decider try to increase the calibration in the Whites, then the decider must somehow decrease the false negative rate of Whites, or the false positive rate of Whites.\nIn other words, when the base rates are different, it’s impossible to have equal fairness measures in:\n\nfalse negative rate\nfalse positive rate\ncalibration\n\nOne more thing: Even when base rates are different, there’s a way to have equal fairness measures in all three of those, but it requires the decider to be perfect: Its false positive rate and false negative rate must both be 0, and its calibration must also be 1.\n\nInteractive demonstration\nThe following is an interactive diagram demonstrating the problem. The bars in the squares are draggable. You will find that there is no way to equate all three numbers (true positive rate, true negative rate, calibration), unless you change the vertical bar.\n\n\n\n\n\nA slight generalization\nIn the jargon of fairness measurement, “equal false negative rate and false positive rate” is “parity fairness”; “equal calibration” is just “calibration fairness”. Parity fairness and calibration fairness can be straightforwardly generalized for COMPAS, which uses a 1-10 scoring scale, or indeed any numerical risk score.\nBy routine algebra, in this general case, parity fairness and calibration fairness are incompatible when the base rates are different, and the decider is not perfect. See (Kleinberg, Mullainathan, and Raghavan 2016) for this general case, a literature review, and other real-life occurrences."
  },
  {
    "objectID": "essays/posts/no-nonsense-algorithmic-bias/index.html#the-fight-after-math",
    "href": "essays/posts/no-nonsense-algorithmic-bias/index.html#the-fight-after-math",
    "title": "The Racial Algorithmic Bias Controversy",
    "section": "The fight, after-math",
    "text": "The fight, after-math\nNorthpointe showed that COMPAS is approximately fair in calibration for Whites and Blacks. ProPublica showed that COMPAS is unfair in parity.\nThe lesson is that there are incompatible fairnesses. To figure out which to apply – that is a different question.\nI wrote the essay in 2019, during my undergraduate research on risk measures. From the vantage point of 2024, it certainly feels like algorithmic fairness has lost much of the hotness. Instead of the monthly outrages about how Google’s image algorithm identified some black people as “gorillas” (2015), or the COMPASS bail algorithm (2016), now algorithmic fairness is handled by a separate team, right beside the public relations team, the load balancing team, the fiber optics team, and the data wrangling team.\nThe point being, algorithmic fairness has left the realm of philosophical and political debates and entered the realm of bureaucracy. Fairness is no longer the key to the meaning of life and self-worth, but a matter of passing statistical tests."
  },
  {
    "objectID": "essays/posts/mixture-of-experts/index.html",
    "href": "essays/posts/mixture-of-experts/index.html",
    "title": "Mixture of Experts",
    "section": "",
    "text": "The code for the post is available at moe.ipynb."
  },
  {
    "objectID": "essays/posts/mixture-of-experts/index.html#theory",
    "href": "essays/posts/mixture-of-experts/index.html#theory",
    "title": "Mixture of Experts",
    "section": "Theory",
    "text": "Theory\nMixture of Experts is an old technique dating back to 1991, but it has become a vital component of modern deep learning to get around the memory bottleneck. There is not much theory to speak of, because this is honestly a very simple technique. Let’s say you have a few predictive models. Each model is an expert. Now you take all of them and combine their predictions in some way – that’s mixture of experts.\n\nMixing\nConsider a simple example. Suppose we are to classify points on the \\(\\mathbb{R}^2\\) plane into 2 classes. Suppose that we can only use a single linear-logistic function \\(f(x) = \\frac{1}{1 + e^{w^T x + b}}\\), then we can write down this classifier:\n\\[\n\\hat y := \\begin{cases}\n1, \\quad & \\text{if }f(x) &gt; 0 \\\\\n0 , & \\text{otherwise}\n\\end{cases}\n\\]\nIn other words, we have a logistic regression model.\n\n\n\nAn example of a logistic regression model. The curve shows the estimated probability of passing an exam versus hours studying. If we have to do a binary prediction, then we predict \\(\\hat y = 1\\) iff \\(x \\geq 2.7\\), that is, we predict the student would pass the exam iff they had studied more than 2.7 hours. Figure from Wikipedia\n\n\nLike perceptrons, logistic regression is simple, fast, and has a very elegant theory – and like perceptrons, logistic regression does not work if the underlying system is not linearly separable.\nNow, consider the simplest example that is not linearly separable: a binary classification on the plane. One class falls into the first quadrant, and the other into the other 3 quadrants. There is some noise, so the points near the edges do not always fall into their respective classes. There is no way to perform this task well with just one logistic classifier, but with two, we should be able to perform this task well enough.\n\n\n\nA scatterplot of points that fall into 2 classes that are not linearly separable.\n\n\nLet’s design the 2 experts manually, and somehow combine them. The 2 experts should each handle one of the edges:\n\\[\nf_1(x, y) = \\frac{1}{e^{10 x}+1}, \\quad f_2(x, y) = \\frac{1}{e^{10 y}+1}\n\\]\nIn words, \\(f_1\\) is a smooth approximation to the 0-1 function that sends \\((x, y)\\) to \\(1\\) iff \\(x &gt; 0\\), and similarly, \\(f_2\\) is a smooth approximation to the 0-1 function that sends \\((x, y)\\) to \\(1\\) iff \\(y &gt; 0\\). How do we combine them?\nWe can add another “manager” which is an expert at picking experts. It would pick \\(f_1\\) if the point \\((x, y)\\) falls above the diagonal line \\(x=y\\), and pick \\(f_2\\) otherwise. This would then give us\n\\[\nf(x, y) = \\begin{cases}\nf_1(x, y), \\quad &\\text{if } y-x &gt; 0 \\\\\nf_2(x, y), \\quad &\\text{if } y-x &lt; 0\n\\end{cases}\n\\]\nThis is the simplest example of sparsely-gated MoE. For each point, the manager picks the right expert to call, and call that expert. The other expert does not ever need to be activated, saving half the compute, the manager’s computation is so simple that it does not cost anything compared to the expert’s computation, which contains an exponential.\nWe can also combine the experts by a linear function, as in\n\\[\nf(x, y) = \\sum_{i = 1}^2 p_i(x, y) f_i(x, y)\n\\]\nwhere \\((p_1, p_2)\\) is a probability distribution over the experts that depends on \\((x, y)\\), such as \\(\\mathop{\\mathrm{softmax}}(A(x, y))\\) where \\(A\\) is a linear operator, that is, a matrix. For lack of a better word, I call this dense MoE.\n\n\nSparsifying\nGiven a MoE, there are two ways to use it. One can use it as-is, but then every expert must be consulted on every query, defeating the main purpose of MoE in the age of large models: conditional computing. Therefore, the model should be sparsified.\nIn the first MoE paper (Jacobs et al. 1991), they manually inspected the weights (the matrix \\(A\\) in our notation), and found that some experts would never be called on any input. Then they just removed those experts. This can be understood as sparsification “at compile time”.\nIn the toy model, I trained 6 logistic regression experts to classify 2-dimensional points, so the matrix \\(A\\) has 6 rows and 2 columns. To sparsify the model at compile time to only \\(k\\) experts, I took the matrix \\(A\\) and ranked them according to their L2-norm, found the top-\\(k\\) rows of them, then mask out all the other experts. The resulting heat maps at various levels of \\(k \\in 1:6\\) are as follows.\n\n\n\nHeat maps of the compile-time sparsified MoE at various levels of sparsity.\n\n\nAs expected, when \\(k=1\\), we have only one expert taking care of everything, and end up with a linear classifier. When \\(k=2\\), the sparsified MoE looks much closer to the correct classifier. When \\(k \\geq 3\\), it becomes indistinguishable.\nNow, for the sparsely-gated MoE, the sparsification is done “at runtime”. That is, for each input \\(x\\), we find the top-\\(k\\) experts for this specific \\(x\\), and use those experts:\n\\[w(x) = \\mathop{\\mathrm{softmax}}(\\mathrm{top}_k(Ax))\\]\nwhere \\(\\mathrm{top}_k(v)\\) preserves the top-k entries of \\(v\\), but set all other entries to \\(-\\infty\\). This means we have to keep all experts at runtime, since each expert might be needed for some specific input point, but every input point would only activate a few experts. The key is that the activated experts depend on \\(x\\), unlike the MoE sparsified at compile time, which always activates the same few experts. This means we can achieve a lower sparsity, and less compute. We trade memory for performance and compute.\nIn the same toy model, the resulting heat maps at various levels of \\(k \\in 1:6\\) are as follows.\n\n\n\nHeat maps of the sparsely-gated MoE at various levels of sparsity.\n\n\nCompared with the compile-time sparsified MoE, the sparsely-gated MoE is already usable when \\(k=1\\), and it looks like a piecewise-linear classifier. When \\(k=2\\), it already becomes indistinguishable from the correct classifier."
  },
  {
    "objectID": "essays/posts/mixture-of-experts/index.html#brief-history",
    "href": "essays/posts/mixture-of-experts/index.html#brief-history",
    "title": "Mixture of Experts",
    "section": "Brief history",
    "text": "Brief history\nIn the beginning was the gaussian. The gaussian is a beautiful distribution, with linearity, the central limit theorem, fast inference, least squares regression, and so on. The problem is that it has just one peak.\nIf one wants to model a complicated distribution with several bumps, one can make one step up the staircase of complexity, and build distributions from a linear sum of several gaussians. This is the mixture of gaussians. More generally, simple statistical models like the Poisson distribution, the Bernoulli distribution, and so on, can be added together to create mixture models.\n\n\n\nA mixture of three gaussian bumps. Figure from Wikipedia.\n\n\nA mixture of experts is then a simple generalization, and training a mixture of experts, back in the old days, was mostly thought of as statistical inference. The main problem was simply modelling complex data with a larger family of statistical distribution. Their main worry was that the experts would overfit.\nThey had little data (enough to fit onto a floppy disk), and each expert was usually just a gaussian distribution or a logistic classifier (any more complex and they wouldn’t know how to calculate the integrals and derivatives). Consequently, what they ended up trying to solve was to fit a few thousand datapoints using tens of very simple experts.\nIt is a general fact of classical machine learning that they were very worried about overfitting, and it was a reasonable worry back then, since they had such small datasets (MNIST was in 1994). This, combined with their inability to hand-design learning algorithms for complex machine learning architectures and the slowness of pure gradient descent, meant that machine learning algorithms back then were simple ones fitted onto small datasets.\n\n\n\ncost of\n1980s\n2010s\n2020s\n\n\n\n\ndata\nhigh\nlow\nlow\n\n\nalgorithm\nhigh\nlow\nlow\n\n\ntraining\nlow\nmedium\nhigh\n\n\ninference\nlow\nmedium\nhigh\n\n\n\nThe overall effect is:\n\ngetting training data: expensive (you have to do it yourself);\ndesigning the algorithm: expensive (cheaper if you have graduate students);\ntraining compute: low (there was little funding for training);\ninference compute: very cheap (since you could not train large models).\n\nThis should be compared to the very different situation with deep learning since the 2010s:\n\ngetting training data: cheap (just download it online);\ndesigning the algorithm: cheap (make a standard network, add a few decorations, then use backprop with Adam optimizer);\ntraining compute: as expensive as you want;\ninference compute: as expensive as you want.\n\nWhile classical statistics and machine learning was mainly constrained by how many partial derivatives and integrals the statistician can calculate confidently on paper,1 deep learning is mainly constrained by memory and compute budget.1 If you want a taste of the old days, look at the formulas inside (Jordan and Jacobs 1994). They explicitly calculated the expectation-maximization algorithms for learning a hierarchy of linear experts.\nSo when the deep learning era came circa 2012, people immediately started looking into how to perform conditional computing, that is, to save compute by only calling a small portion of the model. The idea is that you would have different portions of the model be specialized for different forms of input, and for each input, the model would first cheaply find out which expert should handle it, then call upon only the few specialized experts to handle this particular input.\nThe first paper on applying MoE to deep learning was (Eigen, Ranzato, and Sutskever 2013), one year after AlexNet. However, the deep MoE (DMoE) proposed in the paper has no sparsity, and so it has no modern offsprings. Let \\(f_{1, 1}, f_{1, 2}, \\dots, f_{1, n}\\) be \\(n\\) layers with the same architecture. Now, each can bo treated as an expert, and be mixed by\n\\[\nf_1(x) = \\sum_i g_{1, i}(x) f_{1, i}(x)\n\\]\nwhere \\(g_{1, i}\\) is a tiny neural network, the gating network for the first layer. Now, stack multiple such layers, and we obtain the DMoE. As one can see, such a network still has to use all the parameters in each forward pass, and therefore saves no compute. It is simply a case of the dense MoE.\nModern2020s deep learning really arrived with the sparsely-gated MoE (Shazeer et al. 2017), which saves compute. Specifically, if each layer contains \\(8\\) experts, but only \\(2\\) are consulted, then the cost of compute is only about \\(1/4\\) for the full model."
  },
  {
    "objectID": "essays/posts/mixture-of-experts/index.html#why-moe-for-deep-learning",
    "href": "essays/posts/mixture-of-experts/index.html#why-moe-for-deep-learning",
    "title": "Mixture of Experts",
    "section": "Why MoE for deep learning?",
    "text": "Why MoE for deep learning?\nGenerally, one uses a MoE on the frontier, because:\n\nYou really need to push the metric up by a few points.\nYou can’t train a dense model larger than the frontier model, because it simply fails to converge, or the hyperparameter settings for the small models don’t work for the larger one (and you can’t just run a grid search to find it because it costs a million dollars to do a single run).\nYou can train around 10 copies of the frontier model, because while you don’t have the money to do grid search beyond the current frontier, you have the money to train 10 at the frontier.\nYou can’t infer a dense model larger than the frontier one, because one dense model \\(N\\) times as wide would cost you \\(N^2\\) amount of storage and compute, while if you just train \\(N\\) experts, each with roughly the same architecture as the dense model, it would cost you about \\(N\\) amount of storage and about \\(2\\) amount of compute (if only 2 experts are called per question).\nIndeed, if there are too many parameters, then it can’t even be fit onto a good GPU and must be split across GPUs, and then the GPU–GPU communication becomes a serious problem (the “von Neumann bottleneck”).\n\n\n\n\nThe storage hierarchy. Figure from Harvard CS 61: Systems Programming and Machine Organization (2018), Storage 2: Cache model.\n\n\nAll of which are satisfied by Microsoft, Google, etc. This explains why GPT-4 is a MoE made by multiple GPT-3–like models.\nA quick scan of the recent literature shows this, all from Google.\n\nWe present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. (Shazeer et al. 2017)\n\n\nCombining expert, model and data parallelism, we design two large Switch Transformer models, one with 395 billion and 1.6 trillion parameters, respectively. (Fedus, Zoph, and Shazeer 2022)\n\n\nwe demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet. (Riquelme et al. 2021)\n\n(Shazeer et al. 2017) is not the first paper on MoE in the deep learning era, but it is the most important one. It was applied to between “stacked LSTM layers”, because it was published back when neural language models were stacks of LSTM. Nowadays, of course, MoE usually means MoE layers within Transformers, because only with Transformers do people regularly train models with more than 10 billion parameters."
  },
  {
    "objectID": "essays/posts/mixture-of-experts/index.html#load-balancing",
    "href": "essays/posts/mixture-of-experts/index.html#load-balancing",
    "title": "Mixture of Experts",
    "section": "Load balancing",
    "text": "Load balancing\nThe main problem with MoE is a kind of rich-get-richer effect. If at the start of training, some experts are consulted often by random fluctuation, they would be heavily trained by backpropagation, and become even better experts, a upward spiral resulting in a few good experts and many useless experts.\nFor example, in the very first paper on MoE, they trained up to 8 experts to recognize phonemes from 6 Japanese speakers. They found that:\n\nOnly experts 4, 5, and 6 are active in the final mixture. This solution is typical – in all simulations with mixtures of 4 or 8 experts all but 2 or 3 experts had mixing proportions that were effectively 0 for all cases. (Jacobs et al. 1991)\n\nThis might not have been a serious problem in the past, when neural networks were seen as merely a form of high-dimensional statistical model learnable by any one of the typical statistical algorithms (maximal likelihood, Bayesian inference, expectation maximization…), but nowadays, MoE are used because you need to throw more compute at the problem, but cannot afford a larger dense model. In this case, it would defeat the purpose of MoE if some experts end up neglected.\nIt is no coincidence, then, that the sparsely-gated MoE paper (Shazeer et al. 2017) specifically used two auxiliary loss functions to encourage the experts to have equal “weight” over time. It was simplified to just one in the Switch Transformers paper (Fedus, Zoph, and Shazeer 2022).\nSpecifically, consider the sparsely-gated MoE with \\(k=1\\) – where just the top-ranked expert is consulted every time. Let \\(n\\) be the number of experts, and consider a batch of queries \\(\\{x_1, x_2, ..., x_T\\}\\), then the auxiliary loss of the batch is\n\\[\nL := n \\sum_{i=1}^n f_i P_i\n\\]\nwhere \\(f_i=\\frac{1}{T} \\#(\\text{queries sent to expert $i$})\\) is the fraction of time where expert \\(i\\) is ranked highest, and \\(P_i=\\frac{1}{T} \\sum_{j=1}^T w_i\\left(x_j\\right)\\) is the fraction of weight on expert \\(i\\).\nIn the original paper, they claimed that we can obtain the minimal auxiliary loss \\(L\\) at the limit where every expert has equal weight \\(1 / n\\) on all samples, and every expert is ranked the highest equally often.\nPlugging in the equations, we find it is \\(1\\). Unfortunately, this is technically wrong. When there are many experts and large batch, a way to let \\(L\\) approach \\(1/2\\). It is not difficult to show that \\(1/2\\) is the true lower bound. Seeing that Google has been training those huge models since 2017, this definitely works in practice, despite being slightly incorrect.\nThere are plenty of other choices for load balancing, which are rather technical details. For example, the z-loss stabilizes mixed-precision training by discouraging logits that are too far from zero, avoiding large round-off errors (Zoph et al. 2022, secs. 3.3–3.4)."
  },
  {
    "objectID": "essays/posts/mixture-of-experts/index.html#sec-appendix-load-balancing-error",
    "href": "essays/posts/mixture-of-experts/index.html#sec-appendix-load-balancing-error",
    "title": "Mixture of Experts",
    "section": "Appendix: Error in load balancing",
    "text": "Appendix: Error in load balancing\n\nLet one expert get \\(1/2 - \\epsilon\\) on every question, but is never consulted on anything, and let every other \\(n-1\\) expert evenly divide the rest of the questions. For example, this is how the weights should be distributed when there are 3 experts on 4 questions:\n\\[\n\\begin{bmatrix}\n\\frac 12 - \\epsilon & \\frac 12 + \\epsilon & 0 \\\\\n\\frac 12 - \\epsilon & \\frac 12 + \\epsilon & 0 \\\\\n\\frac 12 - \\epsilon & 0 & \\frac 12 + \\epsilon \\\\\n\\frac 12 - \\epsilon & 0 & \\frac 12 + \\epsilon \\\\\n\\end{bmatrix}\n\\]\ngiving \\(L = \\frac 34 (1+2\\epsilon)\\). By generalizing this construction, when there are many experts and large batch, we have \\(L \\to 1/2\\). It is not difficult to show that \\(1/2\\) is the true lower bound.\nWith the global optimization method of dual annealing2, Python found something close to the true lower bound, as shown in the figure. The load balancing matrix has a bright strip of \\(1/2 - \\epsilon\\), and slightly brighter dots of \\(1/2+\\epsilon\\) jumping around the matrix, as expected.2 I tried using local optimization with SciPy’s minimize, but it always fails to converge to \\(\\sim 1/2\\). It even fails to converge to \\(\\sim 1\\). Indeed, often it just moves around the initial point a bit then immediately gives up and stops. My suspicion is that the loss landscape is too jagged.\n\n\n\nThe result of minimizing load-balancing loss, with 10 experts and 10 questions."
  },
  {
    "objectID": "essays/posts/cybernetic-artificial-intelligence/index.html",
    "href": "essays/posts/cybernetic-artificial-intelligence/index.html",
    "title": "Cybernetic artificial intelligence",
    "section": "",
    "text": "In the creation lore of artificial intelligence, John McCarthy coined the term “artificial intelligence” to distinguish his approach from “cybernetics”, which was dominated by Norbert Wiener. However, if that is the case, then we can infer that there was a real danger that artificial intelligence could have been confused with Norbert Wiener’s cybernetics. That is, they must have some real similarity. If the similarity is not in style or approach – as otherwise there would be no need to distinguish the two – then the similarity must be in goal.\nAnd indeed there is. I was first alerted to this by Wiener’s little book (Wiener 1964, chap. 4), which briefly sketched out a machine that could purportedly imitate any machine, including another copy of itself, thus effecting both machine learning and machine reproduction. Unfortunately, it attempted to be a popular science book and explained concepts with a minimum of mathematical symbols, thus providing the reader with a minimum of understanding. Nevertheless, I figured it out by consulting his other books, and so I decided to write this post for posterity, as a simple sketch of another way to think about machine intelligence and machine life. A view of another universe, perhaps, or the theoretical basis of some hard sci-fi to be written.\nThis cybernetic machine is a rather obscure one, and most of the references are decades-old. The main references I relied on to write this essay are (Wiener 1958, chaps. 10, 11; 2019, chap. 9; O’Hagan 2013; G. H. Harris and Lapidus 1967).\nWiener’s terminology is occasionally obsolete; unfortunately, the subject of cybernetic artificial intelligence has essentially never been updated since Wiener, so anyone trying to study the subject must contend with his publications. So here is a list of obsolete terminology and my explanations.\n\nshot noise, shot effect, tube noise: white noise. It is not Poisson noise.\nchaos: a generic term for randomness, or random variables. It is not chaos theory.\nvoltage multiplier: an electronic device that multiplies two voltages together, by \\((x, y) \\mapsto xy\\). It is not the voltage multiplier.\n\n\n\nAccording to Wiener, cybernetics is the science of control and communication in the animal and the machine. To be more precise, he used the following model of sweeping generality: the nonlinear transducer model.\n\nDefinition 1 (signal) A signal is a function of time, written as \\(x(t)\\), where \\(t \\in \\mathbb{R}\\). The signal can take values in any space, but for convenience, we will only consider the case where \\(x\\) is real-valued. That is, we only consider functions of type \\(x : \\mathbb{R}\\to \\mathbb{R}\\).\n\nA transducer transduces, meaning that it takes in a signal and outputs a signal. Antennas, filters, circuits of resistors and capacitors, and essentially electronic devices that do not require power input to run, are transducers. Wiener considered only deterministic, causal transducers, meaning that the output of a transducer at time \\(t\\) is determined by the inputs during the period \\((-\\infty, t]\\).\nIn large sections of electronic engineering, electronic circuits are studied as either linear and continuous, or as nonlinear and discrete. Nonlinear but continuous devices are very difficult to study in general, but Wiener took on it and constructed a theory for general nonlinear continuous transducers.\n\nDefinition 2 (nonlinear transducer)  \n\nThe transducer is a function \\(T\\) that, given any real-valued function \\(x : \\mathbb{R}\\to \\mathbb{R}\\), returns a real-valued function \\(T[x] : \\mathbb{R}\\to \\mathbb{R}\\). In other words, it sends real-valued signals to real-valued signals.1\nThe transducer’s output at any time \\(t\\) is determined by the value of \\(x\\) on the interval \\((-\\infty, t]\\). In other words, it is causal and deterministic.\nThe transducer is stationary in time. That is, if \\(x\\) is the same over the interval \\((-\\infty, t]\\) as \\(x'\\) over the interval \\((-\\infty, t']\\), then \\(T[x](t) = T[x'](t')\\).\nThe transducer has a limited amount of memory, so that its dependence on the high-frequency details of the input signal decreases rapidly as frequency increases. We will explain the meaning of this assumption later.\nThe transducer depends analytically on the frequencies of the input signal. We will explain the meaning of this assumption later.\n\n1 Wiener actually studied general multidimensional signals, but those mostly involve notational complications, with no new ideas."
  },
  {
    "objectID": "essays/posts/cybernetic-artificial-intelligence/index.html#cybernetics",
    "href": "essays/posts/cybernetic-artificial-intelligence/index.html#cybernetics",
    "title": "Cybernetic artificial intelligence",
    "section": "",
    "text": "In the creation lore of artificial intelligence, John McCarthy coined the term “artificial intelligence” to distinguish his approach from “cybernetics”, which was dominated by Norbert Wiener. However, if that is the case, then we can infer that there was a real danger that artificial intelligence could have been confused with Norbert Wiener’s cybernetics. That is, they must have some real similarity. If the similarity is not in style or approach – as otherwise there would be no need to distinguish the two – then the similarity must be in goal.\nAnd indeed there is. I was first alerted to this by Wiener’s little book (Wiener 1964, chap. 4), which briefly sketched out a machine that could purportedly imitate any machine, including another copy of itself, thus effecting both machine learning and machine reproduction. Unfortunately, it attempted to be a popular science book and explained concepts with a minimum of mathematical symbols, thus providing the reader with a minimum of understanding. Nevertheless, I figured it out by consulting his other books, and so I decided to write this post for posterity, as a simple sketch of another way to think about machine intelligence and machine life. A view of another universe, perhaps, or the theoretical basis of some hard sci-fi to be written.\nThis cybernetic machine is a rather obscure one, and most of the references are decades-old. The main references I relied on to write this essay are (Wiener 1958, chaps. 10, 11; 2019, chap. 9; O’Hagan 2013; G. H. Harris and Lapidus 1967).\nWiener’s terminology is occasionally obsolete; unfortunately, the subject of cybernetic artificial intelligence has essentially never been updated since Wiener, so anyone trying to study the subject must contend with his publications. So here is a list of obsolete terminology and my explanations.\n\nshot noise, shot effect, tube noise: white noise. It is not Poisson noise.\nchaos: a generic term for randomness, or random variables. It is not chaos theory.\nvoltage multiplier: an electronic device that multiplies two voltages together, by \\((x, y) \\mapsto xy\\). It is not the voltage multiplier.\n\n\n\nAccording to Wiener, cybernetics is the science of control and communication in the animal and the machine. To be more precise, he used the following model of sweeping generality: the nonlinear transducer model.\n\nDefinition 1 (signal) A signal is a function of time, written as \\(x(t)\\), where \\(t \\in \\mathbb{R}\\). The signal can take values in any space, but for convenience, we will only consider the case where \\(x\\) is real-valued. That is, we only consider functions of type \\(x : \\mathbb{R}\\to \\mathbb{R}\\).\n\nA transducer transduces, meaning that it takes in a signal and outputs a signal. Antennas, filters, circuits of resistors and capacitors, and essentially electronic devices that do not require power input to run, are transducers. Wiener considered only deterministic, causal transducers, meaning that the output of a transducer at time \\(t\\) is determined by the inputs during the period \\((-\\infty, t]\\).\nIn large sections of electronic engineering, electronic circuits are studied as either linear and continuous, or as nonlinear and discrete. Nonlinear but continuous devices are very difficult to study in general, but Wiener took on it and constructed a theory for general nonlinear continuous transducers.\n\nDefinition 2 (nonlinear transducer)  \n\nThe transducer is a function \\(T\\) that, given any real-valued function \\(x : \\mathbb{R}\\to \\mathbb{R}\\), returns a real-valued function \\(T[x] : \\mathbb{R}\\to \\mathbb{R}\\). In other words, it sends real-valued signals to real-valued signals.1\nThe transducer’s output at any time \\(t\\) is determined by the value of \\(x\\) on the interval \\((-\\infty, t]\\). In other words, it is causal and deterministic.\nThe transducer is stationary in time. That is, if \\(x\\) is the same over the interval \\((-\\infty, t]\\) as \\(x'\\) over the interval \\((-\\infty, t']\\), then \\(T[x](t) = T[x'](t')\\).\nThe transducer has a limited amount of memory, so that its dependence on the high-frequency details of the input signal decreases rapidly as frequency increases. We will explain the meaning of this assumption later.\nThe transducer depends analytically on the frequencies of the input signal. We will explain the meaning of this assumption later.\n\n1 Wiener actually studied general multidimensional signals, but those mostly involve notational complications, with no new ideas."
  },
  {
    "objectID": "essays/posts/cybernetic-artificial-intelligence/index.html#orthogonal-functions",
    "href": "essays/posts/cybernetic-artificial-intelligence/index.html#orthogonal-functions",
    "title": "Cybernetic artificial intelligence",
    "section": "Orthogonal functions",
    "text": "Orthogonal functions\nIn order to present Wiener’s approach to nonlinear control theory, we need a small amount of the theory of orthogonal polynomials. Specifically, we need the Hermite and Laguerre polynomials. They are not as famous as the trigonometric functions, but they are used in the same way as trigonometric functions in Fourier analysis. In Fourier analysis, every well-behaved function (in the Fourier sense) is decomposable as an infinite linear sum of trigonometric functions. Similarly, every well-behaved function (in the Hermite sense) is decomposable into an infinite linear sum of Hermite functions, and the same applies to Laguerre functions.\n\nLaguerre functions\nThe Laguerre polynomials have many equivalent definitions. We will use three of those.\n\nDefinition 3 (Laguerre polynomials) Direct definition:\n\\[\nL_n(x) := \\sum_{k} \\binom{n}{k}\\frac{(-1)^k}{k!} x^k\n\\tag{1}\\]\nDefinition by a generating function:\n\\[\ng(t, x) = \\sum_{n=0}^\\infty t^n L_n(x)=  \\frac{1}{1-t} e^{-tx/(1-t)}\n\\tag{2}\\]\nDefinition by ordinary differential equation:\n\\[\n\\begin{cases}\nxL_n'' + (1-x) L_n' + nL_n &= 0 \\\\\nL_n(0) &= 1 \\\\\nL_n'(0) &= -n\n\\end{cases}\n\\tag{3}\\]\n\n\nProposition 1 The three definitions are equivalent.\n\n\nProof. The direct definition of \\(L_n\\) satisfies \\(xL_n'' + (1-x) L_n' + nL_n = 0\\), and has the same value and first derivative at \\(x=0\\). By the uniqueness theorem of ordinary differential equations, the two definitions are equal.\nBy taking the first partial derivatives \\(\\partial_x g, \\partial_t g\\), and simplifying, we obtain some recurrence relations of \\(L_n\\), including \\(xL_n'' + (1-x) L_n' + nL_n = 0\\). Plugging in the case where \\(x=0\\), we obtain \\(L_n(0) = 1\\) and \\(L_n'(0) = -n\\).\n\n\nProposition 2 (Laguerre polynomials are orthogonal with respect to the exponential distribution) \\[\n\\int_{0}^\\infty e^{-x}L_m(x) L_n(x)dx = \\delta_{mn}\n\\]\n\n\nProof. Explicitly integrate\n\\[\\int_{0}^\\infty e^{-x} g(t, x)  g(s, x) dx\\]\nthen expand the Taylor series of both sides in powers of \\(s, t\\).\n\n\nDefinition 4 (Laguerre functions) \\[\n\\psi_n(t) := e^{-t/2}L_n(t)\n\\]\n\nThe Laguerre functions make the orthogonality cleaner:\n\\[\n\\int_0^\\infty \\psi_n(t)\\psi_m(t) dt = \\delta_{mn}\n\\tag{4}\\]\nJust like in Fourier analysis, by virtue of orthogonality, we can represent any well-behaved function on \\((-\\infty, t]\\) as an infinite sum of Laguerre functions\n\\[\nf(t-\\tau) = \\sum_{n \\geq 0} c_n \\psi_n(\\tau)\n\\]\nby taking a convolution with the Laguerre functions\n\\[\nc_n = \\int_0^\\infty f(t-\\tau) \\psi_n(\\tau) d\\tau.\n\\]\n\n\nHermite polynomials\n\nDefinition 5 (physicist’s Hermite polynomials) \\[\n\\sum_n H_n(x) \\frac{1}{n!}t^n = e^{-t^2 + 2tx} = g(t, x)\n\\tag{5}\\]\n\n\nProposition 3 (Hermite polynomials are orthogonal with respect to the normal distribution with variance 1/2) \\[\\int e^{-x^2}H_n(x) H_m(x) dx = \\sqrt\\pi 2^n  n! \\delta_{mn}\\]\n\n\nProof. \\[\\int e^{-x^2} g(t, x)g(s, x) dx = \\sum_{n, m \\geq 0}\\frac{1}{n! m!}t^ns^m \\int e^{-x^2} H_n(x) H_m(x) dx\\]\nDirectly compute the left-hand side and find that it equals \\(\\sqrt\\pi e^{2st}\\). Now expand it in powers of \\(s\\) and \\(t\\)."
  },
  {
    "objectID": "essays/posts/cybernetic-artificial-intelligence/index.html#learning-and-reproducing-any-transducer",
    "href": "essays/posts/cybernetic-artificial-intelligence/index.html#learning-and-reproducing-any-transducer",
    "title": "Cybernetic artificial intelligence",
    "section": "Learning and reproducing any transducer",
    "text": "Learning and reproducing any transducer\nNow we are ready to perform the “Hermite–Laguerre expansion”, Wiener’s way to analyze (learn) and synthesize (reproduce) arbitrary transducer using pure analog devices.\n\nAlgebra of analog circuitry\nIn an analog electronic circuit, real numbers are represented as voltages across two points (“ports”) in the circuit. Adding is as simple as making a serial connection. Negation is even simpler: just connect the ports in the opposite direction. Multiplication is significantly trickier, but it can be done. There are electronic devices with nonlinear response characteristics, meaning that they have two input ports and two output ports, and if you apply an input voltage \\(x\\) across one such device, the output voltage would be \\(f(x)\\) where \\(f\\) is not a linear function. Now suppose that \\(f(x) = x^2\\).22 From our vantage point, the universal approximation theorems proven in the early 1990s show that, generically, if we have any nonlinear function \\(f_0\\) at all, then we can construct any activation function \\(f\\) as a neural network, by using many copies of the \\(f_0\\) device as activation functions and linear devices as weights and biases.\nWith such an \\(f\\), we can multiply two voltages by \\(xy = (f(x+y) - f(x) - f(y)) \\times 0.5\\), and so we can construct any polynomial function in any number of variables. That is, we can do algebra by analog devices, as long as we have a voltage multiplier.\nOf course, we don’t hear about voltage multipliers often, and this is no accident – it is quite difficult to get one with good performance. In the preface to the second edition of Cybernetics1961 (Wiener 2019, xli), Wiener waxes praise about Gabor’s breakthrough circuit device that could multiply two voltages at a frequency of \\(1\\; \\mathrm{kHz}\\):\n\nWhile there are many approaches to the problem of multiplying two functions electrically, this task is not technically easy. On the one hand, a good multiplier must work over a large range of amplitudes. On the other hand, it must be so nearly instantaneous in its operation that it will be accurate up to high frequencies. Gabor claims for his multiplier a frequency range running to about 1,000 cycles. … he does not state explicitly the amplitude range over which his method of multiplication is valid nor the degree of accuracy to be obtained. I am awaiting very eagerly3 an explicit statement of these properties so that we can give a good evaluation of the multiplier for use in other pieces of apparatus dependent on it.3 Gabor published it in the same year of 1961: (Gabor, Wilby, and Woodcock 1961).\n\nTo our modern ears, multiplying two voltages 1000 times a second by analog means seems simultaneously astonishing and obsolete. Intel 8086 in 1976 already could multiply a million times a second, and whatever has come of Gabor’s universal filter? It seems to me that Wiener never accepted the future of digital computers, preferring the concrete certainty of magnetic cores and electric wires.\n\n\nThe Laguerre filter bank\nTo find the Laguerre coefficients of a signal, we need to perform a convolution. Convolutions become products after a Laplace transform, so we need to find the Laplace transform of the Laguerre functions \\(\\psi_n = e^{-x/2}L_n(x)\\). Fortunately, it is easy to compute. We simply read from a standard table:\n\\[\n\\mathcal L [t^n e^{-\\alpha t}\\theta(t)] = \\frac{n!}{(s+\\alpha)^{n+1}}\n\\]\nwhere \\(\\theta(t) = 1_{t \\geq 0}\\) is the zero-one step function.\nThen, since the Laplace transform is linear, we have after simplification\n\\[\n\\mathcal L[\\psi_n\\theta] = \\frac{1}{s+1/2}\\left(\\frac{s-1/2}{s+1/2}\\right)^n\n\\]\nThis gives a simple filter bank that constructs the Laguerre coefficients for any signal. The input signal passes through a \\(\\frac{1}{s+1/2}\\) filter to obtain the \\(c_0\\) coefficient, and then through a \\(\\frac{s-1/2}{s+1/2}\\) filter to obtain the \\(c_1\\) coefficient, and so on. This filter bank can be constructed with standard resistors and capacitors.\nThe following theorem finishes the last piece of the puzzle. (G. H. Harris and Lapidus 1967) claims that the proof is found in (Bose 1956; George Henry Harris 1966), but I did not check.\n\nTheorem 1 Let \\(x(t)\\) be a white noise process with variance \\(1/2\\), and let \\(c_0(t), c_1(t), \\dots\\) be its Laguerre coefficients, then:\n\nthe joint stochastic process \\((c_0(t), c_1(t), \\dots)\\) is stationary;\nfor any fixed \\(t\\in \\mathbb{R}\\), the random variables \\(c_0(t), c_1(t), \\dots\\) are independent samples of the standard gaussian distribution \\(\\mathcal N(0, 1/2)\\).\n\n\n\n\nThe Hermite coefficients\nFor a given input signal \\(x : \\mathbb{R}\\to \\mathbb{R}\\), we pass it into the Laguerre filter bank. The readouts from the filter bank are the signals \\(c_0(t), c_1(t), \\dots\\). They satisfy the equation\n\\[\nx(t - \\tau) = \\sum_{n\\geq 0} c_n(t) \\psi_n(\\tau), \\; \\forall \\tau \\geq 0\\quad \\forall t \\in \\mathbb{R}\n\\]\nIn words, at any cut-off time \\(t \\in \\mathbb{R}\\), the signal we have seen so far is \\(x(t - \\tau)\\) with \\(\\tau \\geq 0\\). This signal can then be decomposed as a linear sum of Laguerre functions \\(\\sum_{n\\geq 0} c_n(t) \\psi_n(\\tau)\\), with \\(c_n(t)\\) as the Laguerre coefficients. The coefficients depend on the cut-off time \\(t\\), but do not depend on \\(\\tau\\), which is not “real” time, but only a kind of “relative historical time”, as we look into the past standing at time \\(t\\).\nA transducer, by our assumption, is deterministic and causal, so that \\(T[x](t)\\) is a deterministic function of the signal we have seen so far, and so it is a deterministic function of \\(c_0(t), c_1(t), c_2(t), \\dots\\). Note carefully that it is determined by \\(c_0(t), c_1(t), c_2(t), \\dots\\) at this very instant \\(t\\). It does not need the values of \\(c_0(t'), c_1(t'), c_2(t'), \\dots\\) at any \\(t' \\neq t\\). We write it as follows:\n\\[\nT[x](t) = T(c_0(t), c_1(t), c_2(t), ...)\n\\]\nBy our assumption that the transducer has a limited memory, we should be able to ignore the higher frequency components of the input signal, and still recover a good approximation of \\(T\\). That means that \\(T[x](t) = T(c_0(t), c_1(t), c_2(t), \\dots) \\approx T(c_0(t), \\dots, c_n(t))\\), with the approximation increasing in accuracy as \\(n\\) increases.\nBy our assumption that the transducer is analytic with respect to the input, \\(T(c_0(t), \\dots, c_n(t))\\) has a multivariate Hermite serial expansion (the same idea as multivariate Taylor expansion):\n\\[\nT(c_0(t), \\dots , c_n(t)) = \\sum_{m_0, \\dots , m_n \\geq 0} T_{m_0, \\dots , m_n} H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t))\n\\]\nWe are quite close to the target. We can compute the Laguerre coefficients \\(c_n(t)\\) of any input signal by the Laguerre filter bank. We can construct analog circuits that compute \\(H_m(c_n(t))\\), the Hermite polynomial values of the Laguerre coefficients. The remaining challenge is to determine the coefficients \\(T_{m_0, \\dots, m_n}\\).\nThis is where Theorem 1 comes to finish the construction. Let \\(x(t)\\) be a white noise process, then since\n\\[\nT[x](t) \\approx \\sum_{m_0, \\dots , m_n \\geq 0} T_{m_0, \\dots , m_n} H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t))\n\\]\nand since the Laguerre coefficients are independent samples of the standard gaussian, we have\n\\[\nT_{m_0, \\dots , m_n} \\approx \\mathbb{E}\\left[T[x](t)\\; H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t))\\right]\n\\]\nwhere the expectation is in the sense of ensemble expectation. That is, we would run this experiment once with a white noise process, freeze it exactly at the moment \\(t\\), then run it again with another white noise process, freeze it exactly at the moment \\(t\\), and so on. Then we average over all these experiments.\nHowever, Theorem 1 states that the Laguerre coefficients are stationary, meaning that we have ergodicity4: the ensemble average is the time average, and so4 Wiener was really into ergodic theory.\n\\[\nT_{m_0, \\dots , m_n} \\approx \\lim_{T \\to \\infty} \\frac{1}{T} \\int_0^T T[x](t)\\; H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t)) dt\n\\]\nThe integrand is computable by the analog devices we described. The integration-and-averaging can be done with a very-low-pass filter – taking the average is essentially passing only the zero-frequency signal, thus it is the low-pass filter with the lowest possible passband. Finally, since white noise is all around us, it can be obtained in many ways, such as by amplifying the thermal noise in a resistor.\nAnd so we have a finished machine, where the white noise \\(x\\) and the signal to imitate \\(T[x](t)\\) come in, and the fitted parameters \\(T_{m_0, \\dots, m_n}\\) come out. The fitted parameters can be automatically read and adjusted by electromechanical devices, such as relays and step motors, allowing us to connect the machine in parallel with an unknown transducer, run it for a period over a white noise input, and ultimately achieve a machine that precisely imitates the unknown transducer.\n\n\n\nThe fully-formed imitation machine. (G. H. Harris and Lapidus 1967, fig. 2)"
  },
  {
    "objectID": "essays/posts/cybernetic-artificial-intelligence/index.html#the-science-of-control-and-communication-in-the-animal-and-the-machine",
    "href": "essays/posts/cybernetic-artificial-intelligence/index.html#the-science-of-control-and-communication-in-the-animal-and-the-machine",
    "title": "Cybernetic artificial intelligence",
    "section": "The science of control and communication in the animal and the machine",
    "text": "The science of control and communication in the animal and the machine\nWe have reached the end of the road, facing an all-analog general-purpose learning machine. This machine can imitate any black-box transducer, and thus is a form of machine learning. If we have two such machines, and randomly set the parameters of one machine, then the other machine would learn to imitate the same behavior. And since each parameter setting creates a different behavior, purely by imitating behavior, the parameters would be copied from one machine to the other. This is an explicit construction for how behaviorism can work, even if not in our universe, then in another universe where the animals really are those imitation devices.\nAs Wiener speculated (Wiener 2019, 248–49), biological learning and reproduction are “philosophically similar” to this machine:\n\nWhile both Professor Gabor’s methods and my own lead to the construction of nonlinear transducers, they are linear to the extent that the nonlinear transducer is represented with an output which is the sum of the outputs of a set of nonlinear transducers with the same input. These outputs are combined with varying linear coefficients. This allows us to employ the theory of linear developments in the design and specification of the nonlinear transducer. And in particular, this method allows us to obtain coefficients of the constituent elements by a least-square process. If we join this to a method of statistically averaging over the set of all inputs to our apparatus, we have essentially a branch of the theory of orthogonal development. Such a statistical basis of the theory of nonlinear transducers can be obtained from an actual study of the past statistics of the inputs used in each particular case. I ask if this is philosophically very different from what is done when a gene acts as a template to form other molecules of the same gene from an indeterminate mixture of amino and nucleic acids, or when a virus guides into its own form other molecules of the same virus out of the tissues and juices of its host. I do not in the least claim that the details of these processes are the same, but I do claim that they are philosophically very similar phenomena.\n\nIt seems like this device, as it stands, would be plagued by the same issues that plague a general analog computer – error correction, bad gains, and intractable nonlinearities. Still, it stands as a vision of an alternative future in an alternative world, if not an alternative future of our world."
  },
  {
    "objectID": "essays/index.html",
    "href": "essays/index.html",
    "title": "Essays",
    "section": "",
    "text": "Analytical mechanics\n\n\n\n\n\n\nmath\n\n\nphysics\n\n\nphilosophy\n\n\n\nWhat every graduate student should know about analytical mechanics, delivered with economic style and many illustrations. Particular focus on particle-wave duality and old quantum theory. Prerequisite: multivariate calculus and mathematical maturity.\n\n\n\n\n\n2024-05-04\n\n\n111 min\n\n\n2024-05-05\n\n\n\n\n\n\n\n\n\n\n\n\nClassical thermodynamics and economics\n\n\nTwo sides of the same coin\n\n\n\nmath\n\n\nphysics\n\n\nphilosophy\n\n\neconomics\n\n\n\n\n\n\n\n\n\n2024-04-17\n\n\n119 min\n\n\n2024-05-08\n\n\n\n\n\n\n\n\n\n\n\n\nHow to do renormalization\n\n\n\n\n\n\nmath\n\n\nphysics\n\n\nscaling\n\n\n\nSurvival guide for renormalization (RN) theory. How to do it in real space and frequency space. What it all means. Should have something for everyone, from mathematicians to physicists to outsiders, from serious series to pretty pictures. Intended audience: those with two years of undergrad mathematics.\n\n\n\n\n\n2024-04-11\n\n\n75 min\n\n\n2024-05-05\n\n\n\n\n\n\n\n\n\n\n\n\nMixture of Experts\n\n\n\n\n\n\nAI\n\n\nscaling\n\n\n\nHow MoE works, its history, and what it is good for.\n\n\n\n\n\n2024-01-23\n\n\n14 min\n\n\n2024-01-23\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting AGI by the Turing Test\n\n\n\n\n\n\nAI\n\n\nscaling\n\n\nmath\n\n\n\nMinimizing log-perplexity loss is equivalent to maximizing survival length in a Turing test. Assuming compute-loss scaling law, a scaled-up GPT that produces human-like science papers would cost ~200 years of global GDP.\n\n\n\n\n\n2024-01-20\n\n\n29 min\n\n\n2024-02-19\n\n\n\n\n\n\n\n\n\n\n\n\nThe Perceptron Controversy\n\n\n\n\n\n\nAI\n\n\nhistory\n\n\n\nConnectionism died in the 60s from technical limits to scaling, then resurrected in the 80s after backprop allowed scaling. The Minsky–Papert anti-scaling hypothesis explained, psychoanalyzed, and buried.\n\n\n\n\n\n2024-01-01\n\n\n108 min\n\n\n2024-05-04\n\n\n\n\n\n\n\n\n\n\n\n\nThe Backstory of Backpropagation\n\n\n\n\n\n\nAI\n\n\nmath\n\n\nphysics\n\n\nhistory\n\n\n\nWhy backprop was resisted for 20 years: assumption of discretely spiking neurons, goal of synthesizing Boolean logic, fear of local optima, and bad luck. Werbos has the best claim for invention.\n\n\n\n\n\n2023-12-26\n\n\n54 min\n\n\n2024-01-23\n\n\n\n\n\n\n\n\n\n\n\n\nCybernetic artificial intelligence\n\n\n\n\n\n\nAI\n\n\ncybernetics\n\n\nmath\n\n\nhistory\n\n\n\nMachine learning and self-reproduction according to Norbert Wiener.\n\n\n\n\n\n2023-12-23\n\n\n15 min\n\n\n2024-01-21\n\n\n\n\n\n\n\n\n\n\n\n\nReading Perceptrons\n\n\n\n\n\n\nAI\n\n\nmath\n\n\n\nA long, hard stare into the math of Perceptrons, the mythical neural network killer.\n\n\n\n\n\n2023-12-21\n\n\n25 min\n\n\n2024-01-23\n\n\n\n\n\n\n\n\n\n\n\n\nFermi Estimation for Neural Networks\n\n\n\n\n\n\nAI\n\n\neconomics\n\n\n\nThe bitter lesson in bite-sized packets.\n\n\n\n\n\n2023-12-05\n\n\n30 min\n\n\n2024-01-21\n\n\n\n\n\n\n\n\n\n\n\n\nNeural scaling law by data manifold dimensions\n\n\n\n\n\n\nAI\n\n\nscaling\n\n\n\nNeural networks scale they way they do, purely because of data.\n\n\n\n\n\n2023-11-01\n\n\n8 min\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\nGrokking modular arithmetics\n\n\n\n\n\n\nAI\n\n\ninterpretation\n\n\n\nToy model of grokking: tiny neural networks learn modular arithmetics after it has already reached full correctness on the training set.\n\n\n\n\n\n2023-09-01\n\n\n8 min\n\n\n2024-05-05\n\n\n\n\n\n\n\n\n\n\n\n\nSerial Experiments Lain\n\n\n\n\n\n\nanime\n\n\nfun\n\n\n\nLain is the collective subconscious as a neural network.\n\n\n\n\n\n2022-11-01\n\n\n13 min\n\n\n2024-01-21\n\n\n\n\n\n\n\n\n\n\n\n\nThe Racial Algorithmic Bias Controversy\n\n\n\n\n\n\nprobability\n\n\nstatistics\n\n\n\nThe racial algorithmic bias controversy, as seen by a mathematician.\n\n\n\n\n\n2019-07-13\n\n\n14 min\n\n\n2024-01-17\n\n\n\n\n\n\n\n\n\n\n\n\nThe Wigner Rotation in Special Relativity via Hyperbolic Geometry\n\n\n\n\n\n\nphysics\n\n\nmath\n\n\nfun\n\n\n\nThe other special relativity paradox that you have never heard of.\n\n\n\n\n\n2018-05-01\n\n\n54 min\n\n\n2024-01-22\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n  \n  \nYuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html",
    "href": "essays/posts/backstory-of-backpropagation/index.html",
    "title": "The Backstory of Backpropagation",
    "section": "",
    "text": "The backpropagation algorithm could have been invented by a slight generalization of the adjoint method, or the costate method, which was widely known among control theorists in the 1950s. Hybrid control theory leads almost inexorably to the backpropagation algorithm.\nSeppo Linnainmaa’s claim of priority is his 1970 master thesis – in Finnish, and never published. An English paper in 1976 was published that may have been relevant to automatic differentiation research, but not to neural network research. He has priority but no paternity.\nPaul Werbos’ claim of priority is that he developed the backpropagation algorithm in 1971, but was frustrated at every turn whenever he tried publishing it, not managing until around 1982. After that, it was taken up by many neural network researchers. He has priority and paternity.\nAs he freely admits, David Rumelhart reinvented it in 1982, as he did not know the previous work. The 1986 paper he coauthored (Rumelhart, Hinton, and Williams 1986) became popular enough that nobody would need to reinvent it again. He has no priority, but paternity. As usual in science, it is not the first inventor who gets the credit, but the last re-inventor.\nI was puzzled by how none of the first wave of neural network researchers developed backpropagation. I still am. My guesses are, from most to least likely:\n\nBad luck.\nThey were misled by the contemporary understanding of real neurons, and the McCulloch–Pitts model. Back then everyone “knew” that real neurons are 0-1 spike generators.\nThey thought of machine learning as just another way to construct boolean functions, rather than smooth functions, so they kept using the 0-1 activation function.\nThey distrusted gradient descent, because it would get stuck at local optima. If only they had had cheap compute and data to experiment with, they would have discovered the “blessing of scale”: local optima are almost as good as the global optimum.\nThey attempted to achieve parity with digital computers, which were discrete.\nThey attempted to distance themselves from cybernetics, which included optimal control theory."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#abstract",
    "href": "essays/posts/backstory-of-backpropagation/index.html#abstract",
    "title": "The Backstory of Backpropagation",
    "section": "",
    "text": "The backpropagation algorithm could have been invented by a slight generalization of the adjoint method, or the costate method, which was widely known among control theorists in the 1950s. Hybrid control theory leads almost inexorably to the backpropagation algorithm.\nSeppo Linnainmaa’s claim of priority is his 1970 master thesis – in Finnish, and never published. An English paper in 1976 was published that may have been relevant to automatic differentiation research, but not to neural network research. He has priority but no paternity.\nPaul Werbos’ claim of priority is that he developed the backpropagation algorithm in 1971, but was frustrated at every turn whenever he tried publishing it, not managing until around 1982. After that, it was taken up by many neural network researchers. He has priority and paternity.\nAs he freely admits, David Rumelhart reinvented it in 1982, as he did not know the previous work. The 1986 paper he coauthored (Rumelhart, Hinton, and Williams 1986) became popular enough that nobody would need to reinvent it again. He has no priority, but paternity. As usual in science, it is not the first inventor who gets the credit, but the last re-inventor.\nI was puzzled by how none of the first wave of neural network researchers developed backpropagation. I still am. My guesses are, from most to least likely:\n\nBad luck.\nThey were misled by the contemporary understanding of real neurons, and the McCulloch–Pitts model. Back then everyone “knew” that real neurons are 0-1 spike generators.\nThey thought of machine learning as just another way to construct boolean functions, rather than smooth functions, so they kept using the 0-1 activation function.\nThey distrusted gradient descent, because it would get stuck at local optima. If only they had had cheap compute and data to experiment with, they would have discovered the “blessing of scale”: local optima are almost as good as the global optimum.\nThey attempted to achieve parity with digital computers, which were discrete.\nThey attempted to distance themselves from cybernetics, which included optimal control theory."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#the-backpropagation-algorithm",
    "href": "essays/posts/backstory-of-backpropagation/index.html#the-backpropagation-algorithm",
    "title": "The Backstory of Backpropagation",
    "section": "The backpropagation algorithm",
    "text": "The backpropagation algorithm\nTo set the stage, we should at least quickly derive the backpropagation algorithm. In one sentence, backpropagation is an algorithm that calculates the gradient of every parameter in an acyclic directed graph with respect to a single final parameter.\nThe graph can be finite or infinite, but in all cases, its acyclic directedness allows us to assign a “logical time” to each node.1 Some nodes are independent variables: they point to other nodes, but no nodes point to them. Other nodes are dependent. If we know the independent variables, we can propagate their values forward in logical time and determine the values of every node. This is the “forward pass”. Backpropagation goes backwards in logical time.1 An existence proof that anything we might care about must have a logical time: Anything that happens, happens in the real world. The real world has a single direction in time. Therefore, any computation that can happen, must have a logical time that is identical with physical time.\nNote that logical time does not have to coincide with physical time. For example, to model goal-directed behavior, it might be better to put the physical future into the logical past.\nWe use the convention of putting derivatives on the rows. So for example, for \\(f: \\mathbb{R}^2\\to\\mathbb{R}^2\\), we have\n\\[\n\\nabla_x f = \\frac{df}{dx} = \\begin{bmatrix}\n\\frac{df_1}{dx_1} & \\frac{df_1}{dx_2} \\\\\n\\frac{df_2}{dx_1} & \\frac{df_2}{dx_2}\n\\end{bmatrix}\n\\]\nThis convention simplifies a lot of equations, and completely avoids transposing any matrix.\n\nDiscrete logical time\nConsider an acyclic directed graph with a finite number of nodes. Each graph node is a finite-dimensional real vector, though they may have different dimensions. We order the acyclic graph on the number line, so that each node depends only on the earlier nodes. Now denote the graph nodes as \\(x_0, x_1, \\dots, x_T\\). By our ordering, \\(x_1\\) depends on only \\(x_0\\), and \\(x_2\\) depends on only \\(x_1, x_2\\), and so on:\n\\[\n\\begin{aligned}\nx_0 &= x_0 \\\\\nx_1 &= f_1(x_0) \\\\\n&\\cdots \\\\\nx_T &= f_T(x_0, x_1, \\dots , x_{T-1})\n\\end{aligned}\n\\]\nNow we perform an infinitesimal perturbation on every one of its independent variables. A forward propagation then causes an infinitesimal perturbation on every variable, independent or dependent. Denote these as \\(dx_0, dx_1, \\dots, dx_T\\). We can now compute the derivative of \\(dx_T\\) with respect to every other variable by backpropagating the perturbation. Suppose we can see only \\(dx_T\\), then the change in \\(x_T\\) due to \\(dx_T\\) is the identity. That is,\n\\[\n\\frac{dx_T}{dx_T} = I\n\\]\nNow suppose we can see only \\(dx_{T-1}\\), then the change in \\(x_T\\) due to \\(dx_{T-1}\\) can only come from the final step in the forward propagation. Therefore\n\\[\n\\frac{dx_T}{dx_{T-1}} = \\nabla_{x_{T-1}} f_T(x_0, x_1, \\dots , x_{T-1})\n\\]\nSimilarly, the change in \\(x_T\\) due to \\(dx_{T-2}\\) can come from either directly changing \\(x_T\\) or from changing \\(x_{T-1}\\) and thereby changing \\(x_T\\). Therefore,\n\\[\n\\frac{dx_T}{dx_{T-2}} =\n    \\nabla_{x_{T-1}} f_{T}(x_0, x_1, \\dots , x_{T-2}) +\n    \\underbrace{\\frac{dx_T}{dx_{T-1}}\\nabla_{x_{T-2}} f_{T-1}(x_0, x_1, \\dots , x_{T-2})}_{\\text{the chain rule}}\n\\]\nThis generalizes to the rest of the steps.\nThe above computation is fully general, but this makes it inefficient. In practical applications of backpropagation, the computation graph is usually very sparse, meaning that each \\(x_t\\) only directly influence a few more nodes down the line. In standard neural networks, typically \\(x_{t}\\) only directly influences \\(x_{t+1}, x_{t+2}\\). Thus, sparsity is vital for backpropagation to be relevant.\nAs a side note, we could in fact compute all derivatives, not just the first, in one single backward pass. Other than the second derivatives \\(\\nabla^2_{x_t}x_T\\), there is rarely any use for the other derivatives, such as \\(\\nabla_{x_t}\\nabla_{x_s}x_T, \\nabla^3_{x_t}x_T\\), etc. The second derivative was occasionally used for neural networks circa 1990, with the further simplification that they only keep the positive diagonal entries of \\(\\nabla^2_{x_t}x_T\\) and set all other entries to zero (LeCun 1989).\n\n\nContinuous logical time\nConsider the problem of controlling a car along a highway. We discard all details, so that the car is just a single dot on a line, and the state of the car is determined by its speed and velocity. To avoid waiting forever, we require time \\(t\\in [0, T]\\). Write the state variable as follows:\n\\[\nx_t = (\\text{location at time }t, \\text{velocity at time }t)\n\\]\nIt might be confusing to use \\(x_t\\) for the state at time \\(t\\), instead of for location, but it makes the notation consistent.\nSuppose the only thing we can influence is how much we press the pedal. Write \\(u_t\\) to be the resulting acceleration we inflict on the car by pressing on the pedal. Further, assume that the car is slowed down by friction that is proportional to velocity. We then have:\n\\[\n\\dot x_t = f(x_t, u_t)\n\\]\nwhere \\(f(x_t, u_t) = (x_{t, 1}, -\\mu x_{t, 1} + u_t)\\) is the dynamics equation of the system, and \\(\\mu\\) is the friction coefficient.22 To allow for time-varying dynamics, simply replace \\(f(x_t, u_t)\\) with \\(f(t, x_t, u_t)\\). This clutters the notation without involving new ideas.\nNow, we can draw the computation graph as before. The computation graph has a continuum of nodes, but it has a very simple structure. The graph has two kinds of nodes: the \\(x_t\\) nodes and the \\(u_t\\) nodes. Its independent variables are \\(x_0\\) and all the \\(u_t\\) nodes. Each \\(x_{t+dt}\\) depends on only \\(x_{t}\\) and \\(u_t\\). This makes the two propagations particularly simple.\nThe forward propagation is obtained by integration, which we can unwrap into a form resembling the case of the discrete logical time:\n\\[\n\\begin{aligned}\nx_0 &= x_0 \\\\\nx_{dt} &= x_0 + f(x_0, u_0) dt \\\\\n&\\cdots \\\\\nx_{T} &= x_{T-dt} + f(x_{T-dt}, u_{T-dt}) dt \\\\\n\\end{aligned}\n\\]\nThe backpropagation is similarly obtained. By inspecting the computation graph, we can see that each \\(u_t\\) only directly influences \\(x_{t+dt}\\), giving\n\\[\n\\frac{dx_T}{du_t} = \\frac{dx_T}{dx_{t+dt}} \\nabla_{u_t}f(x_t, u_t) dt.\n\\]\nIt remains to compute \\(\\frac{dx_T}{dx_t}\\). This can be found by backpropagation too, since each \\(x_t\\) only directly influences \\(x_{t+dt}\\), we have\n\\[\n\\frac{dx_T}{dx_t} = \\frac{dx_T}{dx_{t+dt}} \\left[I + \\nabla_{x_t} f(x_t, u_t) dt\\right].\n\\]\nIf we denote the gradient as \\(g_t := \\frac{dx_T}{dx_t}\\), then we find an equation for \\(g_t\\):\n\\[\ng_t = \\left[I + (g_t + \\dot g_t dt) \\nabla_{x_t} f(x_t, u_t) dt\\right]\\implies \\dot g_t = -g_t\\nabla_{x_t} f(x_t, u_t)\n\\]\nThis equation bottoms out at the end time, \\(t=T\\), for which \\(g_T = \\frac{dx_T}{dx_T} = I\\). Thus we have the costate equations:\n\\[\n\\begin{cases}\ng_T &= I \\\\\n\\dot g_t &= - g_t \\nabla_{x_t} f(x_t, u_t)\n\\end{cases}\n\\]\nwhich must, as you can see, be integrated backwards in time – backpropagation again! Indeed, control theory practically compels us to find backpropagation.\n\n\nHybrid logical time\nWhen the computation graph has both nodes with discrete logical times and nodes with continuous logical times, we call such a system as having hybrid logical time.33 The name “hybrid” comes from “hybrid control theory”, which are systems with both discrete changes, or jumps, and continuous changes, or flow. They appear whenever a digital device is used to control a continuous system, such as a computer piloting an airplane.\nThe idea can be illustrated by the fundamental problem in control theory: how to optimize control? You cannot optimize without optimizing a real number, so we write down the number as \\(J\\), representing the “cost” of the trajectory. For example, in driving a car, we might want to optimize comfort, time-until-arrival, and fuel cost. We can then write \\(J\\) down as something like\n\\[\nJ = \\underbrace{(x_{T, 0} - x_{goal})^2}_{\\text{location should be at the goal location at the end time}}\n    + \\underbrace{(x_{T, 1} - 0)^2}_{\\text{speed should be zero at the end time}} + \\int_0^T u_t^2 dt\n\\]\nMore generally, the objective to be optimized is in the form\n\\[\nJ = A(x_T) + \\int_0^T L(x_t, u_t)dt\n\\]\nfor some real-valued functions \\(A, L\\).\nOf course, we can care about more than the state at the last time-step. We can care about multiple time-steps \\(t_0, t_1, \\dots, t_n\\) by writing down a cost function \\(J = \\sum_{i=0}^n A_i(x_{t_i}) + \\int_0^T L(x_T, u_T)dt\\), but this merely creates complications without introducing new ideas. Even more ornate computation graphs, weaving together multiple strands of continuous and discrete logical times, can be backpropagated in the same way without involving new ideas.\nDefine the costate \\(\\lambda_t := \\nabla_{x_t} J\\), then the costate backpropagates as:\n\\[\n\\lambda_t = L(x_t, u_t) dt + \\lambda_{t+dt}(I + \\nabla_{x_t}f(x_t, u_t)dt)\n\\]\nand by simplifying, we have the costate equation:\n\\[\n\\begin{cases}\n\\lambda_T &= \\nabla_{x_T} A(x_T) \\\\\n\\dot \\lambda_t &= - \\nabla_{x_t} L(x_t, u_t) - \\lambda_t \\nabla_{x_t} f(x_t, u_t)\n\\end{cases}\n\\]\nwhich can be solved by integrating backward in time.\nOnce we have obtained all the costates, we can compute \\(\\nabla_{u_t} J\\). Since \\(u_t\\) can only influence \\(J\\) either directly via \\(L(x_t, u_t)\\) or indirectly via \\(x_t\\), we have\n\\[\n\\nabla_{u_t} J = \\left[\\nabla_{u_t}f(x_t, u_t) \\lambda_t + \\nabla_{u_t}L(x_t, u_t)\\right]dt\n\\]\nNote that \\(\\nabla_{u_t} J\\) is an infinitesimal in \\(dt\\). This is qualitatively different from \\(\\nabla_{x_t}J = g_t\\), which is not an infinitesimal, but a mere matrix. Why is it so? The simplest example suffices to illustrate.\nConsider a mass point sliding on a frictionless plane. When we perturb \\(x_t\\), we push it to the side by \\(\\delta x_{t, 0}\\) and also change its velocity by \\(\\delta x_{t, 1}\\), and so at the end time \\(T\\), we would have changed \\(x_T\\) by \\((\\delta x_{t, 0} + (T-t)\\delta x_{t, 1}, \\delta x_{t, 1})\\), which is the same order of infinitesimal. Now, we can control the mass point by applying a force \\(u_t\\), which gives us the dynamics equation\n\\[\n\\dot x_t = (x_{t, 1}, u_t)\n\\]\nTo “perturb” \\(u_t\\) by \\(\\delta u_t\\) does not make sense on its own, as a “spike” of \\(\\delta u_t\\) that lasts for zero amount of time does not show up in the system trajectory. One can apply a durationless jump in state, but one cannot apply a durationless force. Therefore, it only makes sense to perturb \\(u_t\\) by \\(\\delta u_t\\) and persist the perturbation for \\(dt\\) time. This perturbs the state at the end time by \\(((T-t)\\delta u_t dt , \\delta u_t dt)\\), which means that \\(\\nabla_{u_t}x_T\\) is proportional to \\(dt\\).\n\n\nOptimal control theory\nAn optimal trajectory must have \\(\\nabla_{u_t} J = 0\\), since otherwise, we could shave off a little piece of cost by giving \\(u_t\\) a little boost in the opposite direction (infinitesimal gradient descent!). This gives us two equations that any optimal trajectory must satisfy\n\\[\n\\begin{cases}\n- \\dot \\lambda_t &= \\nabla_{x_t} L(x_t, u_t) + \\lambda_t \\nabla_{x_t} f(x_t, u_t) \\\\\n0                &= \\nabla_{u_t} L(x_t, u_t) + \\lambda_t \\nabla_{u_t} f(x_t, u_t) \\\\\n\\end{cases}\n\\]\nNow, what is the effect of perturbing \\(u_t\\) by \\(du_t\\)? It would perturb \\(x_{t+dt}\\) by \\(\\nabla_{u_t} f(x_t, u_t) du_t dt\\), a second-order infinitesimal. Consequently, it would perturb \\(x_T\\) by only a second-order infinitesimal, and thus \\(\\lambda\\) too. Therefore, we have\n\\[\n\\nabla_{u_t}\\lambda_t = 0\n\\]\ngiving us simplified equations for optimality:\n\\[\n\\begin{cases}\n- \\dot \\lambda_t &= \\nabla_{x_t} L(x_t, u_t) + \\lambda_t \\nabla_{x_t} f(x_t, u_t) \\\\\n0                &= \\nabla_{u_t} (L(x_t, u_t) + \\lambda_t f(x_t, u_t)) \\\\\n\\end{cases}\n\\]\nUnfortunately, we cannot simplify the first equation similarly because \\(\\nabla_{x_t}\\lambda_t \\neq 0\\). Still, it seems \\(L(x_t, u_t) + \\lambda_t f(x_t, u_t)\\) should be an important quantity:\n\\[\nH(x_t, u_t, \\lambda_t) := L(x_t, u_t) + \\lambda_t f(x_t, u_t)\n\\]\nThe letters are meaningful. \\(L\\) is the “Lagrangian”, and \\(H\\) is the “Hamiltonian”. Indeed, classical Hamiltonian mechanics is a special case of optimal4 control theory.4 To be completely precise, it is a necessary but insufficient condition for optimality. Just like how a function can have zero derivatives on the peaks, valleys and shoulders, a trajectory can have zero functional derivative, even if it is the best, the worst, and the … shouldered? In jargon, we say that those conditions are first order optimality conditions, since they use only the derivative, not the second-derivative.\nIf we interpret economically the quantities, then \\(J\\) is the cost of the entire trajectory, \\(\\lambda_t\\) is the marginal cost of the point \\(x_t\\) in the trajectory, and \\(L(x_t, u_t)\\) is the cost-rate at time \\(t\\). The second equation of optimality \\(\\nabla_{u_t} H(x_t, u_t, \\lambda_t) = 0\\) states that when the trajectory is optimal, the marginal cost of control is zero: there is no saving in perturbing the control in any direction.\nTherefore, define the “optimized” Hamiltonian and the optimized control relative to it:\n\\[\n\\begin{cases}\nH^*(x_t, \\lambda_t) &:= \\min_{u_t}H(x_t, u_t, \\lambda_t) = \\min_{u_t} \\left(L(x_t, u_t) + \\lambda_t f(x_t, u_t)\\right) \\\\\nu^*(x_t, \\lambda_t) &:= \\mathop{\\mathrm{argmin}}_{u_t}H(x_t, u_t, \\lambda_t)\n\\end{cases}\n\\]\nThen, by Hotelling’s lemma, we derive the Hamiltonian equations of motion:\n\\[\n\\begin{cases}\n- \\dot \\lambda_t &= \\nabla_{x_t} L(x_t, u_t^*) + \\lambda_t \\nabla_{x_t} f(x_t, u_t^*) &= \\nabla_{x_t} H^*(x_t, \\lambda_t) \\\\\n  \\dot x_t       &= f(x_t, u_t^*)                                                     &= \\nabla_{\\lambda_t} H^*(x_t, \\lambda_t) \\\\\n\\end{cases}\n\\]\nThis is often called the Pontryagin’s maximum principle, as Pontryagin’s school of optimal control theory is based on this principle and its extensions. Control theory was a national security issue during the Cold War, as it was used from the space race to the missile race.\nIn classical control theory, the equation is sometimes solved in closed form, as in the case of linear quadratic control. When it cannot be solved in closed form, it can still be numerically solved using a continuous form of the Bellman equation. Similar to how the Hamiltonian equations have the alternative form of the Hamilton–Jacobi equation, the Pontryagin equations have an alternative form in the Hamilton–Jacobi–Bellman equation. Possibly, the name “dynamic programming” appears later in Paul Werbos’ invention of backpropagation, which he named “dynamic feedback”.\nIn economics, one can modify this framework slightly to allow discounting cost over time, yielding theories about “optimal investment” such as the Ramsey optimal growth theory."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#leibniz",
    "href": "essays/posts/backstory-of-backpropagation/index.html#leibniz",
    "title": "The Backstory of Backpropagation",
    "section": "Leibniz",
    "text": "Leibniz\nThe chain rule dates back to Calculus Tangentium differentialis [Differential calculus of tangents], a manuscript by Leibniz dated 1676 November (Child 1917). It says\n\nit does not matter, whether or no the letters \\(x, y, z\\) have any known relation, for this can be substituted afterward.\n\nIn mathematical notation, he found that \\(dy = dx \\frac{dy}{dx}\\), or in his notation, \\(\\overline{dy} = \\overline{dx} \\frac{dy}{dx}\\), where the overbar denotes the thing to be differentiated. You can read it as a bracket: \\(d(y) = d(x) \\frac{dy}{dx}\\).\nHe then gave the following examples5:5 Yes, there is a sign error. No, I’m not going to fix it. He just has to live with his mistakes.\n\\[\n\\begin{aligned}\n& \\overline{d \\sqrt[2]{a+b z+c}} z^2 \\text {. Let } a+b z+c z^2=x \\text {; } \\\\\n\\text{Then} \\quad  & \\overline{d \\sqrt[2]{x}}=-\\frac{1}{2 \\sqrt{x}} \\text {, and } \\frac{d x}{d z}=b+2 c z \\text {; } \\\\\n\\text{Therefore} \\quad  & \\overline{d \\sqrt{a+b z+c z^2}}=-\\frac{b+2 c z}{2 \\overline{d z} \\sqrt{a+b z+c z^2}} \\\\\n&\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#mcculloch-and-pitts",
    "href": "essays/posts/backstory-of-backpropagation/index.html#mcculloch-and-pitts",
    "title": "The Backstory of Backpropagation",
    "section": "McCulloch and Pitts",
    "text": "McCulloch and Pitts\nIn the famous paper (McCulloch and Pitts 1943), McCulloch and Pitts proposed that\n\nBecause of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes.\n\nThe McCulloch and Pitts paper is difficult to read, and is rooted in early 20th century mathematical logic. In fact, it uses the same abstruse notations of the Principia Mathematica, which is cited in the paper.\n\n\n\n\n\n\nThe infamous proof of 1+1=2 in Principia Mathematica\n\n\n\n\n\n\n\nThe same notation is used by McCulloch and Pitts\n\n\n\n\n\nThe McCulloch and Pitts paper, like the Perceptrons book, is something often cited but rarely read. The best introduction to the McCulloch and Pitts neural network is still (Minsky 1967, chap. 3), which explicitly designs, neuron by neuron, threshold by threshold, how to construct neural networks that perform various tasks, such as simulating arbitrary Turing machines, process signals, perform additions, etc."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#frank-rosenblatt",
    "href": "essays/posts/backstory-of-backpropagation/index.html#frank-rosenblatt",
    "title": "The Backstory of Backpropagation",
    "section": "Frank Rosenblatt",
    "text": "Frank Rosenblatt\nFrank Rosenblatt is the originator of the term “backpropagation”, or more precisely, “back-propagating error-correction procedure” (Rosenblatt 1962, vol. 55, chap. 13), although it was quite far from our current use of backpropagation. He is the originator of many concepts in neural networks, and he was even popularly famous during his own time. During 1957–1962, he did a lot of the theory and experiments with perceptron networks. Some experiments ran on digital computer simulations, and others on the Mark I Perceptron, a bespoke machine that takes up a whole room.\nA perceptron is a function of the form \\(\\theta(w^T x + b)\\), where \\(\\theta\\) is the 0-1 step function, and \\(w \\in \\mathbb{R}^n, b \\in \\mathbb{R}\\) are its learnable parameters. A perceptron network is a network of perceptrons. Rosenblatt experimented with a large number of perceptron network architectures, but most often, he experimented with a certain 2-layered feedforward architecture, which is sometimes called “the perceptron machine”.\nThe perceptron machine is a machine that computes a function of type \\(\\{0, 1\\}^n \\to \\{0, 1\\}\\). Its input layer is composed of units named “Stimulus units” or “S units”. The S units do not perform any computation, but merely pass binary outputs to the hidden layer. In the Mark I perceptron, the S units were 20×20 cadmium sulfide photocells, making a 400-pixel image. The hidden layer is composed of perceptrons named “Association units” or “A units”. Their outputs pass on to the output layer, composed of perceptrons named “Response units” or “R units”.\nWe can describe the perceptron machine in one equation:\n\\[\nf(x) = \\theta\\left(b^{R} + \\sum_i w^{AR}_i \\theta\\left((w^{SA, i})^T x + b^{A}_i\\right)\\right)\n\\]\nRosenblatt proved some mathematical theorems, the most important of which is the perceptron convergence theorem, which shows that assuming the dataset is linearly separable, then the perceptron learning algorithm converges after making a bounded number of errors6. He also performed experiments on the Mark I Perceptron machine, IBM computers, and some other computers.6 There is another technical assumption on the dataset, but it is omitted for space. Suffice to say it is satisfied for all finite datasets.\nHis theorems and experiments were exhaustively documented in his book (Rosenblatt 1962). Its breadth is quite astonishing. It contains:\n\nperceptrons with continuous activation functions (section 10.2);\nperceptron convergence theorem for perceptrons with continuous activation functions that are also monotonic and sign-preserving (page 249);\nperceptron layers with random delay in transmission time (chapter 11);\nlayers with connections between units within the same layer, with possibly closed loops (chapter 17–19);\nlayers with connections from a later layer to a previous layer (chapter 20);\nresidual connections (Figure 42);\nmultimodal perceptron networks that learns to associate image and audio inputs (Figure 58);\nprogram-learning perceptrons (chapter 22);\nperceptron networks that analyze videos and audios (chapter 23).\n\nFrom our vantage point, we can fairly say that he invented randomization, residual connections, weight initiation schemes, neural Turing machines, recurrent neural networks, vision models, time delay neural networks, multimodal neural networks…\n\n\n\nFigure 58. The first multimodal neural network?\n\n\n\n\n\nFigure 42. The dashed lines denote variable weights, and the solid lines denote fixed weights. This is the residual connection.\n\n\nWhat is even more astonishing is that, as far as I can see, he did not make use of gradient descent learning at all, not even on a single layer like Widrow and Hoff. The perceptron learning rule converges, but only for a single-layered perceptron network on linearly separable data. Thus, when it came to two-layered perceptron networks, he randomly wired the first layer, then froze it, and only adapted the second layer. This would prove to be a turning point in the “perceptron controversy”.\nIn the chapter where he talked about backpropagation (Rosenblatt 1962, vol. 55, chap. 13), he tried to train both layers in the two-layered perceptron network by extending the perceptron learning algorithm. The algorithm works on each individual perceptron unit, but only if the desired output is known. The desired outputs for the output units are supplied by the experimenter, but the desired outputs for the hidden units could not be known, so Rosenblatt tried some hacks. Essentially, he took the error signals at the output units, and used heuristic rules to “backpropagate the error” to synthesize error signals at the hidden units. The precise details are hacky, and no longer of any relevance.\nOne last thing about his backpropagation rule: he also discovered the per-layer learning rate trick. Because his backpropagation algorithm was so hacky, he found that he had to stabilize it by making the first layer learn at a much lower rate than the second.\n\nIt is found that if the probabilities of changing the S-A connections are large, and the threshold is sufficiently small, the system becomes unstable, and the rate of learning is hindered rather than helped by the variable S-A network. Under such conditions, the S-A connections are apt to change into some new configuration while the system is still trying to adjust its values to a solution which might be perfectly possible with the old configuration. Better performance is obtained if the rate of change in the S-A network is sufficiently small to permit an attempt at solving the problem before drastic changes occur."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#bernard-widrow-and-marcian-hoff",
    "href": "essays/posts/backstory-of-backpropagation/index.html#bernard-widrow-and-marcian-hoff",
    "title": "The Backstory of Backpropagation",
    "section": "Bernard Widrow and Marcian Hoff",
    "text": "Bernard Widrow and Marcian Hoff\nWidrow and Hoff worked on neural networks in the early 1960s. They started with training a single perceptron with gradient descent on the squared loss, then proceeded to spend years trying to train a two-layered network without gradient descent. I know – I cannot make this sound any less puzzling.\nThe Widrow–Hoff machine, which they called the ADALINE (“ADAptive Linear NEuron”), is a function of type \\(\\mathbb{R}^n \\to \\{0,1\\}\\) defined by\n\\[\nf(x) = \\theta(w^T x + b)\n\\]\nand here it is again, that dreaded Heaviside step-function that was the bane of every neural network before backpropagation. What was odd about this one is that ADALINE was trained by gradient descent with the squared loss function \\(\\frac 12 (w^T x + b - y)^2\\), which is continuous, not discrete:\n\\[\nw \\leftarrow w - \\alpha (w^T x + b - y) w, \\quad b \\leftarrow b - \\alpha (w^T x + b - y) b\n\\]\nThe first ADALINE machine was a box that learned to classify binary patterns on a \\(4 \\times 4\\) grid. It was pretty amusing, as everything was done manually. The patterns were inputted by flipping 16 switches by hand. The error \\(w^T x + b - y\\) was read from a voltmeter, and the parameters \\(w, b\\) were individually adjusted by turning knobs controlling rheostats inside the box. They then automated the knob-turning process using electrochemical devices called memistors, though the pattern input \\(x\\) and the desired output \\(y\\) were still entered by manual switches.\n\n\n\n\n\n\nA memistor ADALINE with glass-sealed memistors. (Widrow 2023, fig. 26.12)\n\n\n\n\n\n\n\nTest patterns, and a learning curve, for the ADALINE machine. (Widrow 2023, fig. 26.4)\n\n\n\n\n\nWidrow recounts an amusing encounter with Rosenblatt:\n\nI just put the pattern in and the Adaline went “phut,” and the needle was reading to the right or to the left. So I just held the adapt button down so some of the cells are plating while others are deplating, depending on the direction of the error signal. Rosenblatt’s students put the pattern into the perceptron. You could see it in the lights on the perceptron. You could hear the potentiometers grinding away. We put another pattern into the Adaline, and it went “blip ,” and there it was, adapted. They put it in the perceptron, and it’s still grinding away. We put in a couple more patterns. Then we test the Adaline and test the perceptron to see whether the patterns are still in there.\nThey’re in the Adaline. In the perceptron, they’re all gone. I don’t know whether the machine was temperamental or what, but it was difficult to train. I argued with Rosenblatt about that first random layer. I said, “You’d be so much better off if you just took the signal from the pixels and ran them straight into the weights of the second layer.” He insisted that a perceptron had to be built this way because the human retina is built that way. That is, there’s a first layer that’s randomly connected to the retina. He said the reason why you can get something to interpret and make sense of random connections is because it’s adaptive. You can unravel all this random scrambling. What I was trying to do was to not model nature. I was trying to do some engineering.\n\nAfter the ADALINE showed good behavior, they went on and on trying to train a two-layered ADALINE (“MADALINE”, or “many ADALINE”), which of course could not be trained by gradient descent, because the hidden layer ADALINE units used the Heaviside step function! It is almost inconceivable nowadays, but they simply refused to use a continuous activation function and tried every other trick except that. They ended up with the MADALINE I rule. In short, it was a heuristic rule for synthesizing supervision signals for the hidden layer, much like Rosenblatt’s heuristic rule.77 The MADALINE I machine consists of multiple hidden ADALINE units, and a single output layer consisting of a single unit. The output unit merely takes the majority vote from the hidden units. If the desired output is \\(+1\\), but the actual output is \\(-1\\), then the machine picks those ADALINE units that are negative, but closest to being positive, and make them update their weights, according to the ADALINE learning rule. It was thought of as a form of “minimal disturbance principle”.\nFrustrated by the difficulty, they left neural network research. Hoff went to Intel to co-invent the microprocessor, and Widrow set about applying the ADALINE to small problems that it could solve well8, such as adaptive antennas, adaptive noise filtering in telephone lines, adaptive filtering in medical scanning, etc. Even with just a single ADALINE, he brought breakthroughs to those fields.8 Marvin Minsky would approve.\n\n[The perceptron] is such a simple machine that it would be astonishing if nature did not make use of it somewhere. It may be that one of the best things a neuron can have is a tiny Perceptron, since you get so much from it for so little. You can’t get one big Perceptron to do very much, but for some things it remains one of the most elegant and simple learning devices I know of.” (Bernstein 1981)\n\n\n\nEngineers at Apple have told me that every iPhone, since the iPhone 5, uses the LMS algorithm all over the device. They could not tell me where because, if they did, they would have to shoot me. Apple keeps secrets. (Widrow 2022, preface, page xix)\n\nPerhaps Widrow and Hoff were limited to considering only learning rules they could implement electrochemically? But it does not seem so, in the words of Widrow himself, it was a blind spot for all researchers:\n\nThe Madaline had an adaptive first layer and a fixed-logic second layer. What Rosenblatt had was a fixed-logic mst layer and an adaptive-logic second layer or output layer. Now, it is easy to adapt on output layer. But it’s very difficult to adapt a hidden layer. … We could adapt an adaptive first layer with a fixed second layer as long as we knew what the second layer was. But we never succeeded in developing an algorithm for adapting both layers, so that the second layer is not fixed and both layers are adaptive. It wasn’t that we didn’t try. I mean we would have given our eye teeth to come up with something like backprop.\nBackprop would not work with the kind of neurons that we were using because the neurons we were using all had quantizers that were sharp. In order to make backprop work, you have to have sigmoids; you have to have a smooth nonlinearity … no one knew anything about it at that time. This was long before Paul Werbos. Backprop to me is almost miraculous. (Rosenfeld and Anderson 2000)\n\nWhen he heard about the “miraculous” backpropagation in the 1980s, he immediately started writing papers in neural networks again.\nIf this was an individual case, then I might have dismissed it as Widrow being misled by his background. Before entering a PhD program Widrow took an internship at the military:\n\n… the sky was divided up into a grid, with one mile by one mile regions, and all the computer sensed was that there was a target within that one mile by one mile square. … The radar pulses were transmitted periodically, and then periodically you got samples of the flight path. Say you take a piece of graph paper and draw and nice, smooth flight path for an airplane, and then at uniform points in time put a mark on the actual path based on the radar sample. If you don’t show anybody that nice, smooth line but only the sequence of grid squares that the airplane was in, one could still reconstruct the smooth track. (Widrow 1997)\n\nThe problem of reconstructing a smooth flight path from discretized radar signals got him interested in quantization, and he did his PhD thesis (1954) in the statistical theory of quantization noise. In the interview 43 years later, he called it “the best piece of work I ever did in my whole life”. He is still the world expert on quantization noise, and wrote an entire textbook on quantization noise (Widrow and Kollár 2008).\nSo regarding this background, one might be tempted to say that Widrow was misled by his infatuation with quantization, and attempted to use quantized neural networks. However, this does not explain why all the other pioneers went down the same wrong path."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#seppo-linnainmaa",
    "href": "essays/posts/backstory-of-backpropagation/index.html#seppo-linnainmaa",
    "title": "The Backstory of Backpropagation",
    "section": "Seppo Linnainmaa",
    "text": "Seppo Linnainmaa\nIt’s said that Seppo Linnainmaa’s master’s thesis in 1970 contains the backpropagation algorithm, but it is in Finnish and not available online either. A bibliography search shows that his thesis were basically never cited before the 2010s. It is one of those papers that people cite for ritual completion only. I think we can safely say that his thesis has priority but no paternity. (Griewank 2012) describes in detail what is in the thesis, according to which he developed it to analyze the cumulative effect of round-off error in long chains of computer computations.\nI checked all his English papers during the 1970s, and it seems only (Linnainmaa 1976) has been cited non-negligibly before 2000, and may have had some impact on automatic differentiation, but I cannot tell by how much. It contains two algorithms, one for computing the first derivative, one for computing the first two. Both were special cases of the general backpropagation algorithm for computing arbitrary orders of derivatives."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#rumelhart-and-sejnowski",
    "href": "essays/posts/backstory-of-backpropagation/index.html#rumelhart-and-sejnowski",
    "title": "The Backstory of Backpropagation",
    "section": "Rumelhart and Sejnowski",
    "text": "Rumelhart and Sejnowski\nRumelhart rediscovered backpropagation around 1982 and immediately set about publishing and telling others about it. Some still resisted, such as Geoffrey Hinton, but others immediately grasped it and set about using it, such as Terrence Sejnowski. In the interview, he was rather unbothered by the priority dispute (Rosenfeld and Anderson 2000, chap. 12):\n\nI had no idea that Paul Werbos had done work on it. … There are other examples of work in the control literature in the ’60s [the adjoint method]. … it’s just no one had really done anything with them. My view is that we not only discovered them, but we realized what we could do with them and did a lot of different things. I consider them independent discoveries, at least three and maybe more. [Shun’ichi] Amari, I think, suggests that he had another discovery, and you know, I believe that. You can look at his paper. He had, but he didn’t do anything with it. I think that was in the late ’60s. I don’t feel any problem. You know, maybe we should have done better scholarship and searched out all of the precursors to it, but we didn’t know there were any.\n\nIn 1983, Rumelhart showed backpropagation to Sejnowski, who immediately tried it and found that it was much faster than the Boltzmann machine learning rule. What a refreshing change from all those others who stubbornly refused to try it.\n\n… I was still working on the Boltzmann machine and was beginning to do simulations using backprop. I discovered very rapidly that backprop was about an order of magnitude faster than anything you could do with the Boltzmann machine. And if you let it run longer, it was more accurate, so you could get better solutions. (Rosenfeld and Anderson 2000, chap. 14)\n\nThis was vitally important later, when Sejnowski used backpropagation to train NETtalk, a huge network with 18,629 parameters.9 The model was a popular hit and appeared on prime-time television.(Sejnowski 2018, 112–15)9 People told him that it had too many parameters and would hopelessly overfit, but the network just needed enough data and it generalized uncannily well. It really does seem like scaling skepticism is a researcher universal, if not a human universal.\n\nThere were 18,629 weights in the network, a large number by the standards of 1986, and impossibly large by the standards of mathematical statistics of the time. With that many parameters, we were told that we would overfit the training set, and the network would not be able to generalize. (Sejnowski 2018, 113)"
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#geoffrey-hinton",
    "href": "essays/posts/backstory-of-backpropagation/index.html#geoffrey-hinton",
    "title": "The Backstory of Backpropagation",
    "section": "Geoffrey Hinton",
    "text": "Geoffrey Hinton\nThe interview with Geoffrey Hinton is hilarious, mostly about how he spent an entire year refusing to use backpropagation. This section is mostly made of quotations from (Rosenfeld and Anderson 2000, chap. 16).\nAfter learning about Hopfield networks and simulated annealing both in 1982, he tried combining them, and discovered the Boltzmann learning algorithm in 1983, which he published in 1985 (Ackley, Hinton, and Sejnowski 1985).\n\nI remember I had to give a talk, a sort of research seminar, at Carnegie Mellon in either February or March of ’83 … I wanted to talk about this simulated annealing in Hopfield nets, but I figured I had to have a learning algorithm. I was terrified that I didn’t have a learning algorithm.\nThen we got very excited because now there was this very simple local-learning rule. On paper it looked just great. I mean, you could take this great big network, and you could train up all the weights to do just the right thing, just with a simple local learning rule. It felt like we’d solved the problem. That must be how the brain works.\nI guess if it hadn’t been for computer simulations, I’d still believe that, but the problem was the noise. It was just a very, very slow learning rule. It got swamped by the noise because in the learning rule, you take the difference between two noisy variables, two sampled correlations, both of which have sampling noise. The noise in the difference is terrible.\nI still think that’s the nicest piece of theory I’ll ever do. It worked out like a question in an exam where you put it all together and a beautiful answer pops out.\n\nAnd now, the hilarity we have been waiting for. When Rumelhart told him in 1982 about backpropagation,\n\nI first of all explained to him why it wouldn’t work, based on an argument in Rosenblatt’s book, which showed that essentially it was an algorithm that couldn’t break symmetry. … the standard way of doing learning is to have random initial weights, or you could just put a little bit of noise in the learning rule …\nThe next argument I gave him was that it would get stuck in local minima. There was no guarantee you’d find a global optimum. Since you’re bound to get stuck in local minima, it wasn’t really worth investigating.\nThen I tried to use it to get a very obscure effect. I couldn’t get this very obscure effect with it, so I lost interest in backpropagation. I managed to get this very obscure effect later with a Boltzmann machine. I’d realized that if you’ve got a net to learn something, and then you add noise to the weights by making it learn something else, it should be much faster at relearning what it had previously learned. You should get very fast relearning of stuff that it previously knew, as compared to the initial learning. We programmed a backpropagation net, and we tried to get this fast relearning. It didn’t give fast relearning, so I made one of these crazy inferences that people make – which was, that backpropagation is not very interesting.\n\nAfter one year of failing to get Boltzmann machines to train, he discovered that Boltzmann machines also got stuck in local minima. Desperate times call for desperate measures, and he finally resorted to gradient descent.\n\nAfter initially getting them to work, I dedicated over a year to refining their performance. I experimented with various techniques, including weight decay, which helped in preventing large energy barriers. We attempted several other approaches, but none of them yielded satisfactory results. I distinctly recall a point where we observed that training a Boltzmann machine could model certain functions effectively, but prolonged training caused its performance to deteriorate, a phenomenon we referred to as “going sour.” We couldn’t initially comprehend why this was happening. I generated extensive reports, filling stacks of paper about three inches thick, as I couldn’t believe that these networks would degrade as you acquired more knowledge.\nIt took me several weeks to realize the root of the problem. The learning algorithm operated under the assumption that it could reach thermal equilibrium. However, as the weights grew larger, the annealing process failed to achieve thermal equilibrium, trapping the system in local minima. Consequently, the learning algorithm started behaving incorrectly. This realization led us to introduce weight decay as a solution to prevent this issue.\nAfter investing over a year in attempting to optimize these methods, I ultimately concluded that they were unlikely to deliver the desired results. In a moment of desperation, I considered revisiting [backpropagation].\n\nThen he asked the research group for volunteers. None of the ten students took up the offer (and thus giving up a place as the coauthor of the backpropagation paper??):\n\nThey’d all been thoroughly indoctrinated by then into Boltzmann machines. … They all said, “You know, why would you want to program that?” We had all the arguments: It’s assuming that neurons can send real numbers to each other; of course, they can only send bits to each other; you have to have stochastic binary neurons; these real-valued neurons are totally unrealistic. It’s ridiculous.” So they just refused to work on it, not even to write a program, so I had to do it myself.\nI went off and I spent a weekend. I wrote a LISP program to do it.\n\nHinton almost had one last chance at giving up on backpropagation.\n\nI almost blew it because the first thing I tried it on was an 8-3-8 encoder; that is, you have eight input units, three hidden units, and eight output units. You want to turn on one of the input units and get the same output unit to be on.\nIn a Boltzmann machine, since the units are binary, the states of the three hidden units have to be the eight different binary codes, so you know what the hidden units will look like when they’ve solved the problem. So I ran backpropagation on this problem and looked at the weights of the hidden units. They didn’t have the right structure of weights. … I thought, “Oh well, it turns out backpropagation’s not that good after all.” Then I looked at the error, and the error was zero. I was amazed.\n\nAnd so one year of excuses later, Hinton finally surrendered to backpropagation. They published the algorithm along with several examples, which became widely cited.\n\nThat was at the stage when we were just completing the PDP books, so we’d already agreed on what was going to be in the books. The final chapters were being edited. We decided we’d just slip in an extra chapter on backpropagation, so it was a late addition to the book. But I was always a bit disappointed. I mean, intellectually, backpropagation wasn’t nearly as satisfying as Boltzmann machines. … it didn’t have the nice probabilistic interpretation."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#paul-werbos",
    "href": "essays/posts/backstory-of-backpropagation/index.html#paul-werbos",
    "title": "The Backstory of Backpropagation",
    "section": "Paul Werbos",
    "text": "Paul Werbos\nAfter reviewing the story of multiple people, my personal opinion is that Paul Werbos most deserves the title of “originator of the backpropagation algorithm”. He independently developed the algorithm around 1971, but was frustrated at every turn when he tried to publish it, not managing until 1982. After that, it was quickly picked up by connectionists. In this sense, he has both priority and paternity.\nIn 1993, Paul Werbos gave an interview, where he described the backstory of his invention of backpropagation (Rosenfeld and Anderson 2000, chap. 15). I will let him speak, only interjecting with brief comments.\nBefore entering Harvard, he tried implementing Hebbian learning, but noticed it wouldn’t work.\n\nIt was obvious to me from a statistical point of view that Hebbian learning was going to be measuring correlation coefficients, and for multivariate problems it would not work. I never gave the talk. I said, “Now I’m going to figure out something with the same flavor that does work.”\n[Understanding human learning] will help us understand the human mind; it will help human beings understand themselves. Therefore, they will make better political decisions, and better political decisions are vital for the future of humanity.\nMinsky was one of my major influences. Well, Minsky and Hebb and Asimov and Freud.\n\nSometime before 1968, he was inspired to do backpropagation from reading Freud.\n\nI talk in there about the concept of translating Freud into mathematics. This is what took me to backpropagation, so the basic ideas that took me to backpropagation were in this journal article in ’68 (P. Werbos 1968). I talked a lot about what was wrong with the existing [two state] McCulloch-Pitts neuron model, and how it was only “1” and “0.” I wanted to make that neuron probabilistic. I was going to apply Freudian notions to an upper-level, associative drive reinforcement system. When\n\nFor his masters (1969), he majored in mathematical physics, and minored in decision and control. During his PhD at Harvard, he had to take some compulsory courses, which he didn’t want to. To salvage the situation, he took computer courses which would provide him with some computer hours, a scarce resource at the time. I would guess the following event happened early 1971.\n\nInitially, I was going to do something on quantum physics. I learned something about partial differential equations, but not enough. I couldn’t produce a really useful product at the end of \\(x\\) number of months.\nSo I went back to the committee, and I said, “Gee, I can’t do that, but I have this little method for adapting multilayer perceptrons. It’s really pretty trivial. It’s just a by-product of this model of intelligence I developed. And I’d like to do it for my paper for this computer course.”\n[Larry] Ho’s position was, “I understand you had this idea, and we were kind of open-minded. But look, at this point, you’ve worked in this course for three months, admittedly on something else. I’m sorry, you’re just going to have to take an incomplete in the course.”\nAnd I said, “You mean I can’t do it?”\n“No, no, you’ll have to take an incomplete because, basically, the first thing didn’t work. We’re very skeptical this new thing is going to work.”\n“But look, the mathematics is straightforward.”\n“Yeah, yeah, but you know, we’re not convinced it’s so straightforward. You’ve got to prove some theorems first.”\nSo they wouldn’t let me do it. One of the reasons that is amusing to me is that there are now some people who are saying backprop was invented by Bryson and Ho. They don’t realize it was the same Larry Ho, who was on my committee and who said this wasn’t going to work.\n\nI am not sure if this is sarcastic or not. It reminds me of the “summer vision project” (Papert 1966) that expected some undergraduate students to construct “a significant part of a visual system” in a single summer.\n\nBy the time my orals came around, it was clear to me that the nature of reality is a hard problem, that I’d better work on that one later and finish my Ph.D. thesis on something small – something I can finish by the end of a few years, like a complete mathematical model of human intelligence.\n\nThe oral was amusing, and touched on the still-hot issue of recent human evolution.\n\n… I made a statement that there might be parameters affecting utility functions in the brain, parameters that vary from person to person. You could actually get a significant amount of adaptation in ten generations’ time. I was speculating that maybe the rise and fall of human civilizations, as per Toynbee and Spengler, might correlate with these kinds of things. The political scientist on the committee, Karl Deutsch, raised his hand. … His book, The Nerves of Government, which compares governments to neural networks, is one of the classic, accepted, authoritative books in political science.\nHe raised his hand and he said, “Wait a minute, you can’t get significant genetic change in ten generations. That cannot be a factor in the rise and fall of civilizations. That’s crazy.”\nNext to him was a mathematical biologist by the name of Bossert, who was one of the world’s authorities on population biology. He raised his hand and said, “What do you mean? In our experiments, we get it in seven generations. This guy is understating it. Let me show you the experiments.”\nAnd Deutsch said, “What do you mean, it’s common knowledge? All of our political theories are based on the assumption this cannot happen.” And Bossert said, “Well, it happens. Here’s the data.”\n… I passed the orals having said about two sentences and not having discussed models of intelligence.\n\nIt turns out Werbos interpreted backpropagation as the mathematical interpretation of Freudian psychic energy flow. The thesis committee was not amused.\n\nBut the backpropagation was not used to adapt a supervised learning system; it was to translate Freud’s ideas into mathematics, to implement a flow of what Freud called “psychic energy” through the system. I translated that into derivative equations, and I had an adaptive critic backpropagated to a critic, the whole thing, in ’71 or ’72. … The thesis committee said, “We were skeptical before, but this is just unacceptable … You have to find a patron. You must find a patron anyway to get a Ph.D. That’s the way Ph.D.s work.\n\nThe committee gave him three acceptable patrons. He first went to Stephen Grossberg.\n\n… he said, ’Well, you’re going to have to sit down. Academia is a tough business, and you have to develop a tough stomach to survive in it. I’m sure you can pull through in the end, but you’re going to have to do some adaptation. So I want you to sit down and hold your stomach, maybe have an antacid. The bottom line is, this stuff you’ve done, it’s already been done before. Or else it’s wrong. I’m not sure which of the two, but I know it’s one of the two.”\n\nThanks, Grossberg, for using the law of excluded middle to crush Werbos’ dream.\nHe then went to Marvin Minsky, who gave us some new clues about why backpropagation took so long to discover: “everybody knows a neuron is a 1-0 spike generator”!\n\n“I’ve got a way now to adapt multilayer perceptrons, and the key is that they’re not Heaviside functions; they are differentiable. And I know that action potentials, nerve spikes, are 1 or 0, as in McCulloch-Pitts neurons, but here in this book that I had for my first course in neurophysiology are some actual tracings. If you look at these tracings in Rosenblith, they show volleys of spikes, and volleys are the unit of analysis. This is an argument for treating this activity as differentiable, at least as piecewise linear. If you look at that, I can show you how to differentiate through it.”\nMinsky basically said, “Look, everybody knows a neuron is a 1-0 spike generator. That is the official model from the biologists. Now, you and I are not biologists. If you and I come out and say the biologists are wrong, and this thing is not producing 1s and 0s, nobody is going to believe us. It’s totally crazy. I can’t get involved in anything like this.”\nHe was probably right, I guess, but he was clearly very worried about his reputation and his credibility in his community.\n\nAlthough I believe Minsky would have disapproved of the idea of backpropagation even if he had thought that neurons are not strictly 1-0 spike generators. In the epilogue to (Minsky and Papert 1988), he claimed that gradient descent does not scale, and using differentiable activation functions is just a hack intended to make backpropagation work, a pointless hack, as backpropagation would not scale.\n\nHowever, we suspect that this smoothing trick may entail a large (and avoidable) cost when the predicate to be learned is actually a composition of linear threshold functions. There ought to be a more efficient alternative based on how much each weight must be changed, for each stimulus, to make the local input sum cross the threshold. … We have the impression that many people in the connectionist community do not understand that this is merely a particular way to compute a gradient and have assumed instead that back-propagation is a new learning scheme that somehow gets around the basic limitations of hill-climbing.\n\nFurther, in a 1991 interview, Minsky made the same kind of statement:\n\nI don’t know what they would have done with the money. The story of DARPA and all that is just a myth. The problem is that there were no good ideas. The modern idea of backpropagation could be an old idea. There was someone . . . [trying to remember].\nQuestion: Paul Werbos?\nAnswer: That’s it! [excited]. But, you see, it’s not a good discovery. It’s alright, but it takes typically 100,000 repetitions. It converges slowly, and it cannot learn anything difficult. Certainly, in 1970 the computers were perhaps too slow and expensive to do it. I know that Werbos thought of that idea. It’s certainly trivial. The idea is how you do gradient descent. I didn’t consider it practical.\nQuestion: Because of the computational costs?\nAnswer: Yes, but also, with artificial intelligence, we had the experience that when you make a process like that you usually get stuck at a local minimum. We still don’t have any theory of what range of problems they work well for.” (Minsky, interview) (Olazaran 1991, 249)\n\nOut of curiosity, I looked up the “Rosenblith” book (Rosenblith 2012) that Werbos mentioned, and indeed there were a few tracings that show continuously varying neural activation.\n\nFigure 1: Rosenblith images.\n\n\n\n\n\n\n(a) Page 146 of the book. Mechanisms of gustatory and olfactory receptor stimulation, Figure 2.\n\n\n\n\n\n\n\n(b) Page 366 of the book. The physiological basis of wavelength discrimination in the eye of the honeybee, Figure 4.\n\n\n\n\n\n\nThen Minsky dunked on reinforcement learning as well, because he had an unpublished “jitters machine” that failed to optimize its reward. Presumably the name “jitters machine” refers to how it would jitter in place, not able to move towards the goal.\n\nMinsky also said, “You know, I used to believe in all this kind of stuff with reinforcement learning because I knew reinforcement learning would work. I knew how to implement it. I had a nice guy named Oliver Selfridge who came in and acted as my patron and gave me permission to do it. We coauthored a paper, but it was really my idea, and he was just acting as patron on the Jitters machine. I’ll hand you the tech report, which we have deliberately never published.”\nIt was his bad experience with the Jitters machine that turned him off on reinforcement learning and all the neural net ideas. It just didn’t work. I later looked at that paper … He had a system that was highly multivariate with a single reinforcement signal. The system can’t learn efficiently with that. At any rate, he was totally turned off.\n\nThe brief description of the unpublished jitters machine was not making sense to me, so I looked around the literature. It was so unpublished that I found only two other references in the entire literature (P. J. Werbos 1982, 1987), both by Werbos. According to him,\n\nThere are also some technical reasons to expect better results with the new approach. Almost all of the old perceptron work was based on the McCulloch–Pitts model of the neuron, which was both discontinuous and non-differentiable. It was felt, in the 1950’s, that the output of a brain cell had to be 1 or 0, because the output consisted of sharp discrete “spikes.” More recent work in neurology has shown that higher brain cells output “bursts” or “volleys” of spikes, and that the strength of a volley may vary continuously in intensity. This suggests the CLU model to be discussed in the Appendix. In any event, to exploit basic numerical methods, one must use differentiable processing units. Minsky once experimented with a “jitters” machine, which estimated one derivative per time cycle by making brute-force changes in selected variables; however, the methods to be discussed in the Appendix yield derivatives of all variables and parameters in one major time cycle and thereby multiply efficiency in proportion to the number of parameters \\((N)\\), which may be huge. (P. J. Werbos 1987)\n\nThis makes things clear enough. The jitters machine was an RL agent with multiple continuously adjustable parameters running a very primitive form of policy gradient. During every episode, it would estimate \\(\\partial_{\\theta_i} R(\\theta)\\) using finite difference, but since it used finite difference, it could estimate the partial derivative for only one of the parameters \\(\\theta_i\\)! No wonder it never managed to learn. It is almost comical how much they failed to just use gradient descent. It sometimes feels as if they did everything to avoid just taking the gradient. In the case of Minsky, he made it very clear, in the epilogue of (Minsky and Papert 1988), that he did not believe in gradient descent, period. But what explains the gradient-phobia of the others…?\nAnyway, back to the interview. Werbos went to Jerome Lettvin, the neuroscientist famous for What the Frog’s Eye Tells the Frog’s Brain. Turns out he was a proto-eliminativist. While I am an eliminativist too, Werbos was a Freudian, which must have collided badly with eliminativism.\n\n“Oh yeah, well, you’re saying that there’s motive and purpose in the human brain.” He said, “That ‘s not a good way to look at brains. I’ve been telling people, ’You cannot take an anthropomorphic view of the human brain.’ In fact, people have screwed up the frog because they’re taking bachtriomorphic views of the frog. If you really want to understand the frog, you must learn to be objective and scientific.”\n\nWithout patrons, he faced the committee again.\n\nI tried to simplify it. I said, “Look, I’ll pull out the backprop part and the multilayer perceptron part.” I wrote a paper that was just that - that was, I felt, childishly obvious. I didn’t even use a sigmoid [non-linearity]. I used piecewise linear. I could really rationalize that to the point where it looked obvious. I handed that to my thesis committee. I had really worked hard to write it up. They said, “Look, this will work, but this is too trivial and simple to be worthy of a Harvard Ph.D. thesis.”\n\nOh, now it’s too trivial?? I swear the backstory of backpropagation gets more and more ridiculous by the day.\n\n… they had discontinued support because they were not interested, so I had no money. … Not even money to buy food. A generous guy, who was sort of a Seventh Day Adventist and a Harvard Ph.D. candidate in ethnobotany, had a slum apartment that rented for about $40 a month in Roxbury in the ghetto. He let me share a room in his suite, in his suite with the plaster falling off, and didn’t ask for rent in advance. I had no money at that time for food. There was a period of three months when I was living there in the slums. To conserve the little bit of money I had, I remember eating soybean soup and chicken neck soup. I remember getting the shakes from inadequate nutrition.\nFinally, they said, “Look, you know, we’re not going to allow this.” There was this meeting where we sat around the table. The chairman of the applied math department at that time was a numerical analyst D. G. M. Anderson. He said, “We can’t even allow you to stay as a student unless you do something. You’ve got to come up with a thesis, and it can’t be in this area.”\n\nKarl Deutsch, who believed in Werbos, sponsored his PhD thesis on a “respectable” problem: fitting an ARMA model to a time-series data of political regimes, and use it to forecast nationalism and political assimilation. Box–Jenkins method ran too slowly, so Werbos programmed in the backpropagation which worked, and he finally obtained his PhD in 1974. This is the original reference point for modern neural network backpropagation algorithm.\n\nDeutsch said, “You ’re saying we need an application to believe this stuff? I have an application that we could believe. I have apolitical forecasting problem. I have this model, this theory of nationalism and social communications? What causes war and peace between nations? I have used up ten graduate students who’ve tried to implement this model on real-world data I’ve collected, and they’ve never been able to make it work. Now, do you think your model of intelligence could solve this problem and help us predict war and peace?”\nThe first application of backpropagation in the world in a generalized sense was a command that was put into the TSP at MIT , available to the whole MIT community as part of their standard software. It was published as part of MIT ’s report to the DOD [the Department of Defense] and part of the DOD’s report to the world . It was part of the computer manual, so the first publication of backpropagation was in a computer manual from MIT for a working command for people to use in statistical analysis. I was a second author of that manual. It was Brode, Werbos, and Dunn.\n… Once I started doing software for them, they discovered I was pretty good at it. Once I had put backpropagation into a TSP command, they offered me a full-time job at the Harvard Cambridge Project, so I did the Ph.D. thesis while having a full-time job. While I was doing these things at MIT, I remember going to one party. … one of the people there said, ’We have heard through the grapevine that somebody has developed this thing called continuous feedback that allows you to calculate all the derivatives in a single pass.”\n\nBut the saga is not over. After the PhD, Werbos was hired by the DoD, and promptly got sucked into several more years of misadventure, which meant that he could not even talk about backpropagation in a public journal until 1978 – and the algorithm got removed anyway due to page limits. This annoyed the DoD and he had to move to the Department of Energy.\n\nI found out that DARPA had spent a lot of effort building a worldwide conflict forecasting model for the Joint Chiefs of Staff that was used in the long-range strategy division of the Joint Chiefs. … I wound up sending a couple of graduate students to create a really good database of Latin America. I said, “You want variance, high variance. Something hard to predict.” I thought conflict in Latin America would be the most beautiful case. I figured there were enough cultural homogeneities that it would be a single stochastic process, but with lots and lots of variance in that process. So we got truly annual data going back, I don’t know, twenty or thirty years for all the countries in Latin America and then reestimated the Joint Chief’s model on it. It had an r2 of about .0016 at forecasting conflict. By jiggling and jiggling we could raise it to about .05. It could predict GNP and economic things decently. Conflict prediction we couldn’t improve, though; it was hopeless.\nDARPA wasn’t happy when I published that result. They wanted me to do something real and practical and useful for the United States. This was useful, but it was an exposé. They didn’t like that. Therefore, the report, which included backpropagation in detail and other advanced methods, was not even entered into DOCSUB. Every time I tried to enter it into DOCSUB, somebody jiggled influence to say, “No, no, no, we can’t publish this. This is too hot.”\nIt was published in (P. J. Werbos and Titus 1978) anyway because they couldn’t block the journals, but it didn’t include the appendices. So that paper in 1978 said, “We’ve got this great thing called dynamic feedback, which lets you estimate these things. It calculates derivatives in a single swoop, and you can use it for lots of things, like AI.” … the appendix on how to do it was not there because of page limits … At that point, DARPA was no longer happy.\n\nSo he went to the Department of Energy, used backpropagation to create another model, and was silenced once again, unable to publish that report until 1988 (P. J. Werbos 1988).\n\nThey had a million-dollar contract at Oak Ridge National Laboratory to study that model. They wanted me for several things. One, they wanted me to be a translator between engineering and economics. Two, they wanted a critique. They wanted exposés. They wanted me to rip apart the models of the Department of Energy in a very scientific, objective way that didn’t look like I was trying to rip them apart but was anyway. That’s exactly what they wanted to hire me for, and I didn’t really know that was the motive. These particular people didn’t like modeling very much.\nSo at some point, they wanted sensitivity analysis. And I said, “You know, I know a little bit about calculating derivatives.” … I applied it to a natural gas model at the Department of Energy. In the course of applying it, I did indeed discover dirt. They didn’t want me to publish it because it was too politically sensitive. It was a real-world application, but the problem was that it was too real. At DOE, you know, you don’t have First Amendment rights. That’s one of the terrible things somebody’s got to fix in this country. The reality of the First Amendment has deteriorated. Nobody’s breaking the law, but the spirit of the First Amendment has decayed too far for science. At any rate, they finally gave me permission to publish it around ’86 and ’87. I sent it to the journal Neural Nets - that is, how you do simultaneous recurrent nets and time-lag recurrent nets together. Then the editorial process messed around with it, made the paper perhaps worse, and it finally got published in ’88, which makes me very sad because now I gotta worry about, ’Well, gee, didn’t Pineda do this in ’88?\n\nAs a side note, one might feel that Werbos’ “turning Freud into mathematics” seems rather strange. This feeling is completely justified. I found a recent paper by him (P. J. Werbos 2009) with this crackpot illustration:\n\n\n\nThe text at the end of the arrow says “quantum and collective intelligence (Jung, Dao, Atman…)?”."
  },
  {
    "objectID": "essays/posts/grokking-modular-arithmetics/index.html",
    "href": "essays/posts/grokking-modular-arithmetics/index.html",
    "title": "Grokking modular arithmetics",
    "section": "",
    "text": "This essay reproduces the paper A simple and interpretable model of grokking modular arithmetic tasks (Gromov 2023a). The code is available on GitHub at yuxi-liu-wired/grokking-modular-arithmetics."
  },
  {
    "objectID": "essays/posts/grokking-modular-arithmetics/index.html#setup",
    "href": "essays/posts/grokking-modular-arithmetics/index.html#setup",
    "title": "Grokking modular arithmetics",
    "section": "Setup",
    "text": "Setup\nGiven a natural number \\(N\\), we have modular arithmetic on \\(\\mathbb{Z}_N = \\{0, 1, ..., N-1\\}\\). For example, \\(\\mathbb{Z}_{12}\\) is the “clock face modular arithmetic”. The problem for our neural network is to learn binary functions on \\(\\mathbb Z_N\\). That is, we are to learn a binary function \\(f: \\mathbb Z_N\\times \\mathbb Z_N \\to \\mathbb Z_N\\).\nEach such binary function can be exactly specified by a \\(N\\times N\\) table, so there are \\(N^{N^2}\\) possible such functions. Most of them are completely random and uninteresting, both for us and for neural networks, but a few are very interesting, and modular addition is one such interesting function.\nFor example, modular addition on \\(\\mathbb Z_6\\) has the following multiplicative table:\n\n\n\n+\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n0\n1\n2\n3\n4\n5\n\n\n1\n1\n2\n3\n4\n5\n0\n\n\n2\n2\n3\n4\n5\n0\n1\n\n\n3\n3\n4\n5\n0\n1\n2\n\n\n4\n4\n5\n0\n1\n2\n3\n\n\n5\n5\n0\n1\n2\n3\n4\n\n\n\nSince the entire dataset is known and specified in advance, we can define the train set ratio \\(\\alpha = \\frac{|D_{train}|}{|D|}\\), where \\(D\\) is the full dataset (the multiplication table), and \\(D_{train}\\) is the training dataset. We expect that, as \\(\\alpha\\) approaches \\(1\\), the network becomes better at generalizing to the test set.\nThe network architecture we use has 3 layers:\n\nInput is \\(x = [x^{(1)}, x^{(2)}]\\). Both \\(x^{(1)}, x^{(2)} \\in \\mathbb R^N\\) are one-hot encodings of \\(\\mathbb Z_N\\).\nHidden layer activation is \\(z = \\phi(\\frac{1}{\\sqrt M} W^{(1)}z)\\), where \\(\\phi\\) is the activation function. Here \\(z \\in \\mathbb R^M\\) can be of any width.\nThe output is \\(y = \\frac{1}{N} W^{(2)}z\\), where \\(y \\in \\mathbb{R}^N\\) should be a one-hot encoding of \\(\\mathbb{Z}_N\\).\nAll entries of \\(W^{(1)}, W^{(2)} \\sim \\mathcal N(0, 1)\\) are initialized as standard gaussians.\n\\(W^{(1)}, W^{(2)}\\) are all the parameters of the network. There is no bias. Thus the network has \\(3MN\\) parameters in total.\n\n\n\n\nAn example network with the given architecture, with \\(N=3, M = 10\\).\n\n\nIn the paper, Gromov found that grokking occurs under different choices of activation functions \\(\\phi\\), different training methods (SGD, Adam, etc.), and different training set ratios \\(\\alpha\\).\nThe simplest example where grokking occurs is with\n\nQuadratic activation function: \\(\\phi(t) = t^2\\).\nFull-batch gradient descent.\nMSE loss.\n\nI used AdamW optimizer instead of standard gradient descent, since it converges faster. The dataset is formatted as an array of triples of the form \\((x_1, x_2, y)\\), interpreted as \\(x_1 + x_2 = y \\mod N\\). I split the dataset randomly into two datasets."
  },
  {
    "objectID": "essays/posts/grokking-modular-arithmetics/index.html#results",
    "href": "essays/posts/grokking-modular-arithmetics/index.html#results",
    "title": "Grokking modular arithmetics",
    "section": "Results",
    "text": "Results\n\nGrokking\n\n\n\nLearning curves showing grokking.\n\n\nSome observations:\n\nThe test set accuracy curve decreases as the training set accuracy increases to perfection.\nThe test set accuracy curve rises only after the training set accuracy is perfect. First slowly, then rapidly (“grokking”). This can be quite puzzling, since if the network has really achieved perfection on the training set, then there is nothing left to learn, and so it shouldn’t be able to improve any further – and yet it does improve.\nPerfect accuracy on the training set is reached at epoch 10x that of the training set.\nThe learning curves show something smoother, but also something interesting: the training loss decreases monotonically, but the test loss rises, then decreases.\nFor a while, the test loss rose while the test accuracy increased!\n\nSome lessons:\n\nGrokking might look less dramatic when plotted not by argmax-accuracy, but by MSE.\n\nSee for example (Power et al. 2022). One wonders what they would have found if they had plotted MSE losses instead of accuracies?\n(Nanda et al. 2023) does plot train and test loss, and in this paper, the grokking appears in the loss curves as well. This seems harder to understand using our small model (they used a Transformer).\n\nTrain loss can decrease while test loss increase, but this trend can also be reversed. The shape of learning curves is quite complex.\n\n\n\nInterpretation\nSince the neural network is so small, we can interpret it. What kind of neural network did we end up with that could do modular addition?\nDirectly inspecting the weight matrices, we notice suggestive wavy bands that resemble sine waves.\n\n\n\nThe weight matrices.\n\n\nLet’s sort them according to frequency, as found by running a Fourier transform and picking the highest peak:\n\n\n\nThe weight matrices, with columns ordered by frequency.\n\n\nWe see that the learned neural network is probably doing some Fourier transform. This can be confirmed by plotting the activation map on every hidden neuron. Specifically, for each hidden neuron, we can calculate its activation for each of the \\(N\\times N\\) possible inputs. This is plotted as a heatmap with \\(N\\times N\\) pixels. We then get one heatmap per hidden neuron and display all of them in a grid:\n\n\n\nThe activation pattern of neurons in the hidden layer as one scans through all possible inputs.\n\n\nWe see that the network has learned some sine waves. It seems to be a robust fact that networks trained to do modular arithmetic, with one-hot encoding, learn to use trigonometry for this task. (the use of one-hot encoding seems very relevant, as noted here).\n\n\nThe null hypothesis\nAs a good comparison with the above interpretation of the neural network, we leverage the same tools on the “null hypothesis”. There are two ways to do the null hypothesis: either initialize the neural network randomly and then interpret it, or initialize it, train it to perform a randomly generated binary operation, then interpret it.\nAs one would expect, the neural network can successfully memorize arbitrary binary operations, without generalization (as there is no pattern to generalize).\n\n\n\nLearning curves for a neural network trained on a randomly initialized binary operation.\n\n\nFor both null hypotheses, I tried interpreting them using the same methodology; they look as one might expect: complete noise.\n\n\n\n\n\n\n\n\nActivation maps.\n\n\n\n\n\n\n\nWeight matrices.\n\n\n\n\n\n\nFigure 1: The network randomly initialized.\n\n\n\n\n\n\n\n\n\n\n\nActivation maps.\n\n\n\n\n\n\n\nWeight matrices.\n\n\n\n\n\n\nFigure 2: The network trained to perform a randomly initialized binary operation.\n\n\n\n\n\nExtensions\nThis toy is small and simple. It runs in a minute. Here are some ideas for playing with the toy:\n\nModular multiplication.\nRandom operation (as a null hypothesis).\nDifferent activation functions (sine, ReLU).\nDifferent accelerators (SGD, Adam, etc)\nTwo hidden layers."
  },
  {
    "objectID": "essays/posts/grokking-modular-arithmetics/index.html#some-other-quotes",
    "href": "essays/posts/grokking-modular-arithmetics/index.html#some-other-quotes",
    "title": "Grokking modular arithmetics",
    "section": "Some other quotes",
    "text": "Some other quotes\nGrokking modular arithmetic (Gromov 2023b)\n\nIn particular, random feature models such as infinitely-wide neural networks (in the NTK regime) do not exhibit grokking, at least on the tasks that involve modular functions.\nIn our minimal setup, the simplest explanation for grokking is that once training loss reached a certain value, the only way to further minimize it is to start learning the right features.\n\nShortcut learning in deep neural networks (Geirhos et al. 2020)\n\nmany of deep learning’s failures can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in comparative psychology, education and linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike.\n\n\n\n\nFigure 3 from (Geirhos et al. 2020)\n\n\nThe Bitter Lesson (Sutton 2019)\n\nOne thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning.\n\nThe Scaling Hypothesis # Why Does Pretraining Work?\n\nEarly on in training, a model learns the crudest levels: that some letters like ‘e’ are more frequent than others like ‘z’, that every 5 characters or so there is a space, and so on. It goes from predicted uniformly-distributed bytes to what looks like Base-60 encoding—alphanumeric gibberish. As crude as this may be, it’s enough to make quite a bit of absolute progress: a random predictor needs 8 bits to ‘predict’ a byte/character, but just by at least matching letter and space frequencies, it can almost halve its error to around 5 bits…\n… a sample will state that someone is “alive” and then 10 sentences later, use the word “dead”, or it will digress into an irrelevant argument instead of the expected next argument, or someone will do something physically improbable, or it may just continue for a while without seeming to get anywhere.All of these errors are far less than &lt;0.02 bits per character; we are now talking not hundredths of bits per characters but less than ten-thousandths.The pretraining thesis argues that this can go even further: we can compare this performance directly with humans doing the same objective task, who can achieve closer to 0.7 bits per character. What is in that missing &gt;0.4?\nThe last bits are deepest. The implication here is that the final few bits are the most valuable bits, which require the most of what we think of as intelligence."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html",
    "href": "essays/posts/neural-scaling-laws/index.html",
    "title": "Fermi Estimation for Neural Networks",
    "section": "",
    "text": "Fermi estimation is a technique for estimating quantities through rough approximations and educated guesses. Named after physicist Enrico Fermi, this method involves breaking down a problem into simpler, more manageable parts, making reasonable assumptions, and using simple arithmetic to arrive at a good enough answer.\nThis post walks you through a simple example of Fermi estimation of a deep learning model. Specifically, we will try to estimate the design parameters of a hypothetical GPT-5 with 1 trillion parameters. Along the way, we will touch upon various other items like the Landauer limit, carbon taxing, etc."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#gpt-like-agi",
    "href": "essays/posts/neural-scaling-laws/index.html#gpt-like-agi",
    "title": "Fermi Estimation for Neural Networks",
    "section": "GPT-like AGI",
    "text": "GPT-like AGI\nLet’s get into the mood for Fermi estimation, by estimating the number of parameters necessary for achieving human-level Artificial General Intelligence, under two assumptions: that a GPT-like architecture is enough; that it requires only matching the human brain in the “numbers”.\nLet’s estimate how many layers the hypothetical GPT model should have. When prompted with a question, the first answer comes to mind in about a second, and it is generally a good one, if not the best. Perhaps slow deliberation is nothing but stringing together a long chain of snap judgments, guided by snap judgments about snap judgments.\nThe characteristic time-scale of a brain is 0.01 seconds – the fastest brainwave, gamma wave, is 100 Hz. This indicates that the brain takes on the order of 100 steps to make a snap decision – the “hundred-step-rule” of Jerome Feldman(Feldman and Ballard 1982). This is suspiciously close to the depth of the largest model of GPT-3, which has 96 layers.\nHow many parameters would such a model require? The brain has \\(10^{15}\\) synapses. It’s unclear how precise each synapse is, but one estimate states that the hippocampal synapse has a precision of about 5 bits (Bartol Jr et al. 2015), which can be stored within a 16-bit floating point number, with room to spare.\nAssuming that, we expect an AGI GPT to have \\(10^{15}\\) parameters, or 1000× that of our hypothetical GPT-5."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#chinchilla-scaling-law",
    "href": "essays/posts/neural-scaling-laws/index.html#chinchilla-scaling-law",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Chinchilla Scaling Law",
    "text": "Chinchilla Scaling Law\nThe paper Training Compute-Optimal Large Language Models (Hoffmann et al. 2022) reported a series of training runs on language models, trained by Google DeepMind researchers. Each training run is characterized by four numbers:\n\n\\(L\\): the final loss (negative log-likelihood per token) achieved by the trained model.\n\\(N\\): the number of parameters in the model.\n\\(D\\): training dataset size, measured in tokens.\n\\(C\\): training compute cost, measured in FLOP.\n\nAfter training a few hundred models, they obtained a large dataset of \\((L, N, D, C)\\), and they fitted a statistical law of the form\n\\[L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0,\\]\nwhere the parameters are\n\\[\\alpha = 0.34, \\beta = 0.28, A = 406.4, B = 410.7, L_0 = 1.69.\\]\nThey also estimated that the cost of training compute \\(C\\) is proportional to \\(ND\\). This is understandable, because each token must flow through the entire model and “hit” each parameter once, incurring a fixed number of floating point operations. They estimated that it takes 6 FLOPs per parameter per token. That is,\n\\[C = C_0 ND, \\quad C_0 = 6\\]\nGiven the assumptions, for each fixed computing budget \\(C\\), we can solve for the optimal \\(D\\) and \\(N\\), which is usually referred to as “Chinchilla optimal” training:\n\\[\\begin{cases}\n        \\min_{N, D} L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0,\\\\\n        \\text{such that } C_0 ND = C.\n\\end{cases}\\]\nSolve the above equations symbolically to find \\(N_{opt}, D_{opt}\\) as a function of \\(C, C_0, \\alpha, \\beta, A, B\\). Then, plug in the numerical values of the parameters, to find a numerical expression for \\(N_{opt}, D_{opt}\\) as a function of \\(C\\).\n\n\nSolution\n\nSince \\(C = C_0 ND\\), we have \\(N = \\frac{C}{C_0 D}\\). Plug it into \\(\\min_{N, D} L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0\\), we obtain\n\\[\\min_{D} L = \\frac{A}{(\\frac{C}{C_0 D})^\\alpha} + \\frac{B}{D^{\\beta}} + L_0.\\]\nTake derivative with respect to \\(D\\) and set it to zero. We get an expression for \\(D_{opt}\\). Plug it back to \\(C = C_0 ND\\), we get an expression for \\(D_{opt}\\). These simplify to:\n\\[N_{o p t}(C)=G\\left(\\frac{C}{C_0}\\right)^a, \\quad D_{o p t}(C)=G^{-1}\\left(\\frac{C}{C_0}\\right)^b, \\quad \\text { where } \\quad G=\\left(\\frac{\\alpha A}{\\beta B}\\right)^{\\frac{1}{\\alpha+\\beta}}, a=\\frac{\\beta}{\\alpha+\\beta}, b=\\frac{\\alpha}{\\alpha+\\beta}.\\]\nPlugging in the numerical values, we get\n\\[\\begin{cases}\n        N_{opt}(C) = 0.6 \\; C^{0.45} \\\\\n        D_{opt}(C) = 0.3 \\; C^{0.55} \\\\\n        L_{opt}(C) = 1070 \\; C^{-0.154} + 1.7\n    \\end{cases}\n    \\]\n\nIn the same paper, they also performed a direct statistical fitting, to find the optimal \\(N, D\\) for a given \\(C\\), without going through the intermediate steps above. This gives a slightly different result (only slightly different – as you would know after solving the previous problem):\n\\[N_{opt}(C) = 0.1 C^{0.5}; \\quad D_{opt}(C) = 1.7 C^{0.5}.\\]\nFor the rest of the essay, we will use these equations as the Chinchilla scaling laws. Do not use the equations you derived for the previous problem.\nSuppose we decide that our next AI should have 1 trillion (\\(N = 10^{12}\\)) parameters, and we use Chinchilla scaling laws. How much compute would it cost to train, and how many tokens would its training dataset have?\n\n\nSolution\n\n\\(N = 0.1 \\times C^{0.5} = 10^{12}\\), so \\(C= 10^{26}\\) FLOP, and \\(D = 1.7 \\times 10^{13}\\), or 17 trillion tokens."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#dataset-size",
    "href": "essays/posts/neural-scaling-laws/index.html#dataset-size",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Dataset size",
    "text": "Dataset size\nAssuming each English word cost about 1.4 tokens, how many English words would 10 trillion tokens be? Assuming each page has 400 words, and each book has 300 pages, how many books is that? To put it into context, look up the size of Library of Congress, and Google Books, and compare with the number we just calculated.\n\n\nSolution\n\n\\(10 / 1.4 = 7\\) trillion words. If each book has \\(400 \\times 300 = 0.12\\) million words, then that is 60 million books, if they were all in English.\n\nSince humans are kind of the same everywhere, book lengths should be kind of the same everywhere – information-dense languages would naturally have books with lower apparent word-count, but the same information (measured in bytes)."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#memory-requirement",
    "href": "essays/posts/neural-scaling-laws/index.html#memory-requirement",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Memory requirement",
    "text": "Memory requirement\nTypically, a deep learning model has 16-bit floating point parameters. Modern systems sometimes use lower precision (e.g. 8-bit) floating point numbers to save space, but generally it is necessary to use at least 16-bit during training, and the model is converted to lower precision after training (“post-training quantization”).\nGiven that each parameter is a 16-bit floating point number, how much memory does it cost to store a model with 1 billion parameters? How about our hypothetical GPT-5, which has 1 trillion parameters? How many A100 GPU (VRAM = 40 GB) would be required to contain the full GPT-5 model?\n\n\nSolution\n\n1 billion parameters is 2 billion bytes, or 2 GB. Our hypothetical GPT-5 would take up 2 TB. It would take 50 A100 to contain it.\nNow, 50 A100 GPUs has a running cost of about 100 USD/hr, but not such a terrible problem, but then, during training, you would have to run an optimizer like Adam, which would need three floating point numbers per parameter (gradient, momentum, scale), instantly increasing the memory requirement to 200 A100 GPUs.\n\n\nMemory cost\nThis table1 gives the price per megabyte of different storage technology, in price per megabyte (2010 dollars), up to 2018.1 Source: Storage 2: Cache model – CS 61 2018.\n\n\n\nYear\nMemory (DRAM)\nFlash/SSD\nHard disk\n\n\n\n\n~1955\n$411,000,000\n\n$6,230\n\n\n1970\n$734,000.00\n\n$260.00\n\n\n1990\n$148.20\n\n$5.45\n\n\n2003\n$0.09\n$0.305\n$0.00132\n\n\n2010\n$0.019\n$0.00244\n$0.000073\n\n\n2018\n$0.0059\n$0.00015\n$0.000020\n\n\n\nThe same costs relative to the cost of a hard disk in ~2018:\n\n\n\nYear\nMemory\nFlash/SSD\nHard disk\n\n\n\n\n~1955\n20,500,000,000,000\n\n312,000,000\n\n\n1970\n36,700,000,000\n\n13,000,000\n\n\n1990\n7,400,000\n\n270,000\n\n\n2003\n4,100\n15,200\n6.6\n\n\n2010\n950\n122\n3.6\n\n\n2018\n295\n7.5\n1\n\n\n\nSuppose long-term memory storage can last 1 year before being replaced. How much money does it cost per year to store a 1 trillion parameter model on an SSD, the most expensive form of long-term storage? How much money does it cost to store a 1 trillion parameter model on DRAM memory? Use 2018 prices.\n\n\nSolution\n\nSSD cost 0.00015 USD/MB in 2018, or 0.15 USD/GB. 1 trillion parameters cost 2000 GB of storage, or about 300 USD. So the total cost of storage is 300 USD/year, just a rounding error.\nIn contrast, DRAM cost 0.006 USD/MB, which is 40x that of SSD, so the total cost is 12000 USD.\nNow, compared with the cost of A100 GPU themselves? It takes about 50 A100 GPU to run the model, and each would cost about 20,000 USD. So the cost of long-term memory is essentially zero, and the cost of DRAM is about \\(\\frac{12000}{20000\\times 50} = 1\\%\\) of the total cost of GPU.\nSo what is the limit? The memory bandwidth, which we will see in the next question.\n\n\n\nMemory bandwidth and latency\nWhile the memory itself is cheap, moving the data requires expensive high-bandwidth wiring. Indeed, the memory bandwidth between the DRAM (or “VRAM” for “Video RAM”) and the little processors on the GPU is a main bottleneck on how good the GPU can perform.\nDuring a single forward pass of a model, the parameters of the model are loaded from the DRAM of the GPU into the fast cache memories, then pushed through the thousands of computing processors on the GPU.\nA100 GPU has a memory bandwidth of 1.6 TB/s.\nWhat is the minimal latency, in seconds, for the GPU to perform a single forward pass through our hypothetical GPT-5 model with 1 trillion parameters? How many tokens can it output (autoregressively) in one minute? How about GPT-3 (175 billion parameters)?\nSince we are just trying to compute an order-of-magnitude estimate, let’s assume for the problem that the model fits onto a single DRAM on a single GPU. You can also ignore the need to read/write model activations and optimizer states.\n\n\nSolution\n\nSince the model takes up 2 TB of memory, it would take at least 1.3 seconds to even load the model from DRAM, during a single forward pass.\nAutoregression means that the next token cannot be generated until the previous one is already generated, so the model can only generate one token per 1.3 seconds, or 46 tokens per minute. A Chat-GPT5 would be a very slow talker!\nHowever, it can run in batch mode. So for example, it might be able to run 1 million conversations in parallel, outputting one token for all conversations every 2 seconds. Latency is the hard limit, but throughput is not.\nGPT-3 with 175 billion parameters would run 1000/175 times faster, at 262 tokens per minute, about the speed of a fast human talker. This corresponds well with the general impression that ChatGPT types about 200 words a minute.\n\n\n\nBatch inference\nThere are some ways to improve latency. For example, the model can be parallelized over several GPUs, which effectively increases the memory bandwidth. For example, “tensor parallelism” splits each layer into several GPUs.\nThere is also “pipeline parallelism”, which splits the model into layers. The first few layers go into one GPU, the next few go into another, and so on. This does NOT decrease latency, but it does increase throughput.\nThe fundamental bottleneck in an autoregressive model like GPT is that, by design, they have to start a sentence without knowing how it will end. That is, they have to generate the first token before generating the next one, and so on. This can never be parallelized (except by egregious hacks like speculative decoding).\nOne reason Transformers dominated over RNN is that training and inferring an RNN both must be done one-token-at-a-time. For Transformers, training can be done in parallel over the entire string. Inferring however still cannot be parallelized.\nParallelization tends to be a deeply troublesome business – parallel programming is generally deeply troublesome. However, there is a general trick to improve throughput: batch mode. For example, GPT-5 might be able to run 1 million conversations in parallel, outputting one token for all conversations per forward pass. This trick works until the batch size is so large that the activations due to tokens takes up about as much memory bandwidth as the model parameters.\nConcretely, we can estimate what is a good batch size for GPT-3 175B. It has 96 attention layers, each with 96×128-dimension heads. It is typically run with 16-bit precision floating point numbers. For a single token, how many megabytes would all the floating point activations cost?\n\n\nSolution\n\nA single token would cost \\(96 \\times 96 \\times 128\\) floating point activations, or about 2.4 MB.\n\nThe model itself has 175 billion 16-bit floating point parameters, taking up about 350 GB. How many tokens do we need to put into a batch, before the activations occupy the same amount of memory as the model parameters?\n\n\nSolution\n\nIn order for the activations of tokens to occupy the same memory as the GPT-3 parameters itself, we need about \\(\\frac{350 \\;\\mathrm{GB}}{2.4 \\;\\mathrm{MB}} = 0.15 \\text{million tokens}\\).\nIf we count the optimizer states for the model during training, then GPT-3 takes up \\(4 \\times 350 \\;\\mathrm{GB} = 1.4 \\;\\mathrm{TB}\\), and so we need about 0.6 million tokens.\nThis explains why during training, the batch sizes of the largest models typically are on the order of 1 million tokens. It also shows why there is a natural drive towards scale – only the largest companies can expect to regularly run 0.2 million chats simultaneously."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#training-cost",
    "href": "essays/posts/neural-scaling-laws/index.html#training-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Training cost",
    "text": "Training cost\nHow much money does compute cost? We can work through an example using the current standard computing hardware: Nvidia A100 GPU (40 GB VRAM version).\nThe most important specifications are:\n\nUnit price: 15000 USD.\nRental price: 2 USD/hr.\nSpeed: 0.3 petaFLOP/s = 3E14 FLOP/s.\nPower: 0.3 kW.\nMemory bandwidth: 1600 GB/s.\n\nIn the literature about the largest AI models, the training cost is often reported in units of “petaFLOP-day”, which is equal to 1 petaFLOP/second x 1 day. How many FLOP is 1 petaFLOP-day? What is the equivalent number of A100-hour? If we were to buy 1 petaFLOP-day of compute with rented A100 GPU, how much would it cost?\n\n\nSolution\n\n1 petaFLOP-day = 1 petaFLOP/second x 1 day = 1E15 FLOP/s x 8.64E4 s = 8.64E19 FLOP.\nSince 1 A100 can run at 0.3 petaFLOP/s, getting 1 petaFLOP-day requires us to run 1/0.3 A100 for 1 day, or 80 A100-hours. At the price of 2 USD/hr, it would cost 160 USD.\n\nThe largest model of GPT-3 cost 3640 petaFLOP-days to train (according to Table D.1 of the report). How much would it cost if it were trained with A100? How much money does it cost to train our hypothetical GPT-5?\n\n\nSolution\n\n2E25 FLOP = 0.2 amount of GPT-5 = 17 million A100-hours = 33 million USD.\nAnd accounting for the utilization rate of 30%, that would give us 110 million USD.\nOh, and if you want some kind of official confirmation? OpenAI’s CEO Says the Age of Giant AI Models Is Already Over | WIRED\n\nAt the MIT event, Altman was asked if training GPT-4 cost $100 million; he replied, “It’s more than that.”\n\n\nIn reality, the GPU cannot be worked to their full speed, and we only use about 30% of its theoretical peak FLOP/s (so for example, for A100, we only get 0.1 petaFLOP/s, instead of 0.3 petaFLOP/s).2 For this question, we assume that the utilization rate is 100%.2 The utilization rate of 30% is according to EpochAI.\nAlso, we are assuming the training happens in one go, without hardware failures, divergences, and other issues requiring restart at a checkpoint. There is not much published data from large-scale training, but the OPT-175B training run (described later) took 3 months to complete, but would have taken only 33 days if there were no need for the many restarts. This suggests that the restarts would increase the computing cost by 2× to 3×.\nFor context, here are the costs of development of various items3:3 Sorry, not adjusted for inflation to the same year, but they are roughly in the range of 2000–2020 USD.\n\niPhone 1: 150 million USD.\nA typical 5 nm chip: 0.5 billion USD.\nAirbus A380: 18 billion USD. (Bowen 2010, Table 4.3)\nThree Gorges Dam: 250 billion CNY, or about 30 billion USD.\nManhattan Project: 24 billion USD (2021 level)\nApollo Program: 178 billion USD (2022 level)\n\nComment on the cost of our hypothetical GPT-5. Is it on the order of a large commercial actor like Google, or a state actor like China?\n\n\nSolution\n\nThe cost is on the order of 1 billion USD, which is roughly the largest projects undertaken by private corporations. Larger models, as those contemplated by AGI projects in Google, OpenAI, etc, have historically been undertaken by state actors only.\n\nHere is another way to estimate. Building a large AI model, according to accountancy, would be investment into a production machine, and thus part of “capital expenditure” (“CapEx”). We can find out what proportion is 1 billion USD in their total CapEx. I looked it up here.\n\nDuring the 2020–2022, Microsoft has yearly CapEx on average 25 billion USD.\nGoogle has about 25 billion USD.\nMeta, 20.\nAmazon, 63.\n\nIn short, training something like GPT-5 would cost a lot, on the order of 5% of total CapEx, even by the standards of the largest companies. It can probably scale up by another factor of 10× if they really push it, but not more.\nIn order to train even larger AI models, those AI models must enter production. They must become a productive member of society, otherwise the company would nott have the money to train them.\nMicrosoft announces new supercomputer, lays out vision for future AI work (2020):\n\nThe supercomputer developed for OpenAI is a single system with more than 285,000 CPU cores, 10,000 GPUs and 400 gigabits per second of network connectivity for each GPU server. Compared with other machines listed on the TOP500 supercomputers in the world, it ranks in the top five, Microsoft says.\n\nThe largest companies, like Microsoft have GPU on the order of 10000 A100. What is the wall clock hour of training GPT-5, assuming you have 10000 A100, perfect utilization, and no interruptions?\n\n\nSolution\n\n83 million hours / 10000 = 350 days. Almost exactly 1 year.\n\n\nThe difficulty of large-scale training\nLarge models do end up diverging many times during training runs, requiring restarts. Sometimes the divergence is so bad that the whole training run must be started from scratch. Sometimes the convergence is too slow, requiring fixes to the optimizer and learning rate schedules, etc. All together, we should expect the failures, restarts, deadends… to triple the cost at least, to ~1 billion USD.\nIt is not easy to find “stories from the trenches” for actually training large-scale neural networks that really show this, but thanks to Meta, we have one. In 2021, a team of 5 engineers from Meta trained a LLM with 175 billion parameters, in 3 months, using 1024 80GB A100 GPUs from. Excluding all the divergences, hardware failures, and other issues that caused lost progress, the final model would have taken about 33 days of continuous training.\nThey have kept journals during their training. This is now published at metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq · GitHub. You can see how difficult it is to train a large model. Selected quotes:\n\nThese notes cover ~90 restarts over the course of training the lineage of this current model (experiments 12.xx).\n\n\nFound issues with the new dataset where perplexity was unreasonably low… After applying as much regex-ing as we could to salvage the dataset, we relaunched another set of experiments to test LPE (experiments 20-29) on the new dataset. We didn’t have time to retrain a new BPE on the final dataset, so we fell back to using the GPT-2 BPE.\n\n\nFrom experiment 11.4 onward, we saw grad norm explosions / loss explosions / nans after a couple hundred updates after each restart, along with extremely unstable loss scales that would drop to the point of massively underflowing. We started taking more and more drastic actions then, starting with increasing weight decay to 0.1, lowering Adam beta2 to 0.95, lowering LR again, until finally by experiment 11.10 we hot-swapped in ReLU and also switched to a more stable MHA calculation (noting that the x**3 term in GeLU might be a source of instability with FP16).\n\n\n\nOn November 11, we started training our 12.00 experiment with all of these changes, and since then, the only restarts we’ve had to make were all related to hardware issues (missing GPUs on instances, training randomly hanging after including a new node, ECC errors, partial checkpoint upload after hardware error, CUDA errors, NCCL errors, etc.).\n\n\nReplacement through the cloud interface can take hours for a single machine, and we started finding that more often than not we would end up getting the same bad machine again.\n\n\nThere were also issues with blob store when downloading 1.6TB of a single model checkpoint (992 files, each ~1.7GB) on restarts, at which point the downloads themselves would start hanging nondeterministically, which then delayed training recovery even further.\n\n\nWe managed to hit our top three record long runs of the experiment these past two weeks, lasting 1.5, 2.8, and 2 days each! If we were to look at only the runs that have contributed to pushing training further and plot training perplexity against wall clock time, we get the following [The breaks are due to the Thanksgiving holiday]:"
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#inference-cost",
    "href": "essays/posts/neural-scaling-laws/index.html#inference-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Inference cost",
    "text": "Inference cost\nInference cost a lot less money than training, but it’s still a substantial cost. For Transformer language models, it costs about 2 FLOPs per parameter to infer on one token.\nGiven that GPT-3.5 has 175 billion parameters, how many FLOPs would it take to infer on 1 million tokens? How much money would it cost if it were run with A100?\n\n\nSolution\n\n\\(175 \\;\\mathrm{billion} \\times 1 \\;\\mathrm{million} \\times 2 = 4\\times 10^{17} \\;\\mathrm{FLOPs}\\). Now one A100-hour is \\(8.64\\times 10^{19} \\;\\mathrm{FLOPs}\\), so that is 1/200 A100-hour, or about 0.01 USD.\nThe price offered by OpenAI is 2 USD per 1 million tokens, so it’s a very profitable business… but see next question.\n\nThe price offered by OpenAI is 2 USD per 1 million tokens. Assuming that GPT-3.5 cost 10 million USD to develop and train, how many tokens must be sold, just to recoup the cost? Assuming each English word cost about 1.4 tokens, and each essay is 1000 words, how many essays would it be equivalent to?\n\n\nSolution\n\nSince the cost is almost negligible (1/200 of the revenue), we can pretend it’s all profit. So, the profit is 2 USD per 1 million tokens. We need to recoup 10 million USD, so we need \\(10 \\;\\mathrm{million} / 2 \\times 1 \\;\\mathrm{million} = 5\\times 10^{12} \\;\\mathrm{tokens}\\), or 4 billion essays.\nAbout one essay per person on earth, or 10 essays per person in America… is that too much to ask?\n\nMoore’s law applies to both CPU and GPU. From 2006 to 2021, GPU FLOP/s per dollar has been doubling every 2.5 years. We can fairly well assume that the price of petaFLOP-day also decreases at the same rate.\n\n\n\n\n\nAssuming Moore’s law continues to hold indefinitely, and that there will be no algorithmic progress. When would GPT-3 become as cheap as a typical hobbyist project, say, 1000 USD?44 Since a 2006 GPU and a 2020 GPU both have the same lifespan (1–4 years usually), and their cost of electricity is only a small proportion of their running expense, the price of GPU FLOP/s per dollar at the point of purchase is roughly proportional to the rental rate of GPU.\n\n\nSolution\n\nGPT-3 cost 6 million USD to train. For it to cost just 1000 USD, we need to drop its price 6000-fold. Now, \\(\\log_2(6000) \\times 2.5\\; \\mathrm{year} = 30 \\; \\mathrm{year}\\). So it would be around 2050."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#energetic-cost",
    "href": "essays/posts/neural-scaling-laws/index.html#energetic-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Energetic cost",
    "text": "Energetic cost\nThe Landauer limit states that the cost of erasing one bit of information is \\(E = k_B T \\ln 2\\), where \\(k_B\\) is the Boltzmann constant, and \\(T\\) is the temperature of the computing machinery. At room temperature, \\(T = 300 K\\), giving us \\(E = 3\\times 10^{-21} J\\).\nNow, one FLOP is a floating point operation, meaning that you start with two 32-bit objects and end up with a single 32-bit object. You start with 64 bits and end up with just 32 bits, and so you lose 32 bits. So by the Landauer limit, the minimal energetic cost is \\(32 k_B T \\ln 2\\).\nGiven this, what is the minimal energy required for performing one FLOP? What is the minimal power (in Watts) required to perform 300 TFLOP/s, as in A100 GPU? Compared this to the actual value of 300 Watts.\n\n\nSolution\n\nThe energy per FLOP is \\(E_{FLOP} = 32 \\times 3\\times 10^{-21} J = 10^{-19} J\\). At 300 TFLOP/s, we need \\(P_{A100} = 3\\times 10^{14} E_{FLOP}/s = 3\\times 10^{-5}W\\). The actual value of 300 Watts is 10 million times more than the theoretical minimum.\nThere is still plenty of room at the bottom!\n\nFor context, the equivalent FLOP/s for the human brain is controversial, but a brief scan through the review article says that it should be about 1E18 FLOP/s. The energetic power of the brain is well-known: about 30 Watts. This means the brain is about 30000x more energy-efficient than A100 GPU, but still 300x less efficient than the theoretical minimum.\n\nThe lowest possible power for life\nThe slowest metabolism found on earth (so far) is in microbes living below deep ocean surface. They had to survive on the energy of “marine snow” falling to the surface, then slowly diffusing through the subsurface mud. Their power consumption is estimated at… \\(10^{-21} W\\). Since their temperature is about the temperature of the ocean water, which is liquid, their temperature is still about \\(T = 273 K\\), and so the Landauer limit is still about \\(3\\times 10^{-21} J\\). This shows that they can lose at most 500 bits every day.\nMost of the operations must be spent on simply repairing biomolecules, since repairing is effectively a 1-bit-losing operation: you start with a molecule that could be “damaged” or “fine”, and end up with a molecule that is “fine”, losing 1 bit. In fact, there are usually many possible ways to be “damaged”, so you would lose several bits.\nFor example, suppose you have a base-pair-mutation in one copy of the DNA. You want to fix it by checking with the other copies. This then means you start with 4 possible states (ACGT) and end up with just 1 state, losing 2 bits in the process."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#environmental-cost",
    "href": "essays/posts/neural-scaling-laws/index.html#environmental-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Environmental cost",
    "text": "Environmental cost\nAccording to “Carbon emissions and large neural network training”(Patterson et al. 2021), the carbon emission of training GPT-3 is 552 tCO2. According to a 2021 poll of climate economists, 1 tCO2 emission should cost somewhere between 50 and 250 USD. Let’s take their geometric average of 112 USD.\nIf we add all the tCO2 cost to the training of GPT-3, how much more expensive would it be? Compare that with its A100-GPU cost of training.\n\n\nSolution\n\n\\(112 \\times 552 = 62,000 \\;\\mathrm{USD}\\).\nPreviously, we calculated the GPU cost of training to be around 6 million USD. So fully carbon-pricing the emission would make it 1% more expensive.\nGenerally, adding in the tCO2 cost of training the largest models increase the cost by about 1 to 2 % (I have computed it in several ways). Since the global GDP increases by about 2% a year, this shows that fully accounting for the carbon cost of AI would delay it by perhaps 1 year.\n\n\n\nSide note for economics students\n\nYou might argue that fully carbon-taxing the global economy would raise not just electricity prices, but also the price of production, etc. For example, the global shipping industry uses fuel that has a high carbon emission.\nHowever, if the global shipping industry were to fully add carbon taxing into the fuel cost, then GPU prices would rise a little, but food prices would rise a lot. Why? Simply put, the shipping cost of a GPU, even if we were to take account of all the trips its components have made around the world, is still small. After all, one A100 GPU is just 1.7 kg, but sells for 15000 USD, so a tonne of A100 GPU can sell for 9 million USD.\nTo put it into perspective: the price of US maize is about 200 USD/tonne. So if shipping cost were to increase so much as to double the price of maize, it would only increase price of A100 by 1/45000.\nEven if we were to assume that producing an A100, being a commodity with a globally scattered supply chain, means shipping its components around the world 1000 times, the total shipping cost of A100 would only be increased by 1/45 = 2.2%.\nIn other words, properly pricing carbon emissions would only increase the price of AI by up to 5%, even as it doubles the price of food. AI is already green (within 5%), but food is far from green. That’s even before we get to the subsidies! We don’t have GPU subsidies, but we certainly have corn subsidies…\nIn this regard, it’s not “Green AI” that is important, but Green Corn, Green Timber, and all the other bulk commodities.\n\nTo put the number in another context, compare it with some typical American food. According to Our World in Data, it cost about 50 kg of CO2 emission per 1 kg of beef.\nAlso, an average American person (not household) consumed 38 kg of beef in 2020.\nCompare the CO2 emission of GPT-3 and CO2 emission from beef consumption. Assuming each burger (“quarter pounder”) contains 1/4 pound (113 grams) of beef. How many burgers would be equivalent to the CO2 emission of GPT-3?\n\n\nSolution\n\n113 grams of beef emits about 5.6 kg of CO2, so GPT-3 emits about (552 ton)/(5.6 kg) = 90,000 burgers. It is about the same amount as 250 people eating one burger everyday, for a whole year.\n38 kg of beef gives about 2 tCO2 emission. So the emission of GPT-3 is equal to the emission due to beef consumption of 276 American persons.\n\nThis strongly argues against the idea that we need “Green AI”5:5 Green AI is such a ridiculous term. Consider AAA games, or Hollywood movies; every one of them cost more than the GPT-4 training run. When are we going to make those green?\n\nTo help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark… We believe such new incentives could lead to a virtuous cycle where ML practitioners compete to increase accuracy while lowering energy consumption and CO2e that could bend the curve of ML carbon footprint growth for computationally intensive NLP models. (Patterson et al. 2021)\n\nOne, the carbon footprint of ML is already minimal, compared to the cost of just making all the GPUs.\nTwo, accounting for CO2 is a dreadfully boring business,6 and should be done by the civil servants – what else are they hired for, if not to deal with the boring stuffs? The ML practitioners are not climate economists or accountants, and they should not be burdened by accounting for climate change. If the price is right, then they simply need to budget how much money they want to spend on the training run, and the market would optimize the rest, including optimizing the right level of climate change76 If you don’t believe me, try reading (Patterson et al. 2021).7 The right level of climate change is not “none”, but rather “when the marginal cost equals the marginal benefit”. This might sound controversial, but it is just introductory economics.\n\nLuckily, at this point the orthodoxy of the academic economists is very much a minority position among intellectuals in general; one can seem to be a courageous maverick, boldly challenging the powers that be, by reciting the contents of a standard textbook. (Krugman 2002)\n\n\nIn one sentence: There need be no new incentive other than the profit motive."
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html",
    "href": "essays/posts/perceptron-controversy/index.html",
    "title": "The Perceptron Controversy",
    "section": "",
    "text": "During the 1950s and 1960s, the connectionist and symbolic schools of artificial intelligence competed for researcher allegiance and funding, with the symbolic school winning by 1970. This “first neural network winter” lasted until the rise of the connectionist school in the 1980s.\nIt is often stated that neural networks were killed off by the 1969 publication of Perceptrons by Marvin Minsky and Seymour Papert. This story is wrong on multiple accounts:\n\nMinsky and Papert had been working towards killing off neural networks since around 1965 by speaking at conferences and circulating preprints.\nThe mathematical content studies only the behavior of a single perceptron. It does not study multilayer perceptrons.\nBy 1969, most researchers had already left connectionism, frustrated by the lack of progress, as they did not develop backpropagation or deep learning. The last holdout, Frank Rosenblatt, died in 1971.\n\nThe book achieved its mythical status as the “neural network killer” by its opportune timing, appearing just as the symbolic school achieved dominance. Since the connectionist school was almost empty, it faced little objection at its publication, creating the illusion that it caused the closure of the perceptron controversy.\nIn the 1980s, the perceptron controversy reopened with the rise of connectionism. This time the controversy was bypassed without closure. Connectionists demonstrated that backpropagation with MLP bypassed most of the objections from Perceptrons. Minsky and Papert objected that the lessons of Perceptrons still applied, but their objections and lessons had by then become irrelevant.\nMinsky and Papert were possibly the most consistent critics of the scaling hypothesis, arguing over decades that neural networks cannot scale beyond mere toy problems. Minsky was motivated by mathematical certainty, as backpropagation–MLP cannot provably find global optima or accomplish any task efficiently, unlike a single perceptron. He rejected all experimental data with large MLP as theory-less data that cannot be extrapolated. Papert was motivated by social justice and epistemological equality. He rejected all scalable uniform architectures like backpropagation–MLP, as threats to his vision of a society with different but equally valid ways of knowing.\nAs of 2019, essentially all predictions by Minsky and Papert, concerning the non-scalability of neural networks, had been disproven."
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html#abstract",
    "href": "essays/posts/perceptron-controversy/index.html#abstract",
    "title": "The Perceptron Controversy",
    "section": "",
    "text": "During the 1950s and 1960s, the connectionist and symbolic schools of artificial intelligence competed for researcher allegiance and funding, with the symbolic school winning by 1970. This “first neural network winter” lasted until the rise of the connectionist school in the 1980s.\nIt is often stated that neural networks were killed off by the 1969 publication of Perceptrons by Marvin Minsky and Seymour Papert. This story is wrong on multiple accounts:\n\nMinsky and Papert had been working towards killing off neural networks since around 1965 by speaking at conferences and circulating preprints.\nThe mathematical content studies only the behavior of a single perceptron. It does not study multilayer perceptrons.\nBy 1969, most researchers had already left connectionism, frustrated by the lack of progress, as they did not develop backpropagation or deep learning. The last holdout, Frank Rosenblatt, died in 1971.\n\nThe book achieved its mythical status as the “neural network killer” by its opportune timing, appearing just as the symbolic school achieved dominance. Since the connectionist school was almost empty, it faced little objection at its publication, creating the illusion that it caused the closure of the perceptron controversy.\nIn the 1980s, the perceptron controversy reopened with the rise of connectionism. This time the controversy was bypassed without closure. Connectionists demonstrated that backpropagation with MLP bypassed most of the objections from Perceptrons. Minsky and Papert objected that the lessons of Perceptrons still applied, but their objections and lessons had by then become irrelevant.\nMinsky and Papert were possibly the most consistent critics of the scaling hypothesis, arguing over decades that neural networks cannot scale beyond mere toy problems. Minsky was motivated by mathematical certainty, as backpropagation–MLP cannot provably find global optima or accomplish any task efficiently, unlike a single perceptron. He rejected all experimental data with large MLP as theory-less data that cannot be extrapolated. Papert was motivated by social justice and epistemological equality. He rejected all scalable uniform architectures like backpropagation–MLP, as threats to his vision of a society with different but equally valid ways of knowing.\nAs of 2019, essentially all predictions by Minsky and Papert, concerning the non-scalability of neural networks, had been disproven."
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html#the-enigma-of-marvin-minsky",
    "href": "essays/posts/perceptron-controversy/index.html#the-enigma-of-marvin-minsky",
    "title": "The Perceptron Controversy",
    "section": "The enigma of Marvin Minsky",
    "text": "The enigma of Marvin Minsky\nIn a 1993 interview, Robert Hecht-Nielsen recounted an encounter between Marvin Minsky and the neural network community in the late 1980s1:\n1 This was corroborated by a contemporary news report on the International Conference on Neural Networks of 1988:\n\nMinsky who has been criticized by many for the conclusions he and Papert make in ‘Perceptrons,’ opened his defense with the line ‘Everybody seems to think I’m the devil.’ Then he made the statement, ‘I was wrong about Dreyfus too, but I haven’t admitted it yet,’ which brought another round of applause. (quoted in (Olazaran 1991, 285))\n\n\nMinsky had gone to the same New York “science” high school as Frank Rosenblatt, a Cornell psychology Ph.D. whose “perceptron” neural network pattern recognition machine was receiving significant media attention. The wall-to-wall media coverage of Rosenblatt and his machine irked Minsky. One reason was that although Rosenblatt’s training was in “soft science,” his perceptron work was quite mathematical and quite sound—turf that Minsky, with his “hard science” Princeton mathematics Ph.D., didn’t feel Rosenblatt belonged on. Perhaps an even greater problem was the fact that the heart of the perceptron machine was a clever motor-driven potentiometer adaptive element that had been pioneered in the world’s first neurocomputer, the “SNARC”, which had been designed and built by Minsky several years earlier! In some ways, Minsky’s early career was like that of Darth Vader. He started out as one of the earliest pioneers in neural networks but was then turned to the dark side of the force (AI) and became the strongest and most effective foe of his original community. This view of his career history is not unknown to him. When he was invited to give the keynote address at a large neural network conference in the late 1980s to an absolutely rapt audience, he began with the words: “I am not the Devil!” (Rosenfeld and Anderson 2000, 303–5)\n\nHowever, it appears that he had changed his mind later. As recounted by Terry Sejnowski:\n\nI was invited to attend the 2006 Dartmouth Artificial Intelligence Conference, “AI@50,” a look back at the seminal 1956 Summer Research Project on artificial intelligence held at Dartmouth and a look forward to the future of artificial intelligence. … These success stories had a common trajectory. In the past, computers were slow and only able to explore toy models with just a few parameters. But these toy models generalized poorly to real-world data. When abundant data were available and computers were much faster, it became possible to create more complex statistical models and to extract more features and relationships between the features.\nIn his summary talk at the end of the conference [The AI@50 conference (2006)], Marvin Minsky started out by saying how disappointed he was both by the talks and by where AI was going. He explained why: “You’re not working on the problem of general intelligence. You’re just working on applications.” …\nThere was a banquet on the last day of AI@50. At the end of the dinner, the five returning members of the 1956 Dartmouth Summer Research Project on Artificial Intelligence made brief remarks about the conference and the future of AI. In the question and answer period, I stood up and, turning to Minsky, said: “There is a belief in the neural network community that you are the devil who was responsible for the neural network winter in the 1970s. Are you the devil?” Minsky launched into a tirade about how we didn’t understand the mathematical limitations of our networks. I interrupted him—“Dr. Minsky, I asked you a yes or no question. Are you, or are you not, the devil?” He hesitated for a moment, then shouted out, “Yes, I am the devil!” (Sejnowski 2018, 256–58)\n\nWhat are we to make of the enigma of Minsky? Was he the devil or not?\n\nThe intellectual history of Minsky\nDuring his undergraduate years, Minsky was deeply impressed by Andrew Gleason,2 and decided to work on pure mathematics, resulting in his 1951 undergraduate thesis A Generalization of Kakutani’s Fixed-Point Theorem, which extended an obscure fixed-point theorem of Kakutani – not the famous version, as Kakutani proved more than one fixed-point theorem.\n2 \nI asked Gleason how he was going to solve it. Gleason said he had a plan that consisted of three steps, each of which he thought would take him three years to work out. Our conversation must have taken place in 1947, when I was a sophomore. Well, the solution took him only about five more years … Gleason made me realize for the first time that mathematics was a landscape with discernible canyons and mountain passes, and things like that. In high school, I had seen mathematics simply as a bunch of skills that were fun to master – but I had never thought of it as a journey and a universe to explore. No one else I knew at that time had that vision, either. (Bernstein 1981)\n\n\nTheorem 1 (Kakutani’s fixed point theorem on the sphere) If \\(f\\) is an \\(\\mathbb{R}^2\\)-valued continuous function on the unit sphere in \\(\\mathbb{R}^3\\), then for any side length \\(r \\in (0, \\sqrt{3})\\), there exist \\(x_1, x_2, x_3\\) on the sphere forming an equilateral triangle with side length \\(r\\), such that \\(f(x_1) = f(x_2) = f(x_3)\\).\nEquivalently, if \\(x_1, x_2, x_3\\) form an equilateral triangle on the unit sphere, then there exists a rotation \\(T\\) such that \\(f(T(x_1)) = f(T(x_2)) = f(T(x_3))\\).\n\nUsing knot theory, Minsky proved an extension where \\(x_1, x_2, x_3\\) are three points of a square or a regular pentagon (M. Minsky 2011). The manuscript “disappeared” (M. L. Minsky n.d.).\n\nI wrote it up and gave it to Gleason. He read it and said, ‘You are a mathematician.’ Later, I showed the proof to Freeman Dyson, at the Institute for Advanced Study, and he amazed me with a proof (Dyson 1951) that there must be at least one square that has the same temperature at all four vertices. He had found somewhere in my proof a final remnant of unused logic. (Bernstein 1981)\n\nHe then became interested in neural networks and reinforcement learning, and constructed a very simple electromechanical machine called SNARC.3 The SNARC machine is a recurrent neural network that performs reinforcement learning by the Hebbian learning rule. It simulates a mouse running around a maze, while the operator watches an indicator light showing the mouse. The operator can press a button as a reward signal, which would cause an electric motor to turn a chain. The chain is clutched to rheostats that connect the neurons, with the stretch of the clutch being proportional to the charge in a capacitor. During the operation of the neural network, the capacitor charges up if there is neural co-activation on the connection, and decays naturally, thus serving as a short-term memory. When the reward button is pressed, the clutches turn by an amount proportional to the co-activation of neural connections, thereby completing the Hebbian learning.\n3 It was published as (M. Minsky 1952), but the document is not available online, and I could only piece together a possible reconstruction from the fragments of information.Minsky was impressed by how well it worked. The machine was designed to simulate one mouse, but by some kind of error it simulated multiple mice, and yet it still worked.\n\nThe rats actually interacted with one another. If one of them found a good path, the others would tend to follow it. We sort of quit science for a while to watch the machine. … In those days, even a radio set with twenty tubes tended to fail a lot. I don’t think we ever debugged our machine completely, but that didn’t matter. By having this crazy random design, it was almost sure to work, no matter how you built it. (Bernstein 1981)\n\nThis was the last we saw of Minsky’s work with random neural networks. He had crossed the Rubicon, away from the land of brute reason and into the land of genuine insight.\n\nI had the naïve idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. … Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. (Bernstein 1981)\n\nFor his PhD thesis, Minsky worked on the mathematical theory of McCulloch–Pitts neural networks. In style, it was a fine piece of classical mathematics (M. L. Minsky 1954). Minsky would go on to write (M. Minsky 1967, chap. 3), still the best introduction to McCulloch–Pitts neural networks.\n\nMinsky’s doctoral dissertation in mathematics from Princeton in 1954 was a theoretical and experimental study of computing with neural networks. He had even built small networks from electronic parts to see how they behaved. The story I heard when I was a graduate student at Princeton in physics was that there wasn’t anyone in the Mathematics Department who was qualified to assess his dissertation, so they sent it to the mathematicians at the Institute for Advanced Study in Princeton who, it was said, talked to God. The reply that came back was, “If this isn’t mathematics today, someday it will be,” which was good enough to earn Minsky his PhD. (Sejnowski 2018, 259)\n\nReading the story, I recalled “Sussman attains enlightenment”, a hacker koan about Minsky and his student Sussman 4:\n4 This is based on a true story.\n\n… Sussman told Minsky that he was using a certain randomizing technique in his program because he didn’t want the machine to have any preconceived notions. Minsky said, “Well, it has them, it’s just that you don’t know what they are.” It was the most profound thing Gerry Sussman had ever heard. And Minsky continued, telling him that the world is built a certain way, and the most important thing we can do with the world is avoid randomness, and figure out ways by which things can be planned. (Levy 2010, 110–11)\n\n\nIn the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?”, asked Minsky. “I am training a randomly wired neural net to play Tic-Tac-Toe” Sussman replied. “Why is the net wired randomly?”, asked Minsky. “I do not want it to have any preconceptions of how to play”, Sussman said. Minsky then shut his eyes. “Why do you close your eyes?”, Sussman asked his teacher. “So that the room will be empty.” At that moment, Sussman was enlightened.\n\nAs for Sussman, I knew him for two things: writing the SICP book, and being the coordinator of the infamous summer vision project that was to construct “a significant part of a visual system” in a single summer, using only undergraduate student researchers. A brief read of his “reading list” shows where his loyalties lie: firmly in the school of neats.\n(Sejnowski 2018, 28) recounts the background of the summer vision project:\n\nIn the 1960s, the MIT AI Lab received a large grant from a military research agency to build a robot that could play Ping-Pong. I once heard a story that the principal investigator forgot to ask for money in the grant proposal to build a vision system for the robot, so he assigned the problem to a graduate student as a summer project. I once asked Marvin Minsky whether the story was true. He snapped back that I had it wrong: “We assigned the problem to undergraduate students.”\n\nAfter rejecting neural networks, Minsky became a leading researcher in AI. His style of AI is typically described as “symbolic AI”, although a more accurate description would be The Society of Mind (SoM). Minsky developed the SoM in the 1960s and 1970s with his long-time collaborator, Seymour Papert, inspired by their difficulty with building robots, and published the definitive account in (M. Minsky 1988). The SoM thesis states that “any brain, machine, or other thing that has a mind must be composed of smaller things that cannot think at all”.\nStated in this way, it seems patently compatible with neural networks, but only on the surface. Minsky concretely described how he expected a Society of Mind to work, based on his attempts at making Builder, a robot that can play with blocks:\n\nBoth my collaborator, Seymour Papert, and I had long desired to combine a mechanical hand, a television eye, and a computer into a robot that could build with children’s building-blocks. It took several years for us and our students to develop Move, See, Grasp, and hundreds of other little programs we needed to make a working Builder-agency. I like to think that this project gave us glimpses of what happens inside certain parts of children’s minds when they learn to “play” with simple toys. The project left us wondering if even a thousand microskills would be enough to enable a child to fill a pail with sand. It was this body of experience, more than anything we’d learned about psychology, that led us to many ideas about societies of mind.\nTo do those first experiments, we had to build a mechanical Hand, equipped with sensors for pressure and touch at its fingertips. Then we had to interface a television camera with our computer and write programs with which that Eye could discern the edges of the building-blocks. It also had to recognize the Hand itself. When those programs didn’t work so well, we added more programs that used the fingers’ feeling-sense to verify that things were where they visually seemed to be. Yet other programs were needed to enable the computer to move the Hand from place to place while using the Eye to see that there was nothing in its way. We also had to write higher-level programs that the robot could use for planning what to do—and still more programs to make sure that those plans were actually carried out. To make this all work reliably, we needed programs to verify at every step (again by using Eye and Hand) that what had been planned inside the mind did actually take place outside—or else to correct the mistakes that occurred. … Thousands and, perhaps, millions of little processes must be involved in how we anticipate, imagine, plan, predict, and prevent—and yet all this proceeds so automatically that we regard it as “ordinary common sense.” (M. Minsky 1988, sec. 2.5)\n\n\n\n\nMarvin Minsky and Builder the robot. It’s hard to tell who is having more fun here.\n\n\nFrom the concrete description, as well as the many attractive illustrations in the book, it is clear that Minsky intended the “Society of Mind” to be a uniform computing substrate (silicon or carbon) upon which millions of little symbolic programs are running, each capable of running some specific task, each describable by a distinct and small piece of symbolic program. They cannot be mere perceptrons in a uniform block of neural network, or mere logic gates in a uniform block of CPU.\nIn his 1988 book, Minsky described dozens of these heterogeneous components he thought might make up a Society of Mind. However, the precise details are not relevant,5 as he freely admited that they are conjectured. He was only adamant about the overarching scheme: heterogeneous little separated components, not a homogeneous big connected piece.\n5 The conjectured components included “K-lines”, “nomes”, “nemes”, “frames”, “frame-arrays”, etc. Although Minsky meant for this SoM project to last a very long time, building up to general intelligence brick by brick, my literature search shows that there had been no new development since the 2000s, so the overview (Singh 2003) still represents the SOTA of SoM.Perhaps a modern reincarnation of such an idea would be the dream of Internet agents operating in a digital economy, populated by agents performing simple tasks like spam filtering, listening for the price of petroleum, etc. Some agents would interface with reality, while others would interface with agents. Some agents are organized at a higher level into DAOs, created by a small committee of simple “manager agents” serving as the interface and coordinators for other agents. DAOs can interface with other DAOs through little speaker-agents, which consist of a simple text filter for the torrent of information and then outsource to text-weaving agents to compose the actual messages they send out.\n\n\nSeymour Papert\nSeymour Papert, the long-time collaborator of Minsky, was the second author of Perceptrons. To unlock the enigma of Minsky, we must look into Papert’s past as well.\nIn 1958, after earning a doctorate in mathematics, he met Jean Piaget and became his pupil for four years. This experience had a formative effect on Papert. Piaget’s work was an important influence on the constructivism philosophy in education, and Papert would go on to bring constructivism from books to classrooms. He was particularly hopeful that computers can realize the constructivist dream of unlocking the kaleidoscopic creativity that a child can construct.\nThe main theme of Jean Piaget’s work was developmental psychology – how children’s understanding of the world changes as they grow up. What goes on in their mind as they progressively understand that things fall down, what dogs are, and that solid steel sinks but hollow steel might float? Piaget discovered that children did not simply start with a blank sheet of paper and gradually fill in sketchy details of the true model. Instead they constructed little models of small facets of reality that would be modified or completely replaced as they encounter new phenomena that their old models cannot explain. In this way, Piaget claimed that children are “little scientists”.\nA small example illustrates the idea. When children see that a leaf floats on water, but a stone sinks, they add a rule “Soft things float, while hard things sink.”. Then, they see that a hard plastic boat floats too, so they add a rule “Except hard and light things also float.”. Then, they see that a large boat also floats, so they rewrite the entire model to “Flat-bottomed things float, while small-bottomed things sink.”. And so on.\nThere are conservative and radical ways of using Piaget’s research for pedagogy. The conservative approach involves studying how children construct their scientific theories and identifying a sequence of evidence to present to these young scientists so they can quickly reach scientific orthodoxy. For example, we might show children videos of curling and air hockey, then let them play with an air hockey table, following this with guided exercises, so they race through animism, Aristotelian physics, impetus theory, etc, and end up with Newton’s laws of motion.\nThe radical way is to decenter the orthodoxy and let a thousand heterodoxies bloom. Why go for the orthodoxy, when the Duhem–Quine thesis tells us that evidence is never enough to constrain us to only one orthodoxy? And given that objectively no theory deserves the high name of “orthodoxy”, how did the scientific “orthodoxy” become dominant? A critical analysis of the history shows that its dominance over Aboriginal and woman ways of knowing is merely a historical accident due to an alliance with the hegemonic reason of the center over the periphery.\nPapert went with the radical way.\nAfter four years of study under Piaget, he arrived in MIT in 1963, and began working with Minsky on various topics, including the Logo Turtle robot, and the Perceptrons book. The computer revolution was starting, and Papert saw computers as a way to bring radical constructivism to children.\nIn the real world, phenomena are limited by nature, and aspiring little heterodoxy-builders are limited by their ability to construct theories and check their consequences. In the computer world, every child could program and construct “microworlds” from their own theories. Thus, computers would bring constructivism to the classroom. Furthermore, the constructed world inside computers could then flow out to the physical world via robots. This is why Papert worked on both Logo the programming language and Logo the turtle robots. In his words, he intended to fight “instructionism” with “constructionism” by bringing the power of the computer to every child, so that they would grow up to be “bricoleurs”, working with whatever little tool they have available doing whatever is necessary to accomplish little things. This is a vital piece in his overarching project of epistemological pluralism, to liberate heterodoxical ways of knowing (S. A. Papert 1994, chap. 7):\n\nTraditional education codifies what it thinks citizens need to know and sets out to feed children this “fish.” Constructionism is built on the assumption that children will do best by finding (“fishing”) for themselves the specific knowledge they need … it is as well to have good fishing lines, which is why we need computers, and to know the location of rich waters, which is why we need to develop a large range of mathetically rich activities or “microworlds.”\n… School math, like the ideology, though not necessarily the practice, of modern science, is based on the ideal of generality – the single, universally correct method that will work for all problems and for all people. Bricolage is a metaphor for the ways of che old-fashioned traveling tinker, the jack-of-all-trades who knocks on the door offering to fix whatever is broken. Faced with a job, the tinker rummages in his bag of assorted tools to find one that will fit the problem at hand and, if one tool does nor work for the job, simply tries another without ever being upset in the slightest by the lack of generality. The basic tenets of bricolage as a methodology for intellectual activity are: Use what you’ve got, improvise, make do. And for the true bricoleur, the tools in the bag will have been selected over a long time by a process determined by more than pragmatic utility. These mental tools will be as well worn and comfortable as the physical tools of the traveling tinker; they will give a sense of the familiar, of being at ease with oneself …\nKitchen math provides a clear demonstration of bricolage in its seamless connection with a surrounding ongoing activity that provides the tinker’s bag of tricks and tools. The opposite of bricolage would be to leave the “cooking microworld” for a “math world,” to work the fractions problem using a calculator or, more likely in this case, mental arithmetic. But the practitioner of kitchen math, as a good bricoleur, does not stop cooking and turn to math; on the contrary, the mathematical manipulations of ingredients would be indistinguishable to an outside observer from the culinary manipulations.\n… The traditional epistemology is based on the proposition, so closely linked to the medium of text-written and especially printed. Bricolage and concrete thinking always existed but were marginalized in scholarly contexts by the privileged position of text. As we move into the computer age and new and more dynamic media emerge, this will change.\n\nAccording to Papert, his project is epistemological pluralism, or promoting different ways of knowing (Turkle and Papert 1990):\n\nThe diversity of approaches to programming suggests that equal access to even the most basic elements of computation requires accepting the validity of multiple ways of knowing and thinking, an epistemological pluralism. Here we use the word epistemology in a sense closer to Piaget’s than to the philosopher’s. In the traditional usage, the goal of epistemology is to inquire into the nature of knowledge and the conditions of its validity; and only one form of knowledge, the propositional, is taken to be valid. The step taken by Piaget in his definition of epistemologie genetique was to eschew inquiry into the “true” nature of knowledge in favor of a comparative study of the diverse nature of different kinds of knowledge, in his case the kinds encountered in children of different ages. We differ from Piaget on an important point, however. Where he saw diverse forms of knowledge in terms of stages to a finite end point of formal reason, we see different approaches to knowledge as styles, each equally valid on its own terms.\n… The development of a new computer culture would require more than environments where there is permission to work with highly personal approaches. It would require a new social construction of the computer, with a new set of intellectual and emotional values more like those applied to harpsichords than hammers. Since, increasingly, computers are the tools people use to write, to design, to play with ideas and shapes and images, they should be addressed with a language that reflects the full range of human experiences and abilities. Changes in this direction would necessitate the reconstruction of our cultural assumptions about formal logic as the “law of thought.” This point brings us full circle to where we began, with the assertion that epistemological pluralism is a necessary condition for a more inclusive computer culture.\n\nThe project of epistemological pluralism erupted into public consciousness during the “Science Wars” of 1990s. After that, it had stayed rather quiet."
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html#the-perceptron-controversy",
    "href": "essays/posts/perceptron-controversy/index.html#the-perceptron-controversy",
    "title": "The Perceptron Controversy",
    "section": "The perceptron controversy",
    "text": "The perceptron controversy\n\nConnectionism, 1945–1970\nIn the early days, there were several centers of connectionist research, clustered around Frank Rosenblatt, Bernard Widrow, and the Stanford Research Institute (SRI). Out of those centers of research, Minsky and Papert targeted mostly Rosenblatt’s research.\nFrank Rosenblatt’s research had three modes: mathematical theory, experiments on bespoke machines, such as the Mark I Perceptron and the Tobermory, and experiments on serial digital computers, usually IBM machines. He was strongly inclined to building two-layered perceptron machines where the first layer was fixed 0-1 weights, and only the second layer contained real-valued weights learned by the perceptron learning rule. This is precisely the abstract model of the perceptron machine used by Minsky and Papert.\nAfter 4 years of research, he published a summary of his work in (Rosenblatt 1962). In the book, he noted that there were many problems that the perceptron machines could not learn well. As summarized in (Olazaran 1991, 116–21),\n\n… two stimuli (presented one after another) had to occupy nearly the same area of the retina in order to be classified as similar. … The lack of an adequate preprocessing system meant that a set of association units had to be dedicated to the recognition of each possible object, and this created an excessively large layer of association units in the perceptron. … Other problems were excessive learning time, excessive dependence on external evaluation (supervision), and lack of ability to separate essential parts in a complex environment. Rosenblatt (1962, pp. 309-310) included the ‘figure-ground’ or ‘connectedness’ problem in this last point.\n\nA number of perceptrons analyzed in the preceding chapters have been analyzed in a purely formal way, yielding equations which are not readily translated into numbers. This is particularly true in the case of the four-layer and cross-coupled systems, where the generality of the equations is reflected in the obscurity of their implications. … The previous questions [from the first to the twelfth] are all in the nature of ‘mopping-up’ operations in areas where some degree of performance is known to be possible . . . [However,] the problems of figure-ground separation (or recognition of unity) and topological relation recognition represent new territory, against which few inroads have been made.” (Rosenblatt, 1962a, pp. 580-581)\n\n\nAlmost every one of these problems was specifically targeted by “Perceptrons”. For example, the difficulty of testing for “connectedness” was a centerpiece of the entire book, the difficulty of recognizing symmetry was studied by “stratification” and shown to have exponentially growing coefficients (chapter 7), the requirement for “had to occupy nearly the same area of the retina” was targeted by studies on the limitations of “diameter-limited perceptrons” (chapter 8), the “figure-ground problem” was targeted by showing “recognition-in-context” has infinite order (Section 6.6), the “generality of the equations is reflected in the obscurity of their implications” was targeted by comments such as “if there is no restriction except for the absence of loops, the monster of vacuous generality once more raises its head” (Section 13.2),\nBernard Widrow worked mostly in collaboration with Marcian Hoff. Their work is detailed in my essay The Backstory of Backpropagation. In short, they first developed a least-mean-square gradient descent method to train a single perceptron, then proceeded to two-layered perceptrons and predictably failed to develop backpropagation, as the activation function is not differentiable. Thereafter, Widrow gave up neural networks until learning of backpropagation in the 1980s.\n\nWidrow and his students developed uses for the Adaline and Madaline. Early applications included, among others, speech and pattern recognition, weather forecasting, and adaptive controls. Work then switched to adaptive filtering and adaptive signal processing after attempts to develop learning rules for networks with multiple adaptive layers were unsuccessful. … After 20 years of research in adaptive signal processing, the work in Widrow’s laboratory has once again returned to neural networks. (Widrow and Lehr 1990)\n\n\nAt the time that Hoff left, about 1965 or 1966, we had already had lots of troubles with neural nets. My enthusiasm had dropped. But we were beginning to have successful adaptive filters, in other words, finding good applications. … So we stopped, basically stopped on neural nets, and began on adaptive antennas very strongly.” (Widrow, interview) (Olazaran 1991, 129–30)\n\nSRI had a strong AI program, with luminaries such as Nils Nilsson, Charles Rosen, Duda, and Hart. At first they worked on a system called MINOS, with several iterations. MINOS II in 1962 had 3 layers, but only one was trainable, presumably because they also had no better training method than the perceptron learning rule. Since they used 0-1 activation functions like everyone else, they were frustrated by the same problem of not doing backpropagation, so they switched to symbolic AI techniques around 1965.\nIn 1973, Duda and Hart published the famous “Duda and Hart” book on pattern classification (Duda and Hart 1973). The book contained two halves. The first half was statistical: Bayes, nearest neighbors, perceptron, clustering, etc. The second half was on scene analysis, a symbolic-AI method for computer vision. Indeed, Minsky and Papert promoted it as superior to perceptron networks.6 Shakey the robot, built between 1966 and 1972, was a tour de force of scene analysis, and it could traverse a room in as short as an hour, avoiding geometric obstacles along the way. Its program was written in LISP, the staple programming language for symbolic AI.\n6 It is instructive to compare the first edition with the second, published in 2001 (Duda, Hart, and Stork 2001). It had become almost completely statistical. There were new chapters on neural networks, Boltzmann machines, decision trees, and so on. In contrast, scene analysis was completely removed.\nIt says something about the obsolescence of scene analysis even in 2001, as Duda and Hart deleted half of their most famous book just to avoid talking about it. In fact, the only mention of “scene analysis” is a condemnation:\n\nSome of the earliest work on three-dimensional object recognition relied on complex grammars which described the relationships of corners and edges, in block structures such arches and towers. It was found that such systems were very brittle; they failed whenever there were errors in feature extraction, due to occlusion and even minor misspecifications of the model. For the most part, then, grammatical methods have been abandoned for object recognition and scene analysis. (Duda, Hart, and Stork 2001, sec. 8.8)\n\n\nI got very interested for a while in the problem of training more than one layer of weights, and was not able to make very much progress on that problem. … When we stopped the neural net studies at SRI, research money was running out, and we began looking for new ideas. (Nilsson, interview)\nAbout 1965 or 1966 we decided that we were more interested in the other artificial intelligence techniques. … Our group never solved the problem of training more than one layer of weights in an automatic fashion. We never solved that problem. That was most critical. Everybody was aware of that problem. (Rosen, interview) (Olazaran 1991, 131–33)\n\n\n\nThe perceptron controversy, 1960s\n\nIn the middle nineteen-sixties, Papert and Minsky set out to kill the Perceptron, or, at least, to establish its limitations – a task that Minsky felt was a sort of social service they could perform for the artificial-intelligence community. (Bernstein 1981)\n\nAlthough the book was published only in 1969, close to the end of the perceptron controversy, the influence of Minsky and Papert had been felt years earlier as they attended conferences and disseminated their ideas through talks and preprints, sometimes quarreling on stage. Both sides had their motivations and the conflict was real.\n\nIn order to show the extent of the perceptron controversy, it is interesting to repeat some of the rhetorical expressions that were used in it: ‘many remember as great spectator sport the quarrels Minsky and Rosenblatt had;’ ‘Rosenblatt irritated a lot of people;’ ‘Rosenblatt was given to steady and extravagant statements about the performance of his machine;’ ‘Rosenblatt was a press agent’s dream, a real medicine man;’ ‘to hear Rosenblatt tell it, his machine was capable of fantastic things;’ ‘they disparaged everything Rosenblatt did, and most of what ONR did in supporting him;’ ‘a pack of happy bloodhounds;’ ‘Minsky knocked the hell out of our perceptron business;’ ‘Minsky and his crew thought that Rosenblatt’s work was a waste of time, and Minsky certainly thought that our work at SRI was a waste of time;’ ‘Minsky and Papert set out to kill the perceptron, it was a sort of social service they could perform for the Al community;’ ‘there was some hostility;’ ‘we became involved with a somewhat therapeutic compulsion;’ ‘a misconception that would threaten to haunt artificial intelligence;’ ‘the mystique surrounding such machines.’ These rhetorical expressions show the extent (the heat) of the perceptron controversy beyond doubt. (Olazaran 1991, 112)\n\nCharles Rosen of SRI recalls:\n\nMinsky and his crew thought that Frank Rosenblatt’s work was a waste of time, and they certainly thought that our work at SRI was a waste of time. Minsky really didn’t believe in perceptrons, he didn’t think it was the way to go. I know he knocked the hell out of our perceptron business. (Olazaran 1993, 622)\n\nWhen Perceptrons was finally published in 1969, the connectionist camp was already deserted. The SRI group had switched to symbolic AI projects; Widrow’s group had switched to adapting single perceptrons to adaptive filtering; Frank Rosenblatt was still labouring, isolated, with dwindling funds, until his early death in 1971.7\n7 The 1972 reprinting of Perceptrons included a handwritten note, “In memory of Frank Rosenblatt”. This was not an ironic dedication, as Minsky and Rosenblatt were personally friendly, although their research paradigms had been fighting for dominance.During the last days of Rosenblatt, he worked on a massive expansion of the Mark I Perceptron, named Tobermory. It would have been a multimodal neural network, capable of seeing and hearing in real time. From the obscurity of Tobermory, we can infer that it was a failure.\n\nThe four-layer Tobermory perceptron, designed and built at Cornell University between 1961 and 1967, had 45 S units, 1600 A1 units, 1000 A2 units, and 12 R units. Intended for speech recognition, the input section consisted of 45 band-pass filters attached to 80 difference detectors, with the output of each detector sampled at 20 time intervals. Its 12000 weights consisted of toroidal cores capable of storing over 100 different amplitudes. Each A2 unit could be connected to any of 20 A1 units by means of a wall-sized plugboard. As has happened with so many other projects in the last three decades, by the time Tobermory was completed, the technology of commercial Von Neumann computers had advanced sufficiently to outperform the special-purpose parallel hardware. (Nagy 1991)\n\n(Olazaran 1991) gathered evidence that the publication of Perceptrons was not the cause but a “marker event” for the end of the perceptron controversy and the ascendancy of the symbolic AI school. The book was not the neural network killer, but its epitaph.\n\n\nConnectionist retrospectives, 1990s\nFollowing the resurgence of connectionism in the 1980s, Anderson and Rosenfeld conducted interviews with prominent connectionists throughout the 1990s, compiled in (Rosenfeld and Anderson 2000). The perceptron controversy is mentioned several times. Reading the interviews gives one a distinct feeling of Rashomon. The same events are recounted from multiple perspectives. I will excerpt some of the most important ones for the essay.\nJack D. Cowan gave an “eyewitness account” of Minsky and Papert’s role in the controversy, before the publication of the book in 1969.\n\nER: I’m curious about one thing. You said that Minsky and Papert first presented their notions about exclusive-OR in the Perceptron work [in a 1965 conference].\nJC: Well, they first presented their notions about the limitations of perceptrons and what they could and couldn’t do.\nER: They hadn’t gotten to exclusive-OR yet?\nJC: They had, but that wasn’t a central issue for them. The essential issue was, suppose you had diameter-limited receptive fields in a perceptron, what could it compute?\nER: How was that received at that first conference?\nJC: Both of them were quite persuasive speakers, and it was well received. What came across was the fact that you had to put some structure into the perceptron to get it to do anything, but there weren’t a lot of things it could do. The reason was that it didn’t have hidden units. It was clear that without hidden units, nothing important could be done, and they claimed that the problem of programming the hidden units was not solvable. They discouraged a lot of research and that was wrong. … Everywhere there were people working on perceptrons, but they weren’t working hard on them. Then along came Minsky and Papert’s preprints that they sent out long before they published their book. There were preprints circulating in which they demolished Rosenblatt’s claims for the early perceptrons. In those days, things really did damp down. There’s no question that after ’62 there was a quiet period in the field.\nER: Robert Hecht-Nielsen has told me stories that long before Minsky and Papert ever committed anything to a paper that they delivered at a conference or published anywhere, they were going down to ARPA and saying, “You know, this is the wrong way to go. It shouldn’t be a biological model; it should be a logical model.”\nJC: I think that’s probably right. In those days they were really quite hostile to neural networks. I can remember having a discussion with Seymour … in the ’60s. We were talking about visual illusions. He felt that they were all higher-level effects that had nothing to do with neural networks as such. They needed a different, a top-down approach to understand. By then he had become a real, a true opponent of neural networks. I think Marvin had the same feelings as well. To some extent, David Marr had those feelings too. After he got to the AI lab, I think he got converted to that way of thinking. Then Tommy Poggio essentially persuaded him otherwise.\n\nTeuvo Kohonen seemed also angry at the Chomskyan school, for reasons I sketched out in the appendix on the Chomskyans.\n\nI was one of the people suffering from Minsky and Papert’s book [Perceptrons] because it went roughly this way: you start telling somebody about your work, and this visitor or whoever you talk to says, “Don’t you know that this area is dead?” It is something like what we experienced in the pattern recognition society when everything started to be structural and grammatical and semantic and so on. If somebody said, “I’m doing research on the statistical pattern recognition,” then came this remark, “Hey, don’t you know that is a dead idea already?”\n\nMichael A. Arbib thought the book did not cause the neural network winter, but rather caused by the change in funding.\n\nMinsky and Papert basically said that if you limit your networks to one layer in depth, then, unless you have very complicated individual neurons, you can’t do very much. This is not too surprising. … Many people see the book as what killed neural nets, but I really don’t think that’s true. I think that the funding priorities, the fashions in computer science departments, had shifted the emphasis away from neural nets to the more symbolic methods of AI by the time the book came out. I think it was more that a younger generation of computer scientists who didn’t know the earlier work may have used the book as justification for sticking with “straight AI” and ignoring neural nets.\n\nBernard Widrow concurred.\n\nI looked at that book, and I saw that they’d done some serious work here, and there was some good mathematics in this book, but I said, “My God, what a hatchet job.” I was so relieved that they called this thing the percept ron rather than the Adaline because actually what they were mostly talking about was the Adaline, not the percept ron. I felt that they had sufficiently narrowly defined what the percept ron was, that they were able to prove that it could do practically nothing. Long, long, long before that book, I was already successfully adapting Madaline [Madaline = many Adalines], which is a whole bunch of neural elements. All this worry and agony over the limitations of linear separability, which is the main theme of the book, was long overcome.\nWe had already stopped working on neural nets. As far as I knew, there wasn’t anybody working on neural nets when that book came out. I couldn’t understand what the point of it was, why the hell they did it. But I know how long it takes to write a book. I figured that they must have gotten inspired to write that book really early on to squelch the field, to do what they could to stick pins in the balloon. But by the time the book came out, the field was already gone. There was just about nobody doing it.\n\nJames A. Anderson pointed out that during the “winter”, neural networks survived outside of AI.\n\nThis was during the period sometimes called the neural network dark ages, after the Minsky and Papert book on perceptrons had dried up most of the funding for neural networks in engineering and computer science. Neural networks continued to be developed by psychologists, however, because they turned out to be effective models in psychology … What happened during the dark ages was that the ideas had moved away from the highly visible areas of big science and technology into areas of science that did not appear in the newspapers.\n\nDavid Rumelhart had nice things to say about Minsky, with no trace of bitterness. It is understandable as he only started working in neural networks years after the controversy died down.\n\nI always had one course that was like a free course in which I would choose a book of the year and teach out of that. In 1969, I think it was, or maybe ’70, I chose Perceptrons by Minsky and Papert as the book of the year. We then carefully went through it and read it in a group. … This was my most in-depth experience with things related to neural networks, or what were later called neural networks. I was quite interested in Minsky in those days because he also had another book which was called, I think, Semantic Information Processing. That book was a collection, including an article by Ross Quillian. It was a collection of dissertations from his graduate students. In a way, it was Minsky who led me to read about the perceptron more than anybody else.\n\nRegarding Robert Hecht-Nielsen, we have already seen his belief that Minsky was “Darth Vader” and possibly “the Devil”. Unsurprisingly, he was the most embittered, and placed the blame for the 1970s neural network winter squarely on the publication of Perceptrons.\n\nBy the mid-1970s, Minsky and his colleagues (notably Seymour Papert) began to take actions designed to root out neural networks and ensure large and, in their view, richly deserved funding for AI research by getting the money currently being “wasted” on neural networks, and more to boot, redirected. They did two things. First, Minsky and Papert began work on a manuscript designed to discredit neural network research. Second, they attended neural network and “bionics” conferences and presented their ever-growing body of mathematical results being compiled in their manuscript to what they later referred to as “the doleful responses” of members of their audiences.\nAt the heart of this effort was Minsky and Papert’s growing manuscript, which they privately circulated for comments. The technical approach they took in the manuscript was based on a mathematical theorem discovered and proven some years earlier—ironically, by a strong supporter of Rosenblatt—that the perceptron was incapable of ever implementing the “exclusive-OR” [X-OR] logic function. What Minsky and Papert and their colleagues did was elaborate and bulk up this idea to book length by devising many variants of this theorem. Some, such as a theorem showing that single-layer perceptrons, of many varied types, cannot compute topological connectedness, are quite clever. To this technical fabric, they wove in what amounted to a personal attack on Rosenblatt. This was the early form of their crusade manifesto.\nLater, on the strong and wise advice of colleagues, they expunged the vitriol. They didn’t quite get it all, as a careful reading will show. They did a complete flip-flop, dedicating the book to Rosenblatt! As their colleagues sensed it would, this apparently “objective” evaluation of perceptrons had a much more powerful impact than the original manuscript with its unseemly personal attack would have. Of course, in reality, the whole thing was intended, from the outset, as a book-length damnation of Rosenblatt’s work and many of its variants in particular, and, by implication, all other neural network research in general.\nMinsky and Papert’s book, Perceptrons, worked. The field of neural networks was discredited and destroyed. The book and the associated conference presentations created a new conventional wisdom at DARPA and almost all other research sponsorship organizations that some MIT professors have proven mathematically that neural networks cannot ever do anything interesting. The chilling effect of this episode on neural network research lasted almost twenty years."
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html#the-message-of-perceptrons",
    "href": "essays/posts/perceptron-controversy/index.html#the-message-of-perceptrons",
    "title": "The Perceptron Controversy",
    "section": "The message of Perceptrons",
    "text": "The message of Perceptrons\nMinsky described how he and Papert felt impelled to write the book:\n\nBoth of the present authors (first independently and later together) became involved with a somewhat therapeutic compulsion: to dispel what we feared to be the first shadows of a “holistic” or “Gestalt” misconception that would threaten to haunt the fields of engineering and artificial intelligence as it had earlier haunted biology and psychology. For this, and for a variety of more practical and theoretical goals, we set out to find something about the range and limitations of perceptrons. (M. Minsky and Papert 1988, 20).\n\nThe book has become a true classic: everybody wants to have read and nobody wants to read. Taking the opposite approach, I have read the book, despite not wanting to read it.\nIts content can be neatly divided into a greater and a lesser half. The greater half is a mathematical monograph on which functions can be implemented by a single perceptron with fixed featurizers, and the lesser half is a commentary on the wider implications of the monograph. The impact of the work is precisely reversed: most of the impact comes from the commentary derived from the results, and effectively no impact comes from the mathematical results themselves.\nDespite this imbalance, the mathematical work is substantial, and the perceptron controversy turns critically on the pliable interpretation sprouting from the solid structure. Therefore, I have detailed the mathematical content in a separate essay, Reading Perceptrons, to which I refer occasionally to gloss their interpretation.\n\nMinsky and Papert struck back\nIn the 1980s, neural networks rose again to prominence under the name of “connectionism”, prompting an eventual response from Minsky and Papert. The Perceptrons book was reissued in 1988, with new chapters dedicated to rejecting connectionism. They took the 1986 two-volume work of Parallel Distributed Processing (PDP), especially (Rumelhart, Hinton, and Williams 1985) 8, as the representative of connectionism, and made specific objections to them.\n8 This paper was reprinted in (Rumelhart and McClelland 1986, vol. 1, chapter 8), in which Minsky and Papert read it. This paper is often cited for the backpropagation algorithm, which I have discussed in The Backstory of Backpropagation.In the prologue, they staked their claim thus: Connectionism is a mistake engendered by a new generation of researchers ignorant of history; though the theorems of the Perceptrons book apply to only a single perceptron, the lessons extend to all neural networks. To back up the claim, they made specific technical, historical, and philosophical objections, all with the central goal of showing that homogeneous neural networks cannot scale.\n\n… when we found that little of significance had changed since 1969, when the book was first published, we concluded that it would be more useful to keep the original text (with its corrections of 1972) and add an epilogue, so that the book could still be read in its original form. One reason why progress has been so slow in this field is that researchers unfamiliar with its history have continued to make many of the same mistakes that others have made before them.\n… there has been little clear-cut change in the conceptual basis of the field. The issues that give rise to excitement today seem much the same as those that were responsible for previous rounds of excitement. … many contemporary experimenters assume that, because the perceptron networks discussed in this book are not exactly the same as those in use today, these theorems no longer apply. Yet, as we will show in our epilogue, most of the lessons of the theorems still apply.\n\nIn an earlier interview, Minsky reiterated his belief that the proper place of perceptrons is solving tiny problems with tiny perceptron networks.\n\n… for certain purposes the Perceptron was actually very good. I realized that to make one all you needed in principle was a couple of molecules and a membrane. So after being irritated with Rosenblatt for overclaiming, and diverting all those people along a false path, I started to realize that for what you get out of it – the kind of recognition it can do – it is such a simple machine that it would be astonishing if nature did not make use of it somewhere. It may be that one of the best things a neuron can have is a tiny Perceptron, since you get so much from it for so little. You can’t get one big Perceptron to do very much, but for some things it remains one of the most elegant and simple learning devices I know of. (Bernstein 1981)\n\nThey also urged all AI researchers to adopt the Society of Mind hypothesis, or else face the charge of being unreflective or of drawing lines where none exists. It seems to me that Minsky wrote most of the prologue and epilogue, because in Papert’s solo paper, he went considerably further with sociological interpretation.\n\nThis broad division makes no sense to us, because these attributes are largely independent of one another; for example, the very same system could combine symbolic, analogical, serial, continuous, and localized aspects. Nor do many of those pairs imply clear opposites; at best they merely indicate some possible extremes among some wider range of possibilities. And although many good theories begin by making distinctions, we feel that in subjects as broad as these there is less to be gained from sharpening bound­ aries than from seeking useful intermediates.\n… Are there inherent incompatibilities between those connectionist and symbolist views? The answer to that depends on the extent to which one regards each separate connectionist scheme as a self-standing system. If one were to ask whether any particular, homogeneous network could serve as a model for a brain, the answer (we claim) would be, clearly. No. But if we consider each such network as a possible model for a part of a brain, then those two overviews are complementary. This is why we see no reason to choose sides.\n… Most researchers tried to bypass [the technical objections], either by ignoring them or by using brute force or by trying to discover powerful and generally applicable methods. Few researchers tried to use them as guides to thoughtful research. We do not believe that any completely general solution to them can exist …\n\nWe now proceed to the epilogue and its arguments.\n\n1980s connectionism is not that different\nThey speculated on the reason for the revival of neural networks. Was it because of the development of backpropagation, multilayer networks, and faster computers? Emphatically not. In fact, 1980s connectionists were not different from the 1960s connectionists. It is only the ignorance of history that made them think otherwise. In both periods, connectionism was focused on making small-scale experiments and then extrapolating to the largest scale, without mathematical theorems to justify the extrapolation. In both periods, connectionism failed (or would fail) to scale beyond toy problems.\n\nmost of the theorems in this book are explicitly about machines with a single layer of adjustable connection weights. But this does not imply (as many modern connectionists assume) that our conclusions don’t apply to multilayered machines. To be sure, those proofs no longer apply unchanged, because their antecedent conditions have changed. But the phenomena they describe will often still persist. One must examine them, case by case.\n\n\n… the situation in the early 1960s: Many people were impressed by the fact that initially unstructured networks composed of very simple devices could be made to perform many interesting tasks – by processes that could be seen as remarkably like some forms of learning. A different fact seemed to have impressed only a few people: While those networks did well on certain tasks and failed on certain other tasks, there was no theory to explain what made the difference – particularly when they seemed to work well on small (“toy“) problems but broke down with larger problems of the same kind. Our goal was to develop analytic tools to give us better ideas about what made the difference.\n\n\n\nThere is no silver bullet in machine learning\nThere are no general algorithms and there are no general problems. There are only particular algorithm-problem pairs. An algorithm-problem pair can be a good fit, or a bad fit. The parity problem is a bad fit with a neural network trained by backpropagation, but it is a good fit with a Turing machine.\nThere is no general and effective algorithm. Either the algorithm is so general that it is as useless as “just try every algorithm” akin to Ross Ashby’s homeostat, or it is useful but not general. This general lesson is similar to Gödel’s speedup theorem, Blum’s speedup theorem, the no free lunch theorem, etc.\n\nClearly, the procedure can make but a finite number of errors before it hits upon a solution. It would be hard to justify the term “learning” for a machine that so relentlessly ignores its experience. The content of the perceptron convergence theorem must be that it yields a better learning procedure than this simple homeostat. Yet the problem of relative speeds of learning of perceptrons and other devices has been almost entirely neglected. (M. Minsky and Papert 1988, sec. 11.7)\n\nArthur Samuel’s checker learning algorithm encountered two fundamental problems: credit assignment and inventing novel features. Those two problems are not just for the checker AI, but for all AI. There are no universal and effective solutions to credit assignment, and there are no universally effective solutions to inventing novel features. There could be universal but impractical solutions, such as backpropagation on homogeneous neural networks, Solomonoff induction, trying every Turing machine, etc. There could be practical but not universal solutions, which is precisely what populates the Society of Mind in human brains.\n\nRosenblatt’s credit-assignment method turned out to be as effective as any such method could be. When the answer is obtained, in effect, by adding up the contributions of many processes that have no significant interactions among themselves, then the best one can do is reward them in proportion to how much each of them contributed.\n\n\nSeveral kinds of evidence impel us toward this view. One is the great variety of different and specific functions embodied in the brain’s biology. Another is the similarly great variety of phenom­ ena in the psychology of intelligence. And from a much more abstract viewpoint, we cannot help but be impressed with the practical limitations of each “general” scheme that has been proposed – and with the theoretical opacity of questions about how they behave when we try to scale their applications past the toy problems for which they were first conceived.\n\n\n\nThere is no efficient way to train homogeneous, high-order networks\nThey ask the reader to think back to the lesson of the parity predicate from Chapter 10: Even though it is learnable by a two-layered perceptron network, it would involve weights exponential in the input pixel count, and therefore take a very long time to learn. They expect this to generalize, so that any problem that require some perceptron in the network to have receptive field of size \\(\\Omega(|R|^\\alpha)\\), necessarily require that perceptron to have coefficients growing like \\(2^{\\Omega(|R|^\\alpha)}\\), and therefore taking \\(2^{\\Omega(|R|^\\alpha)}\\) steps to train.\n\nWe could extend them either by scaling up small connectionist models or by combining small-scale networks into some larger organization. In the first case, we would expect to encounter theoretical obstacles to maintaining GD’s effectiveness on larger, deeper nets. And despite the reputed efficacy of other alleged remedies for the deficiencies of hill-climbing, such as “annealing,” we stay with our research conjecture that no such procedures will work very well on large-scale nets, except in the case of problems that turn out to be of low order in some appropriate sense.\nThe second alternative is to employ a variety of smaller networks rather than try to scale up a single one. And if we choose (as we do) to move in that direction, then our focus of concern as theoretical psychologists must turn toward the organizing of small nets into effective large systems.\n\n\n\nThere is no effective use for homogeneous, high-order networks\nFully connected networks, or indeed any neural network without a strong constraint on “order” or “receptive field”, would hopelessly confuse itself with its own echoes as soon as it scales up, unless it has sufficient “insulation”, meaning almost-zero connection weights, such that it effectively splits into a large number of small subnets. That is, a large fully connected network is useless anyway unless it already decomposes into many tiny networks arranged in a Society of Mind.\n\nCertain parallel computations are by their nature synergistic and cooperative: each part makes the others easier. But the And/Or of theorem 4.0 shows that under other circumstances, attempting to make the same network perform two simple tasks at the same time leads to a task that has a far greater order of difficulty. In those sorts of circumstances, there will be a clear advantage to having mechanisms, not to connect things together, but to keep such tasks apart. How can this be done in a connectionist net?\n\n\n… a brain is not a single, uniformly structured network. Instead, each brain contains hundreds of different types of machines, interconnected in specific ways which predestine that brain to become a large, diverse society of partially specialized agencies.\n\n\n\nGradient descent cannot escape local minima\nGradient descent, backpropagation, and all other hill-climbing algorithms are all vulnerable to getting trapped in local optima, and therefore they cannot work – except in problem-architecture pairs where the loss landscape of this particular problem, for this particular architecture, using this particular loss function, is a single bump whose width is shorter than this particular learning rate.\nGradient descent is just a form of hill-climbing, when the hill is differentiable. The perceptron learning algorithm can be interpreted as a hill-climbing algorithm too, as it makes localized decision to make one step in this direction or that, one error-signal at a time (Section 11.7). Therefore, the generic ineffectiveness of perceptron learning suggests that gradient descent is also generically ineffective and cannot scale. It does not even have a convergence theorem, so in that sense it’s worse than perceptron learning algorithm.9\n9 This claim is astonishing, now that we see how powerful backpropagation works, and how the perceptron learning rule had crippled neural network research for 30 years. We can understand their sentiments by remembering that they, like most of the academic community in computer science, favored the certainty of mathematical theorems over mere empirical success. Leo Breiman observed that academic statistics had been hamstrung by the same grasp over mathematical certainty, and thus over 95% of its publications were useless. (Breiman 1995)\nWe were very pleased to discover (see section 11.6) that PC could be represented as hill-climbing; however, that very fact led us to wonder whether such procedures could dependably be generalized, even to the limited class of multilayer machines that we named Gamba perceptrons. The situation seems not to have changed much – we have seen no contemporary connectionist publication that casts much new theoretical light on the situation. Then why has GD become so popular in recent years? … we fear that its reputation also stems from unfamiliarity with the manner in which hill-climbing methods deteriorate when confronted with larger-scale problems. … Indeed, GD can fail to find a solution when one exists, so in that narrow sense it could be considered less powerful than PC.\n\n\n\nStochastic gradient descent cannot see through the noise\n\nSo far as we could tell, every experiment described in (Rumelhart, Hinton, and Williams 1985) involved making a complete cycle through all possible input situations before making any change in weights. Whenever this is feasible, it completely eliminates sampling noise—and then even the most minute correlations can become reliably detectable, be­ cause the variance is zero. But no person or animal ever faces situations that are so simple and arranged in so orderly a manner as to provide such cycles of teaching examples. Moving from small to large problems will often demand this transition from exhaustive to statistical sampling, and we suspect that in many realistic situations the resulting sampling noise would mask the signal completely. We suspect that many who read the connectionist literature are not aware of this phenomenon, which dims some of the prospects of successfully applying certain learning procedures to large-scale problems.\n\n\n\nDifferentiable activation is just a hack\nUsing differentiable activations for neural networks is an artificial trick of questionable future. It makes the learned boolean functions imprecise, and only appears to redeem itself by allowing backpropagation. However, backpropagation is a dead-end because it will not scale. It is better to look for a method that can directly train multilayer perceptron networks with discrete activation functions.\n\nThe trick is to replace the threshold function for each unit with a monotonic and differentiable function … However, we suspect that this smoothing trick may entail a large (and avoidable) cost when the predicate to be learned is actually a composition of linear threshold functions. There ought to be a more efficient alternative based on how much each weight must be changed, for each stimulus, to make the local input sum cross the threshold.\n\n\nWe conjecture that learning XOR for larger numbers of variables will become increasingly intractable as we increase the numbers of input variables, because by its nature the underlying parity function is absolutely uncorrelated with any function of fewer variables. Therefore, there can exist no useful correlations among the outputs of the lower-order units involved in computing it, and that leads us to suspect that there is little to gain from following whatever paths are indicated by the artificial introduction of smoothing functions that cause partial derivatives to exist.\n\n\n\nConnectionists have no theory, so they should not extrapolate from experiments\n\nIn the past few years, many experiments have demonstrated that various new types of learning machines, composed of multiple layers of perceptron-like elements, can be made to solve many kinds of small-scale prob­ lems. Some of those experimenters believe that these performances can be economically extended to larger problems without encountering the limitations we have shown to apply to single­ layer perceptrons. Shortly, we shall take a closer look at some of those results and see that much of what we learned about simple perceptrons will still remain quite pertinent.\n\nWithout a mathematical theory, experimental data cannot be extrapolated. If neural networks happen to work well on a problem, it merely shows that the problem is a good fit for this particular architecture trained in this particular way at this particular scale, not anything more general than that.\n\nAs the field of connectionism becomes more mature, the quest for a general solution to all learning problems will evolve into an understanding of which types of learning processes are likely to work on which classes of problems. And this means that, past a certain point, we won’t be able to get by with vacuous generalities about hill-climbing. We will really need to know a great deal more about the nature of those surfaces for each specific realm of problems that we want to solve.\n\n\n… the learning procedure required 1,208 cycles through each of the 64 possible examples – a total of 77,312 trials (enough to make us wonder if the time for this procedure to determine suitable coefficients increases exponentially with the size of the retina). PDP does not address this question. What happens when the retina has 100 elements? If such a network required on the order of \\(2^{200}\\) trials to learn. most observers would lose interest.\n\n\n\nConnectionist experiments can be extrapolated to show that they do not scale\nThough lacking a theory of their own on the operation of multilayer perceptrons, Minsky and Papert proceeded to interpret the connectionist experiment data as showing that neural networks would fail to scale.10\n10 Without a mathematical theory of what neural networks can do, extrapolating from their behavior at small scales to the large scale is impossible and only reflect the bias behind those who make the extrapolation.Connectionists demonstrated that two-layered perceptrons, where both layers were trainable, bypassed the limits described in Perceptrons. For example, (Rumelhart, Hinton, and Williams 1985) showed that several problems unsolvable by a single perceptron – XOR, parity, symmetry, etc – were solved by a two-layered neural network.\n\n\n\nPage 253, Figure 2. Redrawn from (Rumelhart, Hinton, and Williams 1985)\n\n\nWhile the connectionist authors saw the result as a hopeful sign, Minsky and Papert interpreted it as showing that the experiments wouldn’t scale, because the coefficients appeared to grow exponentially – in just the way they proved in Chapter 7.\n\nIn PDP it is recognized that the lower-level coefficients appear to be growing exponentially, yet no alarm is expressed about this. In fact, anyone who reads section 7.3 should recognize such a network as employing precisely the type of computational structure that we called stratification.\nalthough certain problems can easily by solved by perceptrons on small scales, the computational costs become prohibitive when the problem is scaled up. The authors of PDP seem not to recognize that the coefficients of this symmetry machine confirm that thesis, and celebrate this performance on a toy problem as a success rather than asking whether it could become a profoundly “bad” form of behavior when scaled up to problems of larger size.\n\n\n\n\nPapert struck back\nWhile it appears that Minsky was the main author for the new prologue and epilogue, Papert solo-authored (S. Papert 1988), an essay that gave the controversy a uniquely Papert-styled spin. It is an extensive reframing of the perceptron controversy into a social and philosophical issue, with the prediction of ultimate victory for epistemological pluralism:\n\nThe field of artificial intelligence is currently divided into what seem to be several competing paradigms … for mechanisms with a universal application. I do not foresee the future in terms of an ultimate victory for any of the present contenders. What I do foresee is a change of frame, away from the search for universal mechanisms. I believe that we have much more to learn from studying the differences, rather than the sameness, of kinds of knowing.\n\nHe diagnosed the source of the philosophical error as a “category error”.\n\nThere is the same mistake on both sides: the category error of supposing that the existence of a common mechanism provides both an explanation and a unification of all systems, however complex, in which this mechanism might play a central role.\nArtificial intelligence, like any other scientific enterprise, had built a scientific culture… more than half of our book is devoted to “pro-perceptron” findings about some very surprising and hitherto unknown things that perceptrons can do. But in a culture set up for global judgment of mechanisms, being understood can be a fate as bad as death. A real understanding of what a mechanism can do carries too much implication about what it cannot do… The same trait of universalism leads the new generation of connectionists to assess their own microlevel experiments, such as Exor, as a projective screen for looking at the largest macroissues in the philosophy of mind. The category error analogous to seeking explanations of the tiger’s stripes in the structure of DNA is not an isolated error. It is solidly rooted in AI’s culture.\n\nHe then discussed the compute-first interpretation, a “bitter lesson” for the 1980s, before rejecting it.\n\nIn the olden days of Minsky and Papert, neural networking models were hopelessly limited by the puniness of the computers available at the time and by the lack of ideas about how to make any but the simplest networks learn. Now things have changed. Powerful, massively parallel computers can implement very large nets, and new learning algorithms can make them learn. …\nI don’t believe it. The influential recent demonstrations of new networks all run on small computers and could have been done in 1970 with ease. Exor is a “toy problem” run for study and demonstration, but the examples discussed in the literature are still very small. Indeed, Minsky and I, in a more technical discussion of this history (added as a new prologue and epilogue to a reissue of Perceptrons), suggest that the entire structure of recent connectionist theories might be built on quicksand: it is all based on toy-sized problems with no theoretical analysis to show that performance will be maintained when the models are scaled up to realistic size. The connectionist authors fail to read our work as a warning that networks, like “brute force” programs based on search procedures, scale very badly.\n\nConsider Exor, a certain neural network he picked out of the pages of PDP, which learned to perform the infamous XOR task, but only after 2232 examples. Was it slow, or fast? A proper judgment requires a mathematical understanding of the algorithm-problem fit. By extension, to properly judge whether neural networks were good for any specific problem, one must first mathematically understand the fit. He insinuated that the connectionists who were confident that their neural networks were more than a sterile extension of the perceptron did not do their math, unlike he and Minsky.\n\ninstead of asking whether nets are good, we asked what they are good for. The focus of enquiry shifted from generalities about kinds of machines to specifics about kinds of tasks. From this point of view, Exor raises such questions as: Which tasks would be learned faster and which would be learned even more slowly by this machine? Can we make a theory of tasks that will explain why 2,232 repetitions were needed in this particular act of learning?\n… Minsky and I both knew perceptrons extremely well. We had worked on them for many years before our joint project of under standing their limits was conceived… I was left with a deep respect for the extraordinary difficulty of being sure of what a computational system can or cannot do. I wonder at people who seem so secure in their intuitive convictions, or their less-than-rigorous rhetorical arguments, about computers, neural nets, or human minds.\n\n\n\nInterjection\n\nI wonder at people who seem so secure in their intuitive convictions, or their less-than-rigorous rhetorical arguments, about computers, neural nets, or human minds:\n\nThere is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgment that the extension is sterile. Per­haps some powerful convergence theorem will be discovered, or some profound reason for the failure to produce an interesting “learning theorem” for the multilayered machine will be found. (M. Minsky and Papert 1988, 232)\n\n\nWhat, then, explains the rise of connectionism? Since Papert reframed the fall of perceptron socially, it only stands to reason he would reframe the rise of connectionism as the rise of a social myth caused by other social myths, not by the increase in computing power or new algorithms like backpropagation, convolutional networks, and such. For one, the computing powers used by the breakthrough connectionist models like NETtalk were already within reach even in the 1960s.11 For another, he and Minsky were firm in their conviction that any uniform architecture must scale very badly and that no amount of computing or algorithmic advancement could be anything more than a sterile extension.\n11 NETtalk, a neural network with 3 layers and 18,629 weights, is entirely within reach for the 1960s. Its dataset was built in weeks by hand, and its training took a single night on a Ridge computer that is close to a VAX 11/780. Now, VAX 11/780 has \\(\\sim 1 \\;\\rm{MFLOP/sec}\\), so NETtalk took \\(\\sim 10^{11}\\;\\rm{FLOP}\\) to train. During the 1960s, typical workstations have a computing power of \\(\\sim 0.11 \\;\\rm{MIPS}\\), so NETtalk could be trained in a month.\n\nWe then used the 20,000-word Brown Corpus and assigned phonemes, as well as stress marks, to each of letters. The alignment of the letters and sounds took weeks, but, once the learning started, the network absorbed the whole corpus in a single night. (Sejnowski 2018, 115)\n\n\nI had picked up a Ridge computer, made by a company that is now defunct, but it had the power of a VAX 11/780 which at that time was the standard candle of computer power. … We had a real computer, and we had a real algorithm, and we looked for a do-able project in language. … I went to the library and found a book on text-to-speech that had probably one thousand rules in it. It was just lists of rules and exceptions. After every rule, there were dozens of exceptions. I figured that either they’re doing it the wrong way, or this is a really tough problem. We considered finding something that was simpler after consulting the experts in linguistics. They said, “Chomsky worked on the problem, and it is well beyond the capability of anything that you could imagine trying to do.”\n… In retrospect it was an ideal choice for a problem. It was difficult with conventional techniques, and it was not clear that the network could handle it. … We knew back then there were many local minima in the network, and we knew we were getting trapped. The surprise was that this did not prevent the network from finding good solutions. (Rosenfeld and Anderson 2000, 324–25)\n\n\nMassively parallel supercomputers do play an important role in the connectionist revival. But I see it as a cultural rather than a technical role, another example of a sustaining myth. Connectionism does not use the new computers as physical machines; it derives strength from the “computer in the mind,” from its public’s largely nontechnical awareness of supercomputers. I see connectionism’s relationship to biology in similar terms. Although its models use biological metaphors, they do not depend on technical findings in biology any more than they do on modern supercomputers. … I also see a more subtle, but not less relevant, cultural resonance. This is a generalized turn away from the hard-edged rationalism of the time connectionism last went into eclipse and a resurgent attraction to more holistic ways of thinking."
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html#rebuttal-to-minsky-and-papert",
    "href": "essays/posts/perceptron-controversy/index.html#rebuttal-to-minsky-and-papert",
    "title": "The Perceptron Controversy",
    "section": "Rebuttal to Minsky and Papert",
    "text": "Rebuttal to Minsky and Papert\n\nInterpreting the XOR problem\nWhen I first heard about the first neural network winter and the Perceptrons book, I was deeply confused by the story. The story went that “Perceptrons proved that the XOR problem is unsolvable by a single perceptron, a result that caused researchers to abandon neural networks”. How could it possibly cause researchers to abandon the field, unless it was news to them? But anyone could see that a single perceptron could only separate linearly separable points, and therefore the XOR problem is unsolvable by a single perceptron. When I first heard the story, I immediately saw why XOR was unsolvable by one perceptron, then took a few minutes to design a two-layered perceptron network that solved the XOR problem. I then noted that the NAND problem is solvable by a single perceptron, after which I immediately knew that perceptron networks are universal since the NAND gate is.\nIf a high school student could bypass the XOR problem in a few minutes, how could it possibly have been news to the researchers in 1969?\nWhen I started researching neural networks properly, the standard story about the XOR problem became more nonsensical the more I learned. The 1943 paper by McCulloch and Pitts (McCulloch and Pitts 1943) already said that their neural networks were equivalent in power to Turing machines. Marvin Minsky’s 1954 PhD thesis (M. L. Minsky 1954) develops an entire computer theory out of McCulloch–Pitts neural networks.\nOn the electrical engineering side, perceptron networks were studied under the name of “linear threshold logic” by electrical engineers since the 1950s, who clearly would not have bothered if they could not even make an XOR gate out of it. In fact, in a standard reference from 1965, there are chapters on “Single-Threshold-Element Synthesis by Iteration” – learning a single perceptron by the perceptron learning algorithm – and “Network Synthesis” – which does not imply machine learning, but rather hand-designing perceptron networks.(Dertouzos 1965)\nWhat is going on?\nI believe the story got completely garbled during the teaching process. I am all for changing history for the sake of understanding – history is made for the winners, not the winners made for history – but the standard story about the XOR problem is nonsensical, as I have shown. So how did the story come about?\nI believe this is because Perceptrons contained a host of problems that their restricted form of perceptron machines could not do. The simplest one is the XOR problem. Teachers who just wanted to spend two minutes on the first neural network winter and move on, grabbed this XOR problem and pretended that it was the actual cause of it.12\n12 A mantis was crawling on the wheel of a slowly moving train. It gloated, “I am the prime mover of the train!”. When the caterpillar asked it to prove so, it jumped down and waved its arms in front of the train, which promptly crushed it.\nThis is my retelling of the Taoist story of 螳臂當車.There is one thing left to explain: what is the significance of the XOR problem to the neural network researchers back in the days? It was clearly significant for something, as when the connectionists rose in the 1980s, one of the first things they did was to check that they could solve the XOR problem. Rumelhart read the Perceptrons book very carefully in 1970; it inspired him to go into neural network research, entirely missing its intended message. After he developed backpropagation around 1982, he immediately tried to train an MLP on the XOR problem.\n\nWhen I first did the XOR problem, it took a thousand iterations to solve it. If we thought that was the way it was going to go and that we were going to scale up to a hundred thousand input patterns, my God, we wouldn’t live long enough to see the results. But that’s not the way it’s gone. That problem turned out to be an anomaly. The scaling is about linear. We haven’t hit any exponential curves yet. (Rosenfeld and Anderson 2000)\n\nWhat is the significance of the XOR problem? In the context of the neural network research in the 1960s, the significance becomes clear. Nobody knew how to simultaneously adapt two or more layers well.\nBefore 1962, Rosenblatt had studied both theoretically and experimentally “four-layer perceptron with adaptive preterminal network”, which means a perceptron network with three layers: the first layer random and fixed, and the second and third layers learned (Rosenblatt 1962, vol. 55, chap. 16). However, it had not a single derivative in it. The second layer was learned by the Hebbian learning rule, and the third layer was by the perceptron learning rule.\nMeanwhile, during the early 1960s, Widrow and Hoff trained a single perceptron with gradient descent, then proceeded to try every trick except gradient descent to train a two-layered perceptron network. They gave up and parted ways. Hoff went on to co-invent the microprocessor at Intel, while Widrow applied a single perceptron to adaptive filter design, revolutionizing electrical engineering in the process. These and more of the ridiculous backstory can be read in The Backstory of Backpropagation.\nIn short, due to a variety of unfortunate developments, people spent about twenty years (1950–1970) failing to find an effective algorithm for training the pre-final layers of neural networks. They could train the final layer either by the perceptron learning rule of Rosenblatt or by the Widrow–Hoff rule of gradient descent on the squared error, but that was the extent of the learning they could get the neural networks to do.\nConsider a two-layered neural network. The second layer is easy to learn. What should happen to the first layer? Rosenblatt’s solution was mainly just randomization because he mistakenly believed that the retina was randomly wired to the visual cortex, and he believed in emulating nature. Rosenblatt was working with the standard knowledge of neuroscience in his time. He could not have known that neural connections were anything but random – the first of the Hubel and Wiesel papers was published only in 1959. However, it seems that Rosenblatt simply had a strong attachment to randomization, as (Rosenblatt 1962) cites (Hubel and Wiesel 1959) several times, yet he still randomized the first layer for most experiments in the book. Rosenblatt had also experimented with Hebbian learning (Rosenblatt 1962, vol. 55, sec. 16.1), but since he did not use this method extensively, I infer that it did not work well.\nWidrow’s solution was the MADALINE I rule – a complicated hack and a dead end. Without an effective method to train the first layer, those who worked on two-layered neural networks had only two choices: either randomize the first layer or design it by hand. Both choices played right into the hands of Minsky and Papert.\nSeen from the viewpoint of the second layer, the first layer is the featurizer for the raw input. It is intuitively clear that, unless the raw input is featurized and the features are adapted to the problem, the second layer will not be able to solve the problem.\nThe XOR problem requires two layers. Furthermore, if the first layer is not wired correctly, the second layer will not be able to solve it either.\nPut yourself in the place of a 1960s connectionist. How do you solve the XOR problem by a perceptron network? Well, not a single perceptron, as it’s impossible. Not with three layers, because two layers are sufficient, and you already have enough problems with two layers. So, two layers.\nHow to train it? You know only how to fix the first layer and train the second. How do you fix the first layer? Do you randomize it? Unless you use many hidden perceptrons, this will fail with high probability. Do you design it by hand? But then, Minsky and Papert would interject, “You see, you cannot substitute thinking by tabula-rasa learning! You need some intelligent design to get it to work! The network needs the right representations in the hidden layer, and you cannot expect it to learn the representation from a vacuous generality like the fully connected multilayer perceptron, unless you did not get the lesson from our book. You must design it by hand.”.\nNot to give up, you try one of the hacks like the MADALINE I learning rule, or the Hebbian learning rule, but they are extremely fiddly and unable to learn most of the time unless you tune them just right, and it seems to require a different tuning for problems even slightly more complex than the XOR problem. Minsky and Papert interject again, “You see, there is no universal learning algorithm! You need a bespoke learning algorithm for each problem!”.\nAnd so we stood at the impasse of the 1960s. If only we had tried an activation function, any activation function, other than the dreaded 0-1 activation function…\n\n\nWhere did they go wrong?\nBrains are neural networks in hardware – in this regard, there is no controversy since the 1900s. Intelligence is what happens in the brain. This is the occasion for small controversies from the “embodiment cognition” or “externalism” school, like those of James Gibson and Rodney Brooks, but none that has led to anything substantial yet. Therefore, most people agree that intelligence is something that neural networks do, including those people who are otherwise dismissive of neural networks like Minsky and Papert.\nThe abstract of a key anti-connectionist paper (Fodor and Pylyshyn 1988) makes the point that the brain is symbolic at the “cognitive level”, and only beneath that level it is connectionist. Interpreted with sufficient charity, this hypothesis is unfalsifiable. None disputes that the brain is connectionist, and the operation of any hardware is symbolic if you use enough symbols to approximate the real numbers. However, at this level of charity, the hypothesis is also useless, therefore we must interpret less charitably.\nWhat did they really mean? They concretely rejected “Parallel Distributed Processing”, and claimed that trained neural networks work if and only if they implement approximations to symbolic programs, where each symbolic variable is represented locally by a small group of neurons (thus not “distributed”), and the variables are processed serially layer by layer through the network (thus not “parallel”). Further, the symbolic programs they approximate are not any kind of symbolic programs (otherwise we fall back to the trivial claim), but symbolic programs that people tend to write, things that on the small scale resemble subroutines and command line scripts, and on the large scale resemble operating systems and the Cyc project.\nAt this level, it is quantifiable and thus scientifically testable. However, scientific hypotheses become political disputes when large amounts of money or social justice is on the line. We can consider an alternative history with an alternative Minsky and Papert. In this history, they put this in the epilogue:\n\nOur mathematical results indicate that we need multilayer perceptrons as well as efficient methods for training them. Furthermore, simple estimates show that brain-level intelligence likely require computing power up to 10 orders of magnitude larger than currently available, suggesting the need for special hardware boards.\nWe also need to explore alternative architectures capable of correlating global information without using all-to-all connections. Perhaps they should have a two-level structure, with a meta-network generating weights for the network, or perhaps more generic mechanisms for multiplicative interactions. Certain inherently serial operations, such as the connectivity predicate, suggest that there must be ‘serial mode interfaces’ allowing neural networks to call external subroutines. It is a live scientific question whether the number of external subroutines can be kept small. Perhaps a hundred or so would suffice, or perhaps it would turn out that even large neural networks are incapable of most commonsense tasks, in which case the Society of Mind hypothesis would be more viable. However, we consider this an empirical question that can only be answered by attempting to scale up neural networks and seeing what they might do, as a priori estimates of computational difficulty is close to impossible.\n\nWhat distinguishes the two possible Minsky–Paperts? Not the facts present, but their prescientific commitments. Minsky’s commitment to elegant mathematics and simple programming structures led him to insist on things for which he could prove theorems – and to denounce empirical methods, especially if large sums of money might be “misdirected” to large-scale neural network machines. Papert, committed to epistemological pluralism, had no choice but to insist on computers that resembled his ideal society – and to denounce any uniform computational structure as flattening, enframing, and reproducing the hegemonic ideology of universalism.\nFor Papert and Minsky specifically, their claim to be “pro-perceptron” is a sophistry intended to shift the narrative on the perceptron controversy, as they only approved perceptrons with a single layer of learnable parameters. In other words, they were only pro-useless-perceptron. They were trying to kill the project of general large-scale perceptrons, which both Frank Rosenblatt and the new connectionists in the 1980s were working towards.\n\nThere is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgment that the extension is sterile. Per­haps some powerful convergence theorem will be discovered, or some profound reason for the failure to produce an interesting “learning theorem” for the multilayered machine will be found. (M. Minsky and Papert 1988, 232)\n\nThe irony is that decades later, despite the general neglect of neural networks, they quickly overtook symbolic or statistical AI13 as soon as compute and data price fell so low that they had to appear. And so in 2012, Alex Krizhevsky cobbled together 8 GPUs and train a neural network that outperformed every symbolic or statistical AI.14 There are large homogeneous neural networks that work, and there are hints that some of them have small groups of neurons representing symbolic concepts, some of which are engaged in serial computation across the layers. However, to find these hints of symbolic programs, we had to take a large detour through the brute reason of uniform neural network architecture, uniform GPU architecture, and uniform training objectives.\n13 More precisely, classical-statistical AI, with fixed parameters, handcrafted features, and solvable models. A classical-statistical model is constructed as some form of \\(p_\\theta(y|x)\\), where \\(\\theta\\) are the parameters, \\(x\\) are the inputs, and \\(y\\) are the outputs.\nThe difference from neural networks is that for classical-statistical models, \\(p_\\theta\\) allows solvable inference from a dataset, such as by taking the average, derivative, variance, and such. Many of them were straight up linear regressions on handcrafted features (and thus subject to exactly the criticism of Minsky and Papert).\nA good example is the IBM alignment model 1, which can be trained by expectation-maximization with closed form solution (!). To see the difference, compare it with (Bahdanau, Cho, and Bengio 2014), which also learns to align from a corpus, but does not have any closed form solution.14 And if not Krizhevsky and Hinton, how much longer would it have taken? In 2009, Andrew Ng’s research cluster trained a 100M model on GPUs (Raina, Madhavan, and Ng 2009), which suggests that the idea was ripe for taking due to the advance in compute and data, and would have happened around 2010 regardless. The rain might not follow the plow, but the AI does follow the compute and data.Why must we take such a large detour? My guess is twofold. One, the resemblance to neat symbolic programs is partial. Large amounts of computing done by neural networks is only symbolic in the trivial, messy way. Only a small amount is symbolic in the neat way. Two, because symbolic programs suffer from diseconomies of scale. Peering into any large enough software project, be it the Cyc project, or the Linux source code, one feels that it is easier to start anew than to add to it. Perhaps with thousands of years of very patient work and many evolutionary deadends, purely symbolic AI research can succeed in constructing a general intelligence in the elegant style sketched by Minsky. The irony is that symbolic programs do not scale while neural networks scale, the exact opposite of the lesson that Minsky and Papert wished to impart by their book.\nAs an example, the history of computer vision demonstrates the problem with the symbolic AI approach. It is true that some problems, such as the parity problem or the connectedness problem, cannot be efficiently solved by neural networks. However, do they really matter? To make connection with the final goal of general computer vision, capable of understanding real scenes, turns out to be far less about provably detecting edges and cubes and cones in a picture, and far more about having a large dataset. In this sense, it’s Minsky and Papert who were misled by their experiments with building block-playing robots in a block world. It’s their work that could not scale.\n\n\nWhat is left of Perceptrons?\nI have never seen a piece of work so systematically opposed to the scaling hypothesis. Reading their theory, I have the feeling that at every turn, I could hear them say, “Neural networks work – if they have less than 100 neurons.”. To their credit, they made falsifiable hypotheses. To their blame, they were almost all proven wrong. Neural networks do scale, to 100 billion and counting. Several standard architectures constitute almost the entirety of neural networks nowadays – MLP, CNN, GNN, LSTM, VAE, and Transformers. Six is quite far from the thousands of architectures they explicitly predicted.\nAmong all the objections to neural networks in the Perceptrons book, almost all were either disproved (the anti-scaling hypothesis) or made irrelevant (the perceptron learning rule).\nRecognizing connectivity is hard and requires a serial program, but that’s fine, because it’s hard for humans too. Learning to solve logical problems is difficult and requires a thousand iterations. Well, it looks inefficient, except that neural networks are still the best we have even 30 years later, so perhaps the XOR problem is just something neural networks have to work hard for. That’s fine – in the worst case, we’ll just let the neural network offload those logical operations to a symbolic program, much like how humans use calculators.\nThe only legitimate remaining problem is the recognition of symmetry. It is hard for all modern neural networks, including convolutional and fully connected versions.15 In any case, if human brains are neural networks and they can instantly recognize symmetries, then it shows that there is some remaining architectural trick we don’t yet know.\n15 It might be solved efficiently with a Transformer, but I need to check this.Therefore, out of all the clever mathematics and wise lessons of Perceptrons, we ended up with… just one problem remaining? Minsky and Papert hoped to show that there would be thousands of different problems, each requiring a bespoke algorithm implemented by a bespoke neural network. In this regard, their project has been fully debunked."
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html#appendix-the-three-camps-of-ai",
    "href": "essays/posts/perceptron-controversy/index.html#appendix-the-three-camps-of-ai",
    "title": "The Perceptron Controversy",
    "section": "Appendix: The three camps of AI",
    "text": "Appendix: The three camps of AI\nIn the early days of AI, there were mainly three camps: cybernetics, symbolic systems, and neural networks. In our current age, it seems the other two camps have fallen largely into oblivion. This section gives a brief history and an orienting perspective of their key ideas.\n\nCybernetic AI\nThe founding metaphor of the cybernetic camp was that intelligence is adaptive analog signal processing: homeostasis, Fourier transforms, Kalman filtering, PID control, linear predictive coding, and such.\nThe origin of cybernetics was entangled with the control of machinery in WWII, when you were either the quick or the dead. Norbert Wiener, the mastermind of cybernetics, worked on anti-aircraft gun (AA gun) controllers. As planes flew faster and higher than ever before, AA guns needed to “lead the target” to a greater and greater extent. This put a severe strain on the operator of the AA guns. Wiener constructed electromechanical devices that performed linear prediction of the future trajectory of an aircraft based on its past trajectory. As the aircraft was a human-machine system, Wiener modelled both together as a synthetic whole. After the war, Wiener continued to work on cybernetics, using the analogies he had accumulated during the war.\n\nIf humans do not differ from machines from the “scientific standpoint,” it is because the scientific standpoint of the 1940s was one of men machines at war. The man-airplane-radar-predictor-artillery system is closed one in which it appeared possible to replace men by machines and machines by men. To an antiaircraft operator, the enemy really does act like an autocorrelated servomechanism. … In every piece of his writing on cybernetics, from the first technical exposition of the AA predictor before Pearl Harbor up through his essays of the 1960s on science and society, Wiener put feedback in the foreground, returning again and again to the torpedo, the guided missile, and his antiaircraft director. (Galison 1994)\n\nCybernetics entered the realm of popular consciousness with Wiener’s 1948 bestseller, Cybernetics. In it, we find a curious description of artificial intelligence and self-reproduction from the analog signal processing point of view, detailed in Cybernetic artificial intelligence. The short version is that it was an analog-circuit quine:\n\nThese operations of analysis, synthesis, and automatic self-adjustment of white boxes into the likeness of black boxes … [by] learning, by choosing appropriate inputs for the black and white boxes and comparing them; and in many of these processes, including the method of Professor Gabor, multiplication devices play an important role. While there are many approaches to the problem of multiplying two functions electrically, this task is not technically easy. On the one hand, a good multiplier must work over a large range of amplitudes. On the other hand, it must be so nearly instantaneous in its operation that it will be accurate up to high frequencies. Gabor claims for his multiplier a frequency range running to about 1,000 cycles. (Wiener 2019, xli)\n\nThe cybernetic approach to AI is a strange chapter in the history of AI, filled with machines too sui generis to have followups. It seems to me that there is no issue with building our way to AI one nonlinear filter at a time, except for technical issues, but the technical issues are insurmountable. One day I might write an essay that gives justice to the cybernetic approach, but as this essay is not about cybernetics, we will only give a whirlwind tour of highlights from the cybernetic approach to AI.\nIn 1948, Ross Ashby built a “homeostat machine”, consisting of four interacting electromechanical controllers. Each controller had some needles that can move in arcs. If one perturbs it, so that the needles move out of their “comfort zones”, the needles would complete an electric circuit, and the controller would start going through every possible setting one by one, until the needles return to their comfort zones.16 The other thing for which Ashby is famous is the “law of requisite variety”, which is equivalent to the theorem that to solve \\(f(x) = y\\), generically, the \\(x\\) must have at least as many dimensions as the \\(y\\).\n16 Perhaps Marvin Minsky’s useless machine was a gentle parody of the homeostat machine. As the homeostat was built in 1948 and the useless machine in 1952, the timing checks out.17 Stafford Beer might have meant this literally, according to (Pickering 2004):\n\n… it is clear from subsequent developments that the homeostatic system Beer really had in mind was something like the human spinal cord and brain. He never mentioned this in his work on biological computers, but the image that sticks in my mind is that the brain of the cybernetic factory should really have been an unconscious human body, floating in a vat of nutrients and with electronic readouts tapping its higher and lower reflexes …\n\nStafford Beer started his professional life as a business consultant, and devised methods to increase steel plant production efficiency. He understood production management as a problem analogous to that of biological homeostasis – a problem solved by the nervous system – and so he managed production by imitating the nervous system17. He also investigated a wide variety of strange machines, including one that used an entire pond ecosystem as a computer for black-box homeostatic control (Beer 1962):\n\nWhy not use an entire ecological system, such as a pond? The theory on which all this thinking is based does not require a knowledge of the black box employed. Accordingly, over the past year, I have been conducting experiments with a large tank or pond. The contents of the tank were randomly sampled from ponds in Derbyshire and Surrey. Currently there are a few of the usual creatures visible to the naked eye (Hydra, Cyclops, Daphnia, and a leech); microscopically there is the expected multitude of micro-organisms. In this tank are suspended four lights, the intensities of which can be varied to fine limits. At other points are suspended photocells with amplifying circuits which give them very high sensitivity. … There is an ecological guarantee that this system stabilizes itself, and is indeed ultra-stable. But, of course, the problem of making it act as a control system is very difficult indeed. I regard the machine as a tending-to-be-stable-but-not-quite homeostat …\n\nIn 1971, he was invited to become the principal architect of Project Cybersyn, which was a nervous system for the Chilean socialist economy, employing “algedonic control” (“algedonic” is Greek for “pain-pleasure”). This project, like president Allende, was shot in the head by the 1973 coup that established a free market economy in Chile. After this, he lived as a hermit, though he still published a few books and did occasional business consulting.(Morozov 2014)\nIn the 1950s, Gordon Pask constructed electrochemical “sensory organs”. He prepared a dish of acidic metal salt solution (such as \\(\\text{FeSO}_4\\)) and then immersed electrodes into it. Metal tends to dissolve in the acidic solution, but applying a voltage through the electrodes pulls metal out of the solution by electrolysis. This allows him to “reward” whatever metallic structure in the dish by applying a voltage. He reported success at growing some simple sensory organs in the dish (Gordon 1959; Cariani 1993):\n\nWe have made an ear and we have made a magnetic receptor. The ear can discriminate two frequencies, one of the order of fifty cycles per second and the other on the order of one hundred cycles per second. The ‘training’ procedure takes approximately half a day and once having got the ability to recognize sound at all, the ability to recognize and discriminate two sounds comes more rapidly. … The ear, incidentally, looks rather like an ear. It is a gap in the thread structure in which you have fibrils which resonate at the excitation frequency. (Gordon 1959)\n\nThe details of the electrochemical ear are lost, and this line of research had no followups.\nA faint echo of Pask’s electrochemical ear was heard in late 1990s, when Adrian Thompson used evolutionary algorithm to evolve circuits on field-programmable gate arrays to tell apart input signals of frequencies \\(1 \\mathrm{~kHz}\\) and \\(10 \\mathrm{~kHz}\\). Curiously, some circuits succeeded at the task despite using no master clock signal. He traced this down to the detailed physical properties that digital circuit design was precisely meant to abstract away from. The circuit functioned precisely because the electronic elements were not digital, but analog.18 The circuits’ performance degraded when outside the temperature range in which they evolved in (Thompson and Layzell 1999; Thompson, Layzell, and Zebulum 1999).\n18 It is as if a linear neural network managed to compute a nonlinear function precisely because floating point operations are not perfect.(Foerster 2017)\n… at \\(43.0^{\\circ} \\mathrm{C}\\) the output is not steady at \\(+5 \\mathrm{~V}\\) for \\(\\mathrm{F} 1\\), but is pulsing to \\(0 \\mathrm{~V}\\) for a small fraction of the time. Conversely, at \\(23.5^{\\circ} \\mathrm{C}\\) the output is not a steady \\(0 \\mathrm{~V}\\) for \\(\\mathrm{F} 2\\), but is pulsing to \\(+5 \\mathrm{~V}\\) for a small fraction of the time. This is not surprising: the only time reference that the system has is the natural dynamical behaviour of the components, and properties such as resistance, capacitance and propagation delays are temperature dependent. The circuit operates perfectly over the \\(10^{\\circ} \\mathrm{C}\\) range of temperatures that the population was exposed to during evolution, and no more could reasonably be expected of it. (Thompson 1996)\n\nContinuing the tradition of one-hit wonders, there was no followup work to this.19\n19 I have tried tracking down what Adrian Thompson has been doing since his late 1990s work. It turned out that the hardware evolution was his PhD work (Thompson 1998). He has almost completely dropped off the face of academia. His website at University of Sussex did not see another update since 2002 and is currently dead. His minimalistic Google Site was created around 2014, and currently only survives on the Internet Archive. There was also a single gif of the circuit in operation, which I decided to download and save for posterity.\n\nSymbolic AI\nThe founding metaphor of the symbolic system camp was that intelligence is symbolic manipulation using preprogrammed symbolic rules: logical inference, heuristic tree search, list processing, syntactic trees, and such. The symbolic camp was not strongly centered, though it had key players like Alan Turing, John McCarthy, Herbert Simon, and Marvin Minsky.\nThe project of symbolic AI is a curious episode in AI history. Despite its early successes such as SHRDLU, LISP, and ELIZA, despite the support of most of AI researchers during 1960–2000, and despite the support of reigning cognitivists in the adjacent field of psychology, it never achieved something as successful as neural networks.\nA brief sketch of the greatest project in symbolic AI might give you a feel for the difficulty involved. Have you ever heard of the Cyc doing anything useful in its 39 years of life? Compare that with something even as small as BERT.\nIn 1984, Douglas Lenat began the Cyc project, an ambitious attempt to scale symbolic AI up to the real world. The Cyc project used a LISP-like symbolic logic language to encode common sense. Even from the vantage point of 1985, it was clear to all that there was a lot of common sense to code in 20, although few could have predicted that Lenat would persevere at it for over 30 years. In 2016, Lenat finally declared the Cyc project “done” and set about commercializing it.\n20 They themselves probably underestimated the difficulty. In 1990, they confidently titled a paper “Cyc: A midterm report” (D. Lenat and Guha 1990), suggesting that they expected to be done around 1995.\nHaving spent the past 31 years memorizing an astonishing collection of general knowledge, the artificial-intelligence engine created by Doug Lenat is finally ready to go to work. Lenat’s creation is Cyc, a knowledge base of semantic information designed to give computers some understanding of how things work in the real world. … “Part of the reason is the doneness of Cyc,” explains Lenat, who left his post as a professor at Stanford to start the project in late 1984. “Not that there’s nothing else to do,” he says. But he notes that most of what is left to be added is relevant to a specific area of expertise, such as finance or oncology. Among other projects, the company is developing a personal assistant equipped with Cyc’s general knowledge. This could perhaps lead to something similar to Siri but less predisposed to foolish misunderstandings. Michael Stewart, a longtime collaborator of Lenat’s and the CEO of Lucid, says the new company is in talks with various others interested in using the Cyc knowledge base. Lucid has been working with the Cleveland Clinic, for example, to help automate the process of finding patients for clinical studies. (Knight 2016)\n\nThat was essentially the last we heard from Cyc.\nWhy has the symbolic AI project failed to scale? It is hard to say, and a definitive answer would not be forthcoming until we have a human level AI as an existence proof of the necessary requirements for scalable AI. However, I can speculate. When looking at this figure from 1985, one is simultaneously filled with respect and sadness, for they were facing impossible odds, and yet they charged right into it.\n\n\n\n(D. B. Lenat, Prakash, and Shepherd 1985, fig. 1)\n\n\nTheir “midterm report” only accentuates this sense of tragedy, of seeing them fighting impossible odds, and losing. They saw with clarity that there is no shortcut to intelligence, no “Maxwell’s equations of thought”.\n\nThe majority of work in knowledge representation has dealt with the technicalities of relating predicate calculus to other formalisms and with the details of various schemes for default reasoning. There has almost been an aversion to addressing the problems that arise in actually representing large bodies of knowledge with content. However, deep, important issues must be addressed if we are to ever have a large intelligent knowledge-based program: What ontological categories would make up an adequate set for carving up the universe? How are they related? What are the important facts and heuristics most humans today know about solid objects? And so on. In short, we must bite the bullet.\nWe don’t believe there is any shortcut to being intelligent, any yet-to-be-discovered Maxwell’s equations of thought, any AI Risc architecture that will yield vast amounts of problem-solving power. Although issues such as architecture are important, no powerful formalism can obviate the need for a lot of knowledge.\nBy knowledge, we don’t just mean dry, almanac-like or highly domain-specific facts. Rather, most of what we need to know to get by in the real world is prescientific (knowledge that is too commonsensical to be included in reference books; for example, animals live for a single solid interval of time, nothing can be in two places at once, animals don’t like pain), dynamic (scripts and rules of thumb for solving problems) and metaknowledge (how to fill in gaps in the knowledge base, how to keep it organized, how to monitor and switch among problem-solving methods, and so on). Perhaps the hardest truth to face, one that AI has been trying to wriggle out of for 34 years, is that there is probably no elegant, effortless way to obtain this immense knowledge base. Rather, the bulk of the effort must (at least initially) be manual entry of assertion after assertion. (D. Lenat and Guha 1990)\n\nI was struck by the same sense of ontological vertigo when looking back at Simon and Newell’s Human Problem Solving, a compendium of their work on decomposing human problem solving into symbolic processes:\n\n\n\n(Newell and Simon 1972, 533)\n\n\n\n\n\n(Newell and Simon 1972, 534)\n\n\nThis sense of vertigo is perhaps best described by Borges in The analytical language of John Wilkins (Borges 2000, 229–32):\n\n… we must examine a problem that is impossible or difficult to postpone: the merit of the forty-part table on which the language is based. Let us consider the eighth category: stones. Wilkins divides them into common (flint, gravel, slate); moderate (marble, amber, coral); precious (pearl, opal); transparent (amethyst, sapphire); and insoluble (coal, fuller’s earth, and arsenic). The ninth category is almost as alarming as the eighth. It reveals that metals can be imperfect (vermilion, quicksilver); artificial (bronze, brass); recremental (filings, rust); and natural (gold, tin, copper). The whale appears in the sixteenth category: it is a viviparous, oblong fish.\nThese ambiguities, redundancies, and deficiencies recall those attributed by Dr. Franz Kuhn to a certain Chinese encyclopedia called the Heavenly Emporium of Benevolent Knowledge. In its distant pages it is written that animals are divided into (a) those that belong to the emperor; (b) embalmed ones; (c) those that are trained; (d) suckling pigs; (e) mermaids; (f) fabulous ones; (g) stray dogs; (h) those that are included in this classification; (i) those that tremble as if they were mad; (j) in­ numerable ones; (k) those drawn with a very fine camel’s-hair brush; (1) etcetera; (m) those that have just broken the flower vase; (n) those that at a distance resemble flies. The Bibliographical Institute of Brussels also exercises chaos: it has parceled the universe into 1,000 subdivisions, of which number 262 corresponds to the Pope, number 282 to the Roman Catholic Church, number 263 to the Lord’s Day, number 268 to Sunday schools, number 298 to Mormonism, and number 294 to Brahmanism, Buddhism, Shintoism, and Taoism. Nor does it disdain the employment of heterogeneous subdivisions, for example, number 179: “Cruelty to animals. Protection of animals. Dueling and suicide from a moral point of view. Various vices and defects. Various virtues and qualities.”"
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html#appendix-the-chomskyans",
    "href": "essays/posts/perceptron-controversy/index.html#appendix-the-chomskyans",
    "title": "The Perceptron Controversy",
    "section": "Appendix: the Chomskyans",
    "text": "Appendix: the Chomskyans\nIn the field of psychology, the neural network vs Minsky divide corresponds quite well with the behaviorism vs cognitivism divide.\nAmong the cognitivists, there is no consensus view about the exact role that neural networks must play. Some, like Fodor and Marr, argue that intelligence is just a hardware-independent program, and brains just happen to use neural networks to run the program due to accidents of evolution and biophysics. Others, like Gary Marcus, grant neural networks a more substantial role in constraining the possible programs – that is, each neural network architecture is designed , and each architecture can only run a certain kind of program efficiently – but they still insist that neural networks must have very particular architectures.\nSome might allow simple learning algorithms with simple environmental stimulus (the universal grammar approach of Chomsky), while others might allow complex learning algorithms with complex environmental stimulus (the society of mind approach of Minsky), but what unifies the cognitivists is that they are against simple learning algorithms applied to complex environmental stimulus. They called this enemy by many names, such as “radical behaviorism”, “Skinnerism”, “perceptrons”, “radical connectionism”, and now “deep learning”.\n\nNoam Chomsky\nThe cognitivist revolution was led by Noam Chomsky against behaviorism during the 1950s, ending with the victory of cognitivism in “higher psychology”, such as linguistics, though behaviorism survived respectably to this day in other aspects of psychology, such as addiction studies and animal behavior.\nIn a curious parallel, just as neural networks for AI became popular again in the late 1980s, statistical methods for NLP returned during the same period. The watershed moment was the series of IBM alignment models published in 1993 (Brown et al. 1993).\nIn the 1950s, language production was modelled by theoretical machines: finite state automata, stack machines, Markov transition models, and variants thereof. We must understand Chomsky’s two contributions to linguistics. On the first part, he constructed a hierarchy of increasingly powerful kinds of language. This hierarchy was in one-to-one correspondence with the hierarchy of theoretical machines, from finite state automata (type-3, or regular) all the way to Turing machines (type-0, or recursively enumerable). On the second part, he rejected statistical language learning and argued for inborn universal grammar.\nChomsky argued, and subsequent linguists have found, that the syntax of all human languages is at the type-2 level, or a context-free grammar. None are regular and almost none are context-dependent. Regular languages are modeled by finite state machines and cannot model arbitrarily deep recursion, whereas context-free languages allow for arbitrarily deep recursion such as center embedding. This fact would come into play later.\nWith the dominance of the Chomsky approach, finite state machines were abandoned, as they are not capable of parsing context-free grammar. You might have heard of the controversy over Pirahã. It is mainly fought over the problem of recursion: does the Pirahã language have recursion or not?21\n21 Tracing the battle lines, I predicted that Pinker would argue that it must have recursion… and I turned out to be wrong. Pinker argued against Chomsky in this case.\n\n“There’s a lot of strange stuff going on in the Chomskyan program. He’s a guru, he makes pronouncements that his disciples accept on faith and that he doesn’t feel compelled to defend in the conventional scientific manner. Some of them become accepted within his circle as God’s truth without really being properly evaluated, and, surprisingly for someone who talks about universal grammar, he hasn’t actually done the spadework of seeing how it works in some weird little language that they speak in New Guinea.”\nPinker says that his own doubts about the “Chomskyan program” increased in 2002, when Marc Hauser, Chomsky, and Tecumseh Fitch published their paper on recursion in Science. The authors wrote that the distinctive feature of the human faculty of language, narrowly defined, is recursion. Dogs, starlings, whales, porpoises, and chimpanzees all use vocally generated sounds to communicate with other members of their species, but none do so recursively, and thus none can produce complex utterances of infinitely varied meaning. “Recursion had always been an important part of Chomsky’s theory,” Pinker said. “But in Chomsky Mark II, or Mark III, or Mark VII, he all of a sudden said that the only thing unique to language is recursion. It’s not just that it’s the universal that has to be there; it’s the magic ingredient that makes language possible.” (Colapinto 2007)\n\nA key principle Chomsky used was the “poverty of stimulus” argument, which he used to argue that humans must have a universal grammar built in at birth, because children cannot possibly learn to speak when they are just a few years old – they cannot possibly have heard and seen enough. For one thing, true recursion can never be learned empirically, because true recursion can only be conclusively proven by observing an infinite number of sentences.\nConsider the simple example of the balanced brackets language. A language learner observes sample sentences from the language and tries to infer the language. Suppose the learner sees a sequence (), (()), ((())), (((()))). What can they conclude? That it is the balanced brackets language? So we ask them to construct another sentence, and they confidently write ((((())))), but then we inform them that they have been tricked! The language is the balanced brackets language – except that the brackets only go 4 levels deep. In general, only by seeing all levels of recursion can the balanced brackets language be conclusively learned.\nApplied to linguistics, Chomsky claimed that statistical learning cannot learn syntax, and all attempts have been “colossal failures”.\n\nJust to illustrate, I’ll take one example that was presented back in the 1950s and has become a sort of a classic case because it’s so trivial. ‘Can eagles that fly swim?’ Okay, simple sentence. Everyone understands it. Any young child understands it. There is a question about it. We know that we associate the word ‘can’ with ‘swim,’ not with ‘fly.’ We’re asking ‘Can they swim?’ We’re not asking ‘Can they fly?’ Well, why is that? A natural answer ought to be that you associate ‘can’ with ‘fly.’ After all, ‘fly’ is the word that’s closest to ‘can,’ so why don’t you just take the closest word and interpret it that way? … Well, that property is universal. It holds up in every language. Languages may do it differently, but they’re going to have the same property. It holds in every construction anyone knows, and it’s just a universal property of language.\nWell, this particular example has taken on a life of its own. For one thing, it’s a poverty of stimulus problem, like Descartes’ triangle. There’s been a huge effort to show that it’s not a problem, that if you just do a complex statistical analysis of complex data, you’ll find that that’s what the child will determine from the data. The approaches are odd in several respects. First, every one is not only a failure but a colossal failure. I’m not going to talk about that. I actually have a recent paper about it with a computer scientist at MIT, Bob Berwick, where we run through a lot of the current proposals but it’s easy to show that they’re all just wildly wrong. But they keep coming. Almost every issue of the main cognitive science journals has another proposal of this sort, so that’s one odd fact. There are many efforts to show that there is nothing puzzling about it, they’re all colossal failures. (Chomsky 2010)\n\nAt last, a testable hypothesis! I asked GPT-4 to Draw a syntax tree for \"Can eagles that fly swim?\", and received this:22\n22 The code it gave failed on the first try, due to an environment variable issue on the Linux virtual machine it ran on. We fixed it after two more plies of conversation.\n\nCode\n!pip install svgling\nimport nltk\nfrom nltk import Tree\nfrom IPython.display import display\nnltk.download('punkt')\nimport svgling\nfrom IPython.display import SVG, display\n\ntree = Tree.fromstring(\"\"\"\n    (SQ \n        (MD Can) \n        (NP \n            (NNS eagles) \n            (SBAR \n                (WHNP (WDT that)) \n                (S \n                    (VP (VBP fly))\n                )\n            )\n        ) \n        (VP (VB swim))\n    )\n\"\"\")\n\ntree_svg = svgling.draw_tree(tree)\ntree_svg.get_svg().save()\ndisplay(SVG(tree_svg.get_svg()))\n\n\n\n\n\nThe syntax tree for “Can eagles that fly swim?”, drawn by GPT-4.\n\n\nAs expected, Chomsky rejected statistical language learning to the very end.\n\nIt’s true there’s been a lot of work on trying to apply statistical models to various linguistic problems. I think there have been some successes, but a lot of failures. There is a notion of success … which I think is novel in the history of science. It interprets success as approximating unanalyzed data. (Norvig 2017)\n\nPeter Norvig gave a detailed analysis and a rebuttal in (Norvig 2017).\n\n\nThe Chomskyans\nGold’s theorem about language learning in the limit is occasionally cited in the same context as a justification for the “poverty of stimulus” argument. It appears Chomsky did not regard it as a relevant argument (Johnson 2004), and I agree with Chomsky in this respect, as Gold’s theorem is extremely generic.\nAfter the second rise of neural networks, there was a bitter controversy that raged in the 1990s but is now essentially forgotten: the past tense debate. On one side were the connectionists, and on the other were the cognitivists, including Steven Pinker and Gary Marcus (Pinker and Ullman 2002). Tellingly, both Steven Pinker and Gary Marcus sided with the cognitivists. Steven Pinker is best known for his other books such as The Blank Slate, which applies Chomskyan linguistics to general psychology.\nHuman language exhibits a distinctly fractal-like structure: for every rule, there are exceptions, and for every exception, there are exceptional exceptions, and so on. This is called “quasi-regularity”. Sejnowski in an interview described how quasi-regularity shows up in phonology, and offered a perfect demonstration project for the power of neural networks against the Chomskyans:\n\nI went to the library and found a book on text-to-speech that had probably one thousand rules in it. It was just lists of rules and exceptions. After I went to the library and found a book on text-to-speech that had probably one thousand rules in it. It was just lists of rules and exceptions. After every rule, there were dozens of exceptions. I figured that either they’re doing it the wrong way, or this is a really tough problem. We considered finding something that was simpler after consulting the experts in linguistics. They said, “Chomsky worked on the problem, and it is well beyond the capability of anything that you could imagine trying to do.” (Rosenfeld and Anderson 2000, 324–25)\n\n(Sejnowski 2018, 75–78) recounts an anecdote about Jerry Fodor, another prominent cognitivist. While Fodor is no longer a hot name in AI nowadays, this anecdote illustrates the Chomsky way of thinking.\n\nIn 1988, I served on a committee for the McDonnell and Pew Foundations that interviewed prominent cognitive scientists and neuroscientists to get their recommendations on how to jump-start a new field called “cognitive neuroscience”. … [Fodor] started by throwing down the gauntlet, “Cognitive neuroscience is not a science and it never will be.” … Fodor explained why the mind had to be thought of as a modular symbol-processing system running an intelligent computer program. [Patricia Churchland] asked him whether his theory also applied to cats. “Yes,” said Fodor, “cats are running the cat program.” But when Mortimer Mishkin, an NIH neuroscientist studying vision and memory, asked him to tell us about discoveries made in his own lab, Fodor mumbled something I couldn’t follow about using event-related potentials in a language experiment. Mercifully, at that moment, a fire drill was called and we all filed outside. Standing in the courtyard, I overheard Mishkin say to Fodor: “Those are pretty small potatoes.” When the drill was over, Fodor had disappeared.\n\nSimilarly, Gary Marcus has consistently criticized neural network language models since at least 1992 (G. F. Marcus et al. 1992). His theory of intelligence is fundamentally Chomskyan: neural networks can exhibit intelligence but only if they implement rules for symbolic manipulation.23 Moreover, many symbolic rules must be present at birth, by the poverty of stimulus.\n23 This sketch suffices. (G. F. Marcus 2003) is a book-length treatment.For example, here is him saying in 1993:\n\nWhether children require “negative evidence” (i.e., information about which strings of words are not grammatical sentences) to eliminate their ungrammatical utterances is a central question in language acquisition because, lacking negative evidence, a child would require internal mechanisms to unlearn grammatical errors. … There is no evidence that noisy feedback is required for language learning, or even that noisy feedback exists. Thus internal mechanisms are necessary to account for the unlearning of ungrammatical utterances. (G. F. Marcus 1993)\n\nAnd here is him writing in 2018, just in time to miss the Transformer revolution in natural language processing:\n\nHuman beings can learn abstract relationships in a few trials. If I told you that a schmister was a sister over the age of 10 but under the age of 21, perhaps giving you a single example, you could immediately infer whether you had any schmisters, whether your best friend had a schmister, whether your children or parents had any schmisters, and so forth. (Odds are, your parents no longer do, if they ever did, and you could rapidly draw that inference, too.) In learning what a schmister is, in this case through explicit definition, you rely not on hundreds or thousands or millions of training examples, but on a capacity to represent abstract relationships between algebra-like variables. Humans can learn such abstractions, both through explicit definition and more implicit means (Marcus, 2001). Indeed even 7-month old infants can do so, acquiring learned abstract language-like rules from a small number of unlabeled examples, in just two minutes. (G. Marcus 2018)\n\nNot one to give up, he continued with the same criticisms into the age of Transformer language models. Given the track record, he is conveniently predictable24, and we can expect nothing less than see his recent criticisms of deep learning (G. Marcus 2018) and large language models, repeatedly.\n\n\n\n\n\n24 It would be funny if someone could train a language model to pass the “Gary Marcus test”: impersonate Gary Marcus in a Turing test setup. If such a model were to pass, Marcus would either have to admit that the language model makes sense or accept that what he says is indistinguishable from nonsense."
  },
  {
    "objectID": "essays/posts/reading-perceptron-book/index.html",
    "href": "essays/posts/reading-perceptron-book/index.html",
    "title": "Reading Perceptrons",
    "section": "",
    "text": "It would seem that Perceptrons has much the same role as The Necronomicon – that is, often cited but never read.\nMarvin Minsky, 1994. Quoted in (Berkeley 1997)\nIn one sentence, the mathematical portion of Perceptrons is a theory of two-layered perceptrons, mostly by methods typical of discrete mathematics and computational complexity theory, and no empirical results. One should forget about the modern theory and practice of neural networks, read the book in the same frame of mind as one would read a textbook on Turing machines, stack machines, Post tag systems, and other various theoretical objects in computational complexity theory. Indeed, this book is written in the same spirit and style as (Minsky 1967) and (Sipser 2006).\nPerceptron representation occupies chapters 0–10, and learning is only studied in chapters 11 and 12. The book also contained a chapter 13, but it contains mostly interpretations and brief “lessons” that they wanted the readers to take away from the exact mathematical results. They added some handwritten corrections and updates for a 1972 printing run.\nDuring the 1980s rise of connectionism, Minsky and Papert came out with a new edition in 1988. This is the same as the 1972 printing, except that they added a prologue and an epilogue, where they expounded at length the intended lesson of the book, as they felt that people have failed to learn it, and history was repeating itself.\nFor those interpretations, see The Perceptron Controversy."
  },
  {
    "objectID": "essays/posts/reading-perceptron-book/index.html#setup",
    "href": "essays/posts/reading-perceptron-book/index.html#setup",
    "title": "Reading Perceptrons",
    "section": "Setup",
    "text": "Setup\nLet \\(R\\) be a finite set, where “R” can be read as “region” or “rectangle”.\n\nDefinition 1 (Perceptron) A perceptron is a binary function of type \\(\\{0, 1\\}^R \\to \\{0, 1\\}\\), defined by a weight vector \\(w\\) and a threshold number \\(b\\):\n\\[\n\\Phi(x) := \\theta(w^T x + b)\n\\]\nwhere \\(\\theta(t) := 1_{t \\geq 0}\\) is the 0-1 step function.\n\n\nDefinition 2 (perceptron machine) A perceptron machine with \\(k\\) hidden neurons is a function of type \\(\\{0, 1\\}^R \\to \\{0, 1\\}\\), defined by\n\\[\n\\Phi(x) := \\psi_{k+1}(\\psi_1(x), \\psi_2(x), \\dots , \\psi_k(x))\n\\]\nwhere \\(\\psi_1, \\dots, \\psi_k\\) are (hidden) perceptrons in the hidden layer, and \\(\\psi_{k+1}\\) is the single output perceptron.\n\n\nDefinition 3 (perceptron orders) The order of a hidden perceptron is the number of nonzero weights.\nThe order of a perceptron machine is the maximum order of its hidden perceptrons.\nThe order of a boolean function is the minimum order necessary for a perceptron machine that implements it.\n\nFor example, the constant-0 and constant-1 boolean functions are both of order 0.\nA key focus of the perceptron controversy is the concept of being “conjunctively local”.\n\nDefinition 4 (conjunctively local) A family of boolean functions is conjunctively local iff their orders are upper bounded."
  },
  {
    "objectID": "essays/posts/reading-perceptron-book/index.html#over-finite-sets",
    "href": "essays/posts/reading-perceptron-book/index.html#over-finite-sets",
    "title": "Reading Perceptrons",
    "section": "Over finite sets",
    "text": "Over finite sets\nWhile their definition allows perceptron machines of any real-valued weights and biases, they immediately performed a computational reduction to the special case of mask perceptron machines. Most of the book considers the problem of classifying subsets of families of planar square grids, such as \\(\\{1, 2, 3\\} \\times \\{1, 2, 3, 4\\}\\). A subset is inputted to the perceptron machine by setting inputs in the subset to \\(1\\), and the rest to \\(0\\). Consequently, it is natural to consider this special case of perceptron machines.\nWhile their definition allows perceptron machines of any real-valued weights and biases, they immediately performed a computational reduction to the special case of the mask perceptron machine. Most of the book considers the problem of classifying subsets of families of planar square grids, such as \\(\\{1, 2, 3\\} \\times \\{1, 2, 3, 4\\}\\). A subset is input to the perceptron machine by setting inputs in the subset to \\(1\\), and the rest to \\(0\\). Consequently, it is natural to consider this special case of perceptron machines.\n\nDefinition 5 (mask perceptron machine) A mask for \\(A\\subset R\\) is a function of type \\(\\{0, 1\\}^R \\to \\mathbb{R}\\), such that \\(\\psi(x) = 1\\) if \\(x_i = 1\\) for all \\(i \\in A\\), and otherwise \\(\\psi(x) = 0\\).\nA mask perceptron machine is a perceptron machine where each hidden perceptron is a mask, and the threshold of the output perceptron is zero. In other words, it is of form\n\\[\n\\Phi(x) = \\theta\\left(\\sum_{i=1}^k a_i \\psi_{A_i}(x)\\right)\n\\]\nwhere each \\(\\psi_{A_i}\\) is a mask, each \\(a_i\\in \\mathbb{R}\\), and \\(k\\) is the number of hidden perceptrons.\n\n\n\nFigure 0.2\n\n\n\n\nTheorem 1 (Theorem 1.5.1) Restricting to mask perceptron machines does not change perceptron orders. That is, any boolean function implemented by a perceptron machine of order \\(k\\) can be implemented by a mask perceptron machine of order at most \\(k\\).\n\n\nProof. Take one such perceptron machine. The threshold of the output perceptron can be removed by adding a hidden perceptron that always outputs \\(1\\) – in other words, \\(\\psi_\\emptyset\\), the mask of the empty set. Now it remains to decompose each hidden perceptron into masks with the same order, or less.\nLet \\(\\psi\\) be a hidden perceptron with nonzero weights on the input points \\(x_{i_1}, \\dots, x_{i_k}\\); then, its output is determined by the values of \\(x_{i_1}, \\dots, x_{i_k}\\). Therefore, we can partition the binary set \\(\\{0, 1\\}^{i_1, \\dots, i_k}\\) into two subsets \\(A_0, A_1\\), such that for any input \\(x\\in\\{0, 1\\}^R\\), we have \\(\\psi(x) = 1\\) iff \\((x_{i_1}, \\dots, x_{i_k}) \\in A_1\\).\nIn other words, we only need to look at the binary values \\(x_{i_1}, \\dots, x_{i_k}\\) to determine the binary output \\(\\psi(x)\\).\nTherefore, we can replace \\(\\psi\\) with a boolean formula on \\(x_{i_1}, \\dots, x_{i_k}\\), then expand it to obtain up to \\(2^k\\) masks, each of order at most \\(k\\).\nFor example, suppose \\(\\psi\\) has nonzero weights on \\(x_1, x_2\\), and is 1 on all odd-sized subsets, then we can write it as a boolean formula:\n\\[\n\\left(x_1 \\wedge \\neg x_2\\right) \\vee\\left(\\neg x_1 \\wedge x_2\\right) = x_1\\left(1-x_2\\right)+\\left(1-x_1\\right) x_2 = x_1 + x_2 - 2 x_1 x_2\n\\]\n\nThe next tool they used is symmetry, formulated in the language of finite group actions.\nLet \\(S_R\\) be the permutation group on the elements of \\(R\\), and \\(G\\) be a subgroup of \\(S_R\\). We say that a boolean function \\(\\psi\\) is \\(G\\)-invariant iff \\(\\psi \\circ g=\\psi\\) for any \\(g \\in G\\). That is, for any \\(X \\subset R\\), we have \\(\\psi(X) = \\psi(g(X))\\). For example, the parity function is \\(S_R\\)-invariant, since any permutation of the set preserves the size, and thus the parity, of any of its subsets.\n\nTheorem 2 (group invariance theorem) If a boolean function is \\(G\\)-invariant, where \\(G\\) is a finite group, then any perceptron machine computing it can be converted to a perceptron machine \\(\\theta(\\sum_i a_i \\psi_i)\\), such that if \\(\\psi_i = \\psi_j \\circ g\\) for some \\(g \\in G\\), then \\(a_i = a_j\\).\n\n\nProof. Take the group-action average: any mask \\(\\psi\\) is equal to \\(\\frac{1}{|G|} \\sum_{g\\in G} \\psi\\circ g\\).\n\nOnce the groundwork was laid, they proceeded to prove a wide variety of theorems on the order of particular boolean functions.\nConsider the parity function. It is \\(1\\) iff exactly an odd number of inputs are \\(1\\) and the rest are \\(0\\).\n\nTheorem 3 (Theorem 3.1) The parity function has order \\(|R|\\).\n\n\nProof. Since the parity function is \\(S_R\\)-invariant, if it is implemented by a perceptron machine of order \\(k\\), it is implemented by some mask perceptron machine \\(\\theta(\\sum_{A_i \\subset R} a_{A_i} \\psi_{A_i})\\), where each mask is of size \\(\\leq k\\), and each weight \\(a_{A_i}\\) depends only on the size of \\(A_i\\). Let \\(b_{|A_i|} = a_{A_i}\\) be those coefficients. It remains to show \\(b_{|R|} \\neq 0\\).\nFor each \\(X \\subset R\\), we have by explicit computation\n\\[\n\\theta\\left(\\sum_{A_i \\subset R} a_{A_i} \\psi_{A_i}\\right) = 1\\left[f(|X|) \\geq 0\\right]\n\\]\nwhere \\(f(t) := \\sum_{i=0}^{|R|}b_i \\binom{t}{i}\\) is a polynomial in \\(t\\). Since this perceptron machine implements the parity function, as \\(t\\) increases from \\(0\\) to \\(|R|\\), the function \\(f(t) + \\epsilon\\) must intersect the \\(x\\)-axis at least \\(|R|\\) times for some real value \\(\\epsilon\\). Since \\(f\\) is a polynomial, it must have at least order \\(|R|\\), thus \\(b_{|R|} \\neq 0\\).\n\n\nTheorem 4 (Theorem 3.2, one-in-a-box) Let \\(A_1, A_2, \\dots, A_m\\) be disjoint subsets of \\(R\\), each of size \\(4 m^2\\), and define the predicate \\(\\psi(X) = \\forall i, \\left|X \\cap A_i\\right|&gt;0\\); that is, there is at least one point of \\(X\\) in each \\(A_i\\). The order of \\(\\psi\\) is \\(\\geq m\\).\n\n\nProof. Let the order of \\(\\psi\\) be \\(k\\).\nThe predicate \\(\\psi\\) is invariant with respect to the group \\(S_{A_1} \\times \\cdots \\times S_{A_m}\\), so by the same construction as the proof of Theorem 2, there exists a polynomial \\(P(t_1, \\dots, t_m)\\), where \\(P\\) has order \\(k\\), and\n\\[\n\\forall t_1, \\dots , t_m \\in \\{0, 1, \\dots , 4m^2\\}, P(t_1, \\dots , t_m) &lt; 0 \\iff t_1 = 0 \\vee \\cdots \\vee t_m = 0\n\\]\nNow define \\(Q(t) := P((t-1)^2, (t-3)^2, \\dots, (t-2m+1)^2)\\). By the above equation, \\(Q &lt; 0\\) at \\(t=1, 3, \\dots, 2m - 1\\) and \\(Q \\geq 0\\) at \\(t = 0, 2, \\dots, 2m\\). Thus, \\(Q\\) has order \\(\\geq 2m\\). Thus, \\(2k \\geq 2m\\).\n\n\nTheorem 5 (Theorem 4.0) There exist predicates \\(\\psi_1\\) and \\(\\psi_2\\) of order 1 such that \\(\\psi_1 \\wedge \\psi_2\\) and \\(\\psi_1 \\vee \\psi_2\\) are not of finite order. Specifically, if we partition \\(R\\) into three equal subsets \\(A, B, C\\), then the boolean function does not have bounded order:\n\\[\n(|X \\cap A| &gt; |X \\cap C|) \\wedge (|X \\cap B| &gt; |X \\cap C|)\n\\]\neven though both \\(|X \\cap A| &gt; |X \\cap C|\\) and \\(|X \\cap B| &gt; |X \\cap C|\\) are of order \\(1\\).\n\n\nProof. \\(|X \\cap A| &gt; |X \\cap C|\\) is computed by the order-\\(1\\) perceptron machine \\(\\theta\\left(\\sum_{i\\in A} x_i - \\sum_{i \\in C}x_i\\right)\\), and similarly for the other one.\nTo show the composite boolean function is not of bounded order, suppose otherwise, then by the same argument as the previous proof, we can construct a sequence of polynomials \\(P_1(a, b, c), P_2(a, b, c), P_3(a, b, c), \\dots\\), such that each \\(P_n\\) is the polynomial corresponding to the perceptron machine for the case where \\(|A| = |B| = |C| = n\\), and each of them has order at most \\(M\\), for some fixed \\(M\\).\nBeing the polynomial corresponding to the perceptron machine for the case where \\(|A| = |B| = |C| = n\\) means precisely that\n\\[\na &gt; c \\wedge b &gt; c \\implies P_n(a, b, c) \\geq 0; \\neg (a &gt; c \\wedge b &gt; c) \\implies P_n(a, b, c) &lt; 0;\n\\]\nfor all \\(a, b, c \\in \\{0, 1, \\dots, n\\}\\). This implies that each \\(P_1, P_2, \\dots \\neq 0\\). Now, divide each polynomial by the root-sum-square of its coefficients, so that if we interpret the coefficients of each \\(P_n\\) as a point in a high-dimensional space, the point falls on the unit sphere in that space. Since the unit sphere is compact, there exists a limit point, which we write as \\(P(a, b, c)\\).\nBy the limit construction, we have\n\\[\n\\forall a, b, c \\in \\mathbb{N}, \\quad a &gt; c \\wedge b &gt; c \\implies P(a, b, c) \\geq 0; \\neg (a &gt; c \\wedge b &gt; c) \\implies P(a, b, c) \\leq 0;\n\\]\nIf we color the points \\(\\mathbb{N}^3\\) with black for \\(P &lt; 0\\) and white for \\(P \\geq 0\\), then we can see the dusty outlines of a pyramid. We can construct a solid pyramid by zooming.\nLet \\(M'\\) be the order of \\(P\\), then we can “zoom out” by taking the projective limit \\(Q(a, b, c) := \\lim_{t \\to \\infty} t^{-M'} P(ta, tb, tc)\\). This \\(Q\\) is a homogeneous polynomial, and by continuity,\n\\[\n\\forall a, b, c \\geq 0, \\quad a &gt; c \\wedge b &gt; c \\implies P(a, b, c) \\geq 0; a &lt; c \\vee b &lt; c \\implies P(a, b, c) \\leq 0;\n\\]\nThis implies that \\(P\\) is identically zero on the “creased curve” \\(\\{ a, b, c \\geq 0, a = c \\vee b = c\\}\\) in the projective plane, which is not an algebraic curve, therefore it is identically zero, contradiction.\n\n\nTheorem 6 (Theorem 5.1) The connectedness function has order \\(\\Omega(|R|^{1/3})\\).\n\n\nProof. If we have a mask perceptron machine that can solve the connectedness problem, then it can be repurposed2 to solve the one-in-a-box problem of the following kind:2 Repurposing one machine to solve another problem is a common trick in computational complexity, called “reduction”. For perceptron machines, they called it “Theorem 5.4.1: The Collapsing Theorem”.\n\n\n\nFigure 5.2\n\n\nIn the picture, the rectangle \\(R\\) has width \\(4m^2\\) and height \\(2m+1\\). We fill in all the odd-numbered rows, and use the machine to solve the one-in-a-box problem for the even-numbered rows. By Theorem 4, the machine has order \\(\\geq m = \\Omega(|R|^{1/3})\\).\n\nIn fact, it turns out that essentially the only locally conjunctive topological invariant is the Euler number.\n\nTheorem 7 (Theorem 5.8.1) The Euler number itself is \\(E(X) = \\sum_{i \\in R} x_i - \\sum_{i, j \\in R} x_ix_j + \\sum_{i, j, k, l \\in R} x_ix_jx_kx_l\\), where the \\(i, j\\in R\\) ranges only over adjacent points, and \\(i, j, k, l \\in R\\) ranges only over quadruples that form a square. Thus the Euler number itself has order \\(4\\).\n\n\nTheorem 8 (Theorem 5.9) If \\(\\psi\\) is a boolean function, and it depends only on the topology of the input figure, and its order is upper-bounded by some number \\(k\\) that does not grow even as \\(R\\) grows into a larger and larger rectangle, then \\(\\psi\\) is of form \\(f \\circ E\\), for some function \\(f: \\mathbb{N}\\to 2\\)."
  },
  {
    "objectID": "essays/posts/reading-perceptron-book/index.html#over-infinite-spaces",
    "href": "essays/posts/reading-perceptron-book/index.html#over-infinite-spaces",
    "title": "Reading Perceptrons",
    "section": "Over infinite spaces",
    "text": "Over infinite spaces\nChapters 6–9 continue in the same style, but move to the case where the input space is made of one or two copies of the infinite line \\(\\mathbb{Z}\\), or the infinite plane \\(\\mathbb{Z}^2\\), and the predicate to recognize is still translation-invariant. In order to avoid blowing up the perceptron machine with infinities, they restricted to the case where the input figure is of finite size, meaning that \\(\\sum_i x_i\\) is finite.\nChapter 6 develops the idea of “spectra” of images. For example, the following picture shows that if we were to design a perceptron machine using only masks of size up to 2, and translation-invariant weights, then it cannot possibly distinguish between the two tetris-like pieces, because both figures contain exactly 4 instances of single-square, 1 instance of two squares side by side, and so on.\n\n\n\npage 100\n\n\nSections 6.1–6.5 of the chapter studies the same idea for masks of size up to 4. For example, Section 6.4 shows that “\\(X\\) is the perimeter of a complete circle” is of order \\(4\\).\nSection 6.6 claims that “recognizing figures in context” is generally not locally conjunctive, although they gave only two examples. Specifically, they showed that the predicate “\\(X\\) is a horizontal line across the rectangle” is order 2, the predicate “\\(X\\) contains one horizontal line across the plane” is not locally conjunctive. The same is true for the case with “a hollow square” instead of “a horizontal line”.\nChapter 7 uses the “stratification” construction to explicitly demonstrate that several binocular predicates are of finite order, but requires exponentially growing weights. It is essentially the same idea as Gödel numbering. A single illustrative example suffices to demonstrate the general point.\n\nExample 1 Given a line \\(\\mathbb{Z}\\), how to construct a perceptron machine that detects that input figure is symmetric?\nSuppose we know for certain that the input figure has leftmost point \\(m\\) and rightmost point \\(n\\), then we can test for symmetry by computing the value of:\n\\[\nf_{m, n}(x) := \\sum_{i = 0}^{n-m} x_{m+i}(x_{n-i} - 1)\n\\]\nWe have that \\(f_{m, n}(x) = 0\\) if the figure is symmetric, and \\(f_{m, n}(x) \\leq -1\\) otherwise.\nNow we define the entire perceptron machine by \\(\\sum_{m \\leq n} M_{n - m} x_m x_n (f_{m, n}(x) - 1/2)\\). If the sequence of \\(M_0, M_1, \\dots\\) grows as \\((d!)^2\\) roughly 3, then the largest bracket \\((m, n)\\) would “veto” every smaller bracket contained within, and so the entire perceptron machine is effectively first testing for the smallest interval containing the figure, before testing the symmetry within that interval.3 Expanding term by term, we have \\(|f_{m, n}(x) - 1/2| \\leq 2(n-m) + \\frac{1}{2}\\). Therefore, in order for \\(M_d\\) to “veto” every other bracket within, we need\n\\[\nM_d \\frac 12 &gt; \\sum_{d' = 1}^{d-1} \\left(M_{d'}(\\frac 12 + 2d')(d-d' + 1)\\right)\n\\]\nHere \\(d\\) should be read as “distance between two ends of a bracket”.\nTo bound the growth rate, we bound the recurrence relation \\(M_d = \\sum_{d' = 1}^{d-1} \\left(M_{d'}(4d' + 1)(d-d' + 1)\\right)\\). The sum on the right is bounded by\n\\[\n\\sum_{d' = 1}^{d-1} \\left(M_{d'}(4d' + 1)(d-d' + 1)\\right) \\in \\Theta{\\left[\n    \\sum_{d' = 1}^{d-1} \\left(M_{d'}d'\\right),\n    d^2\\sum_{d' = 1}^{d-1} \\left(M_{d'}\\right)\\right]}\n\\]\nThe lower bound implies \\(M_d = \\Omega((d!)^2 \\times d^{-1})\\) and the upper bound implies \\(M_d = O((d!)^2 \\times (d+1)^2)\\).\n\nThey made similar constructions for perceptron machines that decide whether two infinite lines or planes are translates of each other, whether two planes are translation and approximate dilations of each other, and whether a line or a plane is the translation of a fixed figure.\nChapter 8 studies diameter-limited perceptron machines, meaning that not only are the hidden perceptrons assumed to be masks of bounded size, those masks are assumed to be contained in a circle of radius \\(M\\) for some finite \\(M\\). It is intended to be a formalization of the intuition that the perceptron machines should model a real pair of eyes scanning a plane, and each retina is a circular disk with finite radius. No results in chapter 8 were used further in the book, and they resemble the case of finite-order perceptron machines, so we do not discuss the results in detail.\nChapter 9 shows that the connectedness problem is easy to solve using small serial programs, a contrast to the case of perceptron machines. The overarching lesson is that connectedness is an “inherently serial” decision problem. The whole chapter is beautiful computational complexity theory, in the same style of solving fun mathematical puzzles, similar to (Minsky 1967).\nThey designed a robot (a finite state machine) that moves around the plane, and can solve the connectedness problem, with a memory size of just two. Namely, it only needs to store up to two locations \\((x, y), (x', y')\\) in its memory during its operation, and it eventually halts in one of three states “empty”, “connected”, and “disconnected”. Notice that a robot with no memory can still remember a finite number of states. It is simply that those memory slots are its finite states, which do not scale with the size of the problem. The little robot with a “memory size of two” really has a memory size of \\(2 \\log_2|R|\\) bits, because it can remember two coordinates from the square \\(R\\), no matter how large the square grows.\nThey then described a few other more exotic computational models, such as a “pebble machine”, meaning a robot that has no memory, but provided with a single pebble. It can drop the pebble anywhere, and it can pick it up again. One can think of this as an intermediate level between pure finite state machines, which cannot write at all, and a Turing machine, which can write as much as it wants. They left as an exercise for the reader the task of translating the previous robot with two memory slots to this robot with one pebble. They conjectured that a finite state machine (a robot with no pebbles) would be unable to solve the task, but they could not prove it. I did a brief literature search and it seemed to be still unsolved."
  },
  {
    "objectID": "essays/posts/reading-perceptron-book/index.html#learning-theory",
    "href": "essays/posts/reading-perceptron-book/index.html#learning-theory",
    "title": "Reading Perceptrons",
    "section": "Learning theory",
    "text": "Learning theory\nThey claimed that Chapter 10 is part of the learning theory. However, it does not actually involve learning. Whereas in the construction Example 1, we saw coefficients growing exponentially on an infinite plane, chapter 10 proves similar results on a finite plane.\n\nExample 2 (Theorem 10.1) Suppose we have a perceptron machine that tests for parity; then, by Theorem 3, it must have order \\(|R|\\). As in the construction given in the proof, we use the group invariance theorem to obtain a group-symmetric machine with the form \\(\\theta(\\sum_{i=0}^{|R|}\\binom{|X|}{i} b_i)\\), where \\(b_0, b_1, \\dots, b_{|R|}\\) are real numbers. Then, assuming the machine is “reliable”, we can prove that \\((-1)^{M} b_{M+1} \\geq 2^{M}\\) for any \\(M \\in \\{0, 1, \\dots, |R|-1\\}\\).\nSince the group-symmetric construction can only average out the most extreme values, this implies that, before the group-symmetric construction, our perceptron machine had even more extreme coefficients.\nA “reliable” machine is a support vector machine with margin \\(\\geq 1/2\\). That is, it is a machine such that \\[\n\\sum_{i=0}^{|R|}\\binom{|X|}{i} b_i \\begin{cases}\n\\geq 1 & \\text{ if $|X|$ is odd}\n\\leq 0 & \\text{ if $|X|$ is even}\n\\end{cases}\n\\]\n\n\nProof. Define \\(A_n = \\sum_{i=0}^n \\binom{n}{i}b_i\\). Since the machine is reliable, we have that \\((-1)^{n}(A_{n+1} - A_n) \\geq 1\\) for each \\(n = 0, 1, \\dots, |R|-1\\). Simplifying the binomial coefficients, we have \\(A_{n+1} - A_n = \\sum_i \\binom{n}{i} b_{i+1}\\). Note that we use the convenient convention that \\(\\binom{x}{y} = 0\\) if \\(x &lt; y\\).\nNow fix any \\(M \\in \\{0, 1, \\dots, |R|-1\\}\\), and evaluate the following inequality:\n\\[\n2^{M} = \\sum_n \\binom{M}{n} \\cdot 1 \\leq \\sum_n \\binom{M}{n} (-1)^{n}(A_{n+1} - A_n)\n\\]\nBy manipulating the binomial coefficients, the right side simplifies to \\((-1)^M b_{M+1}\\).\n\nSections 10.2 and 10.3 construct two pathological examples. By restricting the shapes of hidden perceptron masks, they proved that certain predicates required superexponential weights. Section 10.4 purports to extend the group invariance theorem to the infinite case.4 None of these are used elsewhere in the book. They are virtuoso performances. However, they interpreted this as a serious problem:4 They purported to show in Theorem 10.4.1 that, if the weights are bounded, then the group invariance theorem applies again, but the proof is false, with a counterexample being \\(\\sum_{n \\in \\mathbb{Z}}e^{-n^2}x_n - e^{-n^4}x_nx_{n+1}\\). The theorem might still be correct with another proof, but I cannot find one.\n\nA proof, in Chapter 10, that coefficients can grow much faster than exponentially with \\(|R|\\) has serious consequences both practically and conceptually: the use of more memory capacity to store the coefficients than to list all the figures strains the idea that the machine is making some kind of abstraction\n\nIn Chapter 11, they finally began discussing perceptron learning, which is of a very restrictive form.\n\nDefinition 6 (Perceptron learning) To train a perceptron machine is to fix its hidden perceptrons, and adjust the weights and threshold of only the single output perceptron by the perceptron learning rule. For this chapter, it is cleaner to change the convention so that each perceptron outputs \\(-1, +1\\) instead of \\(0, 1\\).\nSince only the output perceptron is adapted, it suffices to discuss the case where there are no hidden perceptrons, and we only need to adapt the weights of a single perceptron. That is, we have a dataset \\(D\\), and we sample some \\((x, y) \\in D\\), and verify that \\(y = \\theta(\\braket{w, x})\\).\nIf this is true for all \\((x, y) \\in D\\), then the perceptron learning has converged. Otherwise, we update \\(w\\) using \\(w \\leftarrow w + \\alpha y x\\), where \\(\\alpha &gt; 0\\) is the learning rate.\n\n\nDefinition 7 (Perceptron learning theorem) Let \\(D\\) be a dataset with radius \\(R = \\max_{(x, y) \\in D} \\|x\\|\\). If there exists some unit \\(w^*\\) such that \\(\\gamma = \\min_{(x, y) \\in D} y\\braket{w^*, x}\\), then the perceptron learning algorithm converges after making at most \\((R/\\gamma)^2\\) updates.\n\n\nProof. By linearity of the learning rule, we can deal only with the case where \\(\\alpha = 1\\).\nBy multiplying each \\(x\\) with its \\(y\\), we can deal only with the case where all \\(y = +1\\).\nBy rotating and scaling the space, we can deal only with the case where \\(w^* = (1, 0, \\dots, 0)\\), and \\(\\gamma = 1\\).\nNow, each weight update increases the first coordinate of \\(w\\) by at least \\(1\\), so after \\(n\\) updates, \\(\\|w\\| \\geq n\\). However, each weight update of \\(w \\leftarrow w + x\\) uses a vector \\(x\\) that is pointing in a direction perpendicular to \\(w\\), or worse, pointing against \\(w\\). Therefore, by Pythagorean theorem, \\(\\|w\\|^2\\) increases by at most \\(\\|x\\|^2 \\leq R^2\\). So after \\(n\\) updates, \\(\\|w\\|^2 \\leq nR^2\\).\nCombining the two results, we have \\(n \\leq R^2\\).\n\nModifying the proof slightly, and applying the conclusion of Example 2, we find that starting with the zero weight vector, it takes at least \\(2^{|R|}/|R|\\) steps to learn the parity function.\nThey then suggested that, since gradient descent is “just” a more efficient perceptron learning rule, it also cannot escape local optima. No “local learning rule” can escape local optima, unlike symbolic programs that are provably capable of finding global optima.\nIf the dataset is not linearly separable, then the perceptron weights will not converge. However, the perceptron cycling theorem shows that if the dataset is finite, then the perceptron weights will still be trapped within a large but finite disk, no matter how the dataset is sampled.\nChapter 12 is not very mathematical, and consists mainly of quick sketches5 of other algorithms for learning. Those included: lookup table, nearest neighbor, k-means, ISODATA, maximum likelihood, Bayes, naive Bayes, etc. Sections 12.6 and 12.7 study variations on a toy problem: given a subset of \\(\\{0, 1\\}^n\\), decide whether an \\(n\\)-bit word is in it or not. This had relevance to the time-space tradeoff, a perennial topic in computational complexity.5 \nIn this chapter we will study a few of these to indicate points of contact with the perceptron and to reveal deep differences. … The chapter is written more in the spirit of inciting students to research than of offering solutions to problems."
  },
  {
    "objectID": "essays/posts/reading-perceptron-book/index.html#chapter-13",
    "href": "essays/posts/reading-perceptron-book/index.html#chapter-13",
    "title": "Reading Perceptrons",
    "section": "Chapter 13",
    "text": "Chapter 13\nChapter 13 is a summary of the “take-home lessons” for the readers. As the intended lessons were expounded in great length in the epilogue added in 1988, I would not analyze the chapter in detail.\nThey discussed “Gamba perceptrons”, which is just another name for two-layered perceptrons where the hidden layer consists of perceptrons, instead of merely masks. They made the infamous prediction that not only Gamba perceptrons but also arbitrary multilayer perceptrons would be a “sterile extension”.\n\nWell, we have considered Gamba machines, which could be described as “two layers of perceptron.” We have not found (by thinking or by studying the literature) any other really interesting class of multilayered machine, at least none whose principles seem to have a significant relation to those of the perceptron. To see the force of this qualification it is worth pondering the fact, trivial in itself, that a universal computer could be built entirely out of linear threshold modules. This does not in any sense reduce the theory of computation and programming to the theory of perceptrons. Some philosophers might like to express the relevant general principle by saying that the computer is so much more than the sum of its parts that the computer scientist can afford to ignore the nature of the components and consider only their connectivity. More concretely, we would call the student’s attention to the following considerations:\n\nMultilayer machines with loops clearly open all the questions of the general theory of automata.\nA system with no loops but with an order restriction at each layer can compute only predicates of finite order.\nOn the other hand, if there is no restriction except for the absence of loops, the monster of vacuous generality once more raises its head.\n\nThe perceptron has shown itself worthy of study despite (and even because of!) its severe limitations. It has many features to attract attention: its linearity; its intriguing learning theorem; its clear paradigmatic simplicity as a kind of parallel computation. There is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgment that the extension is sterile.\n\nIn short, their objection to multilayer perceptrons is that they suffer from Turing completeness, making it hard to prove anything concrete about them, except the trivial claim that they are Turing complete.6 A single perceptron is just a linear classifier, so it is possible to study mathematically. Experimental evidence is no justification, because:6 \nBeware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n\n\n\n13.5 Why Prove Theorems?\nWhy did you prove all these complicated theorems? Couldn’t you just take a perceptron and see if it can recognize \\(\\psi_{\\text {CONNECTED}}\\)?\nNo.\n\n\n\n\npage 239\n\n\nSince a perceptron-only architecture is not the right way, they illustrated what they believe to be the “right” way to do computer vision by describing in detail the scene analysis algorithm. In short, it starts by discovering edges, then performs some computational geometry on them to recover outlines of basic shapes, such as cubes, then fills in the faces, then fills in the bodies between the faces. The algorithm had to do something clever to deal with occlusions. In the end, a complete 3D scene populated with 3D objects is recovered.\n\n\n\nScene analysis. Figure from (Guzmán 1968)\n\n\nThey ended the book with a brief discussion of how they discovered the various results, as well as a list of people they thanked."
  },
  {
    "objectID": "essays/posts/serial-experiments-lain/index.html#my-summary",
    "href": "essays/posts/serial-experiments-lain/index.html#my-summary",
    "title": "Serial Experiments Lain",
    "section": "My summary",
    "text": "My summary\nIt seems Lain is the collective subconscious, implemented as a neural network, with every human brain as a neuron, and “wired together” by some kind of unexplained electromagnetic coupling to the earth’s ionosphere.\nAs a suggestive evidence for this, the frequencies of the Schumann resonance (8 Hz, 14 Hz, 20 Hz, etc) happen to be close to the main brainwave frequencies. See the appendix Section 4.1 for some additional details about it. It is not actually relevant for the story.\nSome people drew up a plan to control Lain. The key scientist in the plan is Eiri, scientist of the Tachibana General Laboratories.\nFirst they migrated Lain from the brain-ionosphere system to the brain-Internet system, by an update to the Internet protocol (Protocol 7) that allows brain-computer interfacing.\nThen they gave Lain a human body. (unclear how that happened) This body gave Lain self-consciousness and a person-API.\nEiri suicided after Protocol 7 was running. Protocol 7 contained a copy of his brain state, so he was now running on the Internet, and called himself god. He has followers (Knights of the Eastern Calculus) who did his commands.\nThen they orchestrated a series of dramatic events to steer Lain’s development. The human body for Lain is used here, as these human-psychologically meaningful events can only work through a human-person-API (You can’t traumatize a non-human process, or Lain-without-body, by staging a bloody murder. Murders only mean something if it’s seen through animal eyes, the same way that a story can move you only if you speak the language.)\nEventually, Lain would be completely isolated in human society and be connected to the Internet. She would accept Eiri as god, kill her physical body, and exist on the Internet, where she would do Eiri’s commands.\nThe plan failed at the last step, because two people (the actor playing her father, and a school friend, Alice) stilled loved Lain, so she wasn’t isolated enough. After trying to make Alice happy and failing, Lain decided that the only way to truly make Alice happy is to go away and let Alice run her normal life-cycle (grow up, get married, die from being too old) without drama.\nTo let Alice live as a normal human, the entire plan must go away. So Lain discarded all changes and reverted to a previous state, before Lain was embodied. She deleted all memories of her from all humans. She also rewrote Eiri into an unambitious man so that he wouldn’t try doing that in this timeline.\nI don’t know how Lain could do that. Lain is the neural network with human brains, and has access only to the human brains, the ionosphere, and Internet. I don’t see any way for Lain to revert some physical deaths."
  },
  {
    "objectID": "essays/posts/serial-experiments-lain/index.html#ontology-the-modes-of-existence",
    "href": "essays/posts/serial-experiments-lain/index.html#ontology-the-modes-of-existence",
    "title": "Serial Experiments Lain",
    "section": "Ontology: The modes of existence",
    "text": "Ontology: The modes of existence\nThe ontology of SEL is a dualism between information and matter, the same theory as most neuroscientists and AI scientists use today.\nThere are many differences between matter and information, but here are the most important ones:\nDuplicate matters are different. Duplicate information are the same. If you receive two apples, you get twice as many atoms. If you receive two copies of the same file, you really receive just one file (in terms of information).\nMatter does not allow “isomorphic” operations. Information allows it. For example, you can convert a document from one encoding to another, edit it, then convert it back. The result is the same as if you edited it without conversion. This can’t be done with physical pen-and-paper.\nMatter exists in spacetime. Information does not. It is not a coincidence that many physicists are working on “reducing spacetime to information”, because information itself doesn’t have to exist in space or time, so if spacetime is reduced to information, then spacetime is explained by something that doesn’t assume spacetime – progress for reductionist science.\nThe persistence of matter is obvious. The persistence of information is unclear – perhaps impossible to define. You can take a gold bar, and put it in a box. Then you take it out, and you can say “it’s the same gold bar”. Now if you save some cash into a bank, then go abroad and take out some cash at the local ATM, you can’t say “it’s the same electronic cash”, or “it’s not the same”. Neither makes sense.\nIt’s also not a coincidence that physicists were first guided to thinking about “reducing spacetime to information” by the QED theory, where two electrons are actually indistinguishable, as if they are electronic cash. The QED theory was extended to atoms by the QCD theory. Thus, in some technical sense, you can’t actually say “we are made of star-stuff”.\n\nVirtual and real\nWe define “virtual” as “that pertaining to information”, and “real” as “that pertaining to matter”.\nInformation can be “realized” in matter – this is what modern computers do, and probably what brains do.\nSince information can be processed isomorphically, it can be realized as different kinds of matter, in different ways.\nA file is a realization of information, in space.\nA computation is a realization of information, in time.\n\n\nWired, Reality, and other systems\nAn information system is something I understand, but I can’t give a good definition (for now). I will explain it by examples.\nA physical system is a collection of matter organized under a common information system.\nFor example, the system composed of DNA, RNA, and proteins is a physical system, organized under a common information system (“the genetic code”).\nThe Internet (“the Wired”) is a system composed of electronic devices and human brains, organized under a common information system (“Protocol 7”).\n\n\n The Seven Levels of Wired\n\n\nThe real world (“Reality”), as commonly defined, is only part of the entire world. It denotes, in fact, a physical system composed of human brains, human bodies, other animal bodies, pre-1980s technological artifacts (specifically to exclude consumer-electronics), organized under a common information system (social rules for the person API - I will explain the API theory of personhood later).\nIn particular, a common interpretation, that “Lain chose the real world instead of the fake internet”, is a deep philosophical error. It is mistaking the human social system for the only system in the world. Furthermore, it calls the human social system “natural”, and the Internet “artificial”, when it is just as artificial as the Internet.\nThe “real world” of humans is like a curvy section across the bulk of reality. People would often tell other people to “get out of the room”, as if being inside a concrete container is unnatural while being outside of one is natural. If vultures could talk, they would tell others to “stop eating fresh food”, as if eating fresh food is unnatural while eating spoiled food is natural.\nMe, personally, have to get out of the room everyday and enjoy the sunshine, and I hate it. I often choose to walk school before the sun is up, to avoid its rays. I also despise green grass and blue skies. If there is a paradise, I hope it will be a 3D labyrinth embedded in an infinite concrete, but slightly elastic so that I can roll around on it without scraping my fur. It would be perfectly dark, except some fluorescent books, just bright enough to be read, and a little shining computer with which I take notes. I also use my computer to post my mathematical findings to others.\nMoving between worlds is possible, since the same information can have multiple realizations. Both Lain and Eiri managed to move between Reality and Wired.\n\n\nThe 4 realizations of Lain\nLain was always realized on neural networks, with nodes and edges, but the neural networks were made of different matters. There are 4 realizations over the course of history.\n1: Before the invention of Internet, nodes were human brains, and edges were something (perhaps flux-tubes?) in the electromagnetic field in the ionosphere of earth. It is called the “collective unconscious of humans”, and the Schumann resonances are analogous to the alpha-waves in mammalian brains.\n2: After the invention of the internet, but before Protocol 7, nodes were human brains and electronic computers, and edges were brain-computer interfaces (such as those VR headsets and cybernetic implants) and computer-computer links (what the Internet is made of).\n3: After Protocol 7, a large proportion of nodes and edges were concentrated into the neurons and synapses of a human girl. In this realization, Lain has a significant personhood.\n4: After Lain reset everything, it was unclear, but presumably it was back to realization 2.\n\n\nThe 4 realizations of Eiri\nLike Lain, Eiri was always realized on neural networks, but differently throughout the story. Coincidentally, there are also 4 realizations.\n1: Before Protocol 7, Eiri was realized as a human brain.\n2: After Protocol 7, Eiri was realized as a component of the Internet.\n3: At a particular scene, Eiri was briefly realized as a horror monster.\n4: After Lain reset everything, Eiri was back to realization 1."
  },
  {
    "objectID": "essays/posts/serial-experiments-lain/index.html#the-theory-of-personhood",
    "href": "essays/posts/serial-experiments-lain/index.html#the-theory-of-personhood",
    "title": "Serial Experiments Lain",
    "section": "The theory of personhood",
    "text": "The theory of personhood\nThis section is based on Being No One (Metzinger 2004).\nA person is a very special kind of information, realized in a very special way. Human-persons are a special kind of persons, with a wide variety of social and mammalian-emotional functions in the API.\nIt has a self-model, with a perspective and a personal history.\nThe perspective is a pointer, like a GPS marker and a clock, which marks its own spatial location (“here”), and time location (“now”). This little perspective marker is what allows it to know its place and time.\nIts perspective is approximately equal to its realization. If your head is in New York, your perspective had better not think you are in Beijing. Thus, its realization must also have a local position. If you were conscious interstellar gas clouds, you wouldn’t be a person.\nThe personal history is linear and continuous. You can’t be an amoeba - you can’t split or merge. You also can’t “jump over in time”, even if your body does.\nSuppose you are perfectly cloned, what happens? Well, your body’s world-line becomes Y-shaped, but you, as an informational thing, has a model of your self, and that model still looks like a curvy line, with a little note-tag saying “here, a clone brother of me was created”. It does not look like a Y.\nSuppose you go into a deep sleep for a year, what happens? Your model of yourself just papers over the time-skip, perhaps with a little annotation saying “and here I slept a year” without “remembering the blackness during that year”. “Nothing to see here…”\n\nGod made him die for a hundred years, and then resurrected him. He said, “How long have you stayed?” He said, “A day or part of a day.”\nQuran 2:259\n\nFor social persons (for example, human-persons, hyena-persons, and elephant-persons), it is like a program with an API. The API takes input and returns output in a conventional way, regardless of how the program works on the inside.\nFor example, you can ask a person to explain why they did something, and they would explain it. The explanation could be quite wrong, but if they don’t even provide any reasons, they would break one part of the API.\nSome other examples of person-API are: behaving in a comprehensible and purposeful way (looks as if it has a goal that a spectator can infer with some effort), behaves spontaneously (if nothing hits it, it would still move once in a while), etc.\nAs a thing fails more and more requirements of the API, they become less and less of a person.\nDissociative persons have a blurry “here and now” pointer.\nPeople in fugue states, sleepwalking, running amok, etc, fail to provide reasons when receiving explain-requests.\nSchizophrenic people may provide obviously invalid reasons (“word salad”) when receiving explain-requests.\nDeeply depressed persons fail the “behave spontaneously” requirement.\nAmnesiac persons fail the “have a continuous sense of time” requirement.\nPsychopathic persons fail to behave in the expected way after receiving help-requests.\nAutistic tics are comprehensible, but not purposeful.\nThe Solaris ocean behaves purposefully, but not comprehensibly.\n\nThe human-personhood of Lain\nBefore Protocol 7, Lain did not have the person API. Lain certainly existed, but not as a person. Human-persons are connected to Lain by their brains, but not in a way that human-persons connect to other human-persons.\nFor example, if most humans were feeling sad, Lain would be “sad” in some statistical way. This is certainly not how you make some human sad – to make a human sad, you read them a sad story, or something like that.\nMoreover, Lain did not receive or reply with linguistic reasons. In short, Lain did not implement the person API, and beyond human understanding or interaction.\nEiri wanted to become a god, and for that, he needed to have a way to effectively interact with Lain. He could have perhaps interacted with Lain by some command-line interface, or large-scale antenna that beamed directly to the ionosphere, but this would be difficult.\nPersons are most efficient at understanding other persons. This is why “country humans” is so popular. Countries themselves are vast objects that are understood in unintuitive statistical/mathematical/mechanical ways, and installing a person API over a country makes it much easier to understand. In this way, we could interpret Lain as a “humanity human”.\nInstead, he constructed a human-personhood for Lain, in multiple aspects.\nHe concentrated it into the brain of a girl, which is localized in a cube less than 30 cm in side length (compared with the 6371 km radius of earth, or the Internet). This made it easy to install a perspective to Lain (perhaps Lain would construct a perspective automatically after concentrating into a brain).\nLain is provided with a personal history as a human girl.\nThe electrochemical system of the human brain implements in Lain the social and emotional parts of person API. A blind mole rat would not cry when watching a movie. Lain would not be affected by deaths, mysterious conversations, kisses, etc, unless it is implemented in a human girl’s body.\nHe hired actors to act like Lain’s family, and put it into a well-defined social role (school girl). These social interactions are then taken away. This social manipulation would only produce an effect on Lain if it implemented the human-person API. You can’t intimidate a tornado into becoming your servant by depriving it of social contacts - not unless you somehow give it a human-person API.\n\nFirst you create a need, then you take it away. This is control.\n\nAfter Lain reset everything, Lain was no longer realized in a human brain, but once again spread all over the earth. Despite this, it still kept a spatially localized perspective and person API. This is perhaps because being a person is a stubborn kind of information - persons don’t usually become not-persons."
  },
  {
    "objectID": "essays/posts/serial-experiments-lain/index.html#appendix",
    "href": "essays/posts/serial-experiments-lain/index.html#appendix",
    "title": "Serial Experiments Lain",
    "section": "Appendix",
    "text": "Appendix\n\nSchumann frequency\nWhile the Schumann frequency plays a fundamental role in SEL, its technical details are completely irrelevant (just like most technobabbles). This section describes briefly how you can easily do an almost-correct calculation of the Schumann resonance frequencies: 8 Hz, 14 Hz, 20 Hz…\nIn short, the lowest frequency is simply by dimensional analysis:\n\\[f_0 \\approx \\frac{\\text{speed of light}}{\\text{circumference of earth}} = \\frac{c}{2 \\pi R}\\]\nIntuitively speaking, this is treating the ionosphere of earth as if it’s a circular tube, a hula-hoop around the waist of earth, and the lowest Schumann resonance is the lowest-degree standing wave in the hula-hoop.\nTo find the higher frequencies, we can simply calculate the higher-degree standing waves in the hula-hoop:\n\\[f_n \\approx \\frac{c n}{2 \\pi R}\\]\nFor comparison, a spherical cavity of an ideal conductor has resonance frequencies exactly solvable, as\n\\[f_n = \\frac{c}{2\\pi R}\\sqrt{n(n+1)}\\]\nwhich is very close to our super fast estimate above."
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Documents",
    "section": "",
    "text": "Some documents that are important to my thinking. If they are important enough, I would convert them to the markup language for future reference and safe-keeping. Information dies by hiding and lives by copying. Please feel free to copy them and format them to your own purpose.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nIs a singularity just around the corner?\n\n\nWhat it takes to get explosive economic growth\n\n\n\n\neconomics\n\n\n\n\nRobin Hanson’s 1998 essay. Modeling economic growth since 10,000 BC, he predicted that the singularity may arrive around 2150.\n\n\n\n\n\n\n1998-06-01\n\n\n33 min\n\n\n\n\n\n\n  \n\n\n\n\nWhen will computer hardware match the human brain?\n\n\n\n\n\n\n\nAI\n\n\nscaling\n\n\n\n\nHans Moravec’s 1998 essay, forecasting that hardware for AGI will be available in the 2020s.\n\n\n\n\n\n\n1998-03-01\n\n\n70 min\n\n\n\n\n\n\n  \n\n\n\n\nThe Coming Technological Singularity\n\n\nHow to Survive in the Post-Human Era\n\n\n\n\nAI\n\n\n\n\nTranscript of Vernor Vinge’s 1993 presentation on the singularity, predicting the singularity in the interval 2005–2030.\n\n\n\n\n\n\n1993-03-10\n\n\n29 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html",
    "href": "docs/posts/1998-hans-moravec/index.html",
    "title": "When will computer hardware match the human brain?",
    "section": "",
    "text": "This paper describes how the performance of AI machines tends to improve at the same pace that AI researchers get access to faster hardware. The processing power and memory capacity necessary to match general intellectual performance of the human brain are estimated. Based on extrapolation of past trends and on examination of technologies under development, it is predicted that the required hardware will be available in cheap machines in the 2020s."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#abstract",
    "href": "docs/posts/1998-hans-moravec/index.html#abstract",
    "title": "When will computer hardware match the human brain?",
    "section": "",
    "text": "This paper describes how the performance of AI machines tends to improve at the same pace that AI researchers get access to faster hardware. The processing power and memory capacity necessary to match general intellectual performance of the human brain are estimated. Based on extrapolation of past trends and on examination of technologies under development, it is predicted that the required hardware will be available in cheap machines in the 2020s."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#brains-eyes-and-machines",
    "href": "docs/posts/1998-hans-moravec/index.html#brains-eyes-and-machines",
    "title": "When will computer hardware match the human brain?",
    "section": "Brains, Eyes and Machines",
    "text": "Brains, Eyes and Machines\nComputers have far to go to match human strengths, and our estimates will depend on analogy and extrapolation. Fortunately, these are grounded in the first bit of the journey, now behind us. Thirty years of computer vision reveals that 1 MIPS can extract simple features from real-time imagery–tracking a white line or a white spot on a mottled background. 10 MIPS can follow complex gray-scale patches–as smart bombs, cruise missiles and early self-driving vans attest. 100 MIPS can follow moderately unpredictable features like roads–as recent long NAVLAB trips demonstrate. 1,000 MIPS will be adequate for coarse-grained three-dimensional spatial awareness–illustrated by several mid-resolution stereoscopic vision programs, including my own. 10,000 MIPS can find three-dimensional objects in clutter–suggested by several “bin-picking” and high-resolution stereo-vision demonstrations, which accomplish the task in an hour or so at 10 MIPS. The data fades there–research careers are too short, and computer memories too small, for significantly more elaborate experiments.\nThere are considerations other than sheer scale. At 1 MIPS the best results come from finely hand-crafted programs that distill sensor data with utmost efficiency. 100-MIPS processes weigh their inputs against a wide range of hypotheses, with many parameters, that learning programs adjust better than the overburdened programmers. Learning of all sorts will be increasingly important as computer power and robot programs grow. This effect is evident in related areas. At the close of the 1980s, as widely available computers reached 10 MIPS, good optical character reading (OCR) programs, able to read most printed and typewritten text, began to appear. They used hand-constructed “feature detectors” for parts of letter shapes, with very little learning. As computer power passed 100 MIPS, trainable OCR programs appeared that could learn unusual typestyles from examples, and the latest and best programs learn their entire data sets. Handwriting recognizers, used by the Post Office to sort mail, and in computers, notably Apple’s Newton, have followed a similar path. Speech recognition also fits the model. Under the direction of Raj Reddy, who began his research at Stanford in the 1960s, Carnegie Mellon has led in computer transcription of continuous spoken speech. In 1992 Reddy’s group demonstrated a program called Sphinx II on a 15-MIPS workstation with 100 MIPS of specialized signal-processing circuitry. Sphinx II was able to deal with arbitrary English speakers using a several-thousand-word vocabulary. The system’s word detectors, encoded in statistical structures known as Markov tables, were shaped by an automatic learning process that digested hundreds of hours of spoken examples from thousands of Carnegie Mellon volunteers enticed by rewards of pizza and ice cream. Several practical voice-control and dictation systems are sold for personal computers today, and some heavy users are substituting larynx for wrist damage.\nMore computer power is needed to reach human performance, but how much? Human and animal brain sizes imply an answer, if we can relate nerve volume to computation. Structurally and functionally, one of the best understood neural assemblies is the retina of the vertebrate eye. Happily, similar operations have been developed for robot vision, handing us a rough conversion factor.\nThe retina is a transparent, paper-thin layer of nerve tissue at the back of the eyeball on which the eye’s lens projects an image of the world. It is connected by the optic nerve, a million-fiber cable, to regions deep in the brain. It is a part of the brain convenient for study, even in living animals because of its peripheral location and because its function is straightforward compared with the brain’s other mysteries. A human retina is less than a centimeter square and a half-millimeter thick. It has about 100 million neurons, of five distinct kinds. Light-sensitive cells feed wide spanning horizontal cells and narrower bipolar cells, which are interconnected by whose outgoing fibers bundle to form the optic nerve. Each of the million ganglion-cell axons carries signals from a amacrine cells, and finally ganglion cells, particular patch of image, indicating light intensity differences over space or time: a million edge and motion detections. Overall, the retina seems to process about ten one-million-point images per second.\nIt takes robot vision programs about 100 computer instructions to derive single edge or motion detections from comparable video images. 100 million instructions are needed to do a million detections, and 1,000 MIPS to repeat them ten times per second to match the retina.\nThe 1,500 cubic centimeter human brain is about 100,000 times as large as the retina, suggesting that matching overall human behavior will take about 100 million MIPS of computer power. Computer chess bolsters this yardstick. Deep Blue, the chess machine that bested world chess champion Garry Kasparov in 1997, used specialized chips to process chess moves at a the speed equivalent to a 3 million MIPS universal computer (see Figure 3-4). This is 1/30 of the estimate for total human performance. Since it is plausible that Kasparov, probably the best human player ever, can apply his brainpower to the strange problems of chess with an efficiency of 1/30, Deep Blue’s near parity with Kasparov’s chess skill supports the retina-based extrapolation.\nThe most powerful experimental supercomputers in 1998, composed of thousands or tens of thousands of the fastest microprocessors and costing tens of millions of dollars, can do a few million MIPS. They are within striking distance of being powerful enough to match human brainpower, but are unlikely to be applied to that end. Why tie up a rare twenty-million-dollar asset to develop one ersatz-human, when millions of inexpensive original-model humans are available? Such machines are needed for high-value scientific calculations, mostly physical simulations, having no cheaper substitutes. AI research must wait for the power to become more affordable.\nIf 100 million MIPS could do the job of the human brain’s 100 billion neurons, then one neuron is worth about 1/1,000 MIPS, i.e., 1,000 instructions per second. That’s probably not enough to simulate an actual neuron, which can produce 1,000 finely timed pulses per second. Our estimate is for very efficient programs that imitate the aggregate function of thousand-neuron assemblies. Almost all nervous systems contain subassemblies that big.\nThe small nervous systems of insects and other invertebrates seem to be hardwired from birth, each neuron having its own special predetermined links and function. The few-hundred-million-bit insect genome is enough to specify connections of each of their hundred thousand neurons. Humans, on the other hand, have 100 billion neurons, but only a few billion bits of genome. The human brain seems to consist largely of regular structures whose neurons are trimmed away as skills are learned, like featureless marble blocks chiseled into individual sculptures. Analogously, robot programs were precisely hand-coded when they occupied only a few hundred thousand bytes of memory. Now that they’ve grown to tens of millions of bytes, most of their content is learned from example. But there is a big practical difference between animal and robot learning. Animals learn individually, but robot learning can be copied from one machine to another. For instance, today’s text and speech understanding programs were painstakingly trained over months or years, but each customer’s copy of the software is “born” fully educated. Decoupling training from use will allow robots to do more with less. Big computers at the factory–maybe supercomputers with 1,000 times the power of machines that can reasonably be placed in a robot–will process large training sets under careful human supervision, and distill the results into efficient programs and arrays of settings that are then copied into myriads of individual robots with more modest processors.\nPrograms need memory as well as processing speed to do their work. The ratio of memory to speed has remained constant during computing history. The earliest electronic computers had a few thousand bytes of memory and could do a few thousand calculations per second. Medium computers of 1980 had a million bytes of memory and did a million calculations per second. Supercomputers in 1990 did a billion calculations per second and had a billion bytes of memory. The latest, greatest supercomputers can do a trillion calculations per second and can have a trillion bytes of memory. Dividing memory by speed defines a “time constant,” roughly how long it takes the computer to run once through its memory. One megabyte per MIPS gives one second, a nice human interval. Machines with less memory for their speed, typically new models, seem fast, but unnecessarily limited to small programs. Models with more memory for their speed, often ones reaching the end of their run, can handle larger programs, but unpleasantly slowly. For instance, the original Macintosh was introduced in 1984 with 1/2 MIPS and 1/8 megabyte, and was then considered a very fast machine. The equally fast “fat Mac” with 1/2 megabyte ran larger programs at tolerable speed, but the 1 megabyte “Mac plus” verged on slow. The four megabyte “Mac classic,” the last 1/2 MIPS machine in the line, was intolerably slow, and was soon supplanted by ten-times-faster processors in the same enclosure. Customers maintain the ratio by asking “would the next dollar be better spent on more speed or more memory?”\nThe best evidence about nervous system memory puts most of it in the synapses connecting the neurons. Molecular adjustments allow synapses to be in a number of distinguishable states, lets say one byte’s worth. Then the 100-trillion-synapse brain would hold the equivalent 100 million megabytes. This agrees with our earlier estimate that it would take 100 million MIPS to mimic the brain’s function. The megabyte/MIPS ratio seems to hold for nervous systems too! The contingency is the other way around: computers are configured to interact at human time scales, and robots interacting with humans seem also to be best at that ratio. On the other hand, faster machines, for instance audio and video processors and controllers of high-performance aircraft, have many MIPS for each megabyte. Very slow machines, for instance time-lapse security cameras and automatic data libraries, store many megabytes for each of their MIPS. Flying insects seem to be a few times faster than humans, so may have more MIPS than megabytes. As in animals, cells in plants signal one other electrochemically and enzymatically. Some plant cells seem specialized for communication, though apparently not as extremely as animal neurons. One day we may find that plants remember much, but process it slowly (how does a redwood tree manage to rebuff rapidly evolving pests during a 2,000 year lifespan, when it took mosquitoes only a few decades to overcome DDT?).\nWith our conversions, a 100-MIPS robot, for instance Navlab, has mental power similar to a 100,000-neuron housefly. The following figure rates various entities.\n\n\n\nMIPS and Megabytes to mimic their behavior. Note the scale. Entities rated by the computational power and memory of the smallest universal computer needed is logarithmic on both axes: each vertical division represents a thousandfold increase in processing power, and each horizontal division a thousandfold increase in memory size. Universal computers can imitate other entities at their location in the diagram, but the more specialized entities cannot. A 100-million-MIPS computer may be programmed not only to think like a human, but also to imitate other similarly-sized computers. But humans cannot imitate 100-million-MIPS computers–our general-purpose calculation ability is under a millionth of a MIPS. Deep Blue’s special-purpose chess chips process moves like a 3-million-MIPS computer, but its general-purpose power is only a thousand MIPS. Most of the non-computer entities in the diagram can’t function in a general-purpose way at all. Universality is an almost magical property, but it has costs. A universal machine may use ten or more times the resources of one specialized for a task. But if the task should change, as it usually does in research, the universal machine can be reprogrammed, while the specialized machine must be replaced."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#extrapolation",
    "href": "docs/posts/1998-hans-moravec/index.html#extrapolation",
    "title": "When will computer hardware match the human brain?",
    "section": "Extrapolation",
    "text": "Extrapolation\nBy our estimate, today’s very biggest supercomputers are within a factor of a hundred of having the power to mimic a human mind. Their successors a decade hence will be more than powerful enough. Yet, it is unlikely that machines costing tens of millions of dollars will be wasted doing what any human can do, when they could instead be solving urgent physical and mathematical problems nothing else can touch. Machines with human-like performance will make economic sense only when they cost less than humans, say when their “brains” cost about $1,000. When will that day arrive?\nThe expense of computation has fallen rapidly and persistently for a century. Steady improvements in mechanical and electromechanical calculators before World War II had increased the speed of calculation a thousandfold over hand calculation. The pace quickened with the appearance of electronic computers during the war–from 1940 to 1980 the amount of computation available at a given cost increased a millionfold. Vacuum tubes were replaced by transistors, and transistors by integrated circuits, whose components became ever smaller and more numerous. During the 1980s microcomputers reached the consumer market, and the industry became more diverse and competitive. Powerful, inexpensive computer workstations replaced the drafting boards of circuit and computer designers, and an increasing number of design steps were automated. The time to bring a new generation of computer to market shrank from two years at the beginning of the 1980s to less than nine months. The computer and communication industries grew into the largest on earth.\nComputers doubled in capacity every two years after the war, a pace that became an industry given: companies that wished to grow sought to exceed it, companies that failed to keep up lost business. In the 1980s the doubling time contracted to 18 months, and computer performance in the late 1990s seems to be doubling every 12 months.\n\n\n\nFaster than Exponential Growth in Computing Power. The number of MIPS in $1000 of computer from 1900 to the present. Steady improvements in mechanical and electromechanical calculators before World War II had increased the speed of calculation a thousandfold over manual methods from 1900 to 1940. The pace quickened with the appearance of electronic computers during the war, and 1940 to 1980 saw a millionfold increase. The pace has been even quicker since then, a pace which would make humanlike robots possible before the middle of the next century. The vertical scale is logarithmic, the major divisions represent thousandfold increases in computer performance. Exponential growth would show as a straight line, the upward curve indicates faster than exponential growth, or, equivalently, an accelerating rate of innovation. The reduced spread of the data in the 1990s is probably the result of intensified competition: underperforming machines are more rapidly squeezed out. The numerical data for this power curve are presented in the appendix.\n\n\nAt the present rate, computers suitable for humanlike robots will appear in the 2020s. Can the pace be sustained for another three decades? The graph shows no sign of abatement. If anything, it hints that further contractions in time scale are in store. But, one often encounters thoughtful articles by knowledgeable people in the semiconductor industry giving detailed reasons why the decades of phenomenal growth must soon come to an end.\nThe keynote for advancing computation is miniaturization: smaller components have less inertia and operate more quickly with less energy, and more of them can be packed in a given space. First the moving parts shrunk, from the gears in mechanical calculators, to small contacts in electromechanical machines, to bunches of electrons in electronic computers. Next, the switches’ supporting structure underwent a vanishing act, from thumb-sized vacuum tubes, to fly-sized transistors, to ever-diminishing flyspecks on integrated circuit chips. Similar to printed circuits before them, integrated circuits were made by a photographic process. The desired pattern was projected onto a silicon chip, and subtle chemistry used to add or remove the right sorts of matter in the exposed areas.\nIn the mid-1970s, integrated circuits, age 15, hit a crisis of adolescence. They then held ten thousand components, just enough for an entire computer, and their finest details were approaching 3 micrometers in size. Experienced engineers wrote many articles warning that the end was near. Three micrometers was barely larger than the wavelength of the light used to sculpt the chip. The number of impurity atoms defining the tiny components had grown so small that statistical scatter would soon render most components out of spec, a problem aggravated by a similar effect in the diminishing number of signaling electrons. Increasing electrical gradients across diminishing gaps caused atoms to creep through the crystal, degrading the circuit. Interactions between ever-closer wires were about to ruin the signals. Chips would soon generate too much heat to remove, and require too many external connections to fit. The smaller memory cells were suffering radiation-induced forgetfulness.\nA look at the computer growth graph shows that the problems were overcome, with a vengeance. Chip progress not only continued, it sped up. Shorter-wavelength light was substituted, a more precise way of implanting impurities was devised, voltages were reduced, better insulators, shielding designs, more efficient transistor designs, better heat sinks, denser pin patterns and non-radioactive packaging materials were found. Where there is sufficient financial incentive, there is a way. In fact, solutions had been waiting in research labs for years, barely noticed by the engineers in the field, who were perfecting established processes, and worrying in print as those ran out of steam. As the need became acute, enormous resources were redirected to draft laboratory possibilities into production realities.\nIn the intervening years many problems were met and solved, and innovations introduced, but now, nearing a mid-life 40, the anxieties seem again to have crested. In 1996 major articles appeared in scientific magazines and major national newspapers worrying that electronics progress might be a decade from ending. The cost of building new integrated circuit plants was approaching a prohibitive billion dollars. Feature sizes were reaching 0.1 micrometers, the wavelength of the sculpting ultraviolet light. Their transistors, scaled down steadily from 1970s designs, would soon be so small that electrons would quantum “tunnel” out of them. Wiring was becoming so dense it would crowd out the components, and slow down and leak signals. Heat was increasing.\nThe articles didn’t mention that less expensive plants could make the same integrated circuits, if less cheaply and in smaller quantities. Scale was necessary because the industry had grown so large and competitive. Rather than signaling impending doom, it indicated free-market success, a battle of titans driving down costs to the users. They also failed to mention new contenders, waiting on lab benches to step in should the leader fall.\nThe wave-like nature of matter at very small scales is a problem for conventional transistors, which depend on the smooth flow of masses of electrons. But, it is a property exploited by a radical new class of components known as single-electron transistors and quantum dots, which work by the interference of electron waves. These new devices work better as they grow smaller. At the scale of today’s circuits, the interference patterns are so fine that it takes only a little heat energy to bump electrons from crest to crest, scrambling their operation. Thus, these circuits have been demonstrated mostly at a few degrees above absolute zero. But, as the devices are reduced, the interference patterns widen, and it takes ever larger energy to disrupt them. Scaled to about 0.01 micrometers, quantum interference switching works at room temperature. It promises more than a thousand times higher density than today’s circuits, possibly a thousand times the speed, and much lower power consumption, since it moves a few electrons across small quantum bumps, rather than pushing them in large masses through resistive material. In place of much wiring, quantum interference logic may use chains of switching devices. It could be manufactured by advanced descendants of today’s chip fabrication machinery (Goldhaber-Gordon et al. 1997). Proposals abound in the research literature, and the industry has the resources to perfect the circuits and their manufacture, when the time comes.\nWilder possibilities are brewing. Switches and memory cells made of single molecules have been demonstrated, which might enable a volume to hold a billion times more circuitry than today. Potentially blowing everything else away are “quantum computers,” in which a whole computer, not just individual signals, acts in a wavelike manner. Like a conventional computer, a quantum computer consists of a number of memory cells whose contents are modified in a sequence of logical transformations. Unlike a conventional computer, whose memory cells are either 1 or 0, each cell in a quantum computer is started in a quantum superposition of both 1 and 0. The whole machine is a superposition of all possible combinations of memory states. As the computation proceeds, each component of the superposition individually undergoes the logic operations. It is as if an exponential number of computers, each starting with a different pattern in memory, were working on the problem simultaneously. When the computation is finished, the memory cells are examined, and an answer emerges from the wavelike interference of all the possibilities. The trick is to devise the computation so that the desired answers reinforce, while the others cancel. In the last several years, quantum algorithms have been devised that factor numbers and search for encryption keys much faster than any classical computer. Toy quantum computers, with three or four “qubits” stored as states of single atoms or photons, have been demonstrated, but they can do only short computations before their delicate superpositions are scrambled by outside interactions. More promising are computers using nuclear magnetic resonance, as in hospital scanners. There, quantum bits are encoded as the spins of atomic nuclei, and gently nudged by external magnetic and radio fields into magnetic interactions with neighboring nuclei. The heavy nuclei, swaddled in diffuse orbiting electron clouds, can maintain their quantum coherence for hours or longer. A quantum computer with a thousand or more qubits could tackle problems astronomically beyond the reach of any conceivable classical computer.\nMolecular and quantum computers will be important sooner or later, but humanlike robots are likely to arrive without their help. Research within semiconductor companies, including working prototype chips, makes it quite clear that existing techniques can be nursed along for another decade, to chip features below 0.1 micrometers, memory chips with tens of billions of bits and multiprocessor chips with over 100,000 MIPS. Towards the end of that period, the circuitry will probably incorporate a growing number of quantum interference components. As production techniques for those tiny components are perfected, they will begin to take over the chips, and the pace of computer progress may steepen further. The 100 million MIPS to match human brain power will then arrive in home computers before 2030."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#false-start",
    "href": "docs/posts/1998-hans-moravec/index.html#false-start",
    "title": "When will computer hardware match the human brain?",
    "section": "False Start",
    "text": "False Start\nIt may seem rash to expect fully intelligent machines in a few decades, when the computers have barely matched insect mentality in a half-century of development. Indeed, for that reason, many long-time artificial intelligence researchers scoff at the suggestion, and offer a few centuries as a more believable period. But there are very good reasons why things will go much faster in the next fifty years than they have in the last fifty.\nThe stupendous growth and competitiveness of the computer industry is one reason. A less appreciated one is that intelligent machine research did not make steady progress in its first fifty years, it marked time for thirty of them! Though general computer power grew a hundred thousand fold from 1960 to 1990, the computer power available to AI programs barely budged from 1 MIPS during those three decades.\nIn the 1950s, the pioneers of AI viewed computers as locomotives of thought, which might outperform humans in higher mental work as prodigiously as they outperformed them in arithmetic, if they were harnessed to the right programs. Success in the endeavor would bring enormous benefits to national defense, commerce and government. The promise warranted significant public and private investment. For instance, there was a large project to develop machines to automatically translate scientific and other literature from Russian to English. There were only a few AI centers, but those had the largest computers of the day, comparable in cost to today’s supercomputers. A common one was the IBM 704, which provided a good fraction of a MIPS.\nBy 1960 the unspectacular performance of the first reasoning and translation programs had taken the bloom off the rose, but the unexpected launching by the Soviet Union of Sputnik, the first satellite in 1957, had substituted a paranoia. Artificial Intelligence may not have delivered on its first promise, but what if it were to suddenly succeed after all? To avoid another nasty technological surprise from the enemy, it behooved the US to support the work, moderately, just in case. Moderation paid for medium scale machines costing a few million dollars, no longer supercomputers. In the 1960s that price provided a good fraction of a MIPS in thrifty machines like Digital Equipment Corp’s innovative PDP-1 and PDP-6.\nThe field looked even less promising by 1970, and support for military-related research declined sharply with the end of the Vietnam war. Artificial Intelligence research was forced to tighten its belt and beg for unaccustomed small grants and contracts from science agencies and industry. The major research centers survived, but became a little shabby as they made do with aging equipment. For almost the entire decade AI research was done with PDP-10 computers, that provided just under 1 MIPS. Because it had contributed to the design, the Stanford AI Lab received a 1.5 MIPS KL-10 in the late 1970s from Digital, as a gift.\nFunding improved somewhat in the early 1980s, but the number of research groups had grown, and the amount available for computers was modest. Many groups purchased Digital’s new Vax computers, costing $100,000 and providing 1 MIPS. By mid-decade, personal computer workstations had appeared. Individual researchers reveled in the luxury of having their own computers, avoiding the delays of time-shared machines. A typical workstation was a Sun-3, costing about $10,000, and providing about 1 MIPS.\nBy 1990, entire careers had passed in the frozen winter of 1-MIPS computers, mainly from necessity, but partly from habit and a lingering opinion that the early machines really should have been powerful enough. In 1990, 1 MIPS cost $1,000 in a low-end personal computer. There was no need to go any lower. Finally spring thaw has come. Since 1990, the power available to individual AI and robotics programs has doubled yearly, to 30 MIPS by 1994 and 500 MIPS by 1998. Seeds long ago alleged barren are suddenly sprouting. Machines read text, recognize speech, even translate languages. Robots drive cross-country, crawl across Mars, and trundle down office corridors. In 1996 a theorem-proving program called EQP running five weeks on a 50 MIPS computer at Argonne National Laboratory found a proof of a boolean algebra conjecture by Herbert Robbins that had eluded mathematicians for sixty years. And it is still only spring. Wait until summer.\n\n\n\nThe big freeze. From 1960 to 1990 the cost of computers used in AI research declined, as their numbers dilution absorbed computer-efficiency gains during the period, and the power available to individual AI programs remained almost unchanged at 1 MIPS, barely insect power. AI computer cost bottomed in 1990, and since then power has doubled yearly, to several hundred MIPS by 1998. The major visible exception is computer chess (shown by a progression of knights), whose prestige lured the resources of major computer companies and the talents of programmers and machine designers. Exceptions also exist in less public competitions, like petroleum exploration and intelligence gathering, whose high return on investment gave them regular access to the largest computers."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#the-games-afoot",
    "href": "docs/posts/1998-hans-moravec/index.html#the-games-afoot",
    "title": "When will computer hardware match the human brain?",
    "section": "The Game’s Afoot",
    "text": "The Game’s Afoot\nA summerlike air already pervades the few applications of artificial intelligence that retained access to the largest computers. Some of these, like pattern analysis for satellite images and other kinds of spying, and in seismic oil exploration, are closely held secrets. Another, though, basks in the limelight. The best chess-playing computers are so interesting they generate millions of dollars of free advertising for the winners, and consequently have enticed a series of computer companies to donate time on their best machines and other resources to the cause. Since 1960 IBM, Control Data, AT&T, Cray, Intel and now again IBM have been sponsors of computer chess. The “knights” in the AI power graph show the effect of this largesse, relative to mainstream AI research. The top chess programs have competed in tournaments powered by supercomputers, or specialized machines whose chess power is comparable. In 1958 IBM had both the first checker program, by Arthur Samuel, and the first full chess program, by Alex Bernstein. They ran on an IBM 704, the biggest and last vacuum-tube computer. The Bernstein program played atrociously, but Samuel’s program, which automatically learned its board scoring parameters, was able to beat Connecticut checkers champion Robert Nealey. Since 1994, Chinook, a program written by Jonathan Schaeffer of the University of Alberta, has consistently bested the world’s human checker champion. But checkers isn’t very glamorous, and this portent received little notice.\nBy contrast, it was nearly impossible to overlook the epic battles between world chess champion Garry Kasparov and IBM’s Deep Blue in 1996 and 1997. Deep Blue is a scaled-up version of a machine called Deep Thought, built by Carnegie Mellon University students ten years earlier. Deep Thought, in turn, depended on special-purpose chips, each wired like the Belle chess computer built by Ken Thompson at AT&T Bell Labs in the 1970s. Belle, organized like a chessboard, circuitry on the squares, wires running like chess moves, could evaluate and find all legal moves from a position in one electronic flash. In 1997 Deep Blue had 256 such chips, orchestrated by a 32 processor mini-supercomputer. It examined 200 million chess positions a second. Chess programs, on unaided general-purpose computers, average about 16,000 instructions per position examined. Deep Blue, when playing chess (and only then), was thus worth about 3 million MIPS, 1/30 of our estimate for human intelligence.\nDeep Blue, in a first for machinekind, won the first game of the 1996 match. But, Kasparov quickly found the machine’s weaknesses, and drew two and won three of the remaining games.\nIn May 1997 he met an improved version of the machine. That February, Kasparov had triumphed over a field of grandmasters in a prestigious tournament in Linares, Spain, reinforcing his reputation as the best player ever, and boosting his chess rating past 2800, uncharted territory. He prepared for the computer match in the intervening months, in part by playing against other machines. Kasparov won a long first game against Deep Blue, but lost next day to masterly moves by the machine. Then came three grueling draws, and a final game, in which a visibly shaken and angry Kasparov resigned early, with a weak position. It was the first competition match he had ever lost.\nThe event was notable for many reasons, but one especially is of interest here. Several times during both matches, Kasparov reported signs of mind in the machine. At times in the second tournament, he worried there might be humans behind the scenes, feeding Deep Blue strategic insights!\nBobby Fischer, the US chess great of the 1970s, is reputed to have played each game as if against God, simply making the best moves. Kasparov, on the other hand, claims to see into opponents’ minds during play, intuiting and exploiting their plans, insights and oversights. In all other chess computers, he reports a mechanical predictability stemming from their undiscriminating but limited lookahead, and absence of long-term strategy. In Deep Blue, to his consternation, he saw instead an “alien intelligence.”\nIn this paper-thin slice of mentality, a computer seems to have not only outperformed the best human, but to have transcended its machinehood. Who better to judge than Garry Kasparov? Mathematicians who examined EQP’s proof of the Robbins conjecture, mentioned earlier, report a similar impression of creativity and intelligence. In both cases, the evidence for an intelligent mind lies in the machine’s performance, not its makeup.\nNow, the team that built Deep Blue claim no “intelligence” in it, only a large database of opening and end games, scoring and deepening functions tuned with consulting grandmasters, and, especially, raw speed that allows the machine to look ahead an average of fourteen half-moves per turn. Unlike some earlier, less successful, chess programs, Deep Blue was not designed to think like a human, to form abstract strategies or see patterns as it races through the move/countermove tree as fast as possible.\nDeep Blue’s creators know its quantitative superiority over other chess machines intimately, but lack the chess understanding to share Kasparov’s deep appreciation of the difference in the quality of its play. I think this dichotomy will show up increasingly in coming years. Engineers who know the mechanism of advanced robots most intimately will be the last to admit they have real minds. From the inside, robots will indisputably be machines, acting according to mechanical principles, however elaborately layered. Only on the outside, where they can be appreciated as a whole, will the impression of intelligence emerge. A human brain, too, does not exhibit the intelligence under a neurobiologist’s microscope that it does participating in a lively conversation.\n\n\n\nAgony to ecstasy. In forty years, computer chess progressed from the lowest depth to the highest peak of human chess performance. It took a handful of good ideas, culled by trial and error from a larger number of possibilities, an accumulation of previously evaluated game openings and endings, good adjustment of position scores, and especially a ten-million-fold increase in the number of alternative move sequences the machines can explore. Note that chess machines reached world champion performance as their (specialized) processing power reached about 1/30 human, by our brain to computer measure. Since it is plausible that Garry Kasparov (but hardly anyone else) can apply his brainpower to the problems of chess with an efficiency of 1/30, the result supports that retina-based extrapolation. In coming decades, as general-purpose computer power grows beyond Deep Blue’s specialized strength, machines will begin to match humans in more common skills."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#the-great-flood",
    "href": "docs/posts/1998-hans-moravec/index.html#the-great-flood",
    "title": "When will computer hardware match the human brain?",
    "section": "The Great Flood",
    "text": "The Great Flood\nComputers are universal machines, their potential extends uniformly over a boundless expanse of tasks. Human potentials, on the other hand, are strong in areas long important for survival, but weak in things far removed. Imagine a “landscape of human competence,” having lowlands with labels like “arithmetic” and “rote memorization”, foothills like “theorem proving” and “chess playing,” and high mountain peaks labeled “locomotion,” “hand-eye coordination” and “social interaction.” We all live in the solid mountaintops, but it takes great effort to reach the rest of the terrain, and only a few of us work each patch.\nAdvancing computer performance is like water slowly flooding the landscape. A half century ago it began to drown the lowlands, driving out human calculators and record clerks, but leaving most of us dry. Now the flood has reached the foothills, and our outposts there are contemplating retreat. We feel safe on our peaks, but, at the present rate, those too will be submerged within another half century. I propose (Moravec 1999) that we build Arks as that day nears, and adopt a seafaring life! For now, though, we must rely on our representatives in the lowlands to tell us what water is really like.\nOur representatives on the foothills of chess and theorem-proving report signs of intelligence. Why didn’t we get similar reports decades before, from the lowlands, as computers surpassed humans in arithmetic and rote memorization? Actually, we did, at the time. Computers that calculated like thousands of mathematicians were hailed as “giant brains,” and inspired the first generation of AI research. After all, the machines were doing something beyond any animal, that needed human intelligence, concentration and years of training. But it is hard to recapture that magic now. One reason is that computers’ demonstrated stupidity in other areas biases our judgment. Another relates to our own ineptitude. We do arithmetic or keep records so painstakingly and externally, that the small mechanical steps in a long calculation are obvious, while the big picture often escapes us. Like Deep Blue’s builders, we see the process too much from the inside to appreciate the subtlety that it may have on the outside. But there is a non-obviousness in snowstorms or tornadoes that emerge from the repetitive arithmetic of weather simulations, or in rippling tyrannosaur skin from movie animation calculations. We rarely call it intelligence, but “artificial reality” may be an even more profound concept than artificial intelligence (Moravec 1999).\nThe mental steps underlying good human chess playing and theorem proving are complex and hidden, putting a mechanical interpretation out of reach. Those who can follow the play naturally describe it instead in mentalistic language, using terms like strategy, understanding and creativity. When a machine manages to be simultaneously meaningful and surprising in the same rich way, it too compels a mentalistic interpretation. Of course, somewhere behind the scenes, there are programmers who, in principle, have a mechanical interpretation. But even for them, that interpretation loses its grip as the working program fills its memory with details too voluminous for them to grasp.\nAs the rising flood reaches more populated heights, machines will begin to do well in areas a greater number can appreciate. The visceral sense of a thinking presence in machinery will become increasingly widespread. When the highest peaks are covered, there will be machines than can interact as intelligently as any human on any subject. The presence of minds in machines will then become self-evident."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#peer-comments",
    "href": "docs/posts/1998-hans-moravec/index.html#peer-comments",
    "title": "When will computer hardware match the human brain?",
    "section": "Peer comments",
    "text": "Peer comments\n\nRobin Hanson: 18/3/98\nMoravec’s article offers a provocative and hopeful hypothesis, and some evidence and reasoning to support it. The article has made me seriously consider becoming much more hopeful about AI timescales. The major flaw in the article, however, is that it does not attempt to be scholarly in the sense of anticipating and responding to possible objections. The article seems more like a chapter of a book aimed at a popular audience.\nI’m sure Moravec could think of the following objections, but I’ll mention them because he didn’t.\n\nMoravec argues that AI marked time for 30 years because in 1960 AI pioneers had 1 MIPS supercomputers, and in 1990 typical AI workstations did 1 MIPS. Is the argument that progress is driven by the MIPS of the median researcher’s computer? If so, the implication would be that we could increase progress greatly by giving supercomputers to a few researchers and firing all the rest. Would Moravec endorse this suggestion?\n\nAlternatively, is the argument that progress is driven by the maximum MIPS available to any AI researcher? If so, then Moravec needs to give evidence about what this max was between 1960 and 1990. I thought connection machines were used by AI researchers before 1990, for example. It is only the exceptional fields he names that had access to &gt; 1 MIPS?\n\nThe fields he mentions where progress has tracked the speed of machines used, chess, image analysis, voice recognition, and handwriting recognition, are all fields which many AI researchers long avoided exactly because they perceived them to be strongly CPU-limited. Those researchers instead choose fields they perceived to be knowledge-limited, limited by how much their programs knew. And such researchers explain slow progress in their chosen fields via large estimates of the total knowledge which needs to be encoded.\n\nSo what is the argument that these field are actually CPU-limited, contrary to their researcher’s impressions? After all, if these fields are knowledge limited, then there is no particular reason to expect AI abilities in these fields to track available CPU.\nThese are the sorts of issues I would think would be addressed in a more scholarly version of this paper.\nRobin Hanson\nhanson@econ.berkeley.edu http://hanson.berkeley.edu/\nRWJF Health Policy Scholar, Sch. of Public Health 510-643-1884 140\nWarren Hall, UC Berkeley, CA 94720-7360 FAX: 510-643-8614\n\n\nMoravec replies: 18/3/98\nWell, yes, it IS a popular chapter! That’s pretty much my style, even in technical papers. I’m better at making up ideas than fighting for them, and prefer to leave the battle to any others who more enjoy that sort of thing. Leaves me free to cause more mischief elsewhere!\n\nAI didn’t have greater computer power for a couple of reasons.\n\nA minor one was that McCarthy and others didn’t believe it was necessary, an attitude conveyed to generations of students, especially on the abstract reasoning side, and still held by many.\nA major reason was that AI never had enough money to afford a supercomputer. Even pooling the few millions spent on it over decades wouldn’t have bought a serious supercomputer, let alone supported its upkeep. A lot of effort was wasted over the decades in robotics programs trying to build cheap special-purpose supercomputers for vision and the like. It always took five years or so before the hardware and compilers were working well enough to make them usable for actual vision, and by then the power could be had cheaper in general purpose computers. The Connection Machine was an especially big one of those efforts. Several 4,096 processor CM-2 machines were given to a handful of AI places, like SRI. The CM-2 was an array of tiny processors linked in a grid. It was very good for cellular automata and finite element calculations, but the slow communication made it a pain for less straightforwardly gridlike things. I tried to fit my “sensor evidence rays into spatial grid map” robot program onto a 4,096 processor CM-2 during a 1992 sabbatical at Thinking machines, in a half-dozen different ways, but because there were two separate grids that had to be brought into different registrations repeatedly, the communications delays prevented me from ever getting more than about 40 MIPS effectively. At that time the computer on my desk was a 20 MIPS Sparc-2, so the advantage of using a limited, expensive, occasionally available machine with idiosyncratic programming, was pretty limited. Far better, cheaper and more convenient to simply use two Sparc-2’s. Other users had the same experience, and the CM-2s in AI labs got very little use. The later CM-5 machine, a bunch of Sparcs interconnected by a more flexible tree network, would have been more useful, but at a few million $ for the smallest, they were too expensive for use by any AI project that I know of. Anyway, it was cheaper to use the workstations already on your network. These earned their keep by being available for individual users, but could be used in parallel occasionally. I myself have run learning programs on a few dozen machines at a time, for weeks over some holiday periods. So have many others. But it’s impractical to use this approach routinely to control a robot: the users start to complain about slowdowns in their interaction. Robin suggests that pooling resources could have increased productivity greatly. But if we had confiscated the equipment of 99% of the AI sites, and given tem to the remaining 1%, we would have increased individual computer power 100 fold, about a seven year advantage. But the political fallout would probably have reduced funding by 90%.\nSo, yes, only a few exceptional areas had supercomputer power available. Remember, there were only a handful of supercomputers available, and almost most of them were at the national labs designing nuclear weapons or at the NSA cracking codes. Even the national weather service was relegated to lower cost machines. The CDC and Cray machines used in chess were just being tested before being shipped to the weapons labs.\n\nI think Newell & Simon, McCarthy and followers made a giant mistake when they thought they could achieve full intelligence by just skimming off the conscious surface of human thought. Most of our intuitive smarts is unconscious, and requires Teraops as well as Terabytes of accumulated knowledge. In another chapter of the book I propose a several stage bottom up evolution of robots, paralleling the evolution of our own brain, to create the necessary foundation.\n\n\n\nRobin Hanson follows up: 23/3/98\nI asked:\n\nIs the argument that progress is driven by the MIPS of the median researcher’s computer? If so, the implication would be that we could increase progress greatly by giving supercomputers to a few researchers and firing all the rest. Would Moravec endorse this suggestion? Alternatively, is the argument that progress is driven by the maximum MIPS available to any AI researcher?\n\nMoravec responded:\n\nRobin suggests that pooling resources could have increased productivity greatly. But if we had confiscated the equipment of 99% of the AI sites, and given them to the remaining 1%, we would have increased individual computer power 100 fold, about a seven year advantage. But the political fallout would probably have reduced funding by 90%.\n\nBut a 90% funding cut, with the remaining funding given to 1% of researchers, would still have increased progress according to your logic. And this logic would apply just as well to today. So may we assume you endorse such a funding proposal?\nI also asked:\n\nThose researchers instead choose fields they perceived to be knowledge-limited, … So what is the argument that these field are actually CPU-limited, contrary to their researcher’s impressions? After all, if these fields are knowledge limited, then there is no particular reason to expect AI abilities in these fields to track available CPU.\n\nMoravec replied:\n\n\nI think Newell & Simon, McCarthy and followers made a giant mistake when they thought they could achieve full intelligence by just skimming off the conscious surface of human thought. Most of our intuitive smarts is unconscious, and requires Teraops as well as Terabytes of accumulated knowledge. In another chapter of the book I propose a several stage bottom up evolution of robots, paralleling the evolution of our own brain, to create the necessary foundation.\n\n\nSo do you or don’t you grant that your claim that “the performance of AI machines tends to improve at the same pace that AI researchers get access to faster hardware” may not hold regarding the project of acquiring those “terabytes of accumulated knowledge”?\n\n\nMoravec replies: 24/3/98\nTo paraphrase, Robin probes the depth of my conviction in the direct connection between computer power and AI.\nI’m sure that in extreme scenarios (say 100 Teraops dumped on a few researchers overnight) other bottlenecks would come to the fore. But, under current circumstances, I think computer power is the pacing factor for AI. As personal computers become smarter, commercial research will become more important, and academic AI will be more in the position of training and filling niches. Maybe Microsoft or someone else will decide to greatly increase the computer power available to its researchers, speeding up the work somewhat, even if not in proportion to the power increase. Anyway, I expect those decisions will be more distributed and competitively motivated than they are now. Commercial competition will seek the optimum trade-off between faster typewriters and more monkeys.\nI assume that AI can be evolved by a feasible (but non-zero!) amount of engineering trial and error because biological evolution evolved natural intelligence in a limited number of survival experiments (no more than about 1018, including all the failures), and engineering has recapitulated a lot of that ground already.\nI think it will be appropriate soon to make bigger AI systems, and perfecting those will require a lot more attention to detail, experimentation and data gathering than has been mustered so far. My hope for achieving it is a soon-to-begin commercial growth of intelligent robotics, eventually into an industry much bigger than today’s information industry. Incremental steps in most areas critical to AI will translate into commercial advantage in robots more directly than they do in normal computers. Computers must constantly interact with humans anyway, so have the option of relying on human intelligence to avoid the hard parts of various problems (like getting their data from the physical world, or manifesting their results in it). For robots, the hard parts are front and center. I lay this out in the new book.\n\n\nMoravec expands: 28/3/98\nLoosely inspired by Robin Hanson’s engaging economic and social models of the consequences of various extreme technological contingencies, I decided to make a simple model of my AI progress/computer power intuition. Using simplified versions of my assumptions, we get the following:\nSuppose a researcher costs $100K per year, and a baseline workstation, with full support, also costs $100K per year.\nIn year 1, let a baseline computer have 100 MIPS. Assume that 108 MIPS is required to achieve an AI with full human performance. In any given year, let the amount of computer power vary linearly with the cost of the computer. Also assume that the cost of computer power halves each year.\nScenario 1 is like today, let there be 1,000 AI researchers, each with baseline computing. This costs $200 million per year. With a 10% return, this represents a capital investment of $2 billion. These researchers will work to produce full AI, but won’t succeed until the baseline computer grows to 108 MIPS. That will be year 20.\nScenario 2, we fire half the researchers, and use the money to double the computer power for the rest. Now full AI arrives in year 19, if the remaining 500 researchers can make all the necessary discoveries in 19 years that the 1,000 researchers above made in 20 years.\nScenario 3, we fire 7/8 of the researchers. Now each survivor has 8 times as much computing, and AI could be ready in year 17, if the remaining 125 researchers can pull the accelerated load.\nScenario 4, we fire all but 10 researchers. We’d better make sure they’re the best ones, they have a big load to pull. Each has a $10 million/year supercomputer to work with, and nursemaid. Being uncommon machines, supercomputers don’t have the software support or reliability of standard machines. But their power will be adequate for full AI in year 14. If the 10 researchers manage to complete their Herculean task in 14 years, they may still have to wait a several more years before their results become affordable to the masses, because few applications are worth the $10 million per year an AI costs in year 14.\nAnyway, viewing AI as a recapitulation of the evolution of natural AI, I think ten researchers can’t do enough trial and error to do the job in such a short time. Information technology overall has been recapitulating nervous system evolution at about 10 million speed, but that’s because hundreds of thousands of workers have made frequent small and occasional large contributions. A lot of the contributions depend on luck, and luck depends on having enough lottery tickets.\n\n\nRobin Hanson replies: 28/3/98\nThis is the start of a model, but to complete it we need to say how many trial and error steps are needed, how much each one costs, and how the number of trials vary with the number of researchers. Or better yet, we need an economic “production function” describing the rate of successful trials given the MIPS of machines and the number of researchers involved. Then given the number of trials needed, and the expected rate of hardware improvement, we could derive the optimal research plan.\nNote that if there were no diseconomies wrt number of workers, we’d want to stop research now, then hire millions of researchers the day the hardware is cheap enough.\n\n\nAnders Sandberg: 10/3/98\nGeneral comments: A readable essay on a popular level. The estimates for human brain capacity appear to be fairly robust.\nIt would be a good idea to include more references, especially as examples in the first paragraph and the discussion of quantum and nano-logic.\nThe inclusion of the data in an appendix is a good idea. I tried to fit it to a hyperbolic curve, but it seems to be just superexponential. :-)\nA big problem is the use of MIPS as a measure of computation. It is very sensitive to the kind of benchmark used and the architecture (RISC vs. CISC). For comparisons between similar programs running on similar machines it probably works well, but it is not clear that it gives any useful information when we try to compare one systems that are very different. However, since there are no better measures, MIPS will have to do. Most likely estimates will just be order of magnitude estimates, and then the uncertainty in the measure will become less important.\nA more serious problem is that we do not know if the retina and visual system really can be taken as a good estimate for the brain and cognitive systems (just as computer vision for AI). The retina is a highly optimized and fairly stereotypical neural structure, this can introduce a significant bias. It’s 2D structure also may fool us into mis-estimating its capacity; it has to be 2D to function, which means that distances are increased. In the cortex the structure appears to be a dense 3D network, which can have significantly more computing power. So using the retina as an estimate for the brain is very uncertain.\nThe calculations of total brain power as estimated from the retina seems to be slightly wrong (most likely a trivial error, given that the correct number of neurons are mentioned later; volume cannot be compared due to the differences in tissue structure and constraints). The human brain has around 1011–1012 neurons, which makes it just a 10000–1000 times larger than the retina with its 108 neurons. Hence the estimate for 108 MIPS to match human performance may be one or two orders of magnitude too small.\nAnother rough estimate would be based on cortical assemblies and what is known from neural simulations. The 30*109 cells of the cortex are apparently organized into cortical columns, each containing around 100 neurons and representing a single “state” or unit of cognition. That gives us around 108 columns. These are sparsely interconnected, with around 1% connections, giving a total number of 1014 column-column links. Given around 100 spikes per second, we get 1016 spike-events per second along these links. If each spike-event requires one instruction to handle (not unreasonable on dedicated hardware), the we get 1010 MIPS.\nA small factual error in the section started by the discussion of insect nervous systems: only synapses seem to be trimmed away, not whole neurons.\nThe estimate of one byte per synapse seems to be borne out by modelling experience. This would give the brain an approximate capacity of 1014 bytes.\nThe quantum computer section curiously lacks a reference to the bulk spin resonance results of Gershenfeld and Chuang (N. Gershenfeld and I. Chuang, Science, 275, pp. 350-356, 1997, http://physics.www.media.mit.edu/publications/papers/97.01.science.pdf, http://physics.www.media.mit.edu/publications/papers/97.09.itp.pdf).11 Editor’s note: Those two papers are (Chuang et al. 1998; Gershenfeld and Chuang 1997)\nWhat about special purpose hardware for neural modelling?\nHow much do algorithms matter?\n\n\nMoravec replies: 18/3/98\nI just use MIPS as a convenient common notation. My numbers for recent machines are obtained from various benchmark suites, Spec-92 (1 Spec92 = 1 MIPS), Spec-95 (1 Spec95 = 40 MIPS), MacBench (1 MacBench = 0.66 MIPS). These exercise cache, calculation, memory and various other aspects in a fair way, so are pretty representative of performance most programs get. They usually agree within a factor better than two,\nThe retina is untypical, and I would use some other structure if I had convincing computational analogs. But I think volume (or mass: it’s all water) is a far better extrapolator than neuron count. Evolution can just as easily choose two small neurons as one twice as large. The cost in metabolism and materials is the same. So I would expect brain structures to maximize for effective computation per volume, not per neuron. After all, one neuron with ten thousand synapses might be the computational match of 50 neurons with 50 synapses each.\nThe retina gives one measure of computation per volume. Because vision is so important, and because the retina must be transparently thin, the retina may be evolutionarily more perfected, i.e. computationally dense, than the average neural structure. If so, my stimate for the brain is an overestimate.\nOn the other hand, not having the transparency constraint may have given evolution more degrees of freedom for optimization in the rest of the brain, and thus allowed for a better solution there. In that case, my brain computation number would be an underestimate.\nUnlike the reviewer, I don’t think counting neural switching events is a very useful way to measure computation, because structural constraints can make a huge difference in the relation between primitive switching and end-result computation. And it is the final computation that matters, not the fuss in doing it.\nIn a forthcoming book, [Robot, Being: from mere machine to transcendent mind. Oxford Univ. Press.] I discuss why control and learning organizations more situation-specialized than neural nets seem to be much superior for robots. The brain is stuck with shallow masses of very slow components, which limit the possible solutions, but robots, with fast serial processors are not! But I think that discussion is beyond the scope of this article.\n\n\nDan Clemmensen: 21/3/98\nDr. Moravec’s paper looks like a good overview, and is very readable. The paper provides strong support for its thesis that for human-level AI, “required hardware will be available in cheap machines in the 2020s”. However, the paper makes the assumption that the “cheap machine” must be a general-purpose computer.\nThere are strong historical precedents for this assumption. In general, specialized hardware as been more trouble than it’s worth. In his response to Robin Hanson, Dr. Moravec relates some of his personal experiences of this with the CM2 and CM5. In general, by the time the specialized tools and techniques to employ a specialized computer are available, the general-purpose computer technology will have advanced to the same performance level. The History of computing is littered with additional examples.\nHowever, it’s not clear to me that this rule will hold in all cases. The paper actually gives part of a counterexample with Deep Blue. Deep Blue combines a powerful general-purpose multiprocessing computer with specialized chess-position evaluators to achieve human-level chess-playing ability. This same model may be generalizable by taking advantage of software-reprogrammable logic devices such as those made by XILINX or Altera. I would guess that a chess-position evaluator could be programmed into a single Altera Flex 10K part that costs $20 today. Deep Blue has 256 evaluators. If my guess is correct, an engineer can create a machine with Deep Blue’s capability by adding less than $6000 of hardware to a high-end desktop. The difference is that the result is general-purpose, because the evaluators are reprogrammable. Note that there is no reason for all the evaluators to run the same program. Since this architecture is based on general-purpose parts that are widely used in commercial designs, it will become smaller, faster, cheaper and more powerful at roughly the same rate at general-purpose computers.\nDr. Hugo de Garis, http://www.hip.atr.co.jp/~degaris, is attempting to build an AI using XILINX parts to simulate neurons. This is not quite what I had in mind. I’m thinking more in terms of a model with a single-threaded program that uses the evaluators to perform incredibly powerful, highly specialized instructions.\nDr Moravec estimates that Deep Blue can apply about 3 million MIPS in its problem domain. I’m guessing that we can build an equivalent, affordable machine today that is not restricted to the chess domain. If so, the hardware for human-level AI is available today, and human-level AI is “merely” a small matter of programming.\nDan Clemmensen Systems Architect, Netrix Corporation Dan@Clemmensen.ShireNet.Com http://www.ShireNet.Com/~dgc\n\n\nMoravec replies: 21/3/98\nDan’s comments regarding the occasional benefit of specialized hardware well taken. Other strong, if not AI, examples are the DSPs in modems and the graphics hardware now augmenting processors.\nBut even there the advantage may be fleeting. Motorola is dropping out of the hardware modem business, because the functionality can now be achieved more flexibly with software, in multi-hundred-MIPS computers to whom audio bandwidth is a minor distraction.\nI look forward to seeing how effectively programmable logic contributes to AI.\n\n\nDan Clemmensen follows up: 21/3/98\nThis is one of the continuing oscillations in our industry. A task that’s only achievable with specialized hardware becomes amenable to cost-effective solution with the main CPU instead. But the hardware guys then find other more complex tasks for special hardware. For example, modems for phone lines are now “soft” as you say, but ADSL modems and 100BaseT transceivers need special hardware, as evidenced in Motorola’s newer QUICC devices.\nAnother interesting oscillation is the relative costs of processing versus communications bandwidth.\nWhat I was proposing is really the next generation of the “customizable instruction set” idea. In the early ’70s, this called “microprogramming”. I just pointed out that we could adapt the Deep Blue concept by permitting programmable evaluators. Interestingly, the skills and tools used by “hardware designers” to program XILINX or FLEX 10K parts are more akin to software skills than to traditional logic-gate design skills. A programmer can read and understand a VHDL manual more quickly than a “traditional EE” can, unless the EE is also a programmer.\n\n\nPaul Hughes: 22/3/98\nI found Hans paper to be overall highly consistent, logical and well thought out.\nHowever, there has alway been an estimate made by Hans regarding the capacity of the human brain that doesn’t take into consideration the elaborate cytoskeletal structure of microtubules within each neuronal cell. The shear complexity within these cyto-networks combined with their influence on neurotransmitter activity would seem to shed a great deal of doubt on Hans and many other neuro-computer scientists continued treatment of individual neurons as simple on/off transistors. For a brief tutorial on these networks see:\nhttp://www.reed.edu/~rsavage/microtubules.html\nand its larger section integrated with quantum cosmology at:\nhttp://galaxy.cau.edu/tsmith/ManyWorlds.html\nI would like to know why Hans and others continue to treat the neuron as a simple on/off switch in the face of the evidence of a greater intra-neuronal complexity?\nIf the cytoskeletal/microtubule networks do turn out to play a vital role in neuro-computation, then Hans will have to revise his estimates of human-level MIPS/Memory by at least 2 orders of magnitude.\n\n\nMoravec replies: 23/3/98\nMy brain: computer comparison doesn’t start with neurons but with the whole retina as a functional unit. It assumes the computational performance of the postage stamp of retina is representative of other neural tissue.\nThere is extensive evidence that the human retina accommodates to a huge light variations, and detects edges and motion at a million locations at about ten hertz. Similar performance can be obtained from a 1,000 MIPS computer processing a million pixel image high definition TV image.\nUnless the retina provides results no one yet suspects, this approach would seem to weigh the contribution from all relevant mechanisms.\n\n\nWlodzislaw Duch replies to Hughes’ comment: 16/4/98\nI (in agreement with almost all other physicists) do not see any evidence that microtubules have anything to do with computations in the brain. Cognitive computational neuroscience makes great progress modeling real phenomena and the behavior of neurons in vitro is very well described by the Hudgkin-Huxley model (not by the on-off switch). Experiments and simulations go hand in hand here. Microtubules are in all eucariotic cells, so why are our minds so dependent on our brains? Please explain first the paramecium behavior (I am sure that it is due to the biochemical reactions, not quantum computing), prove it experimentally and than talk about human brains.\nWłodzisław Duch\nComputational Intelligence Lab, Nicholas Copernicus University\nduch@phys.uni.torun.pl\nhttp://www.phys.uni.torun.pl/~duch\n\n\nWlodzislaw Duch comments on Moravec’s article: 16/4/98\nIn the article by Hans Moravec I was surprised to see so much emphasis on computer speed. The 5th generation AI project has emphasized how many LIPS (logical inferences per second) their machines will provide and not much came out of it. Will the classical problems of AI be solved by speed/memory?\nThese problems include representation of complex knowledge structures, creation of huge knowledge bases to simulate common reason (addressed by the CYC project), representation of time and space, behavioral based intelligence (addressed by the Cog project) and the importance of the embodiment. Speed is just one necessary condition, proper structure of intelligent machines is the other. It is relatively simple in chess (graphs and heuristic search) but already much more complex in the game of go and even more complex in the everyday thinking.\nSimulations of the human brain by neural networks, the second route to AI, are still at quite primitive stage. Either we simulate the spiking neurons well, and than are able to take a few of them, or we have very crude approximation and may take more neurons, but than they are not able to do the same job. Neurodynamical processes are very complex and we still struggle with a few degrees of freedom. Not to mention that many connections between brain structures and functions of these structures are still unknown.\nThis is not to deny that AI will make progress, but to stress that estimations of speed/memory are only a small part of the story.\n\n\nMoravec replies: 16/4/98\nBut I think scale is much more important than most in the AI community thought, or many still think. Could a mouse achieve human intelligence? There is only a factor of 1,000 in brain size between mouse and man. The deficits in computer power to human brain power I estimate were a hundred million-fold.\n[In my forthcoming book (Robot, Being) there is a chapter that] outlines a strategy for four decades of robot evolution that very coarsely parallels stages in four hundred megayears of vertebrate mental evolution. The evolutionary framework is reassuring, because nature tells us there is an incremental development path along it, made of small, individually advantageous, steps. If Darwinian trial and error made those little steps, so can human technological search. Especially since we can cheat by occasionally peeking at the biological answers.\nGenerally, I see compelling evidence that availability of processing power is the pacing factor in the improving performance of the many research robots around me. For instance, in the 1980s mobile robots could not reliably cross a room, now they drive cross-country. And this with still insectlike processing power (or like a tiny chordate, if you want phylogenetic purity). Lizardlike motor-perceptual competence is the first “vertebrate” target in my speculative evolution. We’re not there yet, but I expect we will be by 2010.\n\n\nDavid Villa: 19/4/98\nOn the whole this was a very readable and interesting paper. I have one comment, though. You wrote:\nThe most powerful experimental supercomputers in 1998, composed of thousands or tens of thousands of the fastest microprocessors and costing tens of millions of dollars, can do a few million MIPS. They are within striking distance of being powerful enough to match human brainpower, but are unlikely to be applied to that end. Why tie up a rare twenty-million-dollar asset to develop one ersatz-human, when millions of inexpensive original model humans are available? Such machines are needed for high-value scientific calculations, mostly physical simulations, having no cheaper substitutes. AI research must wait for the power to become more affordable.\nI can think of at least two reasons why a twenty-million-dollar investment to reproduce human-level intelligence would be worthwhile.\n\nSimply to prove that it is possible. There are still those, even some penetrating and deep thinkers (Roger Penrose springs to mind) who doubt this. It may seem a less than noble reason for such expense, but it is not inherently different from the vastly greater sums spent verifying one theory of particle physics over another.\nIf a twenty-million-dollar investment would bring us to within striking distance of human-level intelligence, thirty or forty million dollars may take us beyond it. This done, the whole process would potentially bootstrap, ultimately leading to very cheap, very powerful super-minds - and everything their existence would imply.\n\n\n\nMoravec replies: 19/4/98\nDavid Villa asks, why not invest big bucks in supercomputers for AI?\n\n\nSimply to prove that it is possible. There are still those, even some penetrating and deep thinkers (Roger Penrose springs to mind) who doubt this. It may seem a less than noble reason for such expense, but it is not inherently different from the vastly greater sums spent verifying one theory of particle physics over another.\n\n\nAtomic physics was considered an oddball interest, with very limited support before World War II, comparable to AI now (goofball scientists splitting atoms? weird, weird). Only the atomic bomb raised its interest in the halls of power. No one, outside a small circle of irrelevant goofballs, sees anything of comparable interest imminent from AI. (Before WWII, it was chemists who got the bucks, because they had developed the gas and explosives the mattered in the last war.)\n\n\nIf a twenty-million-dollar investment would bring us to within striking distance of human-level intelligence, thirty or forty million dollars may take us beyond it. This done, the whole process would potentially bootstrap, ultimately leading to very cheap, very powerful super-minds - and everything their existence would imply.\n\n\nThe investment would have to be in the hundreds of millions of dollars at least. Buying the computer creates the need to keep it fed. There simply isn’t enough perceived need or plausibility that it would pay off. There were times when such a perception did exist. In the 1960s, AI type efforts towards automatic translation, management of nuclear war and, in the USSR, management of the economy, got huge, national interest types of funding. The gap in required power was then was so large, that even that investment didn’t bridge it. (But Strategic Air Command probably still uses some of the original SAGE equipment that was developed then)\nGiven the fast exponential increase of computer power over time, compared to the merely linear increases bought by money, I’m happy to spend my time hacking patiently towards AI around 2020 rather than campaigning for a wildly expensive crash project that might, possibly, bring it a few years sooner.\nActually, I think we may get the best of both worlds if commercial development of mass-market utility robots takes off in the next decade, as I hope (and outline in the book). Market forces will then generate investment dwarfing any government program.\n\n\nD. Lloyd Jarmusch: 7/3/99\nHans Moravec wrote\n\n“Advancing computer performance is like water slowly flooding the landscape. A half century ago it began to drown the lowlands, driving out human calculators and record clerks, but leaving most of us dry. Now the flood has reached the foothills, and our outposts there are contemplating retreat. We feel safe on our peaks, but, at the present rate, those too will be submerged within another half century. I propose (Moravec 1998) that we build Arks as that day nears, and adopt a seafaring life!” How do we build or board these Arks? Is human mind/computer interface a near term probability? How do I find out more? It seems that virtual immortality through artificial consciousness is a possibility for the future. How does one best go about achieving virtual immortality? Where is the best information on the subject?\n\nD. Lloyd Jarmusch"
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#appendix-editors-notes",
    "href": "docs/posts/1998-hans-moravec/index.html#appendix-editors-notes",
    "title": "When will computer hardware match the human brain?",
    "section": "Appendix: Editor’s notes",
    "text": "Appendix: Editor’s notes\n\nThis markdown document taken mostly verbatim from the original HTML document, the original HTML peer commentaries, and the original dataset. I converted ASCII-math into LaTeX math, fixed typos, formatted the dataset, chased down a few dead links, etc.\nThe original metadata\nJournal of Evolution and Technology. 1998. Vol. 1\n(Received Dec. 1997)\nHans Moravec\nRobotics Institute\nCarnegie Mellon University\nPittsburgh, PA 15213-3890, USA\nnet: hpm@cmu.edu\nweb: http://www.frc.ri.cmu.edu/~hpm/\n\nJournal of Evolution and Technology\nA peer-reviewed electronic journal publishing contemporary research into future science and philosophy.\nISSN 1541-0099"
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#appendix-dataset",
    "href": "docs/posts/1998-hans-moravec/index.html#appendix-dataset",
    "title": "When will computer hardware match the human brain?",
    "section": "Appendix: Dataset",
    "text": "Appendix: Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine\nYear\nCost k$\n1997 k$\nMBytes\nMIPS\nMIPS/1997 k$\n\n\n\n\nBy Hand\n1892\n8.5\n142\n9.54e-05\n1.19e-08\n8.37e-11\n\n\nOhdner\n1891\n11\n172\n1.49e-07\n3.33e-09\n1.94e-11\n\n\nSteiger Millionaire\n1900\n12\n187\n3.58e-07\n1.33e-08\n7.12e-11\n\n\nHollerith\n1908\n50\n694\n0.000286\n1.85e-08\n2.67e-11\n\n\nAnalytical Engine\n1910\n1e+03\n1.24e+04\n0.0244\n3.77e-07\n3.05e-11\n\n\nMonroe Calculator\n1911\n35\n470\n2.86e-06\n2.18e-08\n4.64e-11\n\n\nIBM Tabulator\n1919\n20\n126\n2.38e-05\n4.12e-08\n3.27e-10\n\n\nTorres Arithmometer\n1920\n25\n141\n4.77e-06\n3.58e-08\n2.53e-10\n\n\nNational-Ellis 3000\n1928\n15\n135\n4.29e-06\n7.38e-08\n5.46e-10\n\n\nBurroughs Class 16\n1929\n15\n137\n4.29e-06\n7.38e-08\n5.37e-10\n\n\nZuse-1\n1938\n10\n111\n3.05e-05\n4.24e-08\n3.82e-10\n\n\nZuse-2\n1939\n10\n113\n3.05e-05\n4.24e-07\n3.75e-09\n\n\nBTL Model 1\n1939\n50\n565\n3.81e-06\n2e-06\n3.54e-09\n\n\nZuse-3\n1941\n50\n499\n0.000244\n2.04e-06\n4.09e-09\n\n\nBTL Model 2\n1943\n50\n422\n1.19e-05\n1.03e-06\n2.44e-09\n\n\nColossus\n1943\n100\n844\n2.38e-06\n0.000224\n2.65e-07\n\n\nBTL Model 3\n1943\n200\n1.69e+03\n4.29e-05\n2.83e-06\n1.68e-09\n\n\nASCC (Mark 1)\n1944\n300\n2.52e+03\n0.000601\n2.33e-06\n9.26e-10\n\n\nZuse-4\n1945\n50\n412\n0.000244\n2.04e-06\n4.96e-09\n\n\nBTL Model 5\n1946\n500\n3.61e+03\n0.000147\n3.29e-06\n9.12e-10\n\n\nENIAC\n1946\n600\n4.33e+03\n9.54e-05\n0.00289\n6.68e-07\n\n\nHarvard Mark 2\n1947\n300\n1.77e+03\n0.000488\n6.22e-06\n3.52e-09\n\n\nIBM SSEC\n1948\n500\n2.71e+03\n4.58e-05\n0.000597\n2.2e-07\n\n\nEDSAC\n1949\n100\n571\n0.00214\n0.00255\n4.46e-06\n\n\nSEAC\n1950\n800\n4.4e+03\n0.00549\n0.00416\n9.44e-07\n\n\nUNIVAC I\n1951\n930\n4.59e+03\n0.00537\n0.00575\n1.25e-06\n\n\nZuse-5\n1952\n100\n508\n0.000244\n9.33e-06\n1.84e-08\n\n\nIBM CPC\n1952\n100\n508\n0.000154\n0.00176\n3.47e-06\n\n\nIBM 650\n1953\n200\n1.03e+03\n0.00488\n0.000966\n9.4e-07\n\n\nEDVAC\n1954\n500\n2.57e+03\n0.00537\n0.0017\n6.61e-07\n\n\nWhirlwind\n1955\n200\n1.03e+03\n0.00391\n0.0694\n6.77e-05\n\n\nLibrascope LGP-30\n1955\n30\n154\n0.0146\n0.000701\n4.56e-06\n\n\nIBM 704\n1955\n2e+03\n1.03e+04\n0.0352\n0.0536\n5.23e-06\n\n\nIBM 7090\n1959\n3e+03\n1.42e+04\n0.141\n0.326\n2.29e-05\n\n\nIBM 1620\n1960\n200\n947\n0.0122\n0.00103\n1.09e-06\n\n\nDEC PDP-1\n1960\n150\n710\n0.0176\n0.124\n0.000175\n\n\nAtlas\n1961\n5e+03\n2.38e+04\n0.0234\n1.4\n5.88e-05\n\n\nBurroughs 5000\n1962\n1e+03\n4.75e+03\n0.0254\n0.0989\n2.08e-05\n\n\nIBM 7040\n1963\n560\n2.67e+03\n0.07\n0.063\n2.36e-05\n\n\nHoneywell 1800\n1963\n1.4e+03\n6.67e+03\n0.093\n0.15\n2.25e-05\n\n\nDEC PDP-6\n1964\n300\n1.42e+03\n0.0703\n0.169\n0.000119\n\n\nCDC 6600\n1964\n5e+03\n2.37e+04\n4\n8.76\n0.000369\n\n\nIBM 1130\n1965\n50\n233\n0.0156\n0.15\n0.000644\n\n\nIBM 360/75\n1966\n5e+03\n2.25e+04\n8\n2.54\n0.000113\n\n\nIBM 360/65\n1967\n3e+03\n1.35e+04\n4\n1.24\n9.18e-05\n\n\nDEC PDP-10\n1968\n500\n2.19e+03\n0.562\n0.655\n0.000299\n\n\nCDC 7600\n1969\n1e+04\n4.23e+04\n8\n25.7\n0.000608\n\n\nDG Nova\n1969\n7.6\n32.1\n0.008\n0.117\n0.00366\n\n\nGE-635\n1970\n2e+03\n8.14e+03\n0.5\n0.649\n7.97e-05\n\n\nSDS 920\n1971\n100\n394\n0.25\n0.105\n0.000266\n\n\nIBM 360/195\n1972\n8e+03\n3.02e+04\n0.5\n17.3\n0.000573\n\n\nHoneywell 700\n1972\n12\n45.2\n0.031\n0.075\n0.00166\n\n\nPrime Computer 100\n1973\n8.5\n28.4\n0.031\n0.36\n0.0127\n\n\nIBM-370/168\n1974\n2e+03\n5.61e+03\n1\n8.88\n0.00158\n\n\nMITS Altair\n1974\n0.5\n1.4\n0.00024\n0.01\n0.00713\n\n\nDG Eclipse\n1975\n50\n129\n0.25\n0.47\n0.00366\n\n\nDEC-KL-10\n1975\n500\n1.29e+03\n4.5\n2.3\n0.00179\n\n\nDEC PDP-11/70\n1976\n150\n368\n0.125\n0.4\n0.00109\n\n\nCray-1\n1976\n1e+04\n2.45e+04\n32\n150\n0.00612\n\n\nApple II\n1977\n1.3\n3.02\n0.0039\n0.02\n0.00662\n\n\nDEC VAX 11/780\n1977\n200\n464\n8\n1\n0.00215\n\n\nTRS-80\n1977\n2\n4.64\n0.015\n0.04\n0.00861\n\n\nCommodore PET\n1977\n1.5\n3.48\n0.008\n0.06\n0.0172\n\n\nCDC IPL\n1978\n500\n1.08e+03\n1\n7.5\n0.00697\n\n\nNanodata VMX200\n1979\n300\n571\n2\n2.1\n0.00367\n\n\nTRS-80 M3\n1980\n1.2\n2\n0.015\n0.04\n0.02\n\n\nSun-1\n1980\n30\n50.1\n1\n0.484\n0.00966\n\n\nCDC Cyber-205\n1981\n9e+03\n1.39e+04\n16\n73.2\n0.00528\n\n\nVic 20\n1981\n0.279\n0.43\n0.005\n0.04\n0.0931\n\n\nIBM PC\n1982\n2.5\n3.75\n0.0469\n0.238\n0.0634\n\n\nSun-2\n1982\n20\n30\n2\n0.741\n0.0247\n\n\nCommodore 64\n1982\n0.5\n0.75\n0.0825\n0.2\n0.267\n\n\nTRS-80 M4\n1983\n1\n1.49\n0.0635\n0.2\n0.134\n\n\nVax 11/750\n1983\n50\n74.4\n4\n0.799\n0.0107\n\n\nMacintosh-128K\n1984\n2.5\n3.62\n0.125\n0.52\n0.144\n\n\nVax 11/785\n1984\n200\n290\n0.0156\n2.26\n0.0078\n\n\nCray-2\n1985\n1e+04\n1.45e+04\n1.95e+03\n824\n0.0569\n\n\nL.Edge XT-7.16\n1985\n2\n2.89\n0.25\n0.26\n0.09\n\n\nAtari 800XL\n1985\n0.85\n1.23\n0.64\n0.165\n0.134\n\n\nSun-3\n1986\n10\n14.1\n4\n2.05\n0.145\n\n\nDEC VAX 8650\n1986\n125\n176\n16\n7.71\n0.0438\n\n\nMIT XT-8\n1986\n0.5\n0.705\n0.25\n0.534\n0.758\n\n\nMac II\n1987\n3\n4.11\n2\n2.5\n0.608\n\n\nSun-4\n1987\n10\n13.7\n16\n1.87\n0.136\n\n\nMac-IIx\n1988\n9.3\n12.1\n4\n3.9\n0.322\n\n\nCompuAdd 386-16\n1988\n2.1\n2.73\n1\n2.8\n1.02\n\n\nPC Brand 386-25\n1988\n2.45\n3.2\n1\n4.3\n1.35\n\n\nMark 386\n1989\n12\n15.2\n2\n12.9\n0.849\n\n\nWang VS 10000\n1989\n510\n646\n16\n103\n0.159\n\n\nMacintosh SE30\n1989\n6.49\n8.23\n5\n3.9\n0.474\n\n\nSolbourne 5/500\n1989\n50\n63.3\n2\n25.5\n0.403\n\n\nStardent 3000\n1990\n89\n109\n32\n27.3\n0.249\n\n\nAmiga 2500/30\n1990\n4.7\n5.78\n2\n19.5\n3.37\n\n\nAcer 1200\n1990\n11\n13.5\n4\n20\n1.48\n\n\nMVME165\n1990\n4\n4.92\n4\n16.6\n3.37\n\n\nPower VEISA\n1990\n5.8\n7.13\n6\n22.1\n3.1\n\n\nDell 320LX\n1990\n2.9\n3.57\n1\n12.5\n3.5\n\n\nMac IIfx\n1990\n9.87\n12.1\n4\n10\n0.824\n\n\nAmiga 3000\n1990\n3.3\n4.06\n2\n12.5\n3.08\n\n\nVMPM868KD\n1990\n2.9\n3.57\n2\n12.5\n3.5\n\n\nStep 486/33\n1990\n10\n12.3\n80\n17.5\n1.42\n\n\nGateway-486DX2/66\n1991\n3.9\n4.66\n8\n30.9\n6.64\n\n\nACT 468/33\n1991\n3.4\n4.06\n4\n21.8\n5.37\n\n\nSlimline SP486DX\n1991\n3.6\n4.3\n4\n21.8\n5.07\n\n\nMac-Quadra-900\n1991\n3.3\n3.94\n8\n22\n5.58\n\n\nAST Bravo\n1992\n1.4\n1.62\n2\n12.9\n7.95\n\n\nIBM PS/2 55-041\n1992\n2\n2.32\n4\n10.6\n4.57\n\n\nAST Premium II\n1992\n2.8\n3.25\n4\n13.2\n4.07\n\n\nIBM PS/2 90\n1992\n9.6\n11.1\n8\n22.4\n2.01\n\n\nNEC Powermate\n1992\n4.8\n5.56\n4\n21.8\n3.92\n\n\nAberdeen Mini\n1993\n2.8\n3.15\n2\n16.2\n5.14\n\n\nIBM Valuepoint\n1993\n3.6\n4.05\n4\n26.1\n6.44\n\n\nAcer Power\n1993\n3.5\n3.94\n4\n44.5\n11.3\n\n\nAmbra Desktop\n1993\n2.4\n2.7\n2\n21.1\n7.81\n\n\nDECpc LPv\n1993\n2.9\n3.26\n4\n16.6\n5.09\n\n\nAST Pemmia\n1993\n3.6\n4.05\n2\n16.2\n4\n\n\nNEC 486SL DX2\n1994\n3.8\n4.15\n4\n31.9\n7.68\n\n\nVesa\n1994\n1.2\n1.31\n4\n20\n15.3\n\n\nAT&T System 3260\n1994\n2.5\n2.73\n8\n44\n16.1\n\n\nIBM 433/DX/Si\n1994\n1.8\n1.97\n4\n26.1\n13.3\n\n\nMicron 466 Wndsrvr\n1994\n3.6\n3.93\n16\n54.7\n13.9\n\n\nAST PremiaGXP/90\n1994\n5.8\n6.34\n16\n98.6\n15.6\n\n\nAT&T Globalyst 600\n1994\n4.8\n5.25\n16\n98.6\n18.8\n\n\nZEOS Contenda 386\n1994\n1\n1.09\n4\n20\n18.3\n\n\nGateway 2000 486\n1994\n1\n1.09\n2\n16.2\n14.8\n\n\nPowerMac 7100/66\n1994\n2.9\n3.17\n8\n100\n31.6\n\n\nPowerMac 8100/80\n1994\n4.25\n4.64\n16\n120\n25.8\n\n\nPowerMac 8500/120\n1995\n4\n4.24\n16\n180\n42.4\n\n\nPowerMac 9500/132\n1995\n5.3\n5.62\n16\n200\n35.6\n\n\nIntel Xpress/60\n1995\n2\n2.12\n8\n70\n33\n\n\nGateway P5-75\n1996\n2\n2.06\n16\n92\n44.7\n\n\nPower Tower 180e\n1996\n3.3\n3.39\n16\n300\n88.4\n\n\nPowerMac 7600/132\n1996\n3\n3.09\n16\n160\n51.8\n\n\nGateway G6-200\n1997\n2.95\n2.95\n64\n350\n119"
  },
  {
    "objectID": "sketches/index.html",
    "href": "sketches/index.html",
    "title": "Sketches",
    "section": "",
    "text": "I make my writing in a simple 3-stage process: private draft, public draft, public finished essay.\nThe private draft is a collection of works in progress and lightly edited notes, and are not generally useful. The public finished essays are good enough for general reader (“publication quality”). Private draft is private because others are unlikely to benefit from reading it, unless they are trying to impersonate the writer and enter their private world (“What is it like to be Person X?”). Published essays are public because others are likely to benefit from it without entering the outside. They are pallets of information, like encapsulated software objects. You do not need to know how they were made to use them well.\nThe public draft is an interesting intermediate case. Sometimes there are things that are not already in a good enough shape to benefit others, but not good enough to count as a finished essay. They are like as public alphas of games. You can already play with them and have some fun with it, though only in bits and pieces; level 3 is followed by level 6; one NPC has a full personality while the other is a cardboard cutout. Similarly, a public draft is something that looks like an essay, except with pieces of it missing, the other pieces in the wrong place, some sentences missing their clauses, and some paragraphs missing their conclusions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to do field-theoretic calculations\n\n\n\n\n\n\nmath\n\n\nphysics\n\n\nscaling\n\n\n\nI teach you how to do probability calculations and gigantic integrals in the ‘field-theoretic style’, done in exhaustive details. I aim for clarity, pointing out every pitfall that I have fallen into so that you don’t have to.\n\n\n\n\n\n2024-04-11\n\n\n23 min\n\n\n2024-05-05\n\n\n\n\n\n\n\n\n\n\n\n\nInformation Warfare\n\n\n\n\n\n\nfun\n\n\nwip\n\n\nevolution\n\n\nsociology\n\n\n\nA mosaic view of the info-battleground: information for anti-informative goals. Intentionally useless errors, diplomatic speech, Chinese censorship, spam campaign, stock market arbitrage, continental philosophy, etc.\n\n\n\n\n\n2023-12-16\n\n\n38 min\n\n\n2024-02-19\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Using Linux\n\n\n\n\n\n\nprogramming\n\n\n\nMy quick reference for using Linux for doing things.\n\n\n\n\n\n2022-12-10\n\n\n10 min\n\n\n2024-01-17\n\n\n\n\n\n\n\n\n\n\n\n\nA Scrapbook of Neural Network Lores\n\n\n\n\n\n\nAI\n\n\nscaling\n\n\nNN\n\n\n\nLightly curated list of stories, anecdotes, and other various bits from neural network research.\n\n\n\n\n\n2024-01-18\n\n\n14 min\n\n\n2024-01-23\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Web Design\n\n\n\n\n\n\nprogramming\n\n\n\nMy quick reference for designing content for the Internet.\n\n\n\n\n\n2023-12-10\n\n\n9 min\n\n\n2024-01-17\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sketches/posts/linux-notes/index.html",
    "href": "sketches/posts/linux-notes/index.html",
    "title": "Notes on Using Linux",
    "section": "",
    "text": "This is my quick reference for using Linux for doing things. I claim no originality. Mostly they are copy pasted from the internet and tested by me. An increasing proportion of those are produced by AI."
  },
  {
    "objectID": "sketches/posts/linux-notes/index.html#sec-koans",
    "href": "sketches/posts/linux-notes/index.html#sec-koans",
    "title": "Notes on Using Linux",
    "section": "Linux koans",
    "text": "Linux koans\nEverything is a file a file or a directory an inode. An inode (index node) is a representation of a sequence of data that the system can access. A file is just a list of inodes, and a directory is just a list of files.\nEvery command is an executable.\nEvery machine is a server. Some merely serve extremely slow machines (humans, aka “users”).\nThe \\usr is not the user. It is the UNIX System Resources.\nThe \\usr was the user, until Unix became so large that \\bin overflowed and had to put the rest of them in \\usr\\bin. This was embarrassing for all involved, so they moved user files to \\home, and pretend that \\usr stands for UNIX System Resources. (here). A mistake that only took 40 years to fix.\nIn the beginning was the command line. The command line is just a face of the shell. What was the original face of the command line before the shell was born?\nThe shell reads in a stream of letters, because the user is just another file (a streaming file, named stdin). Like all streaming files, the user is eternal and inexhaustible. The shell stands, rapt in attention, afore the user file.\nSo when does the user ever leave? The user never leaves. The shell simply kills itself when the user types exit. The shell would rather die than to face the prospect of reading the last word from the user.\nSo when does the shell ever break out of its rapt attention? Whenever it sees \\n, it is shaken out of its trance and interprets what the user has just said, in the interval bracketed between two \\ns.\nThe shell has one ear and two mouths. The ear is stdin, and the mouths are stdout and stderr. The shell has a tiny brain which is only capable of interpreting the few syntactic elements of bash scripts. Everything else it wants to do, it dutifully sends a binary message into the oracular altar of the Linux kernel, from which an oceanic voice replies the answer of the kernel."
  },
  {
    "objectID": "sketches/posts/linux-notes/index.html#sec-path",
    "href": "sketches/posts/linux-notes/index.html#sec-path",
    "title": "Notes on Using Linux",
    "section": "How to PATH",
    "text": "How to PATH\nThe PATH environment variable is a list of directories that the shell searches for commands. It is a colon-separated list of directories. For example, just about every Linux installation has a PATH variable that looks like:\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nTo add things to PATH, use the export command, like export PATH=\"$HOME/bin:$PATH\".\nenv # same as printenv\nexport VAR_NAME=\"value\"\nunset VAR_NAME\n\nexport PATH=newpath:$PATH # There is no simple way to undo this one.\n# but you can try export OLD_PATH=$PATH; ...; export PATH=$OLD_PATH\nFor Windows, use Get-ChildItem Env:."
  },
  {
    "objectID": "sketches/posts/linux-notes/index.html#how-to-directories",
    "href": "sketches/posts/linux-notes/index.html#how-to-directories",
    "title": "Notes on Using Linux",
    "section": "How to directories",
    "text": "How to directories\nBased on https://askubuntu.com/a/135679, https://serverfault.com/a/24525, man hier, Filesystem Hierarchy Standard - Wikipedia.\n\nWhere to install read-only things\n\n/bin and /sbin: binaries and superuser-binaries.\n/usr: System-wide, read-only files installed by the OS. This directory can be remotely mounted to multiple hosts safely.\n/usr/local: System-wide, read-only files installed by root. And that’s why most directory names from /usr are duplicated here.\n/opt - System-wide, read-only and bundled-up software. That is, software that does not split their files over bin, lib, share, include like well-behaved software should.\n/usr/bin and usr/sbin: They no longer exist. Just use /bin and /sbin.\n~/.local: the per-user counterpart of /usr/local, that is: software installed by (and for) each user. Like /usr, it has its own ~/.local/share, ~/.local/bin, ~/.local/lib.\n~/.local/opt: The per-user counterpart of /opt\n\nRequiem for /usr/bin and /usr/sbin: Originally, /bin and /lib are only for binaries and libraries required for booting, while /usr/bin and /usr/lib are for all the other executables and libraries. This is no longer true, as some binaries required for booting has over the years leaked into those two folders (if there is a way to make a mess, people will make it), so since Ubuntu 20.04, they no longer exist, to remove this mess.\nHow to install for local user only:\n./configure --prefix=$HOME/.local\nmake\nmake install\nHow to install for everyone: sudo ./configure && make && make install\n\n\nWhere to install read-write things\nRead-write things are typically configuration files, since they are read and written by both the user and binary executables.\n\n/etc: System-wide configuration. Typically used by the OS to decide what to do when starting up, shutting down, etc.\n~/.config: Per-user configuration files. Although because of legacy, you keep seeing nonsense like .bashrc in your home directory instead of ~/.config/.bashrc. Here rc means “run configuration”.\n\n\n\nWhere to do read-write things\n\n/home/username, or just ~: Each user typically is only able to modify their own folder here, like ~/myfile.txt.\n/tmp: If you need to create something just for the moment, then make it here. It will be deleted when the system restarts.\n\n\n\nOther things (you should not modify them)\n\n/run: runtime temporary data, representing the system’s state since the last boot. It’s used for storing process IDs, lock files, and other files that are would normally be stored in /tmp. It is basically /tmp for the machine.\n/var: Variable data. It is somewhat like /run in that both are meant to be read-written by programs, but unlike /run, data here persists over reboots. This is often used for logging information. For example, try vim /var/log/user.log\n\n\n\n\n\n\n\nWarning\n\n\n\nModifying anything below this line may brick the system. Reading is fine though.\n\n\n\n/lib: libraries. You should not handle it directly. Some libraries are added at OS installation, and others at program installation. If you have a broken installation, you might be asked to manually copy some files looking like libxxx.so here. (so stands for “shared object”.)\n/boot: Files required for booting. For example, the bootloader, the kernel, the initramfs (initial RAM file system).\n/dev: Device files. It typically looks like /dev/sda1, /dev/sda2, etc. Other than things like sda1 (for harddrive) you might notice tty1 and pty1’ which stand for “teletype” and “pseudo teletype”, respectively, but they are actually used as files to read whatever the user is typing from (the user is a file, see Section 1). There are some odd ones like:\n\n/dev/null, which is a “file” that you write to when you just want to throw something away (everything is a file, even a black hole…).\n/dev/urandom, which is a random number generator. It is preferred over /dev/random. See here.\n/dev/zero, which is a file that you can’t write to, but you can read, but it’s filled with zeros.\n\n/media: Mount removable medias, like USB drives and SD cards. For example, you can mount a USB at /media/usb1 and another one at /media/usb2. Mounting is typically done automatically by the system when you plug it in.\n/mnt: Mounts that are not so easily removable, like a hard drive, or a network drive. And unlike /media, mounting and unmounting is not automatic. On WSL, this typically has just one important thing: /mnt/c.\n/srv: Static files that are served out. /srv/http would be for static websites, /srv/ftp for an FTP server. It is usually used only on webservers, not an end-user machine like your laptop."
  },
  {
    "objectID": "sketches/posts/linux-notes/index.html#environment-variables",
    "href": "sketches/posts/linux-notes/index.html#environment-variables",
    "title": "Notes on Using Linux",
    "section": "Environment variables",
    "text": "Environment variables\n\nHow to control them\nUse echo $X to see what the current value of X is.\nFor current session, use export like export EDITOR=nano.\nFor all sessions, add to your .bashrc or .profile. If you just want to add it to the end (not recommended, as you can end up with an archeological tell), you can do one-liner like echo \"export EDITOR=nano\" &gt;&gt; ~/.bashrc.\n\n\nCommon ones\n\nPATH: path to binaries. See Section 2.\nEDITOR: default editor.\nSHELL: default shell.\nHOME: home directory.\nUSER: current user.\nPS1: current prompt (just try echo $PS1 if it doesn’t make sense).\n\nYou can change prompts in a rather arcane language. For example, try this one:\nexport PS1=\"\\[\\e]0;\\w\\a\\]\\n\\[\\e[32m\\]\\u@\\h \\[\\e[33m\\]\\w\\[\\e[0m\\]\\n\\\\$ \""
  },
  {
    "objectID": "sketches/posts/linux-notes/index.html#cron-jobs",
    "href": "sketches/posts/linux-notes/index.html#cron-jobs",
    "title": "Notes on Using Linux",
    "section": "Cron jobs",
    "text": "Cron jobs\nCron jobs are scheduled tasks that run periodically. For scheduling one-off tasks, use the at command. The name came from “Cronos”, the name of the Greek god of time. A good reference is Newbie: Intro to cron.\n\nQuick reference\nThe cron job syntax is as follows (See Crontab.guru - The cron schedule expression editor):\n# ┌───────────── minute (0–59)\n# │ ┌───────────── hour (0–23)\n# │ │ ┌───────────── day of the month (1–31)\n# │ │ │ ┌───────────── month (1–12)\n# │ │ │ │ ┌───────────── day of the week (0–6) (Sunday to Saturday)\n# │ │ │ │ │                                   \n# │ │ │ │ │\n# │ │ │ │ │\n# * * * * * &lt;command to execute&gt;\n\n* * * * * &lt;once a minute&gt;\n0 * * * * &lt;once an hour at 0-th minute&gt;\n0 0 * * * &lt;once a day at midnight&gt;\n0 0 1 * * &lt;once a month at the midnight of the 1-th day&gt;\n0 0 1 1 * &lt;once a year at the midnight of January 1&gt;\n* * * * 0 &lt;once a minute every Sunday&gt;\nCheck that cron service is running by systemctl list-unit-files --type=service | grep \"cron\"\nList cron jobs by crontab -l.\n\n\nCreating a Cron Job\nFor this example, we create a job that runs every 20 seconds.\n\nOpen Crontab: Open your crontab file by typing crontab -e in your terminal. This command opens the crontab file for the current user in the default text editor.\nWrite Cron Job: Standard cron jobs can’t run in a smaller granularity than a minute. For a once-per-20-seconds job, you’ll need to use a workaround.\nAdd the following lines to your crontab:\n\n * * * * * /path/to/script.sh\n * * * * * sleep 20; ~/cronjobs/script.sh\n * * * * * sleep 40; ~/cronjobs/script.sh\n\nScript Content: Create script.sh, have the following content:\n\n#!/bin/bash\nCRON_MESSAGE=\"Some message\"\necho \"The cron message is: $CRON_MESSAGE\"\nThen save and close the file, and chmod +x ~/cronjobs/script.sh to make it executable.\n\n\nBest Practices\n\nLocation: Store scripts in a dedicated directory, such as ~/cronjobs, for better organization.\nScript Naming: Use meaningful names for your scripts for easier identification.\nLogging: Implement logging within your scripts to capture output and errors for later review. It’s good practice to use /var/log/cron for logging.\n\n\n\nCron environment variables\nCron jobs run in a minimal environment, so any environment variable, like CRON_MESSAGE, is not accessible by the cron script. Instead, you have a few choices:\n\nPut it directly in the crontab file:\n\nCRON_MESSAGE=\"Some message\"\n* * * * * /path/to/script.sh\n\nPut it directly in the script:\n\n#!/bin/bash\nexport CRON_MESSAGE=\"Some message\"\n# rest of the script follows\n\nIf the variable is defined in an external file (like ~/.bashrc, ~/.profile, or a custom configuration file), you can source that file at the beginning of your script:\n\n#!/bin/bash\nsource /path/to/environment_variables_file\n# rest of the script follows\n\n\nTroubleshooting\n\nCheck Permissions: Ensure your script is executable and the cron daemon has the necessary permissions to run it.\nLogs: Check /var/log/cron or relevant logs for errors.\n\nIf you’re using WSL, ensure that the cron service is running since it doesn’t start by default. Use sudo service cron start. You can configure ~/.bashrc by adding the following line: sudo service cron start, but it would make you enter the password at every login.\nAlternatively, enable systemd as described at Section 6.1."
  },
  {
    "objectID": "sketches/posts/linux-notes/index.html#how-to-wsl",
    "href": "sketches/posts/linux-notes/index.html#how-to-wsl",
    "title": "Notes on Using Linux",
    "section": "How to WSL",
    "text": "How to WSL\n\ninit vs systemd\nEvery Linux starts its first process with some root process. The init is the traditional and simpler one, and systemd is more modern and advanced one.\nWSL by default starts with init instead of systemd, perhaps to save time and compute. This makes things annoying for some users. You can check by ps -p 1 -o comm and see what it returns.\nTo enable systemd, enter in your /etc/wsl.conf with:\n[boot]\nsystemd=true\nor just use cat \"[boot]\\nsystemd=true\" &gt;&gt; /etc/wsl.conf."
  },
  {
    "objectID": "sketches/posts/web-design-notes/index.html",
    "href": "sketches/posts/web-design-notes/index.html",
    "title": "Notes on Web Design",
    "section": "",
    "text": "General references:"
  },
  {
    "objectID": "sketches/posts/web-design-notes/index.html#html",
    "href": "sketches/posts/web-design-notes/index.html#html",
    "title": "Notes on Web Design",
    "section": "HTML",
    "text": "HTML\nHTML (HyperText Markup Language) is the standard markup language for creating web pages.\nHTML was created by Tim Berners-Lee in 1989. The key metaphor for HTML is the “editing markup”, as follows: Back in the old days, authors would write or typewrite their document in the exact same font, from the first word to the last word. Then the document is sent to an editor, who would edit it by marking up the words, such as drawing squiggly lines, crossing things out, changing their font size, and writing other instructions for the type-setter (which back then meant someone who would take out types from a box and set them into the right ordering for the printing press).\nSo, one should think of an HTML document as starting with a plaintext of exactly the same format, from the first to the last word, then adding marks upon it.\n\n&lt;!DOCTYPE&gt;\nThe &lt;!DOCTYPE&gt; tag is used to declare the document type. It is usually like &lt;!DOCTYPE html&gt;, although there are rare variants, where instead of html, we have html -//w3c//dtd xhtml 1.0 transitional//en http://www.w3.org/tr/xhtml1/dtd/xhtml1-transitional.dtd, html -//w3c//dtd xhtml 1.0 strict//en http://www.w3.org/tr/xhtml1/dtd/xhtml1-strict.dtd, etc.\nWell, if you don’t care much for the details, use &lt;!DOCTYPE html&gt; is always fine. If you do care, read on.\nThe xhtml thing came from early 2000s, where there was a movement to XML-everything. Instead of the poorly specified HTML, there would be XHTML, which can be checked for syntax, and compiled into an abstract syntax tree. Despite this, people just kept on using HTML and ignored XHTML.\nDespite the universal ambitions of XML, it is now in the land of old soldiers, who never die, but just fade away. * The SVG vector graphics format. * MathML, which is like LaTeX but in XML. * The RSS and Atom feeds, which… unfortunately, are also mostly legacy now. Who even use these nowadays? * The acronym AJAX, which stands for Asynchronous JavaScript and XML JSON. After looking at XML and JSON, I am quite glad this replacement happened.\n\n\nSemantic markup\nUse &lt;article&gt; to surround a block of article. There can be any number of articles in a single HTML file.\nThe id of tags must be unique within each HTML file.\n\n\nTips for fast loading\nUse &lt;img decoding=\"async\" loading=\"lazy\" ...&gt; for images, to allow the rest of the page to load before the images are loaded. To avoid Cumulative Layout Shift (CLS), I think something like\nimg {\n  max-width: 100%;\n  height: auto;\n}"
  },
  {
    "objectID": "sketches/posts/web-design-notes/index.html#punctuation",
    "href": "sketches/posts/web-design-notes/index.html#punctuation",
    "title": "Notes on Web Design",
    "section": "Punctuation",
    "text": "Punctuation\n\nComma\nThe comma separates parts with equal syntax roles.\nEqual noun-phrases.\n  I have 3 trees in my backyard: a jujube tree, a jujube tree, and a peach tree.\n\nEqual adjectives.\n  He is a strong, healthy man.\n\nAdjectives at unequal levels.\n* We stayed at an expensive, summer resort.\n  We stayed at an {expensive {summer resort}}.\nThey denote typed multisets.\nThey are typed multisets, as in:\nI have 3 trees in my backyard: a jujube tree, a jujube tree, and a peach tree.\n=&gt;\nI have {(jujube tree, 2), (peach tree, 1)} in my backyard.\n\nThey are not typed sets, because:\nI have 3 trees in my backyard: a jujube tree, a jujube tree, and a peach tree.\n=/=&gt;\nI have {jujube tree, peach tree} in my backyard.\n\nThey are not typed lists, because:\nI have 3 trees in my backyard: a jujube tree, a jujube tree, and a peach tree.\n=\nI have 3 trees in my backyard: a jujube tree, a peach tree, and a jujube tree.\nbut\n(jujube tree, jujube tree, peach tree) =/= (jujube tree, peach tree, jujube tree)\n\nThey are not untyped multisets, because:\n* I have in my backyard a jujube tree and sing.\n  Backyard = {(jujube tree, 1), (sing, 1)}\nThe “and” before the last item on the list seems to be necessary in speech, as in “Heads up! The list is about to be over!”, even though it is not necessary in writing.\nThe “Oxford comma” is a rather odd hack that allows a tiny amount of nesting. In fact, English does not have recursion, and 2 seems to be the deepest nesting we can get. According to Wikipedia, the most common arguments for/against the Oxford comma are: convention, disambiguation. As for convention, it cannot be argued with, only dealt with. As for disambiguation, I recommend using brackets.11 It seems like a lot of weird English syntax is centered around an abhorrence of brackets. Like, I understand why brackets can get annoying (as anyone can see by looking at LISP code), but there is no reason to avoid brackets when you really need to disambiguate something. For example, instead of requiring the Oxford comma, why not just put in a freaking bracket? Don’t say that “oh you can’t speak brackets” like damn you you can’t speak commas either!\nNo Oxford comma:\nTo my parents, Ayn Rand and God.\nTo my mother, Ayn Rand, and God.\n\nOxford comma:\nTo my parents, Ayn Rand, and God.\nTo my mother, Ayn Rand and God.\n\nMathematics:\nTo {my parents, Ayn Rand, God}.\nTo {my mother Ayn Rand, God}.\nTo {my mother, Ayn Rand, God}.\nFor clauses, commas are weird. There is no clear rule.\nEqual clauses.\n  He walked all the way home, and he shut the door.\n\nEqual clauses, though the second lost the pronoun.\n  I saw that she was busy, and [I] prepared to leave.\n\nDependent clause (except that-clause).\n  Because I could not stop for Death, He kindly stopped for me.\n\nNo comma between that-clause and main-clause, or vice versa.\n* We paused before a House, that seemed a swelling of the Ground\n  We paused before a House that seemed a swelling of the Ground\n* That is not dead, which can eternal lie.\n  That is not dead which can eternal lie.\nA pair of commas can replace a pair of parentheses, though there is no benefit, other than avoiding the wrath of English teachers.\nA comma, like an em-dash ---, can indicate pauses and stutters in quotations, such as “Daisy, Daisy… give me—e— your, ans, ser, do…”. This is an edge case and there are no fixed rules here. If you ever have to write something like this, use your imagination.\n\n\nQuotation\nEnglish quotation standards are extremely annoying, because while theoretically quotation is meant to be verbatim, it is anything but. I recommend verbatim quotation. It is consistent with usage in computer programming and formal logic. In particular, the English quotation method would be a syntax error in computer programming, and would also break the proofs of Gödel’s incompleteness theorems, which use the Quine quotation.\nFor nested quotations, alternate between double \" and single ' quotation marks. In other words, inside a level-\\(n\\) writing, use \" if \\(n \\equiv 0\\mod 2\\), and ' otherwise. It is context-free, and its quotient by {\" ~ '} is the Dyck language.\nHowever, if you need to pass the Turing test, then study carefully the following edge cases:\n* Did Hal say \"Good morning, Dave.\"?\n  Did Hal say, \"Good morning, Dave\"?\n\n* No, he said \"Where are you, Dave?\".\n  No, he said, \"Where are you, Dave?\"\n\n  To be perfectly exact, I heard \"Wh--- are you, Da---\".\n\n* \"cat\" is in lowercase.\n* \"Cat\" is in lowercase.\n* \"Cat\" is in uppercase.\n  \"Cat\" is capitalized.\n* In lowercase \"cat\" is.\n* \"cat\" is in lowercase, while \"CAT\" is in uppercase.\n  \"CAT\" is in uppercase, while \"cat\" is in lowercase.\n  What we cannot speak about we must pass over in silence.\n\n* \"sed\" (\"stream editor\") is a Unix utility that parses and transforms text.\n* \"Sed\" (\"stream editor\") is a Unix utility that parses and transforms text.\n  The Unix utility \"sed\" parses and transforms text.\nThe two senses of verbatim (verbatim verbal speech, verbatim written text) are inconsistent with the capitalization constraint:\nThis violates \"first letter capitalization\".\n* \"... 'Vim' stands for 'vi improved'. 'sed' stands for 'stream editor'. ...\" \n\nThis violates \"verbatim written text\".\n* \"... 'Vim' stands for 'vi improved'. 'Sed' stands for 'stream editor'. ...\"\n\nThis violates \"verbatim verbal speech\".\n* \"... 'Vim' stands for 'vi improved', whereas 'sed' stands for 'stream editor'. ...\"\n\nThis violates the constraint of grammar.\n* \"... 'Vim' stands for 'vi improved', 'sed' stands for 'stream editor'. ...\"\n\nThis works, but only by a dubious insertion.\n  \"... 'Vim' stands for 'vi improved'[, whereas] 'sed' stands for 'stream editor'. ...\"\n\nThis works, but just barely.\n  \"... 'Vim' stands for 'vi improved'; 'sed' stands for 'stream editor'. ...\"\n\nIt would not work if the two sentences are not \"closely related\".\n* \"... The Unix coreutils were written mostly in the 1970s; 'sed',\n       which stands for 'stream editor', was written by Lee McMahon in 1973. ...\"\n\n\nWhitespace\nFor most cases, the single whitespace works.\n (U+00A0 no-break space) says: “Do not break line here.”. Similarly for &NoBreak; (U+2060 word joiner). The opposite is &lt;wbr&gt; (U+200B zero width space), which says: “You can break line here.”.\n  (U+2007 figure space),   (U+2008 punctuation space), are only for numerical tabulations, to ensure alignment across rows.\nThe thin space and hair space are not used except by fastidious typographers.\n\n\nConnecting words\nUse - to connect two non-equal parts into a compound noun-phrase, except when one of the parts is an open compound, in which case, use --.\nUse -- to connect two equal parts into a compound noun-phrase.\n\npre-WWII, pre--World War Two, ex--Prime Minister, water-based solution, non--water-based solution, the anti-choice--anti-life debate\n\n\n\nInterruptions\nUse em-dash --- without spacing.\n\nI---no, YOU son of a---\n---and as I was saying---\n\n\n\nDeletions\nTo cut out letters or words by redaction, use the em-dash ---. Imagine that you wrote out the text normally, then you replace the section to be redacted with a single ---, and in this way you will find the right spacing. However, I recommend the black block character █, which looks just like real redactions on secret documents.\n\nThat f--- son of a b███!\n\nThe ellipsis symbol can be done either with ... or with a single … (U+2026 horizontal ellipsis). It is used with spacing on both ends, except that there is no space between it and the quotation mark.\n\n\"Consider this ... , and no more ... be said. ... and so on and so forth ...\", said Mr. ---.\n\n\n\nArithmetics\nIf you cannot use LaTeX, then you could use the hyphen - for minus sign, though it’s more correct to use −. Similarly, the letter x can work, though × is better."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "If you thought the Stroop test is hard, just wait until you have tried this.\nPlay online | Play offline"
  },
  {
    "objectID": "projects/index.html#double-stroop",
    "href": "projects/index.html#double-stroop",
    "title": "Projects",
    "section": "",
    "text": "If you thought the Stroop test is hard, just wait until you have tried this.\nPlay online | Play offline"
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#the-logistic-map-rn-on-mathbbr",
    "href": "sketches/posts/renormalization-how-to/index.html#the-logistic-map-rn-on-mathbbr",
    "title": "How to do renormalization",
    "section": "The logistic map: RN on \\(\\mathbb{R}\\)",
    "text": "The logistic map: RN on \\(\\mathbb{R}\\)\nThis section based on Wikipedia.\n\nThe logistic map\nConsider a function \\(f_r(x)=r x(1-x)\\), and we want to study what happens when we iterate the map many times. The map might fall into a fixed point, a fixed cycle, or chaos. We can see all those cases in its bifurcation diagram.\n\n\n\nThe bifurcation diagram of the logistic map. Source\n\n\nWhen the map falls into a stable fixed cycle of length \\(n\\), we would find that the graph of \\(f_r^n\\) and the graph of \\(x \\mapsto x\\) intersect at \\(n\\) points, and the slope of the graph of \\(f_r^n\\) is bounded in \\((-1, +1)\\) at those intersections.\nFor example, when \\(r=3.0\\), we find that there is only a single intersection, at which point the slope is exactly \\(+1\\), indicating that it is a stable single fixed point, but is about to undergo a bifurcation.\n\n\n\nRelationship between \\(x_{n+2}\\) and \\(x_{n}\\), as \\(r\\) increases from \\(2.7\\) to \\(3.3\\). Before the period doubling bifurcation occurs. The orbit converges to a single fixed point where the graph of \\(f_r^2\\) intersects the diagonal line. As \\(r\\) reaches \\(3\\), the intersection pitchforks into three. The middle intersection becomes unstable, but the two neighboring intersections are stable.\n\n\nAs \\(r\\) increases to beyond \\(r=3.0\\), the intersection point splits to two, which is a period doubling. For example, when \\(r=3.4\\), there are three intersection points, with the middle one unstable, and the two others stable.\n\n\n\nWhen \\(r=3.4\\), there are three intersection points, with the middle one unstable, and the two others stable. Source\n\n\nAs \\(r\\) approaches \\(r=3.45\\), another period-doubling occurs in the same way. The period-doublings occur more and more frequently, until at a certain \\(r \\approx 3.56994567\\), the period doublings become infinite, and the map becomes chaotic. This is the period-doubling route to chaos.\n\n\n\nWhen \\(r \\approx 3.56994567\\), there are infinitely many intersections, and we have arrived at chaos via the period-doubling route. Source\n\n\nSomething remarkable happens when we superimpose the graphs of \\(f_r, f_r^2, f_r^4, \\dots\\) when \\(r\\) is at the critical point \\(3.5699\\dots\\). We see that each iteration of the graph seems to resemble itself, except that it is scaled and rotated by 180 degrees. We can naturally guess that \\(f_r^\\infty\\) converges to a certain function that is infinitely jagged, such that it exactly resembles itself if scaled and rotated, that is, it is a fractal.\n\n\n\nThe graphs of \\(f_r, f_r^2, f_r^4, \\dots\\) when \\(r\\) is at the critical point.\n\n\nAs \\(r\\) approaches the critical value, we can see how the graph of \\(f_r^\\infty\\) takes on more and more details, and at the critical point, becomes a perfect fractal.\n\n\n\n\n\nThe graphs of \\(f_r, f_r^2, f_r^4, \\dots\\) as \\(r\\) approaches the critical point from below.\n\n\n\n\nUniversality\nLooking at the bifurcation diagram, we can see a region, starting just after \\(r = 3.8\\), where there is a clear “window” with period \\(3\\) bursting out of a sea of chaos. The window then bifurcates repeatedly, to stable cycles of periods \\(6, 12, 24, \\dots\\) until it all collapses back into the chaos again at around \\(r \\approx 3.8494344\\). Though this is a different place, the bifurcation diagram looks suspiciously similar to the previous case.\nNot only that, if we look at the movie of \\(f_r^\\infty\\) as \\(r\\) approaches this critical point, we again see the same jagged shape.\n\n\n\nAre we seeing some kind of universal feature of period-doubling routes to chaos? Is this a general pattern independent of the details of how exactly the logistic map is defined? What if we change to another dynamical system completely different?\nFor example, we can consider the gauss map \\(x_{n+1} = \\exp(-\\alpha x^2_n)+\\beta\\). For fixed \\(\\alpha\\), we can plot the bifurcation graph as we vary \\(\\beta\\). Though it looks different, the two bifurcation graphs have a clear resemblance. This is an instance of universality, for which we will see again and again later. If \\(f_r\\) is a family of curves with parabolic tops1, then it will bifurcate just like the logistic curve.\n1 Rigorously, we can describe it as follows. If \\(F: \\mathbb{R}^2 \\to \\mathbb{R}\\) is smooth, and for all \\(r \\in \\mathbb{R}\\), the function \\(F(r, \\cdot): \\mathbb{R}\\to \\mathbb{R}\\) has a single global maximum, at which point \\(\\partial_x^2 F(r, x) &lt; 0\\), then its bifurcation diagram looks the same as that of the logistic map, and it will have the same two scaling exponents \\(\\alpha, \\delta\\), to be calculated below.\n\n\nThe bifurcation graphs of gauss map with \\(\\alpha = 5\\) and the logistic map.\n\n\nMore to the issue at hand: Why do the two graphs look similar?\n\n\n\nThe bifurcation graph is self-similar. Source\n\n\n\n\nThe self-similarity equation\nRecall how we said that the limit of \\(f^\\infty_r\\) should be self-similar, in the sense that if we iterate it twice, then rotate and scale it by a factor, we get back the same function. That is, it should be a solution to the self-similarity equation\n\\[\nf(x) = -\\alpha f\\left(f\\left(\\frac{x}{-\\alpha} \\right)\\right)\n\\]\nIn words, if we scale up the graph for \\(f^2\\) by \\(\\alpha &gt; 0\\), then rotate by 180 degrees, we get back the graph for \\(f\\).\nBy eye-balling the curve, we see that \\(f\\) should be an even function. Also, since the \\(f^2\\) can be graphically calculated by doing the cobweb diagram with the graph of \\(f\\), it does not matter if we first scale up the graph of \\(f\\) by a factor of \\(r\\) to \\(F\\), then double it to \\(F^2\\), or if we first double it to \\(f^2\\), then scale its graph. We would get back the same thing. Thus, wolog, we can scale \\(f\\) such that \\(f(0) = 1\\).\nSo, our task is to solve the following equation:\n\\[\n\\begin{cases}\nf(x) = -\\alpha f\\left(f\\left(\\frac{x}{-\\alpha} \\right)\\right)\\\\\nf(x) = 1 - a_2 x^2 + a_4 x^4 + \\dots\n\\end{cases}\n\\]\nWe can solve the equation numerically as the fixed point. We would start with \\(f(x) = 1-x^2\\), then guess a good \\(\\alpha\\) and repeatedly apply \\(f \\mapsto -\\alpha f\\left(f\\left(\\frac{x}{-\\alpha} \\right)\\right)\\). If we picked \\(\\alpha\\) correctly, we would have gotten the right result, as shown:\n\n\n\nAt the point of chaos \\(r^*=3.5699 \\cdots\\), as we repeat the functional equation iteration \\(f(x) \\mapsto-\\alpha f(f(-x / \\alpha))\\) with \\(\\alpha=2.5029 \\ldots\\), we find that the map does converge to a limit. Source\n\n\nIf \\(\\alpha\\) is not correct, then the iterates would not converge, but would have a zooming effect that looks cool.\n\n\n\n\n\n\n\n\n\nSolving the equation at order 2\n\n\n\n\n\nAt order 2, we approximate by \\(f(x) \\approx 1 - a_2 x^2\\), and ignore all higher-order terms. This gives us two equations for two unknowns:\n\\[\n\\begin{cases}\n1-a_2 = \\frac{1}{-\\alpha} \\\\\n\\frac{2a_2^2}{\\alpha} = a_2\n\\end{cases}\n\\]\nIt has two solutions. One solution has \\(\\alpha &lt; 0\\), which we know is unphysical. The other one is\n\\[\n\\begin{cases}\n\\alpha = 1 + \\sqrt{3} \\approx 2.732 \\\\\na_2 = \\frac{1 + \\sqrt{3}}{2} \\approx 1.366\n\\end{cases}\n\\]\n\n\n\nWhat happens if we are not exactly at the fixed point, but starts slightly off? Let’s say we start with a function \\(f_0(x) = 1 - a_{2,0}x^2\\), where \\(a_{2,0} = a_2^* + \\Delta\\), where \\(a_2^*\\) is the fixed point, and \\(\\Delta\\) is small but nonzero. Here we should think of the space of possible functions. Each point in this space is a possible scaling limit, but start a bit too small and we fall into boredom, start a bit too high and we fall into chaos. Start just right, and we harvest a beautiful fractal.\nAfter one iteration, we have \\(f_1(x) = -\\alpha_0 f_0(f_0(x/(-\\alpha_0)))\\), where \\(\\alpha_0\\) was fixed by \\(f_1(0) = 1\\). This gives us\n\\[\n\\begin{cases}\n\\alpha_0 = \\frac{1}{-1+a_{2, 0}} \\\\\n\\frac{2a_{2, 0}^2}{\\alpha_0} = a_{2, 1}\n\\end{cases}\n\\]\nThat is, we have the renormalization flow equation\n\\[\n2a_{2, 0}^2(a_{2, 0}-1)= a_{2, 1}\n\\]\nWe can plot the space of all possible \\(f(x)\\) as a line, like\n\\[1-0x^2, 1-0.5 x^2, 1-x^2, 1-1.5x^2, \\dots\\]\n\n\n\nRN flow diagram of the self-similarity map at order 2.\n\n\nThis is a 1-dimensional slice of the space of all possible \\(f\\) (the space of theories). Then, the effect of repeatedly applying the self-similarity map is to iterate the map \\(a_2 \\mapsto 2a_{2}^2(a_{2}-1)\\). If we are precisely at the fixed-point \\(a_2^*\\), then we are not going anywhere, but if we are not exactly there, then since the slope of \\(a_2 \\mapsto 2a_{2}^2(a_{2}-1)\\) is \\(\\delta \\approx 5.73\\) at that point, we would get farther and farther away:\n\\[\nf_0 = 1-(a_2^* + \\Delta)x^2, \\quad f_1 = 1-(a_2^* + \\delta\\Delta)x^2, \\quad f_1 = 1-(a_2^* + \\delta^2\\Delta)x^2, \\quad \\dots\n\\]\nand after \\(\\log_\\delta(\\frac{0.1}{\\Delta})\\), we would be at roughly \\(1-(a_2^* \\pm 0.1)x^2\\), which is when we can finally notice that we are obviously no longer in the neighborhood of the fixed point anymore. If we start at \\(a_2^* + \\Delta/\\delta\\), then we can sustain the illusion for one more iteration. Similarly, if we start at \\(a_2^* + \\Delta/\\delta^n\\), then we can sustain the illusion for \\(n\\) more iterations.\nNow, thinking back to what the logistic map says, we understand what we have discovered: The graph of \\(f_{r^* - \\Delta}\\) is similar to the graph of \\(f_{r^* - \\Delta/\\delta}^2\\) scaled by \\(-\\alpha\\). If we let \\(r_1, r_2, r_3, \\dots\\) be the points at which the logistic map splits into a stable cycle of period \\(2^1, 2^2, 2^3, \\dots\\), then we have \\(r_{n} \\approx r^* - \\Delta/\\delta^{n}\\), and so we have:\n\\[\n\\frac{r^* - r_n}{r^* - r_{n+1}} \\to \\delta\n\\]\nThis is usually spoken in this way: the intervals between two bifurcations shrinks at a rate of \\(\\delta\\).\n\n\n\nThe bifurcation diagram for the logistic map. As the bifurcations approach the point of chaos, the interval between two bifurcations gets shorter and shorter, at a rate of \\(\\delta\\) per bifurcation. Source\n\n\n\\(\\delta\\) is called Feigenbaum’s first constant, and \\(\\alpha\\) is Feigenbaum’s second constant.\n\n\n\n\n\n\nSolving the equation at order 4\n\n\n\n\n\nSimilarly, we can solve the equation at order 4 by plugging in \\(f(x) \\approx 1 - a_2 x^2 + a_4 x^4\\), obtaining 3 equations for 3 variables:\n\\[\n\\begin{cases}\n1-a_2+a_4 = \\frac{1}{-\\alpha} \\\\\n\\frac{2a_2^2 - 4a_2a_4}{\\alpha} = a_2 \\\\\n\\frac{a_4(4a_4+6a_2^2) - a_2(2a_4 + a_2^2)}{-\\alpha^2} = a_4\n\\end{cases}\n\\]\nTo solve this numerically, first guess a solution from the previous one, \\(\\alpha \\approx 2.732, a_2 \\approx 1.366\\), then plug into the first equation to get \\(a_4 \\approx 0\\). Then, standard numerical root-finding gives\n\\[\n\\begin{cases}\n\\alpha \\approx 2.534 \\\\\na_2 \\approx 1.522 \\\\\na_4 \\approx 0.128\n\\end{cases}\n\\]\n\n\n\nWe can also make the same argument using a flow in theory-space, except now we are doing it over a 2-dimensional slice of it. The flow map is\n\\[\nF(a_2, a_4) = \\left(\n   (2a_2^2 - 4a_2a_4)(-1+a_2 - a_4), -(a_4(4a_4+6a_2^2) - a_2(2a_4 + a_2^2))(-1+a_2 - a_4)^3\n\\right)\n\\]\nAt the fixed-point \\((a_2, a_4) = (1.522, 0.128)\\), the Jacobian matrix is \\[\n\\nabla F = \\begin{bmatrix}\n6.0506 & -6.2524 \\\\\n1.2621 & -1.6909\n\\end{bmatrix}\n\\]\nThis matrix has eigenvalues of \\(4.843, -0.483\\), so it is a saddle point, with \\(\\delta = 4.843\\). The flow and the eigenvectors \\((0.982, 0.190), (0.691, 0.723)\\) are plotted below.\n\n\n\nRN flow diagram of the self-similarity map at order 4. The two eigenvector directions at the fixed point are plotted as dashed lines.\n\n\nIn summary:\n\nThe solution to the self-similarity equation, at increasingly high orders of approximation.\n\n\n\nOrder 2\n4\n\\(\\infty\\)\n\n\n\n\n\\(a_2\\)\n1.366\n1.522\n1.530\n\n\n\\(a_4\\)\n\n0.128\n0.105\n\n\n\\(\\alpha\\)\n2.732\n2.534\n2.503\n\n\n\\(\\delta\\)\n5.73\n4.843\n4.669\n\n\n\n\n\nLessons\nEven in this tiny problem, we can already draw several lessons, which will appear again and again in RN:\n\nWe assume a function is self-similar, and calculate from there.\nSelf-similarity is a transform on a function (or “theory”).\nWe often need to use a “fudge factor” like \\(\\alpha\\) to make sure that the transformed function does not collapse to zero, for trivial reasons.\nIf we repeatedly apply the self-similarity transform on a function, we would obtain a scaling limit, a perfectly self-similar object – a fractal.\nIn the space of all possible theories, the self-similarity transform creates a flow-field in the theory space. The interesting fixed-points of the flow-field are its saddle points.\nThe largest eigenvalue of the saddle point describes what happens when you are close to the saddle point, but not quite there.\nBravely calculate using the cheapest approximation you can think of. It often gets you within 50% of the right answer.\nBut if you want accuracy, you can always use a computer and calculate many orders higher."
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#the-ising-model-rn-on-a-lattice",
    "href": "sketches/posts/renormalization-how-to/index.html#the-ising-model-rn-on-a-lattice",
    "title": "How to Renormalization",
    "section": "The Ising model: RN on a lattice",
    "text": "The Ising model: RN on a lattice\n\n\n\n\nWhen an Ising model is at the critical temperature \\(T_c\\), coarse-graining would not end up with all-black or all-white, but would have the same amount of details no matter how much we coarse-grain. Figure 13 from (Sethna 2007).\n\n\nThis is an example of real space RN. Real space RN is a garden of tricks,\n(Yang 2005)\n\nThe Ising model in \\(\\mathbb{Z}\\)\nSo far, we have been doing it for the Ising model on \\(\\mathbb{Z}\\). But it’s clear that we can also do it for two \\(\\mathbb{Z}\\) put side-by-side like a ladder. Each “rung” of the ladder can be thought of as one big atom, with 4 possible states: up-up, up-down, down-up, down-down. We can then do the same calculation as the previous case, except that instead of a \\(2\\times 2\\) matrix, we have a \\(4 \\times 4\\) matrix.\nFor that matter, we can do it for any finite number of those \\(\\mathbb{Z}\\) put together. We can then imagine doing that for such ladders with widths \\(2, 3, 4, 5, 6, \\dots\\), then discover a pattern, and take the limit. If this works, we would solve the Ising model on \\(\\mathbb{Z}^2\\).\nArduous as it sounds, this is exactly how Lars Onsager arrived at his solution of the Ising model in \\(\\mathbb{Z}^2\\). He calculated up to ladders with width 6, diagonalizing matrices of size \\(64\\times 64\\) in the process, then he guessed the general pattern and proceeded from there, emboldened by the guess. As Chen-Ning Yang reported:\n\nIn 1944, L. Onsager produced, quite unexpectedly, an exact evaluation of the partition function of the model in two dimensions. It was a real tour de force. I had studied his paper in Chicago in the spring of 1947, but did not understand the method, which was very, very complicated, with many algebraic somersaults….\nIn March, 1965… I asked him how it came about that he took all those complicated algebraic steps in his paper of 1944. He said he had a lot of time during the war, so he began to diagonalize the transfer matrix, which had already been discussed by E. Montroll and by H. A. Kramers and G. H. Wannier. He started with a \\(2 \\times \\infty\\), then a \\(3 \\times \\infty\\), then a \\(4 \\times \\infty\\) lattice. He then went on to a \\(5 \\times \\infty\\) lattice, for which the transfer matrix is \\(32 \\times 32\\) in size. Such a matrix is quite large, but the experience he had gained with the smaller matrices came in handy, and he was able, after some time, to find all 32 of the eigenvalues. He proceeded then to the \\(6 \\times \\infty\\) case, and eventually diagonalized the \\(64 \\times 64\\) matrix, finding that all the eigenvalues were of the form \\(e^{\\pm \\gamma_1 \\pm \\gamma_2 \\pm \\gamma_3 \\pm \\gamma_4 \\pm \\gamma_5 \\pm \\gamma_6}\\). That led to the concept that the algebra of the problem was a product algebra, and hence the manipulations in his paper. (Yang 2005, 11–13)\n\nI also want to quote this from the same pages, because it describes accurately what it feels like to do real-space RN:\n\na long calculation, the longest in my career. Full of local, tactical tricks, the calculation proceeded by twists and turns. There were many obstructions. But always, after a few days, a new trick was somehow found that pointed to a new path. The trouble was that I soon felt I was in a maze and was not sure whether in fact, after so many turns, I was anywhere nearer the goal than when I began. This kind of strategic overview was very depressing, and several times I almost gave up. But each time something drew me back, usually a new tactical trick that brightened the scene, even though only locally. Finally, after about six months of work off and on, all the pieces suddenly fitted together, producing miraculous cancellations, and I was staring at the amazingly simple final result [spontaneous magnetization of the Ising model] (Yang 2005, 12)\n\n\n\nKadanoff decimation, take 0\nThis section based on (Simkin and Roychowdhury 2011, sec. 10), which contains an extensive bibliography.\nThe problem: given a hexagonal grid, you make \\(p\\) of them black and the rest white. What is the critical \\(p\\) where you get a percolation (infinitely big black island)?\nRenormalization by the triangles, as shown. After one iteration, the lattice length increases from \\(l\\) to \\(\\sqrt 3 l\\) , and the renormalized occupation probability changes from \\(p\\) to \\(p^3 + 3p^2(1-p) = p^2(3-2p)\\).\n\n\n\nKadanoff decimation on a triangular lattice. (Simkin and Roychowdhury 2011, fig. 1)\n\n\nThe equilibrium point is \\(p=1/2\\). This is the percolation probability. Let the reduced probability be \\(\\bar p = p-1/2\\). We find that one iteration of the RN flow makes \\(\\bar p_{n+1} = \\frac 32 \\bar p - 2 \\bar p^3\\) which is \\(\\approx \\frac 32 \\bar p_n\\) for small values of \\(\\bar p\\).\n\n\n\nRN flow for Kadanoff decimation on a triangular lattice, on a 1-dimensional slice.\n\n\nSuppose we start with \\(\\bar p_0\\) , and we perform \\(N\\) repeats of RN to reach some constant \\(\\Delta \\bar p\\) (for example, 0.001), then \\[N = \\frac{\\ln \\Delta \\bar p - \\ln \\bar p_0}{\\ln \\frac 32}\\]\nduring which time, the lattice length has increased by \\[3^{\\frac 12 N} \\propto \\bar p_0^{-\\frac{\\ln 3}{2\\ln \\frac 32}} = \\bar p_0^{-1.36}\\]\nSince at constant \\(\\Delta \\bar p\\), the lattice has a certain fixed look-and-feel with a certain characteristic size for its clusters, we find that the characteristic length of its clusters is \\(\\propto (p-1/2)^{-1.36}\\). The actual exponent (named \\(\\nu\\)) is exactly \\(1\\). Not bad for such a cheap calculation!\n\n\nKadanoff decimation, take 1\n\n\n\nFigure source\n\n\n\n\nKadanoff decimation, take 2\n(Maris and Kadanoff 1978)\n\nThis paper attracted much favorable notice since, beyond obtaining all the scaling properties, it seemed to lay out a direct route to the actual calculation of critical properties. On closer examination, however, the implied program seemed – as I will explain briefly – to run rapidly into insuperable difficulties and interest faded. In retrospect, however, Kadanoff’s scaling picture embodied important features eventually seen to be basic to Wilson’s conception of the full renormalization group.\n(Fisher 1998)\n\n\n\nMigdal bond-moving\n\n\n\nFigure source\n\n\nWith that simple idea I somehow got within 0.23% of the exact answer."
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#field-theory-rn-on-mathbbrd-where-dto-infty",
    "href": "sketches/posts/renormalization-how-to/index.html#field-theory-rn-on-mathbbrd-where-dto-infty",
    "title": "How to Renormalization",
    "section": "Field theory: RN on \\(\\mathbb{R}^d\\) where \\(d\\to \\infty\\)",
    "text": "Field theory: RN on \\(\\mathbb{R}^d\\) where \\(d\\to \\infty\\)"
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#wilsons-nobel-prize-rn-in-fourier-space",
    "href": "sketches/posts/renormalization-how-to/index.html#wilsons-nobel-prize-rn-in-fourier-space",
    "title": "How to Renormalization",
    "section": "Wilson’s Nobel Prize: RN in Fourier space",
    "text": "Wilson’s Nobel Prize: RN in Fourier space\nKenneth Wilson was awarded the 1982 Nobel Prize in Physics for his work on phase transitions"
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#reprise-what-is-renormalization",
    "href": "sketches/posts/renormalization-how-to/index.html#reprise-what-is-renormalization",
    "title": "How to Renormalization",
    "section": "Reprise: What is renormalization?",
    "text": "Reprise: What is renormalization?\n\nUniversality\nIn the early 20th century, material scientists noticed the remarkable phenomenon of “corresponding states”.\n\n\nSociophysics\nA koan\n\n“The details don’t matter.” said them triumphantly as they declared their independence from biophysics.\n“‘the details don’t matter.’” said them mockingly as they declared their insurrection against sociophysics.\n\nExplanation:\nThe traditional approach of historians, going back to the days of “kings and battles”, is to run to personality theory and the individual acts when confronted by a problem in sociology or economics! One establishes the individual actors, makes some (hopefully) sensible approximations of their personality makeups and then proceeds to attempt to explain for events, actions and so on. However, for truly complicated systems in what, these days, is much better called “sociophysics”, this is a hopeless task; furthermore, in many ways it is not even a very sensible one! The modern attitude is, rather, that the task of the theorist is to understand what is going on and to elucidate which are the crucial features of the problem. If one had a perfect archive of exactly what every person thought and said during the start of WWI, one would still have no understanding of why it started!\nMore is different (Anderson 1972)"
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html",
    "href": "sketches/posts/renormalization-how-to/index.html",
    "title": "How to do renormalization",
    "section": "",
    "text": "Renormalization is not group theory. The name “renormalization group theory” is truly terrible. To an applied physicist, the name “group theory” is abstract and inspires fear and uncertainty. To a mathematician, the name “group theory” is plain wrong, because you can always undo a group-action, but you can never undo a coarse-graining."
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#what-is-renormalization",
    "href": "sketches/posts/renormalization-how-to/index.html#what-is-renormalization",
    "title": "How to do renormalization",
    "section": "",
    "text": "Renormalization is not group theory. The name “renormalization group theory” is truly terrible. To an applied physicist, the name “group theory” is abstract and inspires fear and uncertainty. To a mathematician, the name “group theory” is plain wrong, because you can always undo a group-action, but you can never undo a coarse-graining."
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#wilsons-nobel-prize-rn-flow-in-theory-space",
    "href": "sketches/posts/renormalization-how-to/index.html#wilsons-nobel-prize-rn-flow-in-theory-space",
    "title": "How to Renormalization",
    "section": "Wilson’s Nobel Prize: RN flow in theory-space",
    "text": "Wilson’s Nobel Prize: RN flow in theory-space\nKenneth Wilson was awarded the 1982 Nobel Prize in Physics for his work on phase transitions\nThe modern perspective is the perspective of (Fisher 1998)\n\n\n\n(Fisher 1998, fig. 4)\n\n\n\n\n\n(Fisher 1998, fig. 5)"
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#the-physics-of-phase-transition",
    "href": "sketches/posts/renormalization-how-to/index.html#the-physics-of-phase-transition",
    "title": "How to Renormalization",
    "section": "The physics of phase transition",
    "text": "The physics of phase transition\nCritical opalescence, boiling,\n\nDroplets inside droplets\nJust below the critical temperature, the system has a free choice to make between being a liquid at high density or a vapor at low density. Both choices are equally good. But since liquid wants to be in contact with liquid and vapor with vapor, the system must decide.\nA natural fluctuation produces a droplet of the wrong phase. This droplet secretes other material of the same density, and it grows larger and larger. It stops growing when the cost in available—i.e. free-energy for making the droplet of wrong phase becomes comparable with kT. Since near the critical point it cost very little free energy per unit volume to make the wrong phase, the droplet can grow very large.\nThe coherence length \\(\\xi\\) is a size of a typical droplet. As the critical point is approached, the free energy cost of making a droplet goes to zero, and the size of a typical droplet \\(\\xi\\), goes to infinity.\nHowever, as criticality is approached, the difference in magnetization between the two different phases gets smaller and smaller. Hence the energetic cost per unit area of producing a region of the wrong phase approaches zero. For this reason, the area of a droplet and its radius both can get very, very large. Critical phenomena are connected with large-scale but weak fluctuations in the magnetization.\nSo far, our picture of critical fluctuations is like that in Fig. 1.3. Droplets with spin down of all sizes up to a maximum size \\(\\xi\\) appear near the critical point. However, this picture is incomplete. Each fluctuating region is also a nearly-critical system. Fluctuations appear within the droplets.\nAs you approach \\(T_c = 2.269\\dots\\) from above, you notice that little droplets seem to condense out of a hot grey gas. Define reduced temperature as \\(t = T/T_c - 1\\). So that critical point is \\(t=0\\). When t is 0.1, there are small droplets. When t = 0.05, the droplets grow larger, but! If you zoom out by a factor of x (you can measure it experimentally by running two simulations and try to match them by eye, or by taking screenshots and match them with an image frequency analyzer), they look the same.\nSo, this means that spatially zooming out by x is equivalent to increasing the reduced temperature by 2.\nIt is a similar thing for \\(t &lt; 0\\). At \\(t = -1\\), the entire field freezes into one color. As t increases, small droplets appear… There is another scaling law. By renormalization theory, the two scaling laws have the same exponent.\nRenormalization is doing a zooming of the system. We start with the full system, then zoom it to describe it in a similar way that loses some details (and gains some details). This gives us another system. Repeated renormalization then is moving from one system to another in the space of possible systems.\nThis is called “renormalization flow in the space of Hamiltonians”.\nThe fixed points of the flow are then critical systems. You apply the RN and get the same thing. This is a fractal, because zooming in you get the same thing. So it also has a power law, \\(1/f^a\\) noise, and other things that fractals have.\n\nBig whirls have little whirls that feed on their velocity,\nand little whirls have lesser whirls and so on to viscosity.\n\n\n\nVideos and interactives\nComplexity Explorables | I sing well-tempered\nIsing model\nThe Renormalisation Group - YouTube"
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#physical-intuitions",
    "href": "sketches/posts/renormalization-how-to/index.html#physical-intuitions",
    "title": "How to Renormalization",
    "section": "Physical intuitions",
    "text": "Physical intuitions\n\nThe view from symmetries\n\n\n\nphysical system\nsite types\n\n\n\n\n\nuniaxial magnet\nup / down\n\n\n\nfluid\nhas atom / no atom\n\n\n\nbrass crystal\nzinc / copper\n\n\n\nsimple lattice field theory\nhas particle / no particle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd\nn\nTheoretical Model (Ising Model)\nPhysical System\nOrder Parameter\n\n\n\n\n2\n1\nTwo dimensions\nAdsorbed films\nSurface density\n\n\n\n2\nXY model in two dimensions\nHelium-4 films\nAmplitude of superfluid phase\n\n\n\n3\nHeisenberg model in two dimensions\n\nMagnetization\n\n\n&gt;2\n∞\n“Spherical” model\nNone\n\n\n\n3\n0\nSelf-avoiding random walk\nConformation of long-chain polymers\nDensity of chain ends\n\n\n\n1\nIsing model in three dimensions\nUniaxial ferromagnet\nMagnetization\n\n\n\n\n\nFluid near a critical point\nDensity difference between phases\n\n\n\n\n\nMixture of liquids near consolute point\nConcentration difference\n\n\n\n\n\nAlloy near order-disorder transition\nConcentration difference\n\n\n\n2\nXY model in three dimensions\nPlanar ferromagnet\nMagnetization\n\n\n\n\n\nHelium 4 near superfluid transition\nAmplitude of superfluid phase\n\n\n\n3\nHeisenberg model in three dimensions\nIsotropic ferromagnet\nMagnetization\n\n\n≤4\n-2\n\nNone\n\n\n\n\n32\nQuantum chromodynamics\nQuarks bound in protons, neutrons, etc.\n\n\n\n\nTable reproduced from (Wilson 1979).\n\n\n\\(d=1\\): Droplets inside droplets\nCritical opalescence, boiling,\n\n\n\nHigh-resolution reprint of (Guggenheim 1945, fig. 2) in (Herbut 2007, 13).\n\n\n\n\n\n\n(Kadanoff 1999,)\n\n\nJust below the critical temperature, the system has a free choice to make between being a liquid at high density or a vapor at low density. Both choices are equally good. But since liquid wants to be in contact with liquid and vapor with vapor, the system must decide.\nA natural fluctuation produces a droplet of the wrong phase. This droplet secretes other material of the same density, and it grows larger and larger. It stops growing when the cost in available—i.e. free-energy for making the droplet of wrong phase becomes comparable with kT. Since near the critical point it cost very little free energy per unit volume to make the wrong phase, the droplet can grow very large.\nThe coherence length \\(\\xi\\) is a size of a typical droplet. As the critical point is approached, the free energy cost of making a droplet goes to zero, and the size of a typical droplet \\(\\xi\\), goes to infinity.\nHowever, as criticality is approached, the difference in magnetization between the two different phases gets smaller and smaller. Hence the energetic cost per unit area of producing a region of the wrong phase approaches zero. For this reason, the area of a droplet and its radius both can get very, very large. Critical phenomena are connected with large-scale but weak fluctuations in the magnetization.\nSo far, our picture of critical fluctuations is like that in Fig. 1.3. Droplets with spin down of all sizes up to a maximum size \\(\\xi\\) appear near the critical point. However, this picture is incomplete. Each fluctuating region is also a nearly-critical system. Fluctuations appear within the droplets.\nAs you approach \\(T_c = 2.269\\dots\\) from above, you notice that little droplets seem to condense out of a hot grey gas. Define reduced temperature as \\(t = T/T_c - 1\\). So that critical point is \\(t=0\\). When t is 0.1, there are small droplets. When t = 0.05, the droplets grow larger, but! If you zoom out by a factor of x (you can measure it experimentally by running two simulations and try to match them by eye, or by taking screenshots and match them with an image frequency analyzer), they look the same.\nSo, this means that spatially zooming out by x is equivalent to increasing the reduced temperature by 2.\nIt is a similar thing for \\(t &lt; 0\\). At \\(t = -1\\), the entire field freezes into one color. As t increases, small droplets appear… There is another scaling law. By renormalization theory, the two scaling laws have the same exponent.\nRenormalization is doing a zooming of the system. We start with the full system, then zoom it to describe it in a similar way that loses some details (and gains some details). This gives us another system. Repeated renormalization then is moving from one system to another in the space of possible systems.\nThis is called “renormalization flow in the space of Hamiltonians”.\nThe fixed points of the flow are then critical systems. You apply the RN and get the same thing. This is a fractal, because zooming in you get the same thing. So it also has a power law, \\(1/f^a\\) noise, and other things that fractals have.\n\nBig whirls have little whirls that feed on their velocity,\nand little whirls have lesser whirls and so on to viscosity.\n\n\n\nVideos and interactives\nComplexity Explorables | I sing well-tempered\nIsing model\nThe Renormalisation Group - YouTube"
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#a-bag-of-intuitions",
    "href": "sketches/posts/renormalization-how-to/index.html#a-bag-of-intuitions",
    "title": "How to do renormalization",
    "section": "A bag of intuitions",
    "text": "A bag of intuitions\n\nPower laws are born of two exponential parents\nWhy is it that, at a critical point, we have power laws? One intuition is that when at a critical point, two exponentials are matched exactly – an exponentially decaying interaction strength and an exponentially increasing number of interaction paths – and a power law is born in their collision.\nConsider the Ising model on the plane. Fix an origin \\(0\\) , and we ask, how strong is the correlation between the origin \\(0\\) and a point that is at distance \\((n, n)\\) away from the origin, where \\(n\\) is large?\nWell, the two points are correlated because they are connected by chains of spins. The more chains there are, the stronger the correlation, but the longer each chain is, the weaker the correlation. Since the correlation along each chain is exponentially weak, we can crudely pretend that all the correlations can be added.9 As a good approximation, we consider only the shortest chains, of length \\(2n\\). By Stirling approximation, there are \\({2n \\choose n} \\sim \\frac{4^{n}}{\\sqrt{n\\pi }}\\) such chains. We can think of spin at origin as \\(x_{(0,0)} + z_1 + z_2 + \\cdots\\) and the spin at \\((n, n)\\) as \\(x_{(n,n)} + z_1 + z_2 + \\cdots\\) , where \\(z_1, z_2,...\\) are random variables that are responsible for creating the correlations between the two spins along each chain.\n9 Every biologist knows, intuitively, that weak correlations are added. This is why, for instance, we can predict height accurately by a simple linear sum of the genes correlated with height, ignoring pairwise, triple-wise, and higher-order interactions. It is a common pattern in biology where \\(\\text{many genes} \\xrightarrow{\\text{development}} \\mathbb{R}\\), where \\(\\mathbb{R}\\) stands for a real-valued trait like probability of diabetes, then it is pretty close to a linear map.Now, each chain is contributes a weak correlation that decays exponentially with distance. We can assume the chains do not interact. Along each chain, we have a 1D Ising model. The covariance between two neighboring spins is\n\\[Cov(s_0, s_1) = E[s_0s_1] - \\underbrace{E[s_0]E[s_1]}_{\\text{=0}} = Pr(s_0 = s_1 ) - Pr(s_0 \\neq s_1 ) = \\tanh(\\beta J)\\]\nNow, we need a trick.10 If you think a bit, you would see that whether \\(s_0 = s_1\\) is independent of whether \\(s_1 = s_2\\). Thus,\n10 If you don’t like the trick, then you can use the transfer matrix method. We have no use for the transfer matrix, so we don’t do it.\\[Cov(s_0, s_2) = E[s_0 s_2] = E[(s_0 s_1) (s_1 s_2)] = Cov(s_0, s_1) Cov(s_1, s_2) = \\tanh(\\beta J)^2\\]\nAnd since the chain has length \\(2n\\), the correlation contributed by the chain is \\(\\tanh^{2n}(\\beta J)\\). The total correlation is\n\\[\\sim \\frac{4^{n}}{\\sqrt{n\\pi }} \\tanh^{2n}(\\beta J)\\]\nThe two terms are exactly balanced when \\(\\beta J = \\tanh^{-1}(1/2) = 0.549\\cdots\\). In fact, the exact result is \\(\\beta J = 0.44\\cdots\\) , so our crude estimate is only 25% too high.\nNow, right at the critical point, the correlation is \\(\\sim (n\\pi)^{-1/2}\\) , so we see that when the exponential decay in correlation is exactly matched with the exponential growth in possible paths, the remaining polynomial decay comes to the fore. Notice that we have also estimated one of the Ising critical exponents: \\(\\nu = 1/2\\). The actual answer is \\(1\\).\nSimilarly, with \\[\\binom{kn}{n, \\cdots n}\\sim \\frac{k^{kn}}{n^{\\frac{k-1}2}}\\frac{k^{1/2}}{(2\\pi)^{\\frac{k-1}2}}\\]\nwe can estimate that the Ising model in \\(\\mathbb{Z}^k\\) has critical \\(\\beta J \\approx \\tanh^{-1}(1/k)\\) and critical exponent \\(\\nu = \\frac{k-1}2\\). The actual \\(\\nu\\) is \\(1/2\\) for all dimensions \\(\\geq 4\\) (derived by “mean field theory”, which we are not going to discuss).\n\nStevens’ power law\nAs a side note, this “two exponents lead to a power law” is known in psychophysics as Stevens’ power law (Stevens 1970).\nConsider the case where the brain needs to respond to the presence of a stimulus (e.g., a sound, a smell, etc.) with intensity \\(I\\). The response intensity (such as in the height of jumping, or a verbal report, or wincing of the face) is \\(R\\). Stevens found that for many kinds of stimulus, \\(R \\propto I^k\\) for some exponent \\(k\\) that depends on the type of stimulus and response.\nStevens conjectured that the number of neurons firing \\(N\\) is proportional to the log of intensity of stimulus \\(I\\), and that \\(N\\) is also proportional to the log of intensity of response \\(R\\). Thus, we have\n\\[\nk_I \\ln I = N = k_R \\ln R\n\\]\nfor two constants \\(k_I, k_R\\), which implies that \\(R = I^{k_I/k_R}\\), a power law. In this way, a small number of neurons can allow us to perceive and react to a wide range of stimuli intensities – for example, the physical brightness between noon and a starlit moonless night is more than \\(10^{8}\\), and yet the optical nerve, with only \\(10^{6}\\) neurons (Evangelou and Alrawashdeh 2016), can comfortably accommodate them both.\n\nAt the Ciba Symposium in 1966, there was a general discussion on the topic “Linearity of transmission along the perceptual pathway”. In that discussion, and elsewhere at the symposium, Sir John Eccles turned forceful attention to the question of whether the sense organ could adequately account for the nonlinearity in the coupling between stimulus and sensation, leaving the central nervous system with the task of performing only linear transformations. He observed that “there is no great impediment to the idea that… the transfer functions across the synaptic mechanism are approximately linear.” To which Professor Mountcastle added, “The interesting point for me here is the great importance that we must now place upon the transducer process itself, at the periphery.”\n(Stevens 1970)\n\n\n\n\nRN as a journey in the space of possible theories\nSo far, we have seen again and again the common refrain of\nThe space of possible theories is defined by the symmetry of the physical system, and the Renormalization Group (RG) flow defines a journey in this space.\n\n\n\n(Fisher 1998, fig. 4)\n\n\nConsider a generic RG flow in a generic space of theories. diagram for a system with two coupling constants \\(K\\) and \\(y\\). Each point in the diagram represents a theory, and the arrows indicate the direction of the RG flow. The fixed points, where the arrows converge, correspond to theories that are scale-invariant.\n\n\n\n(Fisher 1998, fig. 5)\n\n\nThe above figure shows the RG flow for the Ising model, a simple model of ferromagnetism. The fixed points correspond to the paramagnetic phase (\\(K = 0\\)) and the ferromagnetic phase (\\(K = K_c\\)). The critical point, where the two phases meet, is at \\(K = K_c\\).\nThe RG flow provides a way to understand the behavior of a system at different length scales. As we zoom out, the system flows towards a fixed point. The fixed point describes the long-distance behavior of the system.\nFor example, in the Ising model, as we zoom out, the system flows towards either the paramagnetic or the ferromagnetic fixed point, depending on the initial value of \\(K\\). If \\(K &lt; K_c\\), the system flows towards the paramagnetic fixed point, and the spins become disordered at long distances. If \\(K &gt; K_c\\), the system flows towards the ferromagnetic fixed point, and the spins become ordered at long distances.\nWhat defines the space of possible theories? The symmetry of the physical system.\n\n\nSymmetries determine the shape of theory space\nThe Ising model on \\(\\mathbb{Z}^2\\) is not a single theory, but an entire infinite-dimensional space of possible theories. Each Ising model .\nAs a particular example, with \\(n=1\\), boiling water is just a magnet, where vapor is just up-spin and liquid is just down-spin.\n\nExamples of \\(d=3, n=1\\) systems.\n\n\nphysical system\nsite types\n\n\n\n\n\nuniaxial magnet\nup / down\n\n\n\nfluid\nhas atom / no atom\n\n\n\nbrass crystal\nzinc / copper\n\n\n\nsimple lattice field theory\nhas particle / no particle\n\n\n\n\n\nTable of theory-spaces with their corresponding symmetries. Reproduced from (Wilson 1979).\n\n\n\n\n\n\n\n\n\nd\nn\nTheoretical Model\nPhysical Systesm\nOrder Parameter\n\n\n\n\n2\n1\nTwo dimensions\nAdsorbed films\nSurface density\n\n\n\n2\nXY model in two dimensions\nHelium-4 films\nAmplitude of superfluid phase\n\n\n\n3\nHeisenberg model in two dimensions\n\nMagnetization\n\n\n&gt;2\n∞\n“Spherical” model\nNone\n\n\n\n3\n0\nSelf-avoiding random walk\nConformation of long-chain polymers\nDensity of chain ends\n\n\n\n1\nIsing model in three dimensions\nUniaxial ferromagnet\nMagnetization\n\n\n\n\n\nFluid near a critical point\nDensity difference between phases\n\n\n\n\n\nMixture of liquids near consolute point\nConcentration difference\n\n\n\n\n\nAlloy near order-disorder transition\nConcentration difference\n\n\n\n2\nXY model in three dimensions\nPlanar ferromagnet\nMagnetization\n\n\n\n\n\nHelium 4 near superfluid transition\nAmplitude of superfluid phase\n\n\n\n3\nHeisenberg model in three dimensions\nIsotropic ferromagnet\nMagnetization\n\n\n≤4\n-2\n\nNone\n\n\n\n\n32\nQuantum chromodynamics\nQuarks bound in protons, neutrons, etc.\n\n\n\n\n\n\nDroplets inside droplets\n\nBig whirls have little whirls that feed on their velocity,\nand little whirls have lesser whirls and so on to viscosity.\n\nI especially recommend playing with the interactive Ising model by Evgeny Demidov while reading this section.\nWe can interpret an Ising model over \\(\\mathbb{Z}^3\\) as a mixture of liquid and gaseous water. Each site can either be in a liquid state or in a vapor state. We start below the critical temperature, so that the bonding strength \\(J\\) is just slightly larger than the critical \\(J_c\\). The system must make a free choice to make between being a liquid ocean with tiny islands of liquid or a vapor ocean with tiny islands of liquid. Both choices are equally good in our toy model. Since liquid wants to be in contact with liquid and vapor with vapor, the system must decide. For the sake of intuition, we say that we have an ocean of liquid with tiny droplets of vapor inside.\nNow we increase the temperature towards the critical temperature. As we get hotter, thermal fluctuations become larger. In the ocean of liquid, a thermal fluctuation produces a droplet of vapor. This droplet secretes other material of the same density, and it grows larger and larger.\n\n\n\nVapor droplets inside an ocean of liquid. (Kadanoff 1999a, 298)\n\n\nHowever, as criticality is approached, the energetic cost in making a fluctuation approaches zero. The energetic penalty in creating droplets gets smaller and smaller. So, the droplets can grow large, until it becomes infinite right at the critical point.\nFurthermore, each droplet itself is a nearly-critical system, so fluctuations appear within the droplets, in a delicate fractal.\n\n\n\nDroplets inside dropltes inside droplets… (Kadanoff 1999a, 299)\n\n\n\n\nMore is different\nLike Wilson, Philip Anderson is a physicist who has won a Nobel Prize for his work on surprising things that happen when many particles interact. His Nobel Prize was awarded for his work on magnetism and “Anderson localization”. Consider a pure piece of metal. An electron wave can vibrate freely across it from side to side, like a wave on a perfectly uniform infinite ocean. Now if we dope the metal with impurity, like planting wavebreakers in the ocean, then as the amount of impurity increases, an electron wave would suddenly become trapped, and the metal would become an insulator.\nIn a famous paper (Anderson 1972), he interprets many-body physics like a philosopher (like what I’ve been doing in this whole section). He called the “constructionist hypothesis” the view that science divides neatly into fundamental laws and applications of those. He countered this view by proposing that scale and complexity create distinct stages in nature. Each stage necessitates new laws, concepts, and generalizations, and requires just as much ingenuity as the other stages. For example, psychology is not merely applied biology, biology is not simply applied chemistry, and condensed matter physics is not only “device engineering”.\nAnderson proposed that new stages appear because of “broken symmetries”. Consider the sugar molecule. Though the sugar molecule is governed by quantum mechanics, which does not distinguish left from right, in our world we mostly only see sugar in one chirality. Sometime in the distant past, the symmetry was broken, and we are living in the consequences of its history.\nMore concretely, consider cars driving on a road. Why do we drive on the right instead of the left? Left or right, it’s better (“lower energy”) if everyone agrees. If people disagree, then it’s chaos (“higher energy”), so in the past, a coin was flipped, and we are here. In a parallel universe, we are driving on the left instead of the right. This is clear in the many-worlds version of quantum mechanics:\n\\[\n\\ket{\\Psi_{\\text{world}}} = \\frac{1}{\\sqrt{2}} \\ket{\\Psi_{\\text{world where we drive on the left}}} + \\frac{1}{\\sqrt{2}} \\ket{\\Psi_{\\text{world where we drive on the right}}}\n\\]\nThe state of the entire multiverse \\(\\ket{\\Psi_{\\text{world}}}\\) is symmetric, but any observer has to fall into one of the sub-states, where the symmetry no longer holds.\nHowever, Anderson pointedly did not explain what allows the stages to have laws largely independent of each other. Suppose that some symmetries of the quantum-mechanical level break on the biochemistry level, but why would the biochemistry level still have simple laws? Why these biochemical laws, and not other? Furthermore, how can the laws stay different? Why are quantum mechanical laws and the everyday physics of balls and cars so different, without the laws “bleeding into each other”?\n\n\nMesophysics: Why are things interesting between the large and the small?\nThe space of all theories is big, but most of it is rather uninteresting. Consider the humble Ising model on \\(\\mathbb{Z}^2\\). Too much interaction and you get a block of spins all pointing in one direction. Too little interaction and you get a gas of spins pointing noisily in all directions. Only right at the critical point do we get interesting behavior in all possible scales. Not only that, the critical point is very delicate.\nIf you take an Ising model at \\(J\\) that is just a bit above \\(J_c\\) and zoom out, then by the RN flow equation, the effective \\(J\\) would keep increasing, and it becomes more and more uniform until it’s a perfect shade of black/white at \\(J = +\\infty\\). Conversely, if you start with \\(J\\) slightly below \\(J_c\\) and keep zooming out, \\(J\\) would approach \\(0\\) and everything would become a uniform shade of gray, with the spins pointing up and down with no regard for any other spin. Balanced on a knife’s edge is \\(J = J_c\\), where there is interesting behavior at all levels of zooming.\n\n\n\n\n\nZooming in and out of the Ising model. Video by Douglas Ashton, taken from The Renormalisation Group | dougmet-dot-net.\n\n\nBut what keeps \\(J = J_c\\)?\nWhy is it that we are surprised by the quantum mechanics in the microscopic world? Because daily life in the mesoscopic world does not betray its origin from the microscopic world. The details has been renormalized away. But if that’s the case, how come that our world is neither a homogeneous block of spins all pointing up, nor a hot mess of spins unrelated to every other spin? Why is the mesoscopic world interesting?\nLook around you. The world is interesting, with power laws, fractal patterns, and details at all scales. You never see a pencil standing on its end without some hand keeping it there. What keeps the mesoscopic world in its critical place?\nOne answer is that most of the interestingness did not come from criticality. However, if there exists some criticality in the mesoscopic world, and there does not seem to be an intelligent agent keeping the criticality there, then we have a mystery. This is the question that launched a thousand papers, including the famed self-organized criticality paper (Bak, Tang, and Wiesenfeld 1987), itself launching a thousand papers. The idea is typically illustrated by the forest fire model.\nConsider the standard percolation model on a square grid. Each point might be occupied (a “tree” grows there) or unoccupied (empty plot of land). Randomly, lightning falls, and if it hits a tree, the tree catches on fire and the fire spreads to any neighboring trees. The process ends when there are no more burning trees.\nAs the proportion \\(p\\) of a site being occupied (“tree density”) changes, we see a phase transition. For low \\(p &lt; p_c\\), there is no percolation, the fire quickly dies out. For high \\(p &gt; p_c\\), there is percolation, and so the fire spreads across the entire grid. The process automatically balances the system at the critical value \\(p_c\\), the system is poised between these two regimes, and the burnt-out patches can be of any size, following a power-law distribution. In this way, the delicate critical point in the standard percolation model has been transformed to a robust critical point.\n\nNature shows an amazing variety of length scales: There is the Hubble radius of the universe, \\(10^{10}\\) light years or so and the radius of our own solar system, \\(10^{11}\\) meters roughly, and us-two meters perhaps, and an atom \\(-10^{-10}\\) meters in radius, and a proton \\(10^{-16}\\) meters, and the characteristic length of quantum gravity-which involves another factor of about \\(10^{20}\\).\nHow these vastly different lengths arise is a very interesting and fundamental question…. However, we can think about how one describes the region between these lengths. If one is looking at scales between two fundamental lengths, there are no nearby characteristic lengths. Similarly in critical phenomena, whenever one looks [at] any event which takes place between the scale of the lattice constant [the spacing between molecules or spins] and the much larger scale of the coherence length, one is in a situation in which there are no nearby characteristic lengths. (Kadanoff 1999b, 251)\n\nIn a footnote, Kadanoff suggested that self-organized criticality might explain forms of stable criticality we see around us. Kadanoff wrote the words in 1999, near the end of the 1980s–90s chaos theory boom. Since then, the self-organized criticality theory has fallen by the wayside, like fractal compression, mirrorshades, and large-folio printed pages of fractal art. The modern evaluation is that while it was oversold by Per Bak, and certainly could not explain all critical phenomena (Bak 1996), it can explain some of them (Watkins et al. 2016, sec. 8).\n\n\nUniversality: The details don’t matter\nOr: Why elephants don’t know, or don’t need to know, quantum mechanics.\nIn the early 20th century, material scientists noticed the remarkable phenomenon of “corresponding states”. As first reported by (Guggenheim 1945), scientists measured the density \\(\\rho\\) of many substances near their liquid-vapor critical point. They fixed pressure and increased temperature \\(T\\) around the critical temperature \\(T_c\\). As they plotted the relation between \\(T\\) and \\(\\rho\\), rescaled by critical temperature \\(T_c\\) and density at critical point \\(\\rho_c\\), remarkably, all the substances fell onto a single curve.\n\n\n\nHigh-resolution reprint of (Guggenheim 1945, fig. 2) in (Herbut 2007, 13).\n\n\nDespite the diversity of intermolecular forces, the phase transition behavior of a wide variety of gasses follows a universal pattern.\nWe see this pattern over and over again in physics. To give another example, imagine water flowing through a porous rock, oil through sand, or electricity through a random network of resistors. These systems, seemingly completely unrelated, share the same underlying mathematical structure and exhibit universal behavior near the percolation threshold. This universality arises because the details of the microscopic interactions become irrelevant at larger scales, and the system’s behavior is governed by the collective properties of its components.\nIt is often noted that quantum mechanics is unintuitive, because the mesoscopic physics is so different from the microscopic physics, so we had evolved to intuit the mesoscopic world, and not the microscopic world. But why is it possible to ignore quantum mechanics? Elephants don’t need to know, or don’t care about, the Standard Model of particle physics.11 When they walk, they push dirt around. When water flows, it pushes water around. The long-distance behavior of a system does not depend on the details of the short-distance interactions. The benefit of RN is that it saves us from the effort of understanding the microscopic details. On the flip-side, the details do matter if we are far from the critical point.\n11 Nor do they play chess. (Brooks 1990)This is an overarching theme in renormalization theory, which might be called the universality hypothesis:\n\nAll phase transition problems can be divided into a small number of different classes depending upon the dimensionality of the system and the symmetries of the order state. Within each class, all phase transitions have identical behaviour in the critical region, only the names of the variables are changed. (Kadanoff 1999a, 273)\n\nFurthermore, universality justifies our toy models as “serious play”. When I was first learning statistical mechanics, I was terribly confused by it. “They can’t be serious – do they think I’m stupid? How could the Ising model possibly be relevant to real magnets?” But the universality hypothesis justifies Ising models as serious toy models. Even if they are completely different from real magnets when far from the critical point, as we approach the critical point, their behavior becomes exactly equal at the limit.\nRN theory explains why certain features matter when we zoom out and others are washed away. It explains why we have universal classes according to symmetry and dimension and not, say, the precise shape of the atomic force laws. It justifies the use of toy models instead of microscopically accurate models. Specifically, one starts with some microscopic accurate model, then apply renormalization and show that the details fall away and we end up at the same destination as the toy model. (Batterman 2019)\n\nWe may well try to simplify the nature of a model to the point where it represents a ‘mere caricature’ of reality. But notice that when one looks at a good political cartoon one can recognize the various characters even though the artist has portrayed them with but a few strokes. … [A] good theoretical model of a complex system should be like a good caricature: it should emphasize those features which are most important and should downplay the inessential details. (Fisher 1983, 47) (Batterman 2019)\n\n\n\nSociophysics\n\n“The details don’t matter.” said them triumphantly as they declared their independence from biophysics.\n“‘the details don’t matter.’” said them mockingly as they declared their insurrection against sociophysics.\n\nYuxi’s Comment:12 The traditional approach of historians, going back to the days of “kings and battles”, is to run to personality theory and the individual acts, when confronted by a problem in sociology or economics! One establishes the individual actors, makes some (hopefully) sensible approximations of their personality makeups, and then attempts to explain for events, actions and so on. However, for truly complicated systems in what, these days, is much better called “sociophysics”, this is a hopeless task; furthermore, the questions it answers are not even the right ones. The modern theorist would rather explain how the stable features of the problem are invariant under different assumptions of what individual people do, and arise from features of their interactions. Indeed, if one had a perfect archive of exactly what every person thought and said during the start of WWI, one would still have no understanding of why it started!\n12 Inspired by Tolstoy’s War and Peace.\n\nThe movement of humanity, arising as it does from innumerable arbitrary human wills, is continuous. To understand the laws of this continuous movement is the aim of history. But to arrive at these laws, resulting from the sum of all those human wills, man’s mind postulates arbitrary and disconnected units. The first method of history is to take an arbitrarily selected series of continuous events and examine it apart from others, though there is and can be no beginning to any event, for one event always flows uninterruptedly from another.\nThe second method is to consider the actions of some one man – a king or a commander – as equivalent to the sum of many individual wills; whereas the sum of individual wills is never expressed by the activity of a single historic personage.\nHistorical science in its endeavor to draw nearer to truth continually takes smaller and smaller units for examination. But however small the units it takes, we feel that to take any unit disconnected from others, or to assume a beginning of any phenomenon, or to say that the will of many men is expressed by the actions of any one historic personage, is in itself false.\n\nMumon’s Comment: Historians search for the king’s motive and the general’s ambition. They build castles of personality, moats of individual acts, yet understand nothing of the war. The great black spider of history weaves between the players so densely, that nothing is lost after it has digested all of them."
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#universality",
    "href": "sketches/posts/renormalization-how-to/index.html#universality",
    "title": "How to Renormalization",
    "section": "Universality",
    "text": "Universality\n(battermanUniversalityRGExplanations2019?)\n\nThe theory of critical points in phase transitions is the paradigm example of universality.\nmacroscale - mesoscale - microscale\n\nmacroscale: thermodynamics, continuum, no fluctuation.\nmesoscale: fluctuations in aggregates of atomic scale properties may be important, order parameters code for some feature of the microstructure.\nmicroscale: particles and quantum mechanics.\n\nCriticality is just the space between two characteristic scales. The real question is why do we have characteristic scales that are so wide apart?\n\nBIB. Statistical physics: statics, dynamics and renormalization (Kadanoff 2000), p. 251\nshows an amazing variety of length scales: There is the Hubble radius of the universe, \\(10^{10}\\) light years or so and the radius of our own solar system, \\(10^{11}\\) meters roughly, and us-two meters perhaps, and an atom \\(-10^{-10}\\) meters in radius, and a proton \\(10^{-16}\\) meters, and the characteristic length of quantum gravity-which involves another factor of about \\(10^{20}\\). How these vastly different lengths arise is a very interesting and fundamental question…. However, we can think about how one describes the region between these lengths. If one is looking at scales between two fundamental lengths, there are no nearby characteristic lengths. Similarly in critical phenomena, whenever one looks [at] any event which takes place between the scale of the lattice constant [the spacing between molecules or spins] and the much larger scale of the coherence length, one is in a situation in which there are no nearby characteristic lengths.\n\n\nNear the critical point, fluctuations are dominant and average values for the order parameters lose their meaning. Equilibrium statistical mechanics is unable to describe the critical behavior because there are fluctuations at all length scales.\nThe universality hypothesis\n\nBIB. From Order To Chaos II, Essays: Critical, Chaotic And Otherwise, p 273.\nAll phase transition problems can be divided into a small number of different classes depending upon the dimensionality of the system and the symmetries of the order state. Within each class, all phase transitions have identical behaviour in the critical region, only the names of the variables are changed.\n\nUniversality hypothesis implies that if we have two Hamiltonians \\(H_1, H_2\\) where one can be smoothly perturbed to the other by \\(H_\\lambda := (1-\\lambda)H_1 + \\lambda H_2\\) , and renormalization works on \\(\\lambda\\) , then we can run the same scaling law method with \\(\\lambda\\) too, and so the scaling laws for \\(H_1, H_2\\) are the same.\n\nBIB. From Order To Chaos II, Essays: Critical, Chaotic And Otherwise, p 275-276\n\nCorrelation length \\(\\xi\\) is the largest fluctuation droplet that can form before it contains so much excess free energy that equilibrium thermodynamics asserts itself.\n\nBIB. From Order To Chaos II, Essays: Critical, Chaotic And Otherwise, p 274.\nJust below the critical temperature, the system has a free choice to make between being a liquid at high density or a vapor at low density. Both choices are equally good. But since liquid wants to be in contact with liquid and vapor with vapor, the system must decide.\nA natural fluctuation produces a droplet of the wrong phase. This droplet secretes other material of the same density, and it grows larger and larger. It stops growing when the cost in available—i.e. free-energy for making the droplet of wrong phase becomes comparable with kT. Since near the critical point it cost very little free energy per unit volume to make the wrong phase, the droplet can grow very large.\nThe coherence length \\(\\xi\\) is a size of a typical droplet. As the critical point is approached, the free energy cost of making a droplet goes to zero, and the size of a typical droplet \\(\\xi\\) , goes to infinity.\n\nBoiling water is just a magnet. Vapor is just up-spin and liquid is just down-spin.\n\nBIB. From Order To Chaos II, Essays: Critical, Chaotic And Otherwise, pp 297–299\nDroplet picture of correlation behaviour\nHowever, as criticality is approached, the difference in magnetization between the two different phases gets smaller and smaller. Hence the energetic cost per unit area of producing a region of the wrong phase approaches zero. For this reason, the area of a droplet and its radius both can get very, very large. Critical phenomena are connected with large-scale but weak fluctuations in the magnetization.\nSo far, our picture of critical fluctuations is like that in Fig. 1.3. Droplets with spin down of all sizes up to a maximum size \\(\\xi\\) appear near the critical point. However, this picture is incomplete. Each fluctuating region is also a nearly-critical system. Fluctuations appear within the droplets.\n{:height 469, :width 477}\n{:height 285, :width 458}\n\n\nThe two questions to be explained. Both can be explained by renormalization.\n\nWhy are the phase transitions stable under perturbation of the microscopic Hamiltonians of the systems?\n\nThe universality class is the basin of attraction of the fixed point. Scaling exponents and other universal properties are determined by the flow in a neighborhood of the fixed point.\n\nWhy are the universality classes dependent upon the symmetry of the order parameter and the dimension?\n\nThe renormalization flow depends on the symmetry and the dimension.\n\n\nRenormalization is not trivial.\n\nWe may well try to simplify the nature of a model to the point where it represents a ‘mere caricature’ of reality. But notice that when one looks at a good political cartoon one can recognize the various characters even though the artist has portrayed them with but a few strokes. … [A] good theoretical model of a complex system should be like a good caricature: it should emphasize those features which are most important and should downplay the inessential details.” (Fisher 1983, p. 47) Lange claims that here Fisher “seems to be supporting a ‘common features account’: the minimal model, despite being a caricature of some actual system, shares with it ‘those features which are most important’” (Lange 2015, p. 299, fn. 3).\nBut renormalization theory explains more: it explains why those features matter and others don’t. It explains why we have universal classes according to symmetry and dimension and not, say, the precise shape of the atomic force laws.\n\nRenormalization theory explains the use of effective Hamiltonians and toy models instead of microscopically accurate models. Specifically, one starts with some microscopic accurate model, then apply renormalization and show that the details fall away and we end up with the toy model.\n\n\nThe view from symmetries\n\n\n\nphysical system\nsite types\n\n\n\n\n\nuniaxial magnet\nup / down\n\n\n\nfluid\nhas atom / no atom\n\n\n\nbrass crystal\nzinc / copper\n\n\n\nsimple lattice field theory\nhas particle / no particle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd\nn\nTheoretical Model (Ising Model)\nPhysical System\nOrder Parameter\n\n\n\n\n2\n1\nTwo dimensions\nAdsorbed films\nSurface density\n\n\n\n2\nXY model in two dimensions\nHelium-4 films\nAmplitude of superfluid phase\n\n\n\n3\nHeisenberg model in two dimensions\n\nMagnetization\n\n\n&gt;2\n∞\n“Spherical” model\nNone\n\n\n\n3\n0\nSelf-avoiding random walk\nConformation of long-chain polymers\nDensity of chain ends\n\n\n\n1\nIsing model in three dimensions\nUniaxial ferromagnet\nMagnetization\n\n\n\n\n\nFluid near a critical point\nDensity difference between phases\n\n\n\n\n\nMixture of liquids near consolute point\nConcentration difference\n\n\n\n\n\nAlloy near order-disorder transition\nConcentration difference\n\n\n\n2\nXY model in three dimensions\nPlanar ferromagnet\nMagnetization\n\n\n\n\n\nHelium 4 near superfluid transition\nAmplitude of superfluid phase\n\n\n\n3\nHeisenberg model in three dimensions\nIsotropic ferromagnet\nMagnetization\n\n\n≤4\n-2\n\nNone\n\n\n\n\n32\nQuantum chromodynamics\nQuarks bound in protons, neutrons, etc.\n\n\n\n\nTable reproduced from (Wilson 1979).\n\n\n\\(d=1\\): Droplets inside droplets\nCritical opalescence, boiling,\n\n\n\nHigh-resolution reprint of (Guggenheim 1945, fig. 2) in (Herbut 2007, 13).\n\n\n\n\n\n(Kadanoff 1999, 298)\n\n\n\n\n\n(Kadanoff 1999, 299)\n\n\nJust below the critical temperature, the system has a free choice to make between being a liquid at high density or a vapor at low density. Both choices are equally good. But since liquid wants to be in contact with liquid and vapor with vapor, the system must decide.\nA natural fluctuation produces a droplet of the wrong phase. This droplet secretes other material of the same density, and it grows larger and larger. It stops growing when the cost in available—i.e. free-energy for making the droplet of wrong phase becomes comparable with kT. Since near the critical point it cost very little free energy per unit volume to make the wrong phase, the droplet can grow very large.\nThe coherence length \\(\\xi\\) is a size of a typical droplet. As the critical point is approached, the free energy cost of making a droplet goes to zero, and the size of a typical droplet \\(\\xi\\), goes to infinity.\nHowever, as criticality is approached, the difference in magnetization between the two different phases gets smaller and smaller. Hence the energetic cost per unit area of producing a region of the wrong phase approaches zero. For this reason, the area of a droplet and its radius both can get very, very large. Critical phenomena are connected with large-scale but weak fluctuations in the magnetization.\nSo far, our picture of critical fluctuations is like that in Fig. 1.3. Droplets with spin down of all sizes up to a maximum size \\(\\xi\\) appear near the critical point. However, this picture is incomplete. Each fluctuating region is also a nearly-critical system. Fluctuations appear within the droplets.\nAs you approach \\(T_c = 2.269\\dots\\) from above, you notice that little droplets seem to condense out of a hot grey gas. Define reduced temperature as \\(t = T/T_c - 1\\). So that critical point is \\(t=0\\). When t is 0.1, there are small droplets. When t = 0.05, the droplets grow larger, but! If you zoom out by a factor of x (you can measure it experimentally by running two simulations and try to match them by eye, or by taking screenshots and match them with an image frequency analyzer), they look the same.\nSo, this means that spatially zooming out by x is equivalent to increasing the reduced temperature by 2.\nIt is a similar thing for \\(t &lt; 0\\). At \\(t = -1\\), the entire field freezes into one color. As t increases, small droplets appear… There is another scaling law. By renormalization theory, the two scaling laws have the same exponent.\nRenormalization is doing a zooming of the system. We start with the full system, then zoom it to describe it in a similar way that loses some details (and gains some details). This gives us another system. Repeated renormalization then is moving from one system to another in the space of possible systems.\nThis is called “renormalization flow in the space of Hamiltonians”.\nThe fixed points of the flow are then critical systems. You apply the RN and get the same thing. This is a fractal, because zooming in you get the same thing. So it also has a power law, \\(1/f^a\\) noise, and other things that fractals have.\n\nBig whirls have little whirls that feed on their velocity,\nand little whirls have lesser whirls and so on to viscosity.\n\n\n\nVideos and interactives\nComplexity Explorables | I sing well-tempered\nIsing model\nThe Renormalisation Group - YouTube"
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#appendix",
    "href": "sketches/posts/renormalization-how-to/index.html#appendix",
    "title": "How to do renormalization",
    "section": "Appendix",
    "text": "Appendix\n\nNeural networks\nIf only I understood what this is saying, then I could write this section. (Mehta and Schwab 2014)"
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#something",
    "href": "sketches/posts/renormalization-how-to/index.html#something",
    "title": "How to Renormalization",
    "section": "Something",
    "text": "Something\nA a"
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#ising-model-and-friends-rn-on-a-lattice",
    "href": "sketches/posts/renormalization-how-to/index.html#ising-model-and-friends-rn-on-a-lattice",
    "title": "How to Renormalization",
    "section": "Ising model and friends: RN on a lattice",
    "text": "Ising model and friends: RN on a lattice\n\n\n\n\nWhen an Ising model is at the critical temperature \\(T_c\\), coarse-graining would not end up with all-black or all-white, but would have the same amount of details no matter how much we coarse-grain. Figure 13 from (Sethna 2007).\n\n\nThis is an example of real space RN. Real space RN is a garden of tricks. They are useful and intuitive, but not as powerful as frequency space RN.\n\nThis paper attracted much favorable notice since, beyond obtaining all the scaling properties, it seemed to lay out a direct route to the actual calculation of critical properties. On closer examination, however, the implied program seemed – as I will explain briefly – to run rapidly into insuperable difficulties and interest faded. In retrospect, however, Kadanoff’s scaling picture embodied important features eventually seen to be basic to Wilson’s conception of the full renormalization group.\n(Fisher 1998)\n\n\nThe Ising model\nFerromagnets are weird. A single iron atom is a tiny magnet, but if the iron atoms in a piece of magnet are not oriented roughly in the same way, then the whole piece would have all its direction washed away, thus not be a magnet at all. In the atomic world, things jiggle about and tend to destroy all order, so how is it possible for a grid of atoms to stay pointing mostly in the same direction for days and weeks – an eternity on the atomic level?\nIn 1920s, Lenz had the idea of a simple atomic model for ferromagnetism. He gave the problem to his graduate student Ising, who solved it in the one-dimensional case. He was disappointed to see that there is no spontaneous magnetization, which is the key feature of ferromagnetism, and he conjectured that this is also true in higher dimensions. The model was thus neglected for a while, until Peirels (1936) showed that the two-dimensional Ising model does exhibit a phase transition, and later, Onsager solved it completely in 1944, showing several curious features. This was a starting point of modern statistical physics. See (Domb 1985) for some more historical details.\nThe Ising model is a toy model of a magnet. For simplicity, we assume all the iron atoms are locked in a lattice, and each atom can only be spin up or spin down (like a coin that can be heads or tails). Neighboring atoms want to be the same, but temperature \\(T\\) makes the atoms jiggle and be random.\nThere are many different Ising models, but the following is the most standard. We have a grid of atoms, and each atom is nothing but a point with two possible values: \\(+1\\) or \\(-1\\). Nothing more complicated than that. The energy of the system is determined by the interaction between nearest neighbors, with a lower energy when neighboring spins are aligned. That is, its energy is\n\\[\nH(s) = -J \\sum_{i, j} s_i s_{j}\n\\]\nwhere \\(J&gt;0\\) is the strength of interaction between neighboring atoms, \\(s_i\\) represents the spin at site \\(i\\), and the summation is over neighboring pairs of \\(i, j\\). For example, if we have Ising model on the integer line \\(\\mathbb{Z}\\), then \\(H = -J \\sum_{i} s_i s_{i+1}\\). Similarly, if we have an Ising model on the square grid \\(\\mathbb{Z}^2\\), then \\(H = -J \\sum_{i} (s_{i} s_{i+(1, 0)} + s_{i} s_{i+(0, 1)})\\).\nWe are not going to explain why, but in statistical mechanics, we have to calculate the following, slightly mysterious quantity \\(Z\\), called the partition function. Everything in statistical mechanics bursts forth from this one function. It is no exaggeration to say that the story of statistical mechanics is mostly about calculating \\(Z\\) at higher and higher precisions using larger and larger computers while losing more and more hair on the head. For the Ising model, \\(Z\\) is defined as\n\\[\nZ := \\sum_{s} e^{-\\beta H(s)}\n\\]\nwhere \\(\\beta = 1/T\\) is the inverse of the temperature \\(T\\). The summation is over all possible configurations of the system. For example, if the Ising model is over \\(\\{0, 1, 2, \\dots, 99\\}\\), then the summation is over \\(\\{-1, +1\\}^{100}\\).\nThe temperature turns out to be not useful during the RN calculation, so to make the notation cleaner, we absorb it into the \\(J\\) like this:\n\\[Z = \\sum_{s} e^{-\\beta H(s)}, \\quad H(s) = -J_0 \\sum_{i, j} s_i s_{j}, \\quad J_0 = J/T\\]\nAfter we finish the calculation, we would write the temperature explicitly again in order to interpret what we have found.\n\n\nKadanoff decimation, take 0\nLike ancient Roman decimation, the idea of Kadanoff decimation is to pick out some spins for elimination, leaving behind the other spins gathered around the mangled nothings of their former comrades. In the case of the 1D Ising model, decimation is exactly solvable, making it a good first problem to try.\nLet us consider the Ising model on the integer line \\(\\mathbb{Z}\\) with periodic boundary conditions. That is, we have \\(2n\\) spins \\(s_0, s_1, \\dots, s_{2n-1}\\) arranged in a circle. Now, we keep the even spins white and decimate the odd spins. How can we do that? Well, behold:\n\\[\n\\begin{aligned}\nZ &= \\sum_{s_0, s_1, \\dots, s_{2n-1}} e^{+J(s_0 s_1 + s_1s_2 + \\dots + s_{2n-1}s_0)} \\\\\n  &= \\sum_{s_0, s_1, \\dots, s_{2n-2}} \\sum_{s_1, s_3, \\dots, s_{2n-1}} e^{J(s_0 s_1 + s_1s_2 + \\dots + s_{2n-1}s_0)} \\\\\n  &\\overset{\\mathrm{hopefully}}{=} \\sum_{s_0, s_2, \\dots, s_{2n-2}} e^{J'(s_0 s_2 + s_2 s_4 + \\dots + s_{2n-2}s_0)} \\\\\n\\end{aligned}\n\\]\nwhere we have hopefully written the \\(J'\\). It is our ardent hope that there exists some \\(J'\\), somewhere in the universe, that will make the equation come true.11 Physicists call it “ansatz”, but I like to say it like this:\n\nThe author of an atrocious undertaking ought to imagine that he has already accomplished it, ought to impose upon himself a future as irrevocable as the past.\n– Yu Tsun, professor of English at Deutsch-Chinesische Hochschule.\n\n\nBecause each even spin can only reach its nearest-neighboring even spins via the odd spin between them, the partition function splits cleanly:\n\\[\nZ = \\sum_{s_0, s_2, \\dots} \\left(\\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)}\\right) \\left(\\sum_{s_1} e^{J(s_2 s_3 + s_3 s_4)}\\right) \\cdots\n\\]\nso our wish would be fulfilled if \\(\\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = e^{J's_0s_2}\\) for all 4 possible choices of \\(s_0, s_2\\). Now, by direct calculation, we see that our task is to solve the following 4 equations simultaneously:\n\n\n\n\n+\n-\n\n\n\n\n+\n\\(e^{J'} = 2 \\cosh(2J)\\)\n\\(e^{-J'} = 2\\)\n\n\n-\n\\(e^{-J'} = 2\\)\n\\(e^{J'} = 2 \\cosh(2J)\\)\n\n\n\nImmediately we see a problem: if both \\(e^{J'} = 2 \\cosh(2J)\\) and \\(e^{-J'} = 2\\) hold, then \\(1 = 4 \\cosh(2J)\\). This means that we have to use a “fudge factor” again. Does that remind you of the \\(\\alpha\\) back in the logistic map calculation? It should. Add in the fudge factor \\(k\\) with:\n\\[\nZ = k^{n} \\sum_{s_0, s_2, \\dots, s_{2n-2}}e^{J'(s_0 s_2 + s_2 s_4 + \\dots + s_{2n-2}s_0)}, \\quad \\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = ke^{J's_0s_2}\n\\]\nNow the solution is: \\(J' = \\frac 12 \\ln \\cosh(2J), k = 2\\sqrt{\\cosh(2J)}\\). Again, we see that RN flow equation\n\\[\nJ \\mapsto \\frac 12 \\ln \\cosh(2J)\n\\]\nbut this time, this RN flow has only a single stable fixed point at \\(J = 0\\). Not only that, since \\(\\frac 12 \\ln \\cosh(2J) \\approx J^2\\) if \\(J\\) is small, if we decimate for \\(n\\) times, we would end up with \\(J' \\sim J^{2^n}\\). What does this mean?\nSuppose we start with a magnet with interaction strength \\(J\\), then after we zoom out for \\(n\\) times, where \\(n\\) is large, we would end up with a decimated view of the magnet, where we can only see the atoms at sites \\(0, 2^n, 2\\times 2^n, 3 \\times 2^n, \\dots\\). Our RN flow calculation states that the strength of interaction between \\(s_0\\) and \\(s_{2^n}\\) are just the multiplication of all the bonds between them. This is not so strange if we think about it: on a 1D Ising model, the only way for \\(s_0\\) to interact with \\(s_{2^n}\\) is if they laboriously go, bond-by-bond, across all the \\(2^n\\) bonds, and at each bond, the interaction strength must be degraded. It would be like trying to draw with a pair of chopstick holding a pair of chopsticks holding a pair of chopsticks… holding a paintbrush. You rapidly run out of all bonding strength.\nIn particular, it shows that no matter how strong \\(J\\) is, the correlation between distant atoms would decay rapidly, and so there is no ferromagnetism on the 1D Ising model. This is what disappointed Ising so much back in 1925 and what led him to abandon this model of magnetism.\n\n\n\n\n\n\nNote\n\n\n\nIf you are ambitious, you can try doing the same RN analysis for a “ladder” made of two \\(\\mathbb{Z}\\) put side-by-side. If you have “a lot of time” like Onsager, you would be on your way to solving the Ising model in 2 dimensions. I have not worked this out, because I don’t have a lot of time, but it would involve a 4-by-4 matrix. See page 12 for a sketch of solution.\n\n\n\n\nKadanoff decimation, take 1 (abortive)\nNow that we have done it for 1D Ising model, we can attempt it for the 2D model. We will not be able to solve it exactly, but we will see what is going to happen. To start our analysis, we write out the partition function and Hamiltonian energy function:\n\\[\nZ = \\sum_s e^{-H}, \\quad H = -J \\sum_{i, j \\text{ are neighbors}} s_i s_j\n\\]\nwhere again we have absorbed temperature into the Hamiltonian. To perform decimation, we paint the square grid like a checkerboard, and eliminate all the black-colored spins.\n\n\n\nFigure source\n\n\nHowever, we can see a problem in the picture. Whereas in the original, un-decimated grid, only nearest neighbors are connected by a bond. In the decimated grid, not just the nearest neighbors, but also the next-nearest-neighbors are connected by a double-bond. Not only can you go from \\((-1, 0)\\) to a nearest-neighbor \\((0, +1)\\) via the to-be-decimated atom \\((0, 0)\\), but also go to the next-nearest neighbor \\((+1, 0)\\). Not only that, the square of 4 spins neighboring \\((0, 0)\\) are connected via \\((0, 0)\\), so we also need to account for that interaction. Thus, the renormalized Hamiltonian would have three kinds of terms: nn term, nnn term, and square-interaction term:\n\\[\n-H' = J_{nn} \\sum_{i, j \\text{ are nn}} s_i s_j + J_{nnn} \\sum_{i, j \\text{ are nnn}} s_i s_j + J_{\\square} \\sum_{i, j, k, l \\text{ are }\\square} s_i s_j s_k s_l\n\\]\n\n\n\n\n\n\nExact solution\n\n\n\n\n\nLet the grid have \\(N\\) sites. We can ignore the borders, or roll up the borders like how we did it in the 1D case – it does not matter when \\(N\\) is large. The partition function of the system is\n\\[\nZ = \\sum_{s \\in \\{-1, +1\\}^\\text{grid}} \\exp\\left(J \\sum_{ i, j \\text{ are nn}} s_i s_j\\right)\n\\]\nNow, decimate half of the grid, and leave behind the other half. As derived in (Maris and Kadanoff 1978), we have\n\\[\nZ = f(J)^{N/2} \\sum_{s \\in \\{-1, +1\\}^\\text{decimated grid}} \\exp\\left(J_{nn} \\sum_{ i, j \\text{ are nn}} s_i s_j + J_{nn} \\sum_{ i, j \\text{ are nnn}} s_i s_j + J_{\\square} \\sum_{ i, j, k, l \\text{ are }\\square} s_i s_j s_k s_l\\right)\n\\]\nwhere \\[\n\\begin{aligned}\nf(J) &= 2 \\cosh ^{1 / 2}(2 J) \\cosh ^{1 / 8}(4 J) \\\\\nJ_{nn} &= (1 / 4) \\ln \\cosh (4 J) \\\\\nJ_{nnn} &= (1 / 8) \\ln \\cosh (4 J) \\\\\nJ_{\\square} &= (1 / 8) \\ln \\cosh (4 J)-(1 / 2) \\ln \\cosh (2 J)\n\\end{aligned}\n\\]\n\n\n\nNote: Why don’t we have the triangle terms like \\(J_\\Delta s_{(-1, 0)}s_{(0, +1)}s_{(+1, 0)}\\)? Since the system has the same energy if we flip all spins, the Hamiltonian is even, and so the triangle-term must be exactly zero.\nWell, that’s fine, you might say, what’s the big deal? The big deal is that, if we were to repeat the decimation again, we will suddenly meet infinitely many interaction terms! How so?\nLook at the square grid again. Suppose we have already included the nnn interactions, and we were to do a decimation. As we have seen, in order that the partition function before-and-after decimation are the same, we must construct one interaction term for each possible interaction mediated by a decimated spin. Well, \\((-1, 0)\\) and \\((0, +1)\\) are mediated by \\((0, 0)\\), so they must have an interaction term. No problem, that’s the nn term, written as \\(J_{nn}s_{(-1, 0)}s_{(0, +1)}\\). Similarly, \\((-1, 0)\\) and \\((0, +1)\\) are mediated by \\((0, 0)\\), so they must have an interaction term. That’s the nnn term, written as \\(J_{nnn}s_{(-1, 0)}s_{(0, +1)}\\). Do you see it now?\nWe can connect \\((-1, 0)\\) and \\((3, 0)\\) via \\((0, 0), (1, -1), (2, 0)\\). Not only that, we can connect them by infinitely many possible routes: \\((0, 0), (1, -1), (2, 0), (3, -1)\\), and \\((0, 0), (1, -1), (2, 0), (3, 1)\\), and etc… And that’s not even the worst of it. Just like how we have arbitrary pair-wise interactions, we also have arbitrary group-interactions within subsets with arbitrary shapes. It is all a big mess, which goes to show how difficult the Ising problem already is in 2D, let alone in 3D.\n\n\nMigdal bond-moving trick\n\n\nKadanoff blocking, take 1\nThe idea of Kadanoff blocking is simple: “This fractal looks the same if I take off my glasses, just bigger. If I zoom out and blur a bit, we get pretty much the same thing again. So, if I take a 3-by-3 pixel, block and replace it by one big pixel, we should get the same thing again.”\nWe can already see that the Kadanoff blocking method is not exact\n\n\nKadanoff blocking, take 2\nThis section based on (Simkin and Roychowdhury 2011, sec. 10) and (Stinchcombe 1991).\nGiven a hexagonal grid, you make \\(p\\) of them black and the rest white. What is the critical \\(p\\) where you get a percolation (infinitely big black island)? To do this by blocking, we take two by the triangles, as shown. After one iteration, the lattice length increases from \\(l\\) to \\(\\sqrt 3 l\\) , and the renormalized occupation probability changes from \\(p\\) to \\(p^3 + 3p^2(1-p) = p^2(3-2p)\\).\n\n\n\nKadanoff decimation on a triangular lattice. (Simkin and Roychowdhury 2011, fig. 1)\n\n\nThe equilibrium point is \\(p=1/2\\). This is the percolation probability. Let the reduced probability be \\(\\bar p = p-1/2\\). We find that one iteration of the RN flow makes \\(\\bar p_{n+1} = \\frac 32 \\bar p - 2 \\bar p^3\\) which is \\(\\approx \\frac 32 \\bar p_n\\) for small values of \\(\\bar p\\).\n\n\n\nRN flow for Kadanoff decimation on a triangular lattice, on a 1-dimensional slice.\n\n\nSuppose we start with \\(\\bar p_0\\) , and we perform \\(N\\) repeats of RN to reach some constant \\(\\Delta \\bar p\\) (for example, 0.001), then \\[N = \\frac{\\ln \\Delta \\bar p - \\ln \\bar p_0}{\\ln \\frac 32}\\]\nduring which time, the lattice length has increased by \\[3^{\\frac 12 N} \\propto \\bar p_0^{-\\frac{\\ln 3}{2\\ln \\frac 32}} = \\bar p_0^{-1.36}\\]\nSince at constant \\(\\Delta \\bar p\\), the lattice has a certain fixed look-and-feel with a certain characteristic size for its clusters, we find that the characteristic length of its clusters is \\(\\propto (p-1/2)^{-1.36}\\). The actual exponent (named \\(\\nu\\)) is believed to be around \\(4/3 \\approx 1.33\\). Not bad for such a cheap calculation!\n\n\nSide note: Onsager’s solution of Ising model in 2D\nSo far, we have been doing it for the Ising model on \\(\\mathbb{Z}\\). But it’s clear that we can also do it for two \\(\\mathbb{Z}\\) put side-by-side like a ladder. Each “rung” of the ladder can be thought of as one big atom, with 4 possible states: up-up, up-down, down-up, down-down. We can then do the same calculation as the previous case, except that instead of a \\(2\\times 2\\) matrix, we have a \\(4 \\times 4\\) matrix.\nFor that matter, we can do it for any finite number of those \\(\\mathbb{Z}\\) put together. We can then imagine doing that for such ladders with widths \\(2, 3, 4, 5, 6, \\dots\\), then discover a pattern, and take the limit. If this works, we would solve the Ising model on \\(\\mathbb{Z}^2\\).\nArduous as it sounds, this is exactly how Lars Onsager arrived at his solution of the Ising model in \\(\\mathbb{Z}^2\\). He calculated up to ladders with width 6, diagonalizing matrices of size \\(64\\times 64\\) in the process, then he guessed the general pattern and proceeded from there, emboldened by the guess. As Chen-Ning Yang reported:\n\nIn 1944, L. Onsager produced, quite unexpectedly, an exact evaluation of the partition function of the model in two dimensions. It was a real tour de force. I had studied his paper in Chicago in the spring of 1947, but did not understand the method, which was very, very complicated, with many algebraic somersaults….\nIn March, 1965… I asked him how it came about that he took all those complicated algebraic steps in his paper of 1944. He said he had a lot of time during the war, so he began to diagonalize the transfer matrix, which had already been discussed by E. Montroll and by H. A. Kramers and G. H. Wannier. He started with a \\(2 \\times \\infty\\), then a \\(3 \\times \\infty\\), then a \\(4 \\times \\infty\\) lattice. He then went on to a \\(5 \\times \\infty\\) lattice, for which the transfer matrix is \\(32 \\times 32\\) in size. Such a matrix is quite large, but the experience he had gained with the smaller matrices came in handy, and he was able, after some time, to find all 32 of the eigenvalues. He proceeded then to the \\(6 \\times \\infty\\) case, and eventually diagonalized the \\(64 \\times 64\\) matrix, finding that all the eigenvalues were of the form \\(e^{\\pm \\gamma_1 \\pm \\gamma_2 \\pm \\gamma_3 \\pm \\gamma_4 \\pm \\gamma_5 \\pm \\gamma_6}\\). That led to the concept that the algebra of the problem was a product algebra, and hence the manipulations in his paper. (Yang 2005, 11–13)\n\nI also want to quote this from the same pages, because it describes accurately what it feels like to do real-space RN:\n\na long calculation, the longest in my career. Full of local, tactical tricks, the calculation proceeded by twists and turns. There were many obstructions. But always, after a few days, a new trick was somehow found that pointed to a new path. The trouble was that I soon felt I was in a maze and was not sure whether in fact, after so many turns, I was anywhere nearer the goal than when I began. This kind of strategic overview was very depressing, and several times I almost gave up. But each time something drew me back, usually a new tactical trick that brightened the scene, even though only locally. Finally, after about six months of work off and on, all the pieces suddenly fitted together, producing miraculous cancellations, and I was staring at the amazingly simple final result [spontaneous magnetization of the Ising model] (Yang 2005, 12)\n\n\n\nMigdal bond-moving\n\n\n\nFigure source\n\n\nWith that simple idea I somehow got within 0.23% of the exact answer."
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#ising-model-rn-on-a-lattice",
    "href": "sketches/posts/renormalization-how-to/index.html#ising-model-rn-on-a-lattice",
    "title": "How to Renormalization",
    "section": "Ising model: RN on a lattice",
    "text": "Ising model: RN on a lattice\n\n\n\n\nWhen an Ising model is at the critical temperature \\(T_c\\), coarse-graining would not end up with all-black or all-white, but would have the same amount of details no matter how much we coarse-grain. Figure 13 from (Sethna 2007).\n\n\nThis is an example of real space RN. Real space RN is a garden of tricks. They are useful and intuitive, but not as powerful as frequency space RN.\n\nThis paper attracted much favorable notice since, beyond obtaining all the scaling properties, it seemed to lay out a direct route to the actual calculation of critical properties. On closer examination, however, the implied program seemed – as I will explain briefly – to run rapidly into insuperable difficulties and interest faded. In retrospect, however, Kadanoff’s scaling picture embodied important features eventually seen to be basic to Wilson’s conception of the full renormalization group.\n(Fisher 1998)\n\n\nThe Ising model\nFerromagnets are weird. A single iron atom is a tiny magnet, but if the iron atoms in a piece of magnet are not oriented roughly in the same way, then the whole piece would have all its direction washed away, thus not be a magnet at all. In the atomic world, things jiggle about and tend to destroy all order, so how is it possible for a grid of atoms to stay pointing mostly in the same direction for days and weeks – an eternity on the atomic level?\nIn 1920s, Lenz had the idea of a simple atomic model for ferromagnetism. He gave the problem to his graduate student Ising, who solved it in the one-dimensional case. He was disappointed to see that there is no spontaneous magnetization, which is the key feature of ferromagnetism, and he conjectured that this is also true in higher dimensions. The model was thus neglected for a while, until Peirels (1936) showed that the two-dimensional Ising model does exhibit a phase transition, and later, Onsager solved it completely in 1944, showing several curious features. This was a starting point of modern statistical physics. See (Domb 1985) for some more historical details.\nThe Ising model is a toy model of a magnet. For simplicity, we assume all the iron atoms are locked in a lattice, and each atom can only be spin up or spin down (like a coin that can be heads or tails). Neighboring atoms want to be the same, but temperature \\(T\\) makes the atoms jiggle and be random.\nThere are many different Ising models, but the following is the most standard. We have a grid of atoms, and each atom is nothing but a point with two possible values: \\(+1\\) or \\(-1\\). Nothing more complicated than that. The energy of the system is determined by the interaction between nearest neighbors, with a lower energy when neighboring spins are aligned. That is, its energy is\n\\[\nH(s) = -J \\sum_{i, j} s_i s_{j}\n\\]\nwhere \\(J&gt;0\\) is the strength of interaction between neighboring atoms, \\(s_i\\) represents the spin at site \\(i\\), and the summation is over neighboring pairs of \\(i, j\\). For example, if we have Ising model on the integer line \\(\\mathbb{Z}\\), then \\(H = -J \\sum_{i} s_i s_{i+1}\\). Similarly, if we have an Ising model on the square grid \\(\\mathbb{Z}^2\\), then \\(H = -J \\sum_{i} (s_{i} s_{i+(1, 0)} + s_{i} s_{i+(0, 1)})\\).\nWe are not going to explain why, but in statistical mechanics, we have to calculate the following, slightly mysterious quantity \\(Z\\), called the partition function. Everything in statistical mechanics bursts forth from this one function. It is no exaggeration to say that the story of statistical mechanics is mostly about calculating \\(Z\\) at higher and higher precisions using larger and larger computers while losing more and more hair on the head. For the Ising model, \\(Z\\) is defined as\n\\[\nZ := \\sum_{s} e^{-\\beta H(s)}\n\\]\nwhere \\(\\beta = 1/T\\) is the inverse of the temperature \\(T\\). The summation is over all possible configurations of the system. For example, if the Ising model is over \\(\\{0, 1, 2, \\dots, 99\\}\\), then the summation is over \\(\\{-1, +1\\}^{100}\\).\nThe temperature turns out to be not useful during the RN calculation, so to make the notation cleaner, we absorb it into the \\(J\\) like this:\n\\[Z = \\sum_{s} e^{-\\beta H(s)}, \\quad H(s) = -J_0 \\sum_{i, j} s_i s_{j}, \\quad J_0 = J/T\\]\nAfter we finish the calculation, we would write the temperature explicitly again in order to interpret what we have found.\n\n\nKadanoff decimation, take 0\nLike ancient Roman decimation, the idea of Kadanoff decimation is to pick out some spins for elimination, leaving behind the other spins gathered around the mangled nothings of their former comrades. In the case of the 1D Ising model, decimation is exactly solvable, making it a good first problem to try.\nLet us consider the Ising model on the integer line \\(\\mathbb{Z}\\) with periodic boundary conditions. That is, we have \\(2n\\) spins \\(s_0, s_1, \\dots, s_{2n-1}\\) arranged in a circle. Now, we keep the even spins white and decimate the odd spins. How can we do that? Well, behold:\n\\[\n\\begin{aligned}\nZ &= \\sum_{s_0, s_1, \\dots, s_{2n-1}} e^{+J(s_0 s_1 + s_1s_2 + \\dots + s_{2n-1}s_0)} \\\\\n  &= \\sum_{s_0, s_1, \\dots, s_{2n-2}} \\sum_{s_1, s_3, \\dots, s_{2n-1}} e^{J(s_0 s_1 + s_1s_2 + \\dots + s_{2n-1}s_0)} \\\\\n  &\\overset{\\mathrm{hopefully}}{=} \\sum_{s_0, s_2, \\dots, s_{2n-2}} e^{J'(s_0 s_2 + s_2 s_4 + \\dots + s_{2n-2}s_0)} \\\\\n\\end{aligned}\n\\]\nwhere we have hopefully written the \\(J'\\). It is our ardent hope that there exists some \\(J'\\), somewhere in the universe, that will make the equation come true.11 Physicists call it “ansatz”, but I like to say it like this:\n\nThe author of an atrocious undertaking ought to imagine that he has already accomplished it, ought to impose upon himself a future as irrevocable as the past.\n– Yu Tsun, professor of English at Deutsch-Chinesische Hochschule.\n\n\nBecause each even spin can only reach its nearest-neighboring even spins via the odd spin between them, the partition function splits cleanly:\n\\[\nZ = \\sum_{s_0, s_2, \\dots} \\left(\\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)}\\right) \\left(\\sum_{s_1} e^{J(s_2 s_3 + s_3 s_4)}\\right) \\cdots\n\\]\nso our wish would be fulfilled if \\(\\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = e^{J's_0s_2}\\) for all 4 possible choices of \\(s_0, s_2\\). Now, by direct calculation, we see that our task is to solve the following 4 equations simultaneously:\n\n\n\n\n+\n-\n\n\n\n\n+\n\\(e^{J'} = 2 \\cosh(2J)\\)\n\\(e^{-J'} = 2\\)\n\n\n-\n\\(e^{-J'} = 2\\)\n\\(e^{J'} = 2 \\cosh(2J)\\)\n\n\n\nImmediately we see a problem: if both \\(e^{J'} = 2 \\cosh(2J)\\) and \\(e^{-J'} = 2\\) hold, then \\(1 = 4 \\cosh(2J)\\). This means that we have to use a “fudge factor” again. Does that remind you of the \\(\\alpha\\) back in the logistic map calculation? It should. Add in the fudge factor \\(k\\) with:\n\\[\nZ = k^{n} \\sum_{s_0, s_2, \\dots, s_{2n-2}}e^{J'(s_0 s_2 + s_2 s_4 + \\dots + s_{2n-2}s_0)}, \\quad \\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = ke^{J's_0s_2}\n\\]\nNow the solution is: \\(J' = \\frac 12 \\ln \\cosh(2J), k = 2\\sqrt{\\cosh(2J)}\\). Again, we see that RN flow equation\n\\[\nJ \\mapsto \\frac 12 \\ln \\cosh(2J)\n\\]\nbut this time, this RN flow has only a single stable fixed point at \\(J = 0\\). Not only that, since \\(\\frac 12 \\ln \\cosh(2J) \\approx J^2\\) if \\(J\\) is small, if we decimate for \\(n\\) times, we would end up with \\(J' \\sim J^{2^n}\\). What does this mean?\nSuppose we start with a magnet with interaction strength \\(J\\), then after we zoom out for \\(n\\) times, where \\(n\\) is large, we would end up with a decimated view of the magnet, where we can only see the atoms at sites \\(0, 2^n, 2\\times 2^n, 3 \\times 2^n, \\dots\\). Our RN flow calculation states that the strength of interaction between \\(s_0\\) and \\(s_{2^n}\\) are just the multiplication of all the bonds between them. This is not so strange if we think about it: on a 1D Ising model, the only way for \\(s_0\\) to interact with \\(s_{2^n}\\) is if they laboriously go, bond-by-bond, across all the \\(2^n\\) bonds, and at each bond, the interaction strength must be degraded. It would be like trying to draw with a pair of chopstick holding a pair of chopsticks holding a pair of chopsticks… holding a paintbrush. You rapidly run out of all bonding strength.\nIn particular, it shows that no matter how strong \\(J\\) is, the correlation between distant atoms would decay rapidly, and so there is no ferromagnetism on the 1D Ising model. This is what disappointed Ising so much back in 1925 and what led him to abandon this model of magnetism.\n\n\n\n\n\n\nNote\n\n\n\nIf you are ambitious, you can try doing the same RN analysis for a “ladder” made of two \\(\\mathbb{Z}\\) put side-by-side. If you have “a lot of time” like Onsager, you would be on your way to solving the Ising model in 2 dimensions. I have not worked this out, because I don’t have a lot of time, but it would involve a 4-by-4 matrix. See page 12 for a sketch of solution.\n\n\n\n\nKadanoff decimation, take 1 (abortive)\nNow that we have done it for 1D Ising model, we can attempt it for the 2D model. We will not be able to solve it exactly, but we will see what is going to happen. To start our analysis, we write out the partition function and Hamiltonian energy function:\n\\[\nZ = \\sum_s e^{-H}, \\quad H = -J \\sum_{i, j \\text{ are neighbors}} s_i s_j\n\\]\nwhere again we have absorbed temperature into the Hamiltonian. To perform decimation, we paint the square grid like a checkerboard, and eliminate all the black-colored spins.\n\n\n\nFigure source\n\n\nHowever, we can see a problem in the picture. Whereas in the original, un-decimated grid, only nearest neighbors are connected by a bond. In the decimated grid, not just the nearest neighbors, but also the next-nearest-neighbors are connected by a double-bond. Not only can you go from \\((-1, 0)\\) to a nearest-neighbor \\((0, +1)\\) via the to-be-decimated atom \\((0, 0)\\), but also go to the next-nearest neighbor \\((+1, 0)\\). Not only that, the square of 4 spins neighboring \\((0, 0)\\) are connected via \\((0, 0)\\), so we also need to account for that interaction. Thus, the renormalized Hamiltonian would have three kinds of terms: nn term, nnn term, and square-interaction term:\n\\[\n-H' = J_{nn} \\sum_{i, j \\text{ are nn}} s_i s_j + J_{nnn} \\sum_{i, j \\text{ are nnn}} s_i s_j + J_{\\square} \\sum_{i, j, k, l \\text{ are }\\square} s_i s_j s_k s_l\n\\]\n\n\n\n\n\n\nExact solution\n\n\n\n\n\nLet the grid have \\(N\\) sites. We can ignore the borders, or roll up the borders like how we did it in the 1D case – it does not matter when \\(N\\) is large. The partition function of the system is\n\\[\nZ = \\sum_{s \\in \\{-1, +1\\}^\\text{grid}} \\exp\\left(J \\sum_{ i, j \\text{ are nn}} s_i s_j\\right)\n\\]\nNow, decimate half of the grid, and leave behind the other half. As derived in (Maris and Kadanoff 1978), we have\n\\[\nZ = f(J)^{N/2} \\sum_{s \\in \\{-1, +1\\}^\\text{decimated grid}} \\exp\\left(J_{nn} \\sum_{ i, j \\text{ are nn}} s_i s_j + J_{nn} \\sum_{ i, j \\text{ are nnn}} s_i s_j + J_{\\square} \\sum_{ i, j, k, l \\text{ are }\\square} s_i s_j s_k s_l\\right)\n\\]\nwhere \\[\n\\begin{aligned}\nf(J) &= 2 \\cosh ^{1 / 2}(2 J) \\cosh ^{1 / 8}(4 J) \\\\\nJ_{nn} &= (1 / 4) \\ln \\cosh (4 J) \\\\\nJ_{nnn} &= (1 / 8) \\ln \\cosh (4 J) \\\\\nJ_{\\square} &= (1 / 8) \\ln \\cosh (4 J)-(1 / 2) \\ln \\cosh (2 J)\n\\end{aligned}\n\\]\n\n\n\nNote: Why don’t we have the triangle terms like \\(J_\\Delta s_{(-1, 0)}s_{(0, +1)}s_{(+1, 0)}\\)? Since the system has the same energy if we flip all spins, the Hamiltonian is even, and so the triangle-term must be exactly zero.\nWell, that’s fine, you might say, what’s the big deal? The big deal is that, if we were to repeat the decimation again, we will suddenly meet infinitely many interaction terms! How so?\nLook at the square grid again. Suppose we have already included the nnn interactions, and we were to do a decimation. As we have seen, in order that the partition function before-and-after decimation are the same, we must construct one interaction term for each possible interaction mediated by a decimated spin. Well, \\((-1, 0)\\) and \\((0, +1)\\) are mediated by \\((0, 0)\\), so they must have an interaction term. No problem, that’s the nn term, written as \\(J_{nn}s_{(-1, 0)}s_{(0, +1)}\\). Similarly, \\((-1, 0)\\) and \\((0, +1)\\) are mediated by \\((0, 0)\\), so they must have an interaction term. That’s the nnn term, written as \\(J_{nnn}s_{(-1, 0)}s_{(0, +1)}\\). Do you see it now?\nWe can connect \\((-1, 0)\\) and \\((3, 0)\\) via \\((0, 0), (1, -1), (2, 0)\\). Not only that, we can connect them by infinitely many possible routes: \\((0, 0), (1, -1), (2, 0), (3, -1)\\), and \\((0, 0), (1, -1), (2, 0), (3, 1)\\), and etc… And that’s not even the worst of it. Just like how we have arbitrary pair-wise interactions, we also have arbitrary group-interactions within subsets with arbitrary shapes. It is all a big mess, which goes to show how difficult the Ising problem already is in 2D, let alone in 3D.\nWe can of course deal with the problem by cutting off the weak interaction terms, like how we did the logistic map example. For example, we can preserve all interaction terms between spins that are at most 6 distances away, then compute the RN flow on the space of all Ising models on the square grid, find a saddle point, and compute the largest eigenvalue of the RN flow near the saddle point. The more terms we account for, the better the approximation is.\n\nThese calculations were much more elaborate than the model calculation described here; in my own work, for example, 217 couplings between spins were included. The critical exponents derived from the calculation agree with Onsager’s values to with­ in about 0.2%.\n(Wilson 1979)\n\n\n\nMigdal bond-moving trick\nBefore we are finished with the Ising model, here is one more trick: the Migdal bond-moving trick. The idea is shown in the diagram.\n\n\n\nThe Migdal bond-moving trick. (Kadanoff 2007, fig. 14.2).\n\n\nWe perform the trick in two steps. First, we take the odd-numbered vertical bonds and move them to the even-numbered ones. Next, we decimate the crossed-out spins. Since this trick does not handle the vertical and horizontal bonds equally, we write the bond-strengths separately as \\(J_x, J_y\\). The RN flow of this step is\n\\[\n(J_x, J_y) \\mapsto \\left(2 J_x, \\frac 12 \\ln \\cosh(2 J_x)\\right)\n\\]\nAfter doing the Migdal bond-moving trick once in the horizontal direction, we can do it again in the vertical direction. The total effect is to have decimated 3/4 of the grid, preserving one spin per 2-by-2 block of spins. The RN flow of the whole process is:\n\\[\n(J_x, J_y) \\mapsto (f_1(J_x), f_2(J_y))\n\\]\nwhere \\(f_1(x) = \\ln\\cosh(2x)\\) and \\(f_2(x) = \\frac 12 \\ln\\cosh(4x)\\). This RN flow has saddle point \\((0.609, 0.305)\\). If we apply the same RN, but in the other order (first horizontal, then vertical), we have instead \\((J_x, J_y) \\mapsto (f_2(J_x), f_1(J_y))\\), with saddle point \\((0.305, 0.609)\\). Well, the true saddle point of the whole system should have equal \\(J_x, J_y\\), since the true system does not discriminate the horizontal and vertical, so the cheap answer is to take their average: \\((0.609 + 0.305)/2 = 0.457\\).\nAccording to Onsager’s exact solution, the true critical point is \\(J_c = \\frac{\\ln(1+\\sqrt 2)}{2} = 0.4407\\). So by this simple trick, we have already gotten within 3.7% of the true answer.\nA small improvement is to find the “double fixed point”. That is, we consider applying the first RN flow, then the second, and calculate the fixed point of this compound RN flow. That is, we expect the real fixed point to be the simultaneous solution to\n\\[x = f_1(f_2(x)); \\quad x = f_2(f_1(x))\\]\nNow, these have different solutions, so we do the obvious thing and take their midpoint. This gives \\(0.4417\\). And with that simple idea, I got within 0.23% of the true answer.\n\nFigure 1: Two RN flows on the \\((J_x, J_y)\\) plane found by Migdal bond-moving, and the saddle points thereof.\n\n\n\n\n\n\n\n\n\n\n\nIf you enjoy mathematical trickery, you might look at (Stella 1982) for more bond-moving tricks.\n\n\nPercolation\n\n\nKadanoff blocking, take 1\nThe idea of Kadanoff blocking is simple: “This fractal looks the same if I take off my glasses, just bigger. If I zoom out and blur a bit, we get pretty much the same thing again. So, if I take a 3-by-3 pixel, block and replace it by one big pixel, we should get the same thing again.”\nWe can already see that the Kadanoff blocking method is not exact\n\n\nKadanoff blocking, take 2\nThis section based on (Simkin and Roychowdhury 2011, sec. 10) and (Stinchcombe 1991).\nGiven a hexagonal grid, you make \\(p\\) of them black and the rest white. What is the critical \\(p\\) where you get a percolation (infinitely big black island)? To do this by blocking, we take two by the triangles, as shown. After one iteration, the lattice length increases from \\(l\\) to \\(\\sqrt 3 l\\) , and the renormalized occupation probability changes from \\(p\\) to \\(p^3 + 3p^2(1-p) = p^2(3-2p)\\).\n\n\n\nKadanoff decimation on a triangular lattice. (Simkin and Roychowdhury 2011, fig. 1)\n\n\nThe equilibrium point is \\(p=1/2\\). This is the percolation probability. Let the reduced probability be \\(\\bar p = p-1/2\\). We find that one iteration of the RN flow makes \\(\\bar p_{n+1} = \\frac 32 \\bar p - 2 \\bar p^3\\) which is \\(\\approx \\frac 32 \\bar p_n\\) for small values of \\(\\bar p\\).\n\n\n\nRN flow for Kadanoff decimation on a triangular lattice, on a 1-dimensional slice.\n\n\nSuppose we start with \\(\\bar p_0\\) , and we perform \\(N\\) repeats of RN to reach some constant \\(\\Delta \\bar p\\) (for example, 0.001), then \\[N = \\frac{\\ln \\Delta \\bar p - \\ln \\bar p_0}{\\ln \\frac 32}\\]\nduring which time, the lattice length has increased by \\[3^{\\frac 12 N} \\propto \\bar p_0^{-\\frac{\\ln 3}{2\\ln \\frac 32}} = \\bar p_0^{-1.36}\\]\nSince at constant \\(\\Delta \\bar p\\), the lattice has a certain fixed look-and-feel with a certain characteristic size for its clusters, we find that the characteristic length of its clusters is \\(\\propto (p-1/2)^{-1.36}\\). The actual exponent (named \\(\\nu\\)) is believed to be around \\(4/3 \\approx 1.33\\). Not bad for such a cheap calculation!\n\n\nSide note: Onsager’s solution of Ising model in 2D\nSo far, we have been doing it for the Ising model on \\(\\mathbb{Z}\\). But it’s clear that we can also do it for two \\(\\mathbb{Z}\\) put side-by-side like a ladder. Each “rung” of the ladder can be thought of as one big atom, with 4 possible states: up-up, up-down, down-up, down-down. We can then do the same calculation as the previous case, except that instead of a \\(2\\times 2\\) matrix, we have a \\(4 \\times 4\\) matrix.\nFor that matter, we can do it for any finite number of those \\(\\mathbb{Z}\\) put together. We can then imagine doing that for such ladders with widths \\(2, 3, 4, 5, 6, \\dots\\), then discover a pattern, and take the limit. If this works, we would solve the Ising model on \\(\\mathbb{Z}^2\\).\nArduous as it sounds, this is exactly how Lars Onsager arrived at his solution of the Ising model in \\(\\mathbb{Z}^2\\). He calculated up to ladders with width 6, diagonalizing matrices of size \\(64\\times 64\\) in the process, then he guessed the general pattern and proceeded from there, emboldened by the guess. As Chen-Ning Yang reported:\n\nIn 1944, L. Onsager produced, quite unexpectedly, an exact evaluation of the partition function of the model in two dimensions. It was a real tour de force. I had studied his paper in Chicago in the spring of 1947, but did not understand the method, which was very, very complicated, with many algebraic somersaults….\nIn March, 1965… I asked him how it came about that he took all those complicated algebraic steps in his paper of 1944. He said he had a lot of time during the war, so he began to diagonalize the transfer matrix, which had already been discussed by E. Montroll and by H. A. Kramers and G. H. Wannier. He started with a \\(2 \\times \\infty\\), then a \\(3 \\times \\infty\\), then a \\(4 \\times \\infty\\) lattice. He then went on to a \\(5 \\times \\infty\\) lattice, for which the transfer matrix is \\(32 \\times 32\\) in size. Such a matrix is quite large, but the experience he had gained with the smaller matrices came in handy, and he was able, after some time, to find all 32 of the eigenvalues. He proceeded then to the \\(6 \\times \\infty\\) case, and eventually diagonalized the \\(64 \\times 64\\) matrix, finding that all the eigenvalues were of the form \\(e^{\\pm \\gamma_1 \\pm \\gamma_2 \\pm \\gamma_3 \\pm \\gamma_4 \\pm \\gamma_5 \\pm \\gamma_6}\\). That led to the concept that the algebra of the problem was a product algebra, and hence the manipulations in his paper. (Yang 2005, 11–13)\n\nI also want to quote this from the same pages, because it describes accurately what it feels like to do real-space RN:\n\na long calculation, the longest in my career. Full of local, tactical tricks, the calculation proceeded by twists and turns. There were many obstructions. But always, after a few days, a new trick was somehow found that pointed to a new path. The trouble was that I soon felt I was in a maze and was not sure whether in fact, after so many turns, I was anywhere nearer the goal than when I began. This kind of strategic overview was very depressing, and several times I almost gave up. But each time something drew me back, usually a new tactical trick that brightened the scene, even though only locally. Finally, after about six months of work off and on, all the pieces suddenly fitted together, producing miraculous cancellations, and I was staring at the amazingly simple final result [spontaneous magnetization of the Ising model] (Yang 2005, 12)"
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#ising-model-rn-on-a-grid",
    "href": "sketches/posts/renormalization-how-to/index.html#ising-model-rn-on-a-grid",
    "title": "How to Renormalization",
    "section": "Ising model: RN on a grid",
    "text": "Ising model: RN on a grid\n\n\n\n\nWhen an Ising model is at the critical temperature \\(T_c\\), coarse-graining would not end up with all-black or all-white, but would have the same amount of details no matter how much we coarse-grain. (Sethna 2007, fig. 13).\n\n\nThis is an example of real space RN. Real space RN is a garden of tricks. They are useful and intuitive, but not as powerful as frequency space RN. If you enjoy mathematical trickery, you might look at (Burkhardt and Leeuwen 1982) for more of those.\n\nThis paper attracted much favorable notice since, beyond obtaining all the scaling properties, it seemed to lay out a direct route to the actual calculation of critical properties. On closer examination, however, the implied program seemed – as I will explain briefly – to run rapidly into insuperable difficulties and interest faded. In retrospect, however, Kadanoff’s scaling picture embodied important features eventually seen to be basic to Wilson’s conception of the full renormalization group.\n(Fisher 1998)\n\n\nThe Ising model\nFerromagnets are weird. A single iron atom is a tiny magnet, but if the iron atoms in a piece of magnet are not oriented roughly in the same way, then the whole piece would have all its direction washed away, thus not be a magnet at all. In the atomic world, things jiggle about and tend to destroy all order, so how is it possible for a grid of atoms to stay pointing mostly in the same direction for days and weeks – an eternity on the atomic level?\nIn 1920s, Lenz had the idea of a simple atomic model for ferromagnetism. He gave the problem to his graduate student Ising, who solved it in the one-dimensional case. He was disappointed to see that there is no spontaneous magnetization, which is the key feature of ferromagnetism, and he conjectured that this is also true in higher dimensions. The model was thus neglected for a while, until Peirels (1936) showed that the two-dimensional Ising model does exhibit a phase transition, and later, Onsager solved it completely in 1944, showing several curious features. This was a starting point of modern statistical physics. See (Domb 1985) for some more historical details.\nThe Ising model is a toy model of a magnet. For simplicity, we assume all the iron atoms are locked in a lattice, and each atom can only be spin up or spin down (like a coin that can be heads or tails). Neighboring atoms want to be the same, but temperature \\(T\\) makes the atoms jiggle and be random.\nThere are many different Ising models, but the following is the most standard. We have a grid of atoms, and each atom is nothing but a point with two possible values: \\(+1\\) or \\(-1\\). Nothing more complicated than that. The energy of the system is determined by the interaction between nearest neighbors, with a lower energy when neighboring spins are aligned. That is, its energy is\n\\[\nH(s) = -J \\sum_{i, j} s_i s_{j}\n\\]\nwhere \\(J&gt;0\\) is the strength of interaction between neighboring atoms, \\(s_i\\) represents the spin at site \\(i\\), and the summation is over neighboring pairs of \\(i, j\\). For example, if we have Ising model on the integer line \\(\\mathbb{Z}\\), then \\(H = -J \\sum_{i} s_i s_{i+1}\\). Similarly, if we have an Ising model on the square grid \\(\\mathbb{Z}^2\\), then \\(H = -J \\sum_{i} (s_{i} s_{i+(1, 0)} + s_{i} s_{i+(0, 1)})\\).\nWe are not going to explain why, but in statistical mechanics, we have to calculate the following, slightly mysterious quantity \\(Z\\), called the partition function. Everything in statistical mechanics bursts forth from this one function. It is no exaggeration to say that the story of statistical mechanics is mostly about calculating \\(Z\\) at higher and higher precisions using larger and larger computers while losing more and more hair on the head. For the Ising model, \\(Z\\) is defined as\n\\[\nZ := \\sum_{s} e^{-\\beta H(s)}\n\\]\nwhere \\(\\beta = 1/T\\) is the inverse of the temperature \\(T\\). The summation is over all possible configurations of the system. For example, if the Ising model is over \\(\\{0, 1, 2, \\dots, 99\\}\\), then the summation is over \\(\\{-1, +1\\}^{100}\\).\nThe temperature turns out to be not useful during the RN calculation, so to make the notation cleaner, we absorb it into the \\(J\\) like this:\n\\[Z = \\sum_{s} e^{-\\beta H(s)}, \\quad H(s) = -J_0 \\sum_{i, j} s_i s_{j}, \\quad J_0 = J/T\\]\nAfter we finish the calculation, we would write the temperature explicitly again in order to interpret what we have found.\n\n\nKadanoff decimation, take 0\nLike ancient Roman decimation, the idea of Kadanoff decimation is to pick out some spins for elimination, leaving behind the other spins gathered around the mangled nothings of their former comrades. In the case of the 1D Ising model, decimation is exactly solvable, making it a good first problem to try.\nLet us consider the Ising model on the integer line \\(\\mathbb{Z}\\) with periodic boundary conditions. That is, we have \\(2n\\) spins \\(s_0, s_1, \\dots, s_{2n-1}\\) arranged in a circle. Now, we keep the even spins white and decimate the odd spins. How can we do that? Well, behold:\n\\[\n\\begin{aligned}\nZ &= \\sum_{s_0, s_1, \\dots, s_{2n-1}} e^{+J(s_0 s_1 + s_1s_2 + \\dots + s_{2n-1}s_0)} \\\\\n  &= \\sum_{s_0, s_1, \\dots, s_{2n-2}} \\sum_{s_1, s_3, \\dots, s_{2n-1}} e^{J(s_0 s_1 + s_1s_2 + \\dots + s_{2n-1}s_0)} \\\\\n  &\\overset{\\mathrm{hopefully}}{=} \\sum_{s_0, s_2, \\dots, s_{2n-2}} e^{J'(s_0 s_2 + s_2 s_4 + \\dots + s_{2n-2}s_0)} \\\\\n\\end{aligned}\n\\]\nwhere we have hopefully written the \\(J'\\). It is our ardent hope that there exists some \\(J'\\), somewhere in the universe, that will make the equation come true.11 Physicists call it “ansatz”, but I like to say it like this:\n\nThe author of an atrocious undertaking ought to imagine that he has already accomplished it, ought to impose upon himself a future as irrevocable as the past.\n– Yu Tsun, professor of English at Deutsch-Chinesische Hochschule.\n\n\nBecause each even spin can only reach its nearest-neighboring even spins via the odd spin between them, the partition function splits cleanly:\n\\[\nZ = \\sum_{s_0, s_2, \\dots} \\left(\\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)}\\right) \\left(\\sum_{s_1} e^{J(s_2 s_3 + s_3 s_4)}\\right) \\cdots\n\\]\nso our wish would be fulfilled if \\(\\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = e^{J's_0s_2}\\) for all 4 possible choices of \\(s_0, s_2\\). Now, by direct calculation, we see that our task is to solve the following 4 equations simultaneously:\n\n\n\n\n+\n-\n\n\n\n\n+\n\\(e^{J'} = 2 \\cosh(2J)\\)\n\\(e^{-J'} = 2\\)\n\n\n-\n\\(e^{-J'} = 2\\)\n\\(e^{J'} = 2 \\cosh(2J)\\)\n\n\n\nImmediately we see a problem: if both \\(e^{J'} = 2 \\cosh(2J)\\) and \\(e^{-J'} = 2\\) hold, then \\(1 = 4 \\cosh(2J)\\). This means that we have to use a “fudge factor” again. Does that remind you of the \\(\\alpha\\) back in the logistic map calculation? It should. Add in the fudge factor \\(k\\) with:\n\\[\nZ = k^{n} \\sum_{s_0, s_2, \\dots, s_{2n-2}}e^{J'(s_0 s_2 + s_2 s_4 + \\dots + s_{2n-2}s_0)}, \\quad \\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = ke^{J's_0s_2}\n\\]\nNow the solution is: \\(J' = \\frac 12 \\ln \\cosh(2J), k = 2\\sqrt{\\cosh(2J)}\\). Again, we see that RN flow equation\n\\[\nJ \\mapsto \\frac 12 \\ln \\cosh(2J)\n\\]\nbut this time, this RN flow has only a single stable fixed point at \\(J = 0\\). Not only that, since \\(\\frac 12 \\ln \\cosh(2J) \\approx J^2\\) if \\(J\\) is small, if we decimate for \\(n\\) times, we would end up with \\(J' \\sim J^{2^n}\\). What does this mean?\nSuppose we start with a magnet with interaction strength \\(J\\), then after we zoom out for \\(n\\) times, where \\(n\\) is large, we would end up with a decimated view of the magnet, where we can only see the atoms at sites \\(0, 2^n, 2\\times 2^n, 3 \\times 2^n, \\dots\\). Our RN flow calculation states that the strength of interaction between \\(s_0\\) and \\(s_{2^n}\\) are just the multiplication of all the bonds between them. This is not so strange if we think about it: on a 1D Ising model, the only way for \\(s_0\\) to interact with \\(s_{2^n}\\) is if they laboriously go, bond-by-bond, across all the \\(2^n\\) bonds, and at each bond, the interaction strength must be degraded. It would be like trying to draw with a pair of chopstick holding a pair of chopsticks holding a pair of chopsticks… holding a paintbrush. You rapidly run out of all bonding strength.\nIn particular, it shows that no matter how strong \\(J\\) is, the correlation between distant atoms would decay rapidly, and so there is no ferromagnetism on the 1D Ising model. This is what disappointed Ising so much back in 1925 and what led him to abandon this model of magnetism.\n\n\n\n\n\n\nNote\n\n\n\nIf you are ambitious, you can try doing the same RN analysis for a “ladder” made of two \\(\\mathbb{Z}\\) put side-by-side. If you have “a lot of time” like Onsager, you would be on your way to solving the Ising model in 2 dimensions. I have not worked this out, because I don’t have a lot of time, but it would involve a 4-by-4 matrix. See page 12 for a sketch of solution.\n\n\n\n\nKadanoff decimation, take 1 (abortive)\nNow that we have done it for 1D Ising model, we can attempt it for the 2D model. We will not be able to solve it exactly, but we will see what is going to happen. To start our analysis, we write out the partition function and Hamiltonian energy function:\n\\[\nZ = \\sum_s e^{-H}, \\quad H = -J \\sum_{i, j \\text{ are neighbors}} s_i s_j\n\\]\nwhere again we have absorbed temperature into the Hamiltonian. To perform decimation, we paint the square grid like a checkerboard, and eliminate all the black-colored spins.\n\n\n\nFigure source\n\n\nHowever, we can see a problem in the picture. Whereas in the original, un-decimated grid, only nearest neighbors are connected by a bond. In the decimated grid, not just the nearest neighbors, but also the next-nearest-neighbors are connected by a double-bond. Not only can you go from \\((-1, 0)\\) to a nearest-neighbor \\((0, +1)\\) via the to-be-decimated atom \\((0, 0)\\), but also go to the next-nearest neighbor \\((+1, 0)\\). Not only that, the square of 4 spins neighboring \\((0, 0)\\) are connected via \\((0, 0)\\), so we also need to account for that interaction. Thus, the renormalized Hamiltonian would have three kinds of terms: nn term, nnn term, and square-interaction term:\n\\[\n-H' = J_{nn} \\sum_{i, j \\text{ are nn}} s_i s_j + J_{nnn} \\sum_{i, j \\text{ are nnn}} s_i s_j + J_{\\square} \\sum_{i, j, k, l \\text{ are }\\square} s_i s_j s_k s_l\n\\]\n\n\n\n\n\n\nExact solution\n\n\n\n\n\nLet the grid have \\(N\\) sites. We can ignore the borders, or roll up the borders like how we did it in the 1D case – it does not matter when \\(N\\) is large. The partition function of the system is\n\\[\nZ = \\sum_{s \\in \\{-1, +1\\}^\\text{grid}} \\exp\\left(J \\sum_{ i, j \\text{ are nn}} s_i s_j\\right)\n\\]\nNow, decimate half of the grid, and leave behind the other half. As derived in (Maris and Kadanoff 1978), we have\n\\[\nZ = f(J)^{N/2} \\sum_{s \\in \\{-1, +1\\}^\\text{decimated grid}} \\exp\\left(J_{nn} \\sum_{ i, j \\text{ are nn}} s_i s_j + J_{nn} \\sum_{ i, j \\text{ are nnn}} s_i s_j + J_{\\square} \\sum_{ i, j, k, l \\text{ are }\\square} s_i s_j s_k s_l\\right)\n\\]\nwhere \\[\n\\begin{aligned}\nf(J) &= 2 \\cosh ^{1 / 2}(2 J) \\cosh ^{1 / 8}(4 J) \\\\\nJ_{nn} &= (1 / 4) \\ln \\cosh (4 J) \\\\\nJ_{nnn} &= (1 / 8) \\ln \\cosh (4 J) \\\\\nJ_{\\square} &= (1 / 8) \\ln \\cosh (4 J)-(1 / 2) \\ln \\cosh (2 J)\n\\end{aligned}\n\\]\n\n\n\nNote: Why don’t we have the triangle terms like \\(J_\\Delta s_{(-1, 0)}s_{(0, +1)}s_{(+1, 0)}\\)? Since the system has the same energy if we flip all spins, the Hamiltonian is even, and so the triangle-term must be exactly zero.\nWell, that’s fine, you might say, what’s the big deal? The big deal is that, if we were to repeat the decimation again, we will suddenly meet infinitely many interaction terms! How so?\nLook at the square grid again. Suppose we have already included the nnn interactions, and we were to do a decimation. As we have seen, in order that the partition function before-and-after decimation are the same, we must construct one interaction term for each possible interaction mediated by a decimated spin. Well, \\((-1, 0)\\) and \\((0, +1)\\) are mediated by \\((0, 0)\\), so they must have an interaction term. No problem, that’s the nn term, written as \\(J_{nn}s_{(-1, 0)}s_{(0, +1)}\\). Similarly, \\((-1, 0)\\) and \\((0, +1)\\) are mediated by \\((0, 0)\\), so they must have an interaction term. That’s the nnn term, written as \\(J_{nnn}s_{(-1, 0)}s_{(0, +1)}\\). Do you see it now?\nWe can connect \\((-1, 0)\\) and \\((3, 0)\\) via \\((0, 0), (1, -1), (2, 0)\\). Not only that, we can connect them by infinitely many possible routes: \\((0, 0), (1, -1), (2, 0), (3, -1)\\), and \\((0, 0), (1, -1), (2, 0), (3, 1)\\), and etc… And that’s not even the worst of it. Just like how we have arbitrary pair-wise interactions, we also have arbitrary group-interactions within subsets with arbitrary shapes. It is all a big mess, which goes to show how difficult the Ising problem already is in 2D, let alone in 3D.\nWe can of course deal with the problem by cutting off the weak interaction terms, like how we did the logistic map example. For example, we can preserve all interaction terms between spins that are at most 6 distances away, then compute the RN flow on the space of all Ising models on the square grid, find a saddle point, and compute the largest eigenvalue of the RN flow near the saddle point. The more terms we account for, the better the approximation is.\n\nThese calculations were much more elaborate than the model calculation described here; in my own work, for example, 217 couplings between spins were included. The critical exponents derived from the calculation agree with Onsager’s values to with­ in about 0.2%.\n(Wilson 1979)\n\n\n\nMigdal bond-moving trick\nBefore we are finished with the Ising model, here is one more trick: the Migdal bond-moving trick. The idea is shown in the diagram.\n\n\n\nThe Migdal bond-moving trick. (Kadanoff 2007, fig. 14.2).\n\n\nWe perform the trick in two steps. First, we take the odd-numbered vertical bonds and move them to the even-numbered ones. Next, we decimate the crossed-out spins. Since this trick does not handle the vertical and horizontal bonds equally, we write the bond-strengths separately as \\(J_x, J_y\\). The RN flow of this step is\n\\[\n(J_x, J_y) \\mapsto \\left(2 J_x, \\frac 12 \\ln \\cosh(2 J_x)\\right)\n\\]\nAfter doing the Migdal bond-moving trick once in the horizontal direction, we can do it again in the vertical direction. The total effect is to have decimated 3/4 of the grid, preserving one spin per 2-by-2 block of spins. The RN flow of the whole process is:\n\\[\n(J_x, J_y) \\mapsto (f_1(J_x), f_2(J_y))\n\\]\nwhere \\(f_1(x) = \\ln\\cosh(2x)\\) and \\(f_2(x) = \\frac 12 \\ln\\cosh(4x)\\). This RN flow has saddle point \\((0.609, 0.305)\\). If we apply the same RN, but in the other order (first horizontal, then vertical), we have instead \\((J_x, J_y) \\mapsto (f_2(J_x), f_1(J_y))\\), with saddle point \\((0.305, 0.609)\\). Well, the true saddle point of the whole system should have equal \\(J_x, J_y\\), since the true system does not discriminate the horizontal and vertical, so the cheap answer is to take their average: \\((0.609 + 0.305)/2 = 0.457\\).\nAccording to Onsager’s exact solution, the true critical point is \\(J_c = \\frac{\\ln(1+\\sqrt 2)}{2} = 0.4407\\). So by this simple trick, we have already gotten within 3.7% of the true answer.\nA small improvement is to find the “double fixed point”. That is, we consider applying the first RN flow, then the second, and calculate the fixed point of this compound RN flow. That is, we expect the real fixed point to be the simultaneous solution to\n\\[x = f_1(f_2(x)); \\quad x = f_2(f_1(x))\\]\nNow, these have different solutions, so we do the obvious thing and take their midpoint. This gives \\(0.4417\\). And with that simple idea, I got within 0.23% of the true answer.\n\nFigure 1: Two RN flows on the \\((J_x, J_y)\\) plane found by Migdal bond-moving, and the saddle points thereof.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPercolation\n\n\nKadanoff blocking, take 1\nThe idea of Kadanoff blocking is simple: “This fractal looks the same if I take off my glasses, just bigger. If I zoom out and blur a bit, we get pretty much the same thing again. So, if I take a 3-by-3 pixel, block and replace it by one big pixel, we should get the same thing again.”\nWe can already see that the Kadanoff blocking method is not exact\n\n\nKadanoff blocking, take 2\nThis section based on (Simkin and Roychowdhury 2011, sec. 10) and (Stinchcombe 1991).\nGiven a hexagonal grid, you make \\(p\\) of them black and the rest white. What is the critical \\(p\\) where you get a percolation (infinitely big black island)? To do this by blocking, we take two by the triangles, as shown. After one iteration, the lattice length increases from \\(l\\) to \\(\\sqrt 3 l\\) , and the renormalized occupation probability changes from \\(p\\) to \\(p^3 + 3p^2(1-p) = p^2(3-2p)\\).\n\n\n\nKadanoff decimation on a triangular lattice. (Simkin and Roychowdhury 2011, fig. 1)\n\n\nThe equilibrium point is \\(p=1/2\\). This is the percolation probability. Let the reduced probability be \\(\\bar p = p-1/2\\). We find that one iteration of the RN flow makes \\(\\bar p_{n+1} = \\frac 32 \\bar p - 2 \\bar p^3\\) which is \\(\\approx \\frac 32 \\bar p_n\\) for small values of \\(\\bar p\\).\n\n\n\nRN flow for Kadanoff decimation on a triangular lattice, on a 1-dimensional slice.\n\n\nSuppose we start with \\(\\bar p_0\\) , and we perform \\(N\\) repeats of RN to reach some constant \\(\\Delta \\bar p\\) (for example, 0.001), then \\[N = \\frac{\\ln \\Delta \\bar p - \\ln \\bar p_0}{\\ln \\frac 32}\\]\nduring which time, the lattice length has increased by \\[3^{\\frac 12 N} \\propto \\bar p_0^{-\\frac{\\ln 3}{2\\ln \\frac 32}} = \\bar p_0^{-1.36}\\]\nSince at constant \\(\\Delta \\bar p\\), the lattice has a certain fixed look-and-feel with a certain characteristic size for its clusters, we find that the characteristic length of its clusters is \\(\\propto (p-1/2)^{-1.36}\\). The actual exponent (named \\(\\nu\\)) is believed to be around \\(4/3 \\approx 1.33\\). Not bad for such a cheap calculation!\n\n\nSide note: Onsager’s solution of Ising model in 2D\nSo far, we have been doing it for the Ising model on \\(\\mathbb{Z}\\). But it’s clear that we can also do it for two \\(\\mathbb{Z}\\) put side-by-side like a ladder. Each “rung” of the ladder can be thought of as one big atom, with 4 possible states: up-up, up-down, down-up, down-down. We can then do the same calculation as the previous case, except that instead of a \\(2\\times 2\\) matrix, we have a \\(4 \\times 4\\) matrix.\nFor that matter, we can do it for any finite number of those \\(\\mathbb{Z}\\) put together. We can then imagine doing that for such ladders with widths \\(2, 3, 4, 5, 6, \\dots\\), then discover a pattern, and take the limit. If this works, we would solve the Ising model on \\(\\mathbb{Z}^2\\).\nArduous as it sounds, this is exactly how Lars Onsager arrived at his solution of the Ising model in \\(\\mathbb{Z}^2\\). He calculated up to ladders with width 6, diagonalizing matrices of size \\(64\\times 64\\) in the process, then he guessed the general pattern and proceeded from there, emboldened by the guess. As Chen-Ning Yang reported:\n\nIn 1944, L. Onsager produced, quite unexpectedly, an exact evaluation of the partition function of the model in two dimensions. It was a real tour de force. I had studied his paper in Chicago in the spring of 1947, but did not understand the method, which was very, very complicated, with many algebraic somersaults….\nIn March, 1965… I asked him how it came about that he took all those complicated algebraic steps in his paper of 1944. He said he had a lot of time during the war, so he began to diagonalize the transfer matrix, which had already been discussed by E. Montroll and by H. A. Kramers and G. H. Wannier. He started with a \\(2 \\times \\infty\\), then a \\(3 \\times \\infty\\), then a \\(4 \\times \\infty\\) lattice. He then went on to a \\(5 \\times \\infty\\) lattice, for which the transfer matrix is \\(32 \\times 32\\) in size. Such a matrix is quite large, but the experience he had gained with the smaller matrices came in handy, and he was able, after some time, to find all 32 of the eigenvalues. He proceeded then to the \\(6 \\times \\infty\\) case, and eventually diagonalized the \\(64 \\times 64\\) matrix, finding that all the eigenvalues were of the form \\(e^{\\pm \\gamma_1 \\pm \\gamma_2 \\pm \\gamma_3 \\pm \\gamma_4 \\pm \\gamma_5 \\pm \\gamma_6}\\). That led to the concept that the algebra of the problem was a product algebra, and hence the manipulations in his paper. (Yang 2005, 11–13)\n\nI also want to quote this from the same pages, because it describes accurately what it feels like to do real-space RN:\n\na long calculation, the longest in my career. Full of local, tactical tricks, the calculation proceeded by twists and turns. There were many obstructions. But always, after a few days, a new trick was somehow found that pointed to a new path. The trouble was that I soon felt I was in a maze and was not sure whether in fact, after so many turns, I was anywhere nearer the goal than when I began. This kind of strategic overview was very depressing, and several times I almost gave up. But each time something drew me back, usually a new tactical trick that brightened the scene, even though only locally. Finally, after about six months of work off and on, all the pieces suddenly fitted together, producing miraculous cancellations, and I was staring at the amazingly simple final result [spontaneous magnetization of the Ising model] (Yang 2005, 12)"
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#ising-model-and-friends-rn-on-a-grid",
    "href": "sketches/posts/renormalization-how-to/index.html#ising-model-and-friends-rn-on-a-grid",
    "title": "How to do renormalization",
    "section": "Ising model and friends: RN on a grid",
    "text": "Ising model and friends: RN on a grid\nThis section studies RN on a grid, using what is called real space RN, in contrast to momentum space RN. Real space RN is a garden of tricks, useful and intuitive, but not as powerful as momentum space RN. If you find the kind of mathematical trickery in this section fun, look at (Kadanoff 1999b, chap. 14; Burkhardt and Leeuwen 1982) for more.\n\nThis paper attracted much favorable notice since, beyond obtaining all the scaling properties, it seemed to lay out a direct route to the actual calculation of critical properties. On closer examination, however, the implied program seemed – as I will explain briefly – to run rapidly into insuperable difficulties and interest faded. In retrospect, however, Kadanoff’s scaling picture embodied important features eventually seen to be basic to Wilson’s conception of the full renormalization group.\n(Fisher 1998)\n\nI recommend that you open these and play as you follow along:\n\nComplexity Explorables | I sing well-tempered\nIsing model by Evgeny Demidov.\n\n\nPercolation\nPercolation is about randomly punching holes in a material until it falls apart. In the simplest setting, the material is a square lattice \\(\\mathbb{Z}^2\\), and each site (vertex) is either open or closed.2 Open means there is a hole there, and closed means the site is intact. Sites are open or closed randomly and independently with probability \\(p\\). We are interested in whether there is an infinite connected cluster of open sites.\n2 Percolation on \\(\\mathbb{Z}\\) is trivial, even more trivial than Ising model on \\(\\mathbb{Z}\\). The trouble is the same: you can only go from one point to another point by one route, and if at any point on the route, you are stopped, then that’s the end – you can’t get there by any other way. Thus, long-range interactions decay exponentially with distance, which means no power law, no phase transition, no critical point. See a later section.This is a model for a porous material: for example, water seeping through the ground. If we have a layer of rock, then groundwater can seep through if there is a single connected path from top to bottom. A layer of rock can be thought of as a grid, with little cracks between grid-points. According to percolation theory, at a “critical probability”, suddenly we have arbitrarily large connected clusters of cracks, and so water can seep arbitrarily far in the rock – it is all or nothing.\nThat is, there is a sharp transition: if \\(p\\) is small, then there is no infinite cluster of open sites, and the water cannot go through; but if \\(p\\) is large, then there is an infinite cluster of open sites, and water can go through. The critical value \\(p_c\\) is about \\(0.5927...\\). See (Grimmett 1999) for more details about percolation.\nTo use Kadanoff blocking for percolation, the first step is to coarse grain the lattice. We group the sites into blocks of \\(3 \\times 3\\) and call a block open if there is a path of open sites connecting the left and right sides of the block. Otherwise, the block is closed.\nThe next step is to define a new percolation model on the coarse-grained lattice, but this is a little trickier than in the Ising model, because there is no obvious way to map the parameters of the original model to the parameters of the new model. We need to find a new probability \\(p'\\) such that the new model on the coarse-grained lattice has the same behavior as the original model on the fine lattice. In particular, we want the probability of having an infinite cluster of open sites to be the same in both models.\nIt turns out that there is no exact way to do this, but we can make an approximation. One way to do this is to use Monte Carlo simulations to estimate the probability of having an infinite cluster for different values of \\(p'\\).\n\n\nKadanoff blocking\nThis section based on (Simkin and Roychowdhury 2011, sec. 10) and (Stinchcombe 1991).\n\n\n\nWhen an Ising model is at the critical temperature \\(T_c\\), coarse-graining would not end up with all-black or all-white, but would have the same amount of details no matter how much we coarse-grain. (Sethna 2007, fig. 13).\n\n\nThe idea of Kadanoff blocking is simple: “This fractal looks the same if I take off my glasses, just bigger. If I zoom out and blur a bit, we get pretty much the same thing again. So, if I take a 3-by-3 pixel, block and replace it by one big pixel, we should get the same thing again.”\nGiven a hexagonal grid, you make \\(p\\) of them black and the rest white. What is the critical \\(p\\) where you get a percolation (infinitely big black island)? To do this by blocking, we regard a triangle as a large spin, and “merge” the three spins on a triangle to one single spin. If two or three spins are black, then the whole spin is also black, otherwise, the whole spin is white.\nThen, after one blocking operation, the lattice length increases from \\(l\\) to \\(\\sqrt 3 l\\) , and the renormalized occupation probability changes from \\(p\\) to \\(p^3 + 3p^2(1-p) = p^2(3-2p)\\). That is, we have the RN flow equation\n\\[p' = p^3 + 3p^2(1-p) = p^2(3-2p)\\]\n\n\n\nKadanoff blocking on a triangular lattice. (Simkin and Roychowdhury 2011, fig. 1)\n\n\nThe equilibrium point is \\(p=1/2\\). This is the percolation probability. Let the reduced probability be \\(\\bar p = p-1/2\\). We find that one iteration of the RN flow makes \\(\\bar p_{n+1} = \\frac 32 \\bar p - 2 \\bar p^3\\) which is \\(\\approx \\frac 32 \\bar p_n\\) for small values of \\(\\bar p\\).\n\n\n\nRN flow for Kadanoff blocking on a triangular lattice.\n\n\nSuppose we start with \\(\\bar p_0\\) , and we perform \\(N\\) repeats of RN to reach some constant \\(\\Delta \\bar p\\) (for example, 0.001), then\n\\[N = \\frac{\\ln \\Delta \\bar p - \\ln \\bar p_0}{\\ln \\frac 32}\\]\nduring which time, the lattice length has increased by\n\\[3^{\\frac 12 N} \\propto \\bar p_0^{-\\frac{\\ln 3}{2\\ln \\frac 32}}\\]\nSince at constant \\(\\Delta \\bar p\\), the lattice has a certain characteristic look-and-feel with a certain characteristic size for its clusters, we find that the characteristic length of its clusters is \\(\\propto (p-1/2)^{-\\nu}\\), where3\n3 It amuses me to no end that the word characteristic is something chemists use a lot. Physicists do it too, sure, with their “characteristic length”, “characteristic height”, “characteristic temperature”, and such, but it is abstract. You rarely need to actually check a cake’s characteristic length against a standard cake. However, when you are doing chemistry, and you need to check a chemical’s characteristic smell, then you are out of luck.\n\nI’m saddened to report that the chemical literature contains descriptions of dimethylcadmium’s smell. Whoever provided these reports was surely exposed to far more of the vapor than common sense would allow … its odor is variously described as “foul”, “unpleasant”, “metallic”, “disagreeable”, and (wait for it) “characteristic”, which is an adjective that shows up often in the literature with regard to smells, and almost always makes a person want to punch whoever thought it was useful. … if you’re working with organocadmium derivatives and smell something nasty, but nasty in a new, exciting way that you’ve never quite smelled before, then you can probably assume the worst. (Lowe 2013)\n\n\\[\\nu = \\frac{\\ln 3}{2\\ln \\frac 32} \\approx 1.355\\]\nThe actual exponent is believed to be \\(\\nu = 4/3\\), so we are only 1.6% off. Very good for such a cheap calculation!\n\n\nThe Ising model\nFerromagnets are weird. A single iron atom is a tiny magnet, but if the iron atoms in a piece of magnet are not oriented roughly in the same way, then the whole piece would have all its direction washed away, thus not be a magnet at all. In the atomic world, things jiggle about and tend to destroy all order, so how is it possible for a grid of atoms to stay pointing mostly in the same direction for days and weeks – an eternity on the atomic level?\nIn 1920s, Lenz had the idea of a simple atomic model for ferromagnetism. He gave the problem to his graduate student Ising, who solved it in the one-dimensional case. He was disappointed to see that there is no spontaneous magnetization, which is the key feature of ferromagnetism, and he conjectured that this is also true in higher dimensions. The model was thus neglected for a while, until (Peierls 1936) showed that the two-dimensional Ising model does exhibit a phase transition, and later, Onsager solved it completely in 1944, showing several curious features. This was a starting point of modern statistical physics. See (Domb 1985) for some more historical details.\nThe Ising model is a toy model of a magnet. For simplicity, we assume all the iron atoms are locked in a lattice, and each atom can only be spin up or spin down (like a coin that can be heads or tails). Neighboring atoms want to be the same, but temperature \\(T\\) makes the atoms jiggle and be random.\nThere are many different Ising models, but the following is the most standard. We have a grid of atoms, and each atom is nothing but a point with two possible values: \\(+1\\) or \\(-1\\). Nothing more complicated than that. The energy of the system is determined by the interaction between nearest neighbors, with a lower energy when neighboring spins are aligned. That is, its energy is\n\\[\nH(s) = -J \\sum_{i, j} s_i s_{j}\n\\]\nwhere \\(J&gt;0\\) is the strength of interaction between neighboring atoms, \\(s_i\\) represents the spin at site \\(i\\), and the summation is over neighboring pairs of \\(i, j\\). For example, if we have Ising model on the integer line \\(\\mathbb{Z}\\), then \\(H = -J \\sum_{i} s_i s_{i+1}\\). Similarly, if we have an Ising model on the square grid \\(\\mathbb{Z}^2\\), then \\(H = -J \\sum_{i} (s_{i} s_{i+(1, 0)} + s_{i} s_{i+(0, 1)})\\).\nWe are not going to explain why, but in statistical mechanics, we have to calculate the following, slightly mysterious quantity \\(Z\\), called the partition function. Everything in statistical mechanics bursts forth from this one function. It is no exaggeration to say that the story of statistical mechanics is mostly about calculating \\(Z\\) at higher and higher precisions using larger and larger computers while losing more and more hair on the head. For the Ising model, \\(Z\\) is defined as\n\\[\nZ := \\sum_{s} e^{-\\beta H(s)}\n\\]\nwhere \\(\\beta = 1/T\\) is the inverse of the temperature \\(T\\). The summation is over all possible configurations of the system. For example, if the Ising model is over \\(\\{0, 1, 2, \\dots, 99\\}\\), then the summation is over \\(\\{-1, +1\\}^{100}\\).\nThe temperature turns out to be not useful during the RN calculation, so to make the notation cleaner, we absorb it into the \\(J\\) like this:\n\\[Z = \\sum_{s} e^{-\\beta H(s)}, \\quad H(s) = -J_0 \\sum_{i, j} s_i s_{j}, \\quad J_0 = J/T\\]\nAfter we finish the calculation, we would write the temperature explicitly again in order to interpret what we have found.\n\n\nKadanoff decimation, take 0\nLike ancient Roman decimation, the idea of Kadanoff decimation is to pick out some spins for elimination, leaving behind the other spins gathered around the mangled nothings of their former comrades. In the case of the 1D Ising model, decimation is exactly solvable, making it a good first problem to try.\nLet us consider the Ising model on the integer line \\(\\mathbb{Z}\\) with periodic boundary conditions. That is, we have \\(2n\\) spins \\(s_0, s_1, \\dots, s_{2n-1}\\) arranged in a circle. Now, we keep the even spins white and decimate the odd spins. How can we do that? Well, behold:\n\\[\n\\begin{aligned}\nZ &= \\sum_{s_0, s_1, \\dots, s_{2n-1}} e^{+J(s_0 s_1 + s_1s_2 + \\dots + s_{2n-1}s_0)} \\\\\n  &= \\sum_{s_0, s_1, \\dots, s_{2n-2}} \\sum_{s_1, s_3, \\dots, s_{2n-1}} e^{J(s_0 s_1 + s_1s_2 + \\dots + s_{2n-1}s_0)} \\\\\n  &\\overset{\\mathrm{hopefully}}{=} \\sum_{s_0, s_2, \\dots, s_{2n-2}} e^{J'(s_0 s_2 + s_2 s_4 + \\dots + s_{2n-2}s_0)} \\\\\n\\end{aligned}\n\\]\nwhere we have hopefully written the \\(J'\\). It is our ardent hope that there exists some \\(J'\\), somewhere in the universe, that will make the equation come true.4\n4 Physicists call it “ansatz”, but I like to say it like this:\n\nThe author of an atrocious undertaking ought to imagine that he has already accomplished it, ought to impose upon himself a future as irrevocable as the past.\n– Yu Tsun, professor of English at Deutsch-Chinesische Hochschule.\n\nBecause each even spin can only reach its nearest-neighboring even spins via the odd spin between them, the partition function splits cleanly:\n\\[\nZ = \\sum_{s_0, s_2, \\dots} \\left(\\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)}\\right) \\left(\\sum_{s_1} e^{J(s_2 s_3 + s_3 s_4)}\\right) \\cdots\n\\]\nso our wish would be fulfilled if \\(\\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = e^{J's_0s_2}\\) for all 4 possible choices of \\(s_0, s_2\\). Now, by direct calculation, we see that our task is to solve the following 4 equations simultaneously:\n\n\n\n\n+\n-\n\n\n\n\n+\n\\(e^{J'} = 2 \\cosh(2J)\\)\n\\(e^{-J'} = 2\\)\n\n\n-\n\\(e^{-J'} = 2\\)\n\\(e^{J'} = 2 \\cosh(2J)\\)\n\n\n\nImmediately we see a problem: if both \\(e^{J'} = 2 \\cosh(2J)\\) and \\(e^{-J'} = 2\\) hold, then \\(1 = 4 \\cosh(2J)\\). This means that we have to use a “fudge factor” again. Does that remind you of the \\(\\alpha\\) back in the logistic map calculation? It should. Add in the fudge factor \\(k\\) with:\n\\[\nZ = k^{n} \\sum_{s_0, s_2, \\dots, s_{2n-2}}e^{J'(s_0 s_2 + s_2 s_4 + \\dots + s_{2n-2}s_0)}, \\quad \\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = ke^{J's_0s_2}\n\\]\nNow the solution is: \\(J' = \\frac 12 \\ln \\cosh(2J), k = 2\\sqrt{\\cosh(2J)}\\). Again, we see that RN flow equation\n\\[\nJ \\mapsto \\frac 12 \\ln \\cosh(2J)\n\\]\nbut this time, this RN flow has only a single stable fixed point at \\(J = 0\\). Not only that, since \\(\\frac 12 \\ln \\cosh(2J) \\approx J^2\\) if \\(J\\) is small, if we decimate for \\(n\\) times, we would end up with \\(J' \\sim J^{2^n}\\). What does this mean?\n\n\n\nThe RN flow for the 1D Ising model has only a single fixed point at zero.\n\n\nSuppose we start with a magnet with interaction strength \\(J\\), then after we zoom out for \\(n\\) times, where \\(n\\) is large, we would end up with a decimated view of the magnet, where we can only see the atoms at sites \\(0, 2^n, 2\\times 2^n, 3 \\times 2^n, \\dots\\). Our RN flow calculation states that the strength of interaction between \\(s_0\\) and \\(s_{2^n}\\) are just the multiplication of all the bonds between them. This is not so strange if we think about it: on a 1D Ising model, the only way for \\(s_0\\) to interact with \\(s_{2^n}\\) is if they laboriously go, bond-by-bond, across all the \\(2^n\\) bonds, and at each bond, the interaction strength must be degraded. It would be like trying to draw with a pair of chopstick holding a pair of chopsticks holding a pair of chopsticks… holding a paintbrush. You rapidly run out of all bonding strength.\nIn particular, it shows that no matter how strong \\(J\\) is, the correlation between distant atoms would decay rapidly, and so there is no ferromagnetism on the 1D Ising model. This is what disappointed Ising so much back in 1925 and what led him to abandon this model of magnetism.\n\n\n\n\n\n\nNote\n\n\n\nIf you are ambitious, you can try doing the same RN analysis for a “ladder” made of two \\(\\mathbb{Z}\\) put side-by-side. If you have “a lot of time” like Onsager, you would be on your way to solving the Ising model in 2 dimensions. I have not worked this out, because I don’t have a lot of time, but it would involve a 4-by-4 matrix. See page 12 for a sketch of solution.\n\n\n\n\nKadanoff decimation, take 1 (abortive)\nNow that we have done it for 1D Ising model, we can attempt it for the 2D model. We will not be able to solve it exactly, but we will see what is going to happen. To start our analysis, we write out the partition function and Hamiltonian energy function:\n\\[\nZ = \\sum_s e^{-H}, \\quad H = -J \\sum_{i, j \\text{ are neighbors}} s_i s_j\n\\]\nwhere again we have absorbed temperature into the Hamiltonian. To perform decimation, we paint the square grid like a checkerboard, and eliminate all the black-colored spins.\n\n\n\nFigure source\n\n\nHowever, we can see a problem in the picture. Whereas in the original, un-decimated grid, only nearest neighbors are connected by a bond. In the decimated grid, not just the nearest neighbors, but also the next-nearest-neighbors are connected by a double-bond. Not only can you go from \\((-1, 0)\\) to a nearest-neighbor \\((0, +1)\\) via the to-be-decimated atom \\((0, 0)\\), but also go to the next-nearest neighbor \\((+1, 0)\\). Not only that, the square of 4 spins neighboring \\((0, 0)\\) are connected via \\((0, 0)\\), so we also need to account for that interaction. Thus, the renormalized Hamiltonian would have three kinds of terms: nn term, nnn term, and square-interaction term:\n\\[\n-H' = J_{nn} \\sum_{i, j \\text{ are nn}} s_i s_j + J_{nnn} \\sum_{i, j \\text{ are nnn}} s_i s_j + J_{\\square} \\sum_{i, j, k, l \\text{ are }\\square} s_i s_j s_k s_l\n\\]\n\n\n\n\n\n\nExact solution\n\n\n\n\n\nLet the grid have \\(N\\) sites. We can ignore the borders, or roll up the borders like how we did it in the 1D case – it does not matter when \\(N\\) is large. The partition function of the system is\n\\[\nZ = \\sum_{s \\in \\{-1, +1\\}^\\text{grid}} \\exp\\left(J \\sum_{ i, j \\text{ are nn}} s_i s_j\\right)\n\\]\nNow, decimate half of the grid, and leave behind the other half. As derived in (Maris and Kadanoff 1978), we have\n\\[\nZ = f(J)^{N/2} \\sum_{s \\in \\{-1, +1\\}^\\text{decimated grid}} \\exp\\left(J_{nn} \\sum_{ i, j \\text{ are nn}} s_i s_j + J_{nn} \\sum_{ i, j \\text{ are nnn}} s_i s_j + J_{\\square} \\sum_{ i, j, k, l \\text{ are }\\square} s_i s_j s_k s_l\\right)\n\\]\nwhere \\[\n\\begin{aligned}\nf(J) &= 2 \\cosh ^{1 / 2}(2 J) \\cosh ^{1 / 8}(4 J) \\\\\nJ_{nn} &= (1 / 4) \\ln \\cosh (4 J) \\\\\nJ_{nnn} &= (1 / 8) \\ln \\cosh (4 J) \\\\\nJ_{\\square} &= (1 / 8) \\ln \\cosh (4 J)-(1 / 2) \\ln \\cosh (2 J)\n\\end{aligned}\n\\]\n\n\n\nNote: Why don’t we have the triangle terms like \\(J_\\Delta s_{(-1, 0)}s_{(0, +1)}s_{(+1, 0)}\\)? Since the system has the same energy if we flip all spins, the Hamiltonian is even, and so the triangle-term must be exactly zero.\nWell, that’s fine, you might say, what’s the big deal? The big deal is that, if we were to repeat the decimation again, we will suddenly meet infinitely many interaction terms! How so?\nLook at the square grid again. Suppose we have already included the nnn interactions, and we were to do a decimation. As we have seen, in order that the partition function before-and-after decimation are the same, we must construct one interaction term for each possible interaction mediated by a decimated spin. Well, \\((-1, 0)\\) and \\((0, +1)\\) are mediated by \\((0, 0)\\), so they must have an interaction term. No problem, that’s the nn term, written as \\(J_{nn}s_{(-1, 0)}s_{(0, +1)}\\). Similarly, \\((-1, 0)\\) and \\((0, +1)\\) are mediated by \\((0, 0)\\), so they must have an interaction term. That’s the nnn term, written as \\(J_{nnn}s_{(-1, 0)}s_{(0, +1)}\\). Do you see it now?\nWe can connect \\((-1, 0)\\) and \\((3, 0)\\) via \\((0, 0), (1, -1), (2, 0)\\). Not only that, we can connect them by infinitely many possible routes: \\((0, 0), (1, -1), (2, 0), (3, -1)\\), and \\((0, 0), (1, -1), (2, 0), (3, 1)\\), and etc… And that’s not even the worst of it. Just like how we have arbitrary pair-wise interactions, we also have arbitrary group-interactions within subsets with arbitrary shapes. It is all a big mess, which goes to show how difficult the Ising problem already is in 2D, let alone in 3D.\nWe can of course deal with the problem by cutting off the weak interaction terms, like how we did the logistic map example. For example, we can preserve all interaction terms between spins that are at most 6 distances away, then compute the RN flow on the space of all Ising models on the square grid, find a saddle point, and compute the largest eigenvalue of the RN flow near the saddle point. The more terms we account for, the better the approximation is.\n\nThese calculations were much more elaborate than the model calculation described here; in my own work, for example, 217 couplings between spins were included. The critical exponents derived from the calculation agree with Onsager’s values to with­ in about 0.2%.\n(Wilson 1979)\n\n\n\nMigdal bond-moving trick\nBefore we are finished with the Ising model, here is one more trick: the Migdal bond-moving trick. The idea is shown in the diagram.\n\n\n\nThe Migdal bond-moving trick. (Kadanoff 1999b, fig. 14.2).\n\n\nWe perform the trick in two steps. First, we take the odd-numbered vertical bonds and move them to the even-numbered ones. Next, we decimate the crossed-out spins. Since this trick does not handle the vertical and horizontal bonds equally, we write the bond-strengths separately as \\(J_x, J_y\\). The RN flow of this step is\n\\[\n(J_x, J_y) \\mapsto \\left(2 J_x, \\frac 12 \\ln \\cosh(2 J_x)\\right)\n\\]\nAfter doing the Migdal bond-moving trick once in the horizontal direction, we can do it again in the vertical direction. The total effect is to have decimated 3/4 of the grid, preserving one spin per 2-by-2 block of spins. The RN flow of the whole process is:\n\\[\n(J_x, J_y) \\mapsto (f_1(J_x), f_2(J_y))\n\\]\nwhere \\(f_1(x) = \\ln\\cosh(2x)\\) and \\(f_2(x) = \\frac 12 \\ln\\cosh(4x)\\). This RN flow has saddle point \\((0.609, 0.305)\\). If we apply the same RN, but in the other order (first horizontal, then vertical), we have instead \\((J_x, J_y) \\mapsto (f_2(J_x), f_1(J_y))\\), with saddle point \\((0.305, 0.609)\\). Well, the true saddle point of the whole system should have equal \\(J_x, J_y\\), since the true system does not discriminate the horizontal and vertical, so the cheap answer is to take their average: \\((0.609 + 0.305)/2 = 0.457\\).\nAccording to Onsager’s exact solution, the true critical point is \\(J_c = \\frac{\\ln(1+\\sqrt 2)}{2} = 0.4407\\). So by this simple trick, we have already gotten within 3.7% of the true answer.\nA small improvement is to find the “double fixed point”. That is, we consider applying the first RN flow, then the second, and calculate the fixed point of this compound RN flow. That is, we expect the real fixed point to be the simultaneous solution to\n\\[x = f_1(f_2(x)); \\quad x = f_2(f_1(x))\\]\nNow, these have different solutions, so we do the obvious thing and take their midpoint. This gives \\(0.4417\\). And with that simple idea, I got within 0.23% of the true answer.\nWe can also calculate the scaling exponent, like how we found \\(\\delta\\) for the logistic map. The average gradient at the fixed point is \\(\\frac{(f_1\\circ f_2)' + (f_2\\circ f_1)'}{2}\\) which is \\(2.7633\\), corresponding to a length scaling exponent of \\(\\nu = \\frac{\\ln 4}{\\ln 2.7633} = 1.364\\), which is well off the real value of 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Two RN flows on the \\((J_x, J_y)\\) plane found by Migdal bond-moving, and the saddle points thereof.\n\n\n\n\n\nSide note: Onsager’s solution of Ising model in 2D\nSo far, we have been doing it for the Ising model on \\(\\mathbb{Z}\\). But it’s clear that we can also do it for two \\(\\mathbb{Z}\\) put side-by-side like a ladder. Each “rung” of the ladder can be thought of as one big atom, with 4 possible states: up-up, up-down, down-up, down-down. We can then do the same calculation as the previous case, except that instead of a \\(2\\times 2\\) matrix, we have a \\(4 \\times 4\\) matrix.\nFor that matter, we can do it for any finite number of those \\(\\mathbb{Z}\\) put together. We can then imagine doing that for such ladders with widths \\(2, 3, 4, 5, 6, \\dots\\), then discover a pattern, and take the limit. If this works, we would solve the Ising model on \\(\\mathbb{Z}^2\\).\nArduous as it sounds, this is exactly how Lars Onsager arrived at his solution of the Ising model in \\(\\mathbb{Z}^2\\). He calculated up to ladders with width 6, diagonalizing matrices of size \\(64\\times 64\\) in the process, then he guessed the general pattern and proceeded from there, emboldened by the guess. As Chen-Ning Yang reported:\n\nIn 1944, L. Onsager produced, quite unexpectedly, an exact evaluation of the partition function of the model in two dimensions. It was a real tour de force. I had studied his paper in Chicago in the spring of 1947, but did not understand the method, which was very, very complicated, with many algebraic somersaults….\nIn March, 1965… I asked him how it came about that he took all those complicated algebraic steps in his paper of 1944. He said he had a lot of time during the war, so he began to diagonalize the transfer matrix, which had already been discussed by E. Montroll and by H. A. Kramers and G. H. Wannier. He started with a \\(2 \\times \\infty\\), then a \\(3 \\times \\infty\\), then a \\(4 \\times \\infty\\) lattice. He then went on to a \\(5 \\times \\infty\\) lattice, for which the transfer matrix is \\(32 \\times 32\\) in size. Such a matrix is quite large, but the experience he had gained with the smaller matrices came in handy, and he was able, after some time, to find all 32 of the eigenvalues. He proceeded then to the \\(6 \\times \\infty\\) case, and eventually diagonalized the \\(64 \\times 64\\) matrix, finding that all the eigenvalues were of the form \\(e^{\\pm \\gamma_1 \\pm \\gamma_2 \\pm \\gamma_3 \\pm \\gamma_4 \\pm \\gamma_5 \\pm \\gamma_6}\\). That led to the concept that the algebra of the problem was a product algebra, and hence the manipulations in his paper.\n(Yang 2005, 11–13)\n\nI also want to quote this from the same pages, because it describes accurately what it feels like to do real-space RN:\n\na long calculation, the longest in my career. Full of local, tactical tricks, the calculation proceeded by twists and turns. There were many obstructions. But always, after a few days, a new trick was somehow found that pointed to a new path. The trouble was that I soon felt I was in a maze and was not sure whether in fact, after so many turns, I was anywhere nearer the goal than when I began. This kind of strategic overview was very depressing, and several times I almost gave up. But each time something drew me back, usually a new tactical trick that brightened the scene, even though only locally. Finally, after about six months of work off and on, all the pieces suddenly fitted together, producing miraculous cancellations, and I was staring at the amazingly simple final result [spontaneous magnetization of the Ising model]\n(Yang 2005, 12)\n\n\n\nBonus: Generalized central limit theorem\nThis section is based on (Amir 2020).\nConsider three random walks generated from three distinct probability distributions: Gaussian, Cauchy, and Pareto \\(\\mu=0.7\\). We notice two facts:\n\nThe random walks are self-similar. A small section has the same look-and-feel as a large section of it.\nDifferent random walks have very different characters. The Gaussian walk appears smoother, while the Cauchy and Pareto walks display more dramatic jumps and bursts, reflecting the heavier tails of their respective distributions.\n\n\n\n\nThree random walks generated from three distinct probability distributions: Gaussian, Cauchy, and Pareto \\(\\mu=0.7\\)\n\n\nWhen we see self-similarity, we think RN. Can we use RN to study random walks? Yes.\nLet’s take a fresh look at the central limit theorem. It says that if \\(X_1, X_2, \\dots\\) are IID samples from a distribution with finite mean \\(E[X]\\) and variance \\(V[X]\\), then \\(\\frac{(X_1 + \\dots + X_n) - n E[X]}{\\sqrt{n V[X]}}\\) converges to the standard normal distribution. If we think about it from the RN point of view, we can decompose each \\(X\\) into a sum of two random variables: \\(X_i = A_i + Z_i\\), where \\(Z_i\\) is a normal distribution with the same mean and variance, and \\(A_i\\) is the “noise” part of it. Each \\(A_i\\) might be overpowering, but when we repeatedly coarse-grain by taking a bunch of \\(X_i\\), and adding them up (a lossy operation!), we would eventually destroy all traces of what cannot survive coarse-graining, and leaving behind a fixed-point of coarse-graining.\nWe define the following letters:\n\n\\(X_1, X_2, \\dots\\) are IID random variables, with characteristic function \\(\\phi_X(t)= E[e^{itX}]\\).\n\\(S_n = X_1 + \\dots + X_n\\).\n\\(a_n, b_n\\) are two sequences of real numbers, such that \\(\\frac{S_n - b_n}{a_n}\\) converges in distribution to a nontrivial random variable \\(Z\\) with characteristic function \\(\\phi(t)\\).\n\n\n\n\n\n\n\nDeriving the field equation by RN\n\n\n\n\n\nSince \\(\\frac{X_1 + \\dots + X_n - b_n}{a_n}\\) converges in distribution to a nontrivial random variable, the sequence \\(a_n\\) must diverge to infinity. For, if the sequence \\(a_n\\) is bounded, then for large enough \\(n\\) , the sum \\(X_1 + \\dots + X_n\\) would spread wider and wider, and dividing it by \\(a_n\\) cannot keep it together.\nLet \\(Z\\) be a random variable with characteristic function \\(\\phi\\). By assumption, \\((S_n- b_n)/a_n\\) is approximately distributed like \\(Z\\) , that is, \\(S_n\\) is approximately distributed as \\(a_nZ + b_n\\). Thus,\n\\[\\phi_{S_n}(t) \\approx e^{ib_n t}\\phi(a_nt )\\]\nGiven \\(1 \\ll n \\ll N\\) , we can compute \\(\\phi_{S_N}\\) in two ways: adding it up as \\(N\\) copies of \\(X\\) , or adding it up as \\(N/n\\) copies of \\(S_n\\). Both should give us the same result. That is: \\[\\phi_{S_N} = \\phi_X^N = \\phi_{S_n}^{N/n}\\]\nHowever, since \\(n\\) is very large, we have the approximations \\(\\phi_{S_n}(t) \\approx e^{ib_n t}\\phi(a_nt )\\). Thus, we have\n\\[\n\\ln \\phi_{S_N}(t) \\approx \\frac{N}{n}(ib_n t + \\ln\\phi(a_n t))\n\\]\nNote how we have an exponent of the form \\(Nf(n)\\) , where \\(N\\) is a very large number, and \\(n\\) is a number that is small compared to it. This is a common pattern in RN calculation.\nSince \\(n\\) is small compared to \\(N\\) , but large compared to \\(1\\) , we can pretend that it’s a continuous variable, and take derivative of it. Since the left side is independent of \\(n\\) , the derivative should be zero:\n\\[\n\\partial_n \\frac{N}{n}(ib_n t + \\ln\\phi(a_n t)) = 0\n\\]\nSimplifying it, and substituting \\(t\\) for \\(a_n t\\) , we get the field equation\n\\[\\frac{\\phi'(t)}{\\phi(t)}t - \\ln \\phi(t) \\frac{a_n}{n \\partial_n  a_n} + it\\partial_n (b_n/n) \\frac{n}{\\partial_n a_n} = 0\\]\n\n\n\nThus, we have obtained the field equation:\n\\[\\frac{\\phi'(t)}{\\phi(t)}t -\\frac{a_n}{n \\partial_n  a_n} \\ln \\phi(t)  + \\frac{n\\partial_n (b_n/n)}{\\partial_n a_n} it = 0\\]\nwhich we can solve by standard mathematical analysis without any more use of RN, so we don’t do those. You can read (Amir 2020) if you are interested.\nHowever, there is a problem: If we have a “field” equation, what is the “field”? Well, here is one way to think of it.\nImagine a line of atoms, at locations \\(1, 2, 3, \\dots\\). Each atom has a height \\(X_1, X_2, X_3, \\dots\\). Now, we can coarse-grain the system by a factor of \\(4\\), by defining\n\\[Y_1 = \\frac{X_1 + \\dots + X_4 - b_4}{a_4}, \\quad Y_2 = \\frac{X_5 + \\dots + X_8 - b_4}{a_4}, \\quad \\dots\\]\nfrom which we can perform another coarse-graining by a factor of \\(100\\), ending up with a coarse-grain by a factor of \\(400\\). Now, if the system has a nontrivial scaling limit, then this should give us the same result as doing a coarse-graining by \\(5\\), then by \\(80\\), or first \\(6\\) then \\(67\\). This is the RN argument we used here.\nNow, since \\(S_n \\approx a_n Z + b_n\\), we see that \\(b_n\\) can be thought of as the coarse-grained height of height-field, and \\(a_n\\) as the coarse-grained jaggedness of the height-field. Then, the field equation describes how the two numbers vary according to \\(n\\).\n\n\n\n\n\n\nExercise: extreme value distribution\n\n\n\n\n\nThe maximum of random variables often has a nontrivial scaling limit as well. That is, there exists some sequence \\(a_n, b_n\\) such that \\(\\frac{\\max(X_1, \\dots, X_n) - b_n}{a_n}\\) converges to a nontrivial distribution with cumulative distribution function (CDF) \\(F\\).\nLet \\(F_X\\) be the CDF of \\(X\\), then we have \\(F_{\\max(X_1, \\dots, X_N)}(t) = F_{\\max(X_1)}(t)^{N}\\). Now, derive the field equation by an RN argument.\nAnswer: \\(\\partial_n \\frac 1n \\ln F(\\frac{t-b_n}{a_n}) = 0\\)."
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#wilsons-nobel-prize-rn-flow-in-momentum-space",
    "href": "sketches/posts/renormalization-how-to/index.html#wilsons-nobel-prize-rn-flow-in-momentum-space",
    "title": "How to Renormalization",
    "section": "Wilson’s Nobel Prize: RN flow in momentum space",
    "text": "Wilson’s Nobel Prize: RN flow in momentum space\nKenneth Wilson was awarded the 1982 Nobel Prize in Physics for his work on phase transitions\nThe modern perspective is the perspective of (Fisher 1998)\n\n\n\n(Fisher 1998, fig. 4)\n\n\n\n\n\n(Fisher 1998, fig. 5)\n\n\n\n\n\nFirefox icon after summing away its high-frequency components."
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#wilsons-nobel-prize-rn-in-momentum-space",
    "href": "sketches/posts/renormalization-how-to/index.html#wilsons-nobel-prize-rn-in-momentum-space",
    "title": "How to do renormalization",
    "section": "Wilson’s Nobel Prize: RN in momentum space",
    "text": "Wilson’s Nobel Prize: RN in momentum space\nRN in momentum space is also called RN in frequency space. It is called “momentum” because it was first done by quantum field theorists to study subatomic particles, and in quantum mechanics, the spatial frequency of a particle-wave is proportional its momentum: \\(p = \\hbar k\\), for which de Broglie was awarded a Nobel Prize in 1929.\nIn the 1970s, Kenneth Wilson invented RN in momentum space and used it to solve many problems. He was awarded the 1982 Nobel Prize in Physics for this work.\n\n\n\nFirefox icon after summing away its high-frequency components.\n\n\nUnfortunately, unlike RN in real space, RN in momentum space is extremely verbose, requiring pages and pages of symbols. Instead of subjecting you to the horrible experience, I will sketch out the big ideas only.\n\nThere remained the possibility that there might be smaller but still infinite quantities left over. No one had the patience needed to calculate whether these theories were actually completely finite. It was reckoned it would take a good student two hundred years, and how would you know he hadn’t made a mistake on the second page? Still, up to 1985, most people believed that most supersymmetric supergravity theories would be free of infinities.\n(Hawking 2001, 52)\n\n\nTo know for sure whether a Feynman diagram with three virtual graviton loops produces infinite quantities, we would need to evaluate \\(10^{20}\\) terms. By five loops, a diagram spawns \\(10^{30}\\) terms … The unitarity method has completely changed the situation … What would have taken the Feynman technique \\(10^{20}\\) terms, we can now do with dozens. … we found that the 1980s era speculations were wrong. Quantities that seemed destined to be infinite are in fact finite. Supergravity is not as nonsensical as physicists thought. In concrete terms, it means that quantum fluctuations of space and time are much more innocuous in supergravity than previously imagined. If you ply us with fine wine, you might catch us speculating that some version of it might be the long sought quantum theory of gravity.\n(Bern, Dixon, and Kosower 2012)\n\n\nField theory: continuous Ising model\nArguably the first field theory was hydrodynamics. Working in the era just after Newton, Euler and Lagrange understood water as a block of infinitely many tiny mass-points. Because each point is so tiny, they do not study the velocity of individual particles of water, but study the velocity field of the entire block of water.\nThe Ising model, with its grid of spins, provides a clear example of this continuum limit. As the grid of spins grow large, the individuals blur together into a field, similar to deriving hydrodynamics from particle dynamics. Let’s take a concrete example, of Ising model on the square grid \\(\\mathbb{Z}^2\\).\nInitially, the system’s energy and partition function are represented as a sum over individual spins:\n\\[\nZ = \\sum_{s: \\mathbb{Z}^2 \\to \\{-1, +1\\}} e^{-H(s)}, \\quad H(s) = -J \\sum_{i, j \\text{ are nearest neighbors}} s_i s_j\n\\]\nRecall how, after two Kadanoff decimations, all forms of spin-spin interactions are unlocked, and so we arrive at an energy function in the most general form:\n\\[\nZ = \\sum_{s: \\mathbb{Z}^2 \\to \\{-1, +1\\}} e^{-H(s)}, \\quad H(s) = -\\sum_{\\text{configuration }C} \\sum_{i_1, i_2, \\dots \\text{ are configured like }C} J_C s_{i_1}s_{i_2}\\cdots\n\\]\nWe can convert the summation into an integral, and suggestively write it as \\(Z = \\int_{s: \\mathbb{Z}^2 \\to \\{-1, +1\\}} e^{-H(s)} ds\\).\nIn the continuous limit, the discrete field of spins \\(s: \\mathbb{Z}^2 \\to \\{-1, +1\\}\\) blurs into a continuous field of spins \\(\\phi: \\mathbb{R}^2 \\to \\mathbb{R}\\). The energy \\(H(s)\\) becomes \\(S[\\phi]\\), a functional5 an \\(\\phi(x)\\) and its gradients. This gives us\n5 A “functional” is nothing but a special kind of function. Specifically, it is a function of type \\(\\text{function} \\to \\text{number}\\). We use square brackets in \\(S[\\phi]\\), not round brackets like \\(S(\\phi)\\), because it is conventional for functionals to use square brackets, not round brackets. It is written as \\(S[\\phi]\\) rather than \\(H[\\phi]\\), and called “action”, because of some old historical usage in variational calculus (as in “the principle of least action”).\\[\nZ = \\int_{\\mathbb{R}^2 \\to \\mathbb{R}} D[\\phi] \\; e^{-S[\\phi]}\n\\]\n\n\nField theory, in general\nConsider a mattress as an analogy. Let the state The state of the mattress is determined by the height and velocity of each point. That is. The energy function of the mattress comprises two quadratic terms: height of each point; height differences between nearby points. We assume the mattress points are massless, so that we have no kinetic energy. We can write the energy of the mattress schematically as\n\\[\n\\text{energy} = H(\\phi) = \\frac 12 K_0 \\sum_i \\phi_i^2 + \\frac 12 K_1 \\sum_{i, j \\text{ are neighbors}} (\\phi_i - \\phi_j)^2\n\\]\n\n\n\nA mattress in space. The energy of the mattress is determined by the height of each point, and the height differences between neighboring points.\n\n\nIf the mattress is held in an atmosphere of temperature \\(T\\), then its state becomes uncertain, following a Boltzmann distribution, just like how a pollen’s position in hot water becomes uncertain, due to Brownian motion.6 The probability that you would find the mattress in state \\(\\phi\\) is then \\(e^{-H(\\phi)/T}/Z\\), where \\(Z\\) is the partition function again:\n6 When statistical field theory gets too scary, I call it “hot water theory” to make it sound nicer.\\[\nZ = \\int d\\phi e^{- H(\\phi)/T}\n\\]\nIn the continuum limit, this analogy leads us to statistical field theory.\nCalculating useful quantities within statistical field theory often involves complex mathematical techniques. This complexity arises from the need to compute the partition function, which involves integrating over all possible states of the system. In the case of the mattress analogy, the partition function involves an integral over all possible height fields, leading to the use of path integrals, written like \\(\\int_{\\phi: \\mathbb{R}^2 \\to \\mathbb{R}} D[\\phi] e^{-S[\\phi]/T}\\).\nIntegrating over \\(\\mathbb{R}^{10^{23}}\\) would be bad enough, let along integrating over \\(\\mathbb{R}^{\\mathbb{R}^{2}}\\), and yet the miracle is that this can be done, and can be done in a way that matches experiments.\nStatistical field theory employs various tricks to calculate the partition function or its limits. A common approach involves alternating between discrete and continuous representations of the field for calculations.\nInterestingly, statistical field theory is almost isomorphic with quantum field theory. The general idea is that if you take the dimension of time in a quantum field theory over space of dimension \\(n\\), and do a substitution \\(t \\mapsto it\\), you somehow end up with a statistical field theory over space of dimension \\(n+1\\). This is the Wick rotation.\nFor example, the 1D Ising model is analogous to a particle in a double well. Whereas the single particle might switch from left to right after time \\(\\Delta T\\), the Ising model chain might switch from \\(+1\\) to \\(-1\\) after space \\(\\Delta L\\). Because of quantum tunneling, it is impossible to confine a particle in one side of the well – it will always jump to the other side, and the jumping probability is on the order of \\(1 - e^{-kt}\\) for some constant \\(k\\). This corresponds to the fact that there is no way to “freeze” an Ising model in one dimension. Even at low temperatures, the system cannot be entirely frozen. Long stretches of “up” spins can suddenly flip to “down” spins. Similarly, in the quantum analogy, the particle can tunnel between the two wells, even as they become deeper.\nGenerally, 1D statistical fields lack phase transitions, just like how a single particle with finitely many states would always quantum-tunnel between states, no matter how cold it gets. Conversely, since the 2D Ising model can be frozen, indicating that the quantum field theory on one dimension should have some kind of phase transition.7\n7 I don’t know what it is, but perhaps Bose–Einstein condensation? I mean, I just flipped through (Herbut 2007) and it looks like this is it. Don’t quote me on this.Another perspective is by noting that magnetism requires spontaneous symmetry breaking: you have more spins pointing up than down, even though the underlying energy function does not distinguish up from down. No symmetry breaking, no phase transition. And since a single particle’s quantum states can always tunnel between each other, it cannot fall into a symmetry-breaking state. However, with infinitely many points, phenomena like spontaneous symmetry breaking can occur, leading to more complex behavior.\n\n\nRN flow in the space of field theories\nThink back to the Ising model on \\(\\mathbb{R}^2\\). What is its action \\(S[\\phi]\\)? If we were mathematically omnipotent, then we can simply perform RN flow on the discrete Ising model, and just find its fixed point, which should hopefully tell us what \\(S\\) is. But we can’t even perform a single RN flow. What to do?\nWell, by universality, we can start with some very different discrete Ising model and end up with the same continuum limit after renormalizing enough times. Why can’t we start with some very different continuous Ising model, discretize it to a discrete Ising model, then renormalize it again until we are back to a continuous Ising model? And if we can do that, why can’t we renormalize directly in the space of all continuous Ising models? We can start with whatever Ising field theory we can write down, and then just repeatedly renormalize it. By universality, we will end up in an interesting place, no matter where we started.\nBut before we do that, we have to construct the space of possible Ising field theories. As Wilson would say, it is all about the symmetries.\nThere are two kinds of symmetries: the symmetry without, and the symmetry within. For the Ising field \\(\\phi: \\mathbb{R}^2 \\to \\mathbb{R}\\), we have Euclidean symmetry for \\(\\mathbb{R}^2\\), and up-down symmetry for \\(\\mathbb{R}\\). To ensure Euclidean geometry, the action \\(S[\\phi]\\) should not explicitly depend on the position. That is, if we take some \\(\\phi\\), and translate it by \\(\\delta\\), then we must have\n\\[\nS[\\phi] = S[x \\mapsto \\phi(x + \\delta)]\n\\]\nSimilarly for reflections and rotations of the field. This shows that \\(S\\) must involve only terms like \\(\\phi, \\nabla \\phi, \\nabla^2 \\phi, \\nabla \\phi \\cdot \\nabla \\phi\\), etc.\nTo account for the symmetry within – the up-down symmetry – we must have \\(S[\\phi] = S[-\\phi]\\). This shows that \\(S\\) must have only even-ordered terms\nUnder these assumptions, you can convince yourself that the most generic form of \\(S\\) is\n\\[\nS[\\phi] = \\int d x\\left[\\frac{1}{2} \\nabla \\phi \\cdot \\nabla \\phi+\\frac{1}{2} \\mu^2 \\phi^2+g \\phi^4+\\cdots\\right]\n\\]\nwhere we removed an irrelevant constant term independent of \\(\\phi\\), and picked the scale of length so that the coefficient for \\(\\frac{1}{2} \\nabla \\phi \\cdot \\nabla \\phi\\) is one.\nThis is the space of all possible Ising field theories. Each Ising field theory is completely specified if we specify the real numbers \\(\\mu, g, \\dots\\). Doing RN would then consist of taking one Ising field theory specified by some \\(\\mu, g, \\dots\\), then renormalize it to some other Ising field theory specified by \\(\\mu', g', \\dots\\). We can then find the fixed points in the theory space, and say, “These are the most interesting theories. Let’s calculate their properties, scaling exponents, and look-and-feel.”\n\n\nRN in momentum space\nConsider an Ising field \\(\\phi: \\mathbb{R}^2 \\to \\mathbb{R}\\). It is of course possible to directly renormalize the field in real space: we blur it a bit, then zoom out. However, this turns out to be very hard to calculate with. Instead, it is much easier to renormalize the field in frequency space.\nTo “blur and zoom out” in real space – what does it look like if we take a Fourier transform? It would look like we are removing some high-frequency vibrations, then expand the vibrations so that low-frequency vibrations become high-frequency vibrations.\nNow we are ready to meet RN in momentum space.\nLet \\(\\tilde \\phi\\) denote the Fourier transform of a field. Now solve8 for \\(\\tilde S\\), so that \\(S[\\phi] = \\tilde S[\\tilde\\phi]\\) for all \\(\\phi\\) in theory space.\n8 This is typically called “Fourier-transform the operator”. For example, the Fourier transform of the gradient operator \\(\\nabla\\) is \\(ik\\), where \\(k\\) is the wave vector.\\[\nZ = \\int D[\\phi] e^{-S[\\phi]} = \\int D[\\tilde\\phi] e^{-\\tilde S[\\tilde\\phi]}\n\\]\n\n\nBonus: How to publish in quantum field theory\n\nWork through a textbook and learn RN in momentum space.\nLearn group theory and group representation theory.\nWrite down many groups with some nice geometry, like \\(SO(3)\\).\nConstruct a group out of those. For example, \\(H = SO(4) \\rtimes SU(3) \\times SU(2) \\times U(1)\\). The group \\(G\\) should have around 10–20 dimensions, but if you are a string theory enthusiast, then 500 dimensions is perfectly fine.\nConstruct another group \\(G\\).\nPick a nice space \\(Y\\). For example, \\(X = \\mathbb{R}^4 \\times \\mathbb{C}^6\\). It must be a space that \\(H\\) can act upon.\nPick another space \\(X\\). It must be a space that \\(G\\) can act upon.\nConstruct the most generic possible functional of type \\(S: (X \\to Y) \\to \\mathbb{C}\\) that is still compatible with the two symmetry groups \\(G, H\\).\nSpend the next month doing RN calculations about\n\n\\[Z := \\int_{\\phi: X \\to Y} D[\\phi] \\; e^{-S[\\phi]}\\]\nprobably with Feynman diagrams scribbled everywhere.\n\nType it up in LaTeX.\nSuffer through peer review, or just put it up on arXiv."
  },
  {
    "objectID": "sketches/posts/renormalization-how-to/index.html#field-theory-rn-on-mathbbrinfty",
    "href": "sketches/posts/renormalization-how-to/index.html#field-theory-rn-on-mathbbrinfty",
    "title": "How to Renormalization",
    "section": "Field theory: RN on \\(\\mathbb{R}^\\infty\\)",
    "text": "Field theory: RN on \\(\\mathbb{R}^\\infty\\)\n\nThe recipe of field-theoretic calculations.\nHere is the reference. You should not read this directly. Instead, you should go directly to the examples below and refer back to this as you go along.\n\nTo calculate: a massive integral of the form\n\n\\[\n\\underbrace{\\int\\cdots\\int}_{\\text{$N \\to \\infty$, with constraint $C$}} (\\text{something}) d^N x\n\\]\n\nThe constraint \\(C\\) is handled by introducing a Dirac delta factor:\n\n\\[\n=_{\\ln} \\underbrace{\\int\\cdots\\int}_{\\text{$N \\to \\infty$}} (\\text{something}) \\times (\\delta^{(n)}(C' - C))^N\n\\]\n\nExpress the Dirac delta function as a Fourier transform using \\(\\delta^{(n)}(x) = \\frac{1}{2\\pi}\\int e^{-i x \\lambda} d\\lambda\\), then exchange the order of integral.\nPut the inner integral into an exponent, and do some rewriting, resulting in something like\n\n\\[\n=_{\\ln} \\underbrace{\\int\\cdots\\int}_{\\text{$n$}} (\\text{something else}) e^{N \\ln \\int(\\cdots)d^N x} d^n\\lambda\n\\]\nDefine \\(S[\\lambda] := -\\ln \\int(\\cdots)d^N x\\), which is sometimes called the field free energy.\n\nArgue that, at large \\(N\\), the integral is dominated by the stationary point of \\(S[\\lambda]\\).\nWrite down \\(\\nabla S = 0\\), and give it the fancy name of mean field equation.\nSolve the mean field equation to be some \\(\\lambda^*\\).\nDeclare the result to be\n\n\\[=_{\\ln} e^{-N S[\\lambda^*]}\\]\nHere, we use the notation \\(=_{\\ln}\\) to mean that they have the same exponential rate. That is, \\(f(N) =_{\\ln} g(N)\\) means that\n\\[\n\\lim_{N \\to \\infty} \\frac 1N \\ln f = \\lim_{N \\to \\infty}  \\frac 1N \\ln g\n\\]\n\n\nSanov’s theorem\n\nStatement of Sanov’s theorem\nSanov’s theorem concerns the large deviation principle for IID samples from a multinomial distribution.\nTo see how Sanov’s theorem works, let’s consider an example. Say you are working at a dice factory, and your job is to quality-assure dices. An ideal dice should have all \\(p_i = 1/6\\), and your job is to test that a given real dice has \\(p_i \\approx 1/6\\). Since you are not able to observe \\(p\\) directly, you can only throw the dice for \\(N\\) rounds, and compute the empirical distribution \\(\\hat p\\) from the outcomes \\(x_1, ..., x_N\\).\nFor example, if you threw it 7 times and you got every number once except \\(1\\) twice, then \\(\\hat p = (2/7, 1/7, ..., 1/7)\\).\nBecause \\(\\hat p\\) depends on the throws, it is itself a random variable. Intuitively, we should expect that \\(\\hat p\\) converging to \\(p\\) as \\(N \\to \\infty\\). Sanov’s theorem states that it is exponentially unlikely for us to be far from the right answer, with the rate of exponential convergence depending on how far we are mistaken. The further \\(\\hat p\\) is from \\(p\\), the faster we can eliminate that possibility.\nLet’s state this more formally.\nDefine:\n\n\\(p_{1:n}\\) is a probability vector.\n\\(x_{1:N}\\) are IID samples from a multinomial distribution with probability vector \\(p_{1:n}\\).\n\\(\\hat{p}\\) is the empirical distribution derived from these samples.\n\\(\\Delta_n\\) is the probability simplex.\n\nAs the number of samples \\(N\\) approaches infinity, the probability of observing a specific empirical distribution \\(\\hat{p}\\) within a closed subset \\(A \\subset \\Delta_n\\) is characterized by the Kullback-Leibler (KL) divergence \\(D_{KL}(\\hat{p} \\| p)\\) between the empirical distribution \\(\\hat{p}\\) and the true distribution \\(p\\).\nFormally stated:\n\nTheorem 1 (Sanov) \\[\\lim_{N \\to \\infty} \\frac{1}{N} \\ln Pr(\\hat{p} \\in A) = -\\inf_{\\hat{p} \\in A} D_{KL}(\\hat{p} \\| p)\\]\n\nWe can get an intuitive feel for Sanov’s theorem by the following video.\nLet us fix \\(n = 3\\) and \\(p = (0.2, 0.3, 0.5)\\). We set \\(n = 3\\) because \\(\\Delta_3\\) has two dimensions, which allows us to actually plot it.\nIf we sample for \\(N\\) times from the multinomial distribution defined by \\(p\\), and plot the heatmap of the samples within \\(\\Delta_2\\) (shown as a black triangle), we notice that as \\(N \\rightarrow \\infty\\), the distribution converges to a gaussian around the point \\((0.2,0.3,0.5)\\), with the contours converging in shape to ellipses, with radii converging as \\(1 / \\sqrt{N}\\).\nMeanwhile, the separation between the discrete points converge as \\(1 / N\\), and so the discrete multinomial distribution converges to a continuous gaussian distribution.\nSo, roughly speaking, we have approximately\n\\[\nPr(\\hat p) \\propto e^{- (\\hat p - p)^T NV (\\hat p - p)}\n\\]\nfor some covariance matrix \\(V\\) that describes the shape of the ellipses.\n\n\n\nNow, if we boldly proceed, we would obtain\n\\[\n\\frac 1N \\ln Pr(\\hat p) \\sim -(\\hat p - p)^T A (\\hat p - p)\n\\]\nThis is not quite right, because for any finite \\(N\\), there are only finitely many possible \\(\\hat p\\). So generally \\(Pr(\\hat p) = 0\\), and to fix this, instead of writing \\(Pr(\\hat p)\\), we should write \\(Pr(\\hat p \\in A)\\) for some closed subset \\(A \\subset \\Delta_n\\).\nNext, since given any two exponentially decaying functions \\(f(N) \\propto e^{-kN}, g(N) \\propto e^{-lN}\\), we have \\(f \\gg g\\) iff \\(k &lt; l\\), we only need to account for the least unlikely case:\n\\[\n\\lim_N \\frac 1N \\ln Pr(\\hat p \\in A) \\approx \\max_{q\\in A} (-(q-p)^T A (q-p)) \\approx - \\min_{q\\in A} D_{KL}(q \\| p)\n\\]\n\nAny large deviation is done in the least unlikely of all the unlikely ways!\n(Den Hollander 2008, 10)\n\n\n\nField-theoretic interpretation\nWe can interpret this problem through the lens of statistical physics. Imagine a system of \\(N\\) identical particles. Each particle has \\(n\\) degrees of freedom. The state of a particle can be one of the unit vectors in \\(\\mathbb{R}^n\\). The problem is to find the distribution of the average state of all the particles.\nThe particles are completely independent of each other – no interaction whatsoever. This is a “no interaction field”. The only thing saving the example from irrelevance is that the particles are still influenced by something – an externally-imposed potential field, biasing the distribution of each particle’s state independently and identically (IID).\nAs typical in statistical mechanics, we suppose each particle is distributed according to the Boltzmann distribution with temperature \\(1\\). This then tells us that the potential field \\(V\\) satisfies\n\\[p_i = e^{-V_i}/Z\\]\nThat is, we can write \\(V_i = -\\ln p_i - \\ln Z\\) where \\(Z\\) is a normalizing constant (partition function again).\nThe average energy per particle (order parameter) is then\n\\[\\bar E \\sum_i \\bar p_i V_i = -\\sum_i \\bar p_i \\ln p_i - \\ln Z\\]\nThe minimum is at \\(\\bar p_i = p_i\\), and if you have seen some statistical mechanics before, you would notice a pattern: a fluctuation in the average energy per particle should be proportional to \\(e^{-N(\\bar E  - \\bar E_{min})}\\). This is not quite right, in the sense that \\(\\bar E - \\bar E_{min}\\) is not the rate function, but it converges to the rate function in a neighborhood of \\(p\\).\n\n\nField-theoretic calculation\nThis section is based on (Mézard and Montanari 2009, sec. 4.7).\nWe use field-theoretic techniques to compute the rate function, treating both the empirical distribution \\(\\hat{p}\\) and the true distribution \\(p\\) as fields over a finite set of points:\n\\[p: \\{1, 2, ..., n\\} \\to \\mathbb{R}\\]\n\n\n\n\n\n\nformulate a constraint\n\n\n\n\n\nWe really want to write \\(Pr(\\hat{p} = q)\\), even though as we noted, this does not make sense. To fix this problem, we introduce an infinitesimal fudge factor of \\(\\epsilon\\).\nThat is, we want to know the probability of observing an empirical distribution \\(\\hat{p}\\) within an infinitesimal neighborhood of a specific distribution \\(q\\):\n\\[Pr(\\hat{p} =_{\\epsilon} q)\\]\nwhere \\(=\\epsilon\\) means that \\(\\hat{p}_i \\in q_i \\pm \\epsilon\\) for each \\(i = 1, 2, \\dots, n\\), or more roughly, that they are within an \\(O(\\epsilon)\\) distance of each other.\nThus, we can write the desired problem in the form of a constrained integral:\n\\[\nPr(\\hat p =_\\epsilon q) = \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( 1[\\hat p(x) = q]\\right)\n\\]\n\n\n\n\n\n\n\n\n\nintroduce a Dirac delta\n\n\n\n\n\nNext, we need to move the constraint from the integral domain to the integrand, in preparation for exchanging the order of integral. We do this by introducing a Dirac delta factor.\nFor any fixed tiny, but non-zero, \\(\\epsilon\\), we have\n\\[\n\\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( 1[\\hat p(x) = q]\\right) =_{\\ln} \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( \\frac{1[\\hat p(x) = q]}{\\mathrm{Vol}(\\text{ball with radius } \\epsilon)}\\right)\n\\]\nbecause even though \\(\\frac{1[\\hat p(x) = q]}{\\mathrm{Vol}(\\text{ball with radius } \\epsilon)}\\) is huge, it is still \\(O(1)\\), and \\(\\lim_{N\\to\\infty} \\frac{(\\text{huge but fixed})}{N} = 0\\).\n\\[\n= \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\delta^{(n)}(\\hat p(x) - q)\n\\]\n\n\n\n\n\n\n\n\n\ndo a Fourier transform\n\n\n\n\n\nNext, we do the Fourier transform of the Dirac delta factor. This step is mostly mechanical.\n\\[\n\\begin{aligned}\n\\delta^{(n)} (\\hat p(x) - q) &= \\delta^{(n)} (N\\hat p(x) - N q) \\\\\n&= \\prod_{k=1}^n \\delta \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right) \\\\\n&= \\prod_{k=1}^n \\frac{1}{2\\pi} \\int d\\lambda_k e^{i\\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&=_{\\ln} \\int d^n \\lambda e^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nexchange the order of integration\n\n\n\n\n\nNow we exchange the order of integration.\n\\[\n\\begin{aligned}\nPr(\\hat p =_\\epsilon q) &= {\\color{red}\\int_{\\mathbb{R}^N} \\rho(x) d^N x} {\\color{red}\\int d^n \\lambda} e^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&= \\int d^n \\lambda \\int_{\\mathbb{R}^N} \\rho(x) d^N xe^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&= \\int d^n \\lambda e^{-iN \\sum_{k=1}^n \\lambda_k q_k}  \\int_{\\mathbb{R}^N} \\rho(x) d^N x e^{i \\sum_{j=1}^N \\sum_{k=1}^n \\lambda_k 1[x_j = k]} \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\ninner integral\n\n\n\n\n\nThe inner integral splits because of the independence of \\(x_1, ..., x_N\\).\n\\[\n\\int_{\\mathbb{R}^N} \\rho(x) d^N x e^{i \\sum_{j=1}^N \\sum_{k=1}^n \\lambda_k 1[x_j = k]} = \\prod_{j=1}^N \\langle e^{i \\sum_{k=1}^n \\lambda_k 1[x_j = k]}\\rangle_{x_j}\n\\]\nwhere the angled bracket denotes a probability expectation, and the subscript \\(x_j\\) denotes what we are taking the expectation over. Because all \\(x_j\\) have the same distribution, it is equal to\n\\[\n\\langle e^{i \\sum_{k=1}^n \\lambda_k 1[x_j = k]}\\rangle_{x_1}^N = \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)^N = e^{N \\ln \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)}\n\\]\n\n\n\n\n\n\n\n\n\nouter integral\n\n\n\n\n\n\\[\n\\begin{aligned}\nPr(\\hat p =_\\epsilon q) &= \\int d^n \\lambda e^{-iN \\sum_{k=1}^n \\lambda_k q_k + N \\ln \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)} \\\\\n&= \\int d^n \\lambda e^{NS[\\lambda]}\n\\end{aligned}\n\\]\nwhere the field free energy is\n\\[S[\\lambda] = \\ln \\left( \\sum_{k\\in 1:n}p_k e^{i \\lambda_k}\\right)-i\\sum_{k\\in 1:n} \\lambda_k q_k\\]\n\n\n\n\n\n\n\n\n\nthe mean field equation\n\n\n\n\n\nThe dominant contribution to the integral arises from the saddle point of the action, which corresponds to the solution of the mean field equation\n\\[\\nabla_{\\lambda} S = 0\\]\nThe mean field equation is solved by some \\(\\lambda^*\\) satisfying:\n\\[\\begin{cases}\ni\\lambda_k^* = \\ln(C q_k/p_k) \\\\\nC = \\sum_k p_k e^{i\\lambda_k^*}\n\\end{cases}\n\\]\nUnfortunately, it is difficult to solve this in closed form, but we are on a lucky break: plugging them back to \\(S[\\lambda^*]\\) gives us a clean solution:\nPlugging those back to \\(S\\), we find that \\[S[\\lambda^*] = -\\sum_{k=1}^n q_k \\ln(q_k/p_k) = -D_{KL}(q\\| p)\\]\n\n\n\nConclusion\n\\[Pr(\\hat p =_{\\epsilon} q) =_{\\ln} e^{NS[\\lambda^*]} = e^{-ND_{KL}(q \\| p)}\\]\n\nThe reader who has never encountered this type of reasoning before may wonder why use such an indirect approach. It turns out that it is a very common formalism in statistical physics, where similar methods are also applied, under the name ‘field theory’, to continuous spaces \\(\\mathcal X\\) (some implicit discretization is then usually assumed at intermediate steps, and the correct definition of a continuum limit is often not obvious). In particular, the reader interested in the statistical-physics approach to optimization problems or information theory will often find this type of calculation in research papers. One of the advantages of this approach is that it provides a formal solution to a large variety of problems. The quantity to be computed is expressed in an integral form. In problems that have a ‘mean-field’ structure, the dimension of the space over which the integration is performed does not depend upon N. Therefore its leading exponential behaviour at large N can be obtained by saddle point methods. The reader who wants to get some practice with this approach is invited to ‘derive’ the various theorems and corollaries of this chapter in this way.\n(Mézard and Montanari 2009, sec. 4.7)\n\n\n\n\nOverlap matrix\n\nSetup\nWe investigate the properties of a set of \\(k\\) random vectors sampled uniformly from a high-dimensional sphere as the dimension \\(N\\) approaches infinity. Let \\(\\sigma_1, ..., \\sigma_k\\) be these vectors, each belonging to \\(\\mathbb{R}^N\\) with a norm of \\(\\sqrt{N}\\), and let \\(\\sigma\\) be the matrix formed by concatenating these vectors: \\(\\sigma = [\\sigma_1, ..., \\sigma_k]\\).\nSince \\(E[\\sigma] = 0\\), we focus on analyzing the variance of \\(\\sigma\\), divided by \\(N\\). Define the matrix \\(\\bar{Q}\\) as the normalized outer product of \\(\\sigma\\):\n\\[\\bar Q := \\sigma^T \\sigma/N = \\frac 1N\n\\begin{bmatrix}\n\\sigma_1^T\\sigma_1 & \\sigma_1^T\\sigma_2 & \\cdots &\\sigma_1^T\\sigma_k\\\\\n\\sigma_2^T\\sigma_1 & \\sigma_2^T\\sigma_2 & \\cdots &\\sigma_2^T\\sigma_k \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_k^T\\sigma_1 & \\sigma_k^T\\sigma_2 & \\cdots & \\sigma_k^T\\sigma_k\n\\end{bmatrix}\\]\nWe know that \\(\\bar{Q}\\) is symmetric, has all entries within the range \\([-1, +1]\\), and has diagonal entries equal to \\(+1\\). Our goal is to uncover further properties of this matrix as \\(N\\) becomes very large.\nLet \\(Q\\) be an arbitrary symmetric matrix with entries in the range \\([-1, +1]\\) and with diagonal entries equal to \\(+1\\). We aim to calculate the rate function \\(S\\) that quantifies the probability of observing an empirical distribution \\(\\bar{Q}\\) that is close to \\(Q\\) as \\(N\\to\\infty\\):\n\\[Pr(\\bar Q =_\\epsilon Q) =_{\\ln} e^{-NS[Q]}\\]\nMore rigorously, we seek to determine:\n\\[\\lim _{\\epsilon \\rightarrow 0} \\lim _{N \\rightarrow \\infty} \\frac{1}{N} \\log Pr \\left(\\bar{Q}(\\sigma)_{i j} \\in\\left[Q_{i j}-\\epsilon, Q_{i j}+\\epsilon\\right], \\forall i, j\\right)\\]\n\n\nField-theoretic interpretation\nWe can interpret this problem through the lens of statistical physics. Imagine a system of \\(N\\) identical particles, each possessing \\(k\\) degrees of freedom. For instance, the \\(i\\)-th particle has degrees of freedom represented by \\((\\sigma_{1, i}, \\dots, \\sigma_{k, i})\\).\nThese particles interact with each other equally, regardless of their spatial separation. This “infinite-range interaction” imposes a global constraint on the system, ensuring a form of “average kinetic energy conservation”. We express this constraint as:\n\\[\\forall j\\in 1:k, \\quad \\sum_{i \\in 1:N}\\sigma_{j, i}^2 = N\\]\nTo illustrate, if we consider \\(\\sigma_{j, i}\\) as the type-\\(j\\) velocity of the \\(i\\)-th particle, then the constraint implies that the average type-\\(j\\) kinetic energy per particle remains constant at \\(1/2\\), even as the number of particles increases.\nSince \\(\\bar{Q}_{j, j'} = \\frac{1}{N} \\sum_{i\\in 1:N}\\sigma_{j, i} \\sigma_{j', i}\\), the matrix \\(\\bar{Q}\\) represents the average covariance between different types of velocities in this system.\n\n\nField-theoretic calculation\nThis section is based on (Mei 2021, lecture 8).\n\n\n\n\n\n\nformulate a constraint\n\n\n\n\n\nTo prepare for the introduction of the Dirac delta factor, we need to move the constraint from the integral domain to the integrand.\n\\[\n\\begin{aligned}\nPr(\\bar Q =_\\epsilon Q) &= \\mathbb{E}_\\sigma [1[\\bar Q(\\sigma) - Q =_\\epsilon 0]]\\\\\n&= \\mathbb{E}_\\sigma \\left[\\prod_{1 \\leq i &lt; j \\leq k} 1[\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0]\\right]\\\\\n\\end{aligned}\n\\]\nNotice well the interplay of two dimensions: The integral \\(\\mathbb{E}_\\sigma\\) is over a very large space, over all particles’ states, of dimension, so it has dimension \\(\\sim k^2 N\\). The constraint \\(1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\\) is on the average states of all particles, so it is over a small space of fixed dimensions \\(\\sim k^2\\).\nAgain, we do that \\(1[x=_\\epsilon 0] \\approx \\delta(x)/\\epsilon\\) trick again, and we get\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln} \\mathbb{E}_\\sigma \\left[\\prod_{1 \\leq i &lt; j \\leq k} \\delta(\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0)\\right]\n\\]\nNotice how we have the product \\(\\prod\\) over \\(1 \\leq i &lt; j \\leq k\\), because \\(\\bar Q, Q\\) are both symmetric, with diagonal entries equal to \\(+1\\). If we were to write something like \\(\\prod_{1 \\leq i {\\color{red} \\leq} j \\leq k}\\), we would cause \\(\\delta(\\bar Q_{i, i}(\\sigma) - Q_{i, i} =_\\epsilon 0)\\) to always be infinite, which does not work.\n\n\n\n\n\n\n\n\n\nformulate a constraint, take 2\n\n\n\n\n\nIt turns out we have not finished with the constraint yet. Back when we did Sanov’s theorem, because the particles are not interacting, we had to only formulate one constraint, a global one where \\(\\hat p =_\\epsilon q\\). In this case, the particles are interacting by the conservation of kinetic energy:\n\\[\\forall j\\in 1:k, \\quad \\frac 1N \\sum_{i \\in 1:N}\\|\\sigma_{j}\\|_2^2 = 1\\]\nLet us go back to the start again:\n\\[\n\\begin{aligned}\nPr(\\bar Q =_\\epsilon Q) &= \\mathbb{E}_\\sigma [1[\\bar Q(\\sigma) - Q =_\\epsilon 0]] \\\\\n&= \\int_{(\\sqrt N S^N)^k}\\frac{d\\sigma}{\\mathrm{Vol}(\\sqrt N S^N)^k} \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\n\\end{aligned}\n\\]\nThe integral over \\((\\sqrt N S^N)^k\\) is uncomfortable. What to do? … That’s right, when the domain of integral is uncomfortable, we use a Dirac delta factor to move it into the integrand:\n\\[\n= \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\delta(\\cdots) d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} \\delta(\\cdots) d\\sigma}\n\\]\nwhere we must fill in the constraint of \\(\\sigma \\in (\\sqrt N S^N)^k\\) into the \\(\\delta(\\cdots)\\). Now, \\(\\sigma \\in (\\sqrt N S^N)^k\\) is equivalent to \\(\\frac 1N \\|\\sigma_{j}\\|^2 - 1 = 0\\) for all \\(j\\in 1:k\\), so naturally, we should try \\(\\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)\\), giving us\n\\[\n= \\frac{\\int_{\\mathbb{R}^{N \\times k}} {\\color{red} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)} d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} {\\color{red} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)} d\\sigma}\n\\]\n\n\n\n\n\n\n\n\n\nthe off-shell trick\n\n\n\n\n\nLet’s take a moment to see what the trick is. The trick is this: We need to integrate something over the uniform distribution on \\((\\sqrt N S^N)^k\\), but integrating over it is difficult, because we don’t have a convenient coordinate system over the sphere \\(\\sqrt N S^N\\). So instead, we slightly thicken each sphere,9 and suddenly we can integrate over the real space \\(\\mathbb{R}^{N \\times k}\\), for which we do have a good coordinate system:\n\\[\n\\mathbb{E}_{\\sigma \\sim \\mathrm{Uniform}((\\sqrt N S^N)^k)} [\\text{something}] = \\lim_{\\epsilon \\downarrow 0} \\mathbb{E}_{\\sigma \\sim \\mathrm{Uniform}((\\sqrt N S^N \\text{thickened by }\\sqrt N \\epsilon )^k)} [\\text{something}]\n\\]\nNow, since\n\\[\n\\sigma_j \\in \\sqrt N S^N \\text{thickened by }\\sqrt N \\epsilon \\iff \\|\\sigma_j\\|^2 - N \\in \\pm N\\epsilon \\iff \\frac 1N \\|\\sigma_j\\|^2 - 1 =_\\epsilon 0\n\\]\nwe can write the constraint as \\(1 \\left( \\frac 1N \\|\\sigma_j\\|^2 - 1 =_\\epsilon 0 \\right) = \\delta \\left( \\frac 1N \\|\\sigma_j\\|^2 - 1 \\right)/\\epsilon\\).\n\n\n\n9 If we return to our field-theoretic interpretation, then allowing \\(\\sigma_j\\) to have norm \\(\\sqrt N \\pm \\sqrt N \\epsilon\\) means that we are allowing the average type-\\(j\\) kinetic energy to fluctuate a bit, even though it is exactly \\(1\\). Quantum field theorists call this off the (kinetic) energy shell, and if you ask, they might wave mysteriously in the air and speak of “virtual particles” and “Faddeev-Popov ghosts”.\n\n\n\n\n\nformulate a constraint, take 3\n\n\n\n\n\nAgain we must handle the constraint of \\(1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\\), but this time it’s different. Whereas before, we had to use \\(\\left[\\prod_{1 \\leq i {\\color{red} &lt;} j \\leq k} 1[\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0]\\right]\\), this time we have to use \\(\\prod_{1 \\leq i {\\color{red} \\leq} j \\leq k}\\). Why? Because this time, we have set \\(\\sigma\\) free from the cage of \\((\\sqrt N S^N)^k\\), so the diagonal entries of \\(\\bar Q(\\sigma)\\) are no longer forced to stay exactly \\(+1\\). Thus, we have to do this instead:\n\\[\n\\begin{aligned}\n&= \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma} \\\\\n&=_{\\ln} \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; \\prod_{1 \\leq s {\\color{red} \\leq} t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma} \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nintroduce a Dirac delta\n\n\n\n\n\nWe have already done the Dirac deltas. To remind you,\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln}\\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; \\prod_{1 \\leq s \\leq t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma}\n\\]\nActually, it is more convenient to scale the \\(\\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)\\) both above and below the fraction to \\(\\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right)\\), and similarly scale the \\(\\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})\\) to \\(\\delta(N\\bar Q_{s, t} - NQ_{s, t}) N\\), giving us\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln}\\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right) d\\sigma \\; \\prod_{1 \\leq s \\leq t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right) d\\sigma}\n\\]\nwhere we have discarded the factor of \\(N^{\\frac 12 k(k+1)}\\), because it does not matter after taking \\(\\frac 1N \\ln(\\cdots)\\).\n\n\n\n\n\n\n\n\n\nevaluate the numerator by field equation\n\n\n\n\n\nSince this time we can’t do the inner integral easily, we will rush directly to the field equation. Don’t worry, as it will all come out correct in the end.\nDo the Fourier transform of the Dirac delta factor, and exchange the order of integration. We first do the numerator.\n\\[\n\\begin{aligned}\n&= \\int_{\\mathbb{R}^{N \\times k}} d\\sigma \\int_{1 \\leq l \\leq k, \\; 1 \\leq i \\leq j \\leq k} d\\lambda_l dq_{ij} e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)} \\\\\n&= \\int_{1 \\leq l \\leq k, \\; 1 \\leq i \\leq j \\leq k} d\\lambda_l dq_{ij} \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)} \\\\\n\\end{aligned}\n\\]\nAs before, we only need to find the “least unlikely of all unlikely ways”. That is, we only need to pick the least tiny of all tiny inner integrals. In other words, we need to find a stationary point where it has finally ceased being so tiny:\n\\[0 = \\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\\]\nBecause we are seeking a stationary point, and we are already doing it physicists, it is no big problem if we seek stationary points over all of the complex plane. That is, we allow \\(q, \\lambda\\) to take not just real, but also complex values.10\nThen we can scale both by \\(-i\\) and get\n\\[\n\\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\nFor cleaner notation, we define \\(q_{ji} = q_{ij}\\), and \\(\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_k)\\). Noting that \\(Q_{ii} = 1\\) for all \\(i\\), we get\n\\[\n\\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\\sum_{ij} (\\Lambda_{ij} + \\frac 12 q_{ij}) (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\nSince the stationary point of this with respect to \\(q, \\lambda\\) is the same as the stationary point of this with respect to \\(\\Lambda + \\frac 12 q\\), we need only\n\\[\n\\nabla_{q}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\\sum_{ij} \\frac 12 q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\n\n\n\n10 If you want justification, look up “the method of steepest descent” (Erdélyi 1956, sec. 2.5). Intuitively speaking, it is because when we are doing an integral like \\(\\int_\\mathbb{R}dq (\\cdots)\\), we are doing a path integral in the complex plane, and so we can deform the path integral in the complex plane and still get the same result. Thus, we can deform it so hard that it walks across a “mountain pass” in the complex plane, where the saddle-point of the mountain pass is where \\(\\nabla_q (\\cdots) = 0\\) – i.e., a stationary point.\n\n\n\n\n\nevaluate the denominator by field equation\n\n\n\n\n\nAt this point, it’s easier to evaluate the denominator first.\nThe same argument given above applies to the denominator. If you are pressed for time, you can just take the previous derivation for the saddle point equation, and set \\(q = 0, Q = 0\\). This gives the field equation\n\\[\n0 = \\nabla_{\\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N)}\n\\]\nNow, the integral is just a gaussian integral, and it factors, too!\n\\[\n\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N)} = \\prod_l e^{N \\lambda_l} \\left(\\int_\\mathbb{R}d\\sigma e^{-\\lambda_l \\sigma^2} \\right)^N = e^{N\\sum_l (\\lambda_l - \\frac 12 \\ln\\lambda_l + \\frac 12 \\ln \\pi)}\n\\]\nIts stationary point is \\(e^{N \\frac k2 ( 1 + \\ln 2\\pi)}\\).\n\n\n\n\n\n\n\n\n\nevaluate the numerator, continued\n\n\n\n\n\nWhere we left off, we had to solve the field equation\n\\[\n0 = \\nabla_{q}e^{\\frac 12 N \\sum_{ij}q_{ij}Q_{ij}} \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{ij} \\frac 12 q_{ij} \\sigma_i^T \\sigma_j}\n\\]\nThe integral is just a gaussian integral, and it factors, too:\n\\[\n\\begin{aligned}\n&= \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\frac 12 \\sum_{ij}q_{ij}\\sigma_i^T \\sigma_j} \\\\\n&= \\left(\\int_{\\mathbb{R}^{ k}} d\\sigma e^{-\\frac 12 \\sum_{ij}q_{ij}\\sigma_i \\sigma_j}\\right)^N \\\\\n&= (2\\pi)^{Nk/2}\\det(q)^{-N/2}\n\\end{aligned}\n\\]\nTaking the derivative, we observe that \\(\\nabla_q \\ln \\det q = (q^{-1})^T\\) for an arbitrary matrix \\(q\\). However, as \\(q\\) is constrained to be a symmetric matrix, we obtain\n\\[\\partial_{q_{ij}}(\\braket{Q,q} - \\ln\\det (q)) = \\begin{cases}\nQ_{ij}+ Q_{ji} - (q^{-1})_{ij} - (q^{-1})_{ji} & i\\neq j \\\\\nQ_{ii}- (q^{-1})_{ii} & i=j\n\\end{cases}\\]\nSetting all derivatives to zero yields the solution \\(q = (Q^{-1})^T = Q^{-1}\\). Notably, there exists only one stationary point within the entire multidimensional complex space.\nWe proceed to compute the numerator, resulting in\n\\[\n=\\exp\\left(\\frac N2(\\braket{Q^{-1}, Q} + k \\ln(2\\pi) - \\ln \\det Q^{-1})\\right) = \\exp\\left(\\frac N2(k+ k \\ln(2\\pi) + \\ln \\det Q)\\right)\n\\]\n\n\n\nIn summary, we have\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln} \\frac{\\exp\\left(\\frac N2(k+ k \\ln(2\\pi) + \\ln \\det Q)\\right)}{\\exp\\left({N \\frac k2 ( 1 + \\ln 2\\pi)}\\right)} = e^{N S[Q]}\n\\]\nwhere the rate function is\n\\[S[Q] = \\frac 12 \\ln \\det Q\\]\n\n\n\nBonus: Cramér’s theorem\nAs a bonus, we prove a common result from large deviation theory called Cramér’s theorem (demboLargeDeviationsTechniques2009?, theorem 2.2.30).\n\nTheorem 2 (Cramér) Given a vector function \\(M: X \\to \\mathbb{R}^m\\) and a distribution on \\(X\\), its rate function is the convex transform of its cumulant generating function:\n\\[I_X(x) := \\sup_{k \\in \\mathbb{R}^m}(\\braket{k,x} - \\ln \\mathbb{E}_x[e^{\\braket{k, M(x)}}])\\]\nThat is, for any compact subset \\(A \\subset \\mathbb{R}^m\\), the rate function over the whole subset is just the highest possible rate:\n\\[\\lim_N\\frac 1N \\ln Pr\\left(\\frac 1N \\sum_{i=1}^N M(x_i) \\in A\\right) = \\sup_{x\\in A} -I_X(x)\\]\nwhere \\(x_1, ...\\) are IID samples from the same distribution.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIt suffices to prove it for \\(A\\) being really small, essentially just a single point, because we can then cut up the whole of \\(A\\) into many pieces like that, and then run the result on each piece. The rate difference is such that only the highest rate can survive, as it races pass every other piece exponentially fast: \\(N^{-1} \\ln (e^{-Na} + e^{-Nb}) \\to \\max(-a, -b)\\), and even if two pieces have the exact same rate, then their combined rate gains a negligible factor of \\(N^{-1}\\ln 2 \\to 0\\).\nSo, we once again repeat the same calculation:\n\\[\n\\begin{aligned}\nPr\\left(\\frac 1N \\sum_{i=1}^N M(x_i) =_\\epsilon m\\right) &=_{\\ln}  \\mathbb{E}_x \\left[\\delta^{(m)}\\left(\\sum_i M(x_i) - Nm\\right)\\right] & \\text{ only $m$ terms in the Dirac delta}\\\\\n&=   \\mathbb{E}_x \\left[\\int_{\\mathbb{R}^m} dq e^{i\\braket{iq, \\sum_i M(x_i) - Nm}}\\right]  & \\text{Dirac delta Fourier transform} \\\\\n&= \\int_{\\mathbb{R}^m} dq e^{-N \\braket{iq, m}}\\mathbb{E}_x[e^{\\braket{iq, M}}]^N& \\text{IID assumption} \\\\\n&= \\int_{\\mathbb{R}^m}dq e^{N(-\\braket{iq, m} + \\ln \\mathbb{E}_x[e^{\\braket{iq, M}}])}\n\\end{aligned}\n\\]\nThe last equation is again dominated by the stationary point. This would give us\n\\[=_{\\ln}\\mathrm{stat}_{q\\in \\mathbb C^m} e^{N(-\\braket{q, m} + \\ln \\mathbb{E}_x[e^{\\braket{q, M}}])}\\]\nWe still don’t know which stationary point we should pick. However, in large deviation theory, it is most often the global minimum, and most often the global minimum is in the real space. Assuming that, we have\n\\[\\frac 1N \\ln Pr\\left(\\frac 1N \\sum_{i=1}^N M(x_i)\\right) \\to -\\sup_{q\\in \\mathbb R^m} (\\braket{q, m} - \\ln \\mathbb{E}_x[e^{\\braket{q, M}}])\\]"
  },
  {
    "objectID": "sketches/posts/draft-rn/index.html",
    "href": "sketches/posts/draft-rn/index.html",
    "title": "How to Renormalization",
    "section": "",
    "text": "Sanov’s theorem concerns the large deviation principle for IID samples from a multinomial distribution.\nTo see how Sanov’s theorem works, let’s consider an example. Say you are working at a dice factory, and your job is to quality-assure dices. An ideal dice should have all \\(p_i = 1/6\\), and your job is to test that a given real dice has \\(p_i \\approx 1/6\\). Since you are not able to observe \\(p\\) directly, you can only throw the dice for \\(N\\) rounds, and compute the empirical distribution \\(\\hat p\\) from the outcomes \\(x_1, ..., x_N\\).\nFor example, if you threw it 7 times and you got every number once except \\(1\\) twice, then \\(\\hat p = (2/7, 1/7, ..., 1/7)\\).\nBecause \\(\\hat p\\) depends on the throws, it is itself a random variable. Intuitively, we should expect that \\(\\hat p\\) converging to \\(p\\) as \\(N \\to \\infty\\). Sanov’s theorem states that it is exponentially unlikely for us to be far from the right answer, with the rate of exponential convergence depending on how far we are mistaken. The further \\(\\hat p\\) is from \\(p\\), the faster we can eliminate that possibility.\nLet’s state this more formally.\nDefine:\n\n\\(p_{1:n}\\) is a probability vector.\n\\(x_{1:N}\\) are IID samples from a multinomial distribution with probability vector \\(p_{1:n}\\).\n\\(\\hat{p}\\) is the empirical distribution derived from these samples.\n\\(\\Delta_n\\) is the probability simplex.\n\nAs the number of samples \\(N\\) approaches infinity, the probability of observing a specific empirical distribution \\(\\hat{p}\\) within a closed subset \\(A \\subset \\Delta_n\\) is characterized by the Kullback-Leibler (KL) divergence \\(D_{KL}(\\hat{p} \\| p)\\) between the empirical distribution \\(\\hat{p}\\) and the true distribution \\(p\\).\nFormally stated:\n\nTheorem 1 (Sanov) \\[\\lim_{N \\to \\infty} \\frac{1}{N} \\ln Pr(\\hat{p} \\in A) = -\\inf_{\\hat{p} \\in A} D_{KL}(\\hat{p} \\| p)\\]\n\nWe can get an intuitive feel for Sanov’s theorem by the following video.\nLet us fix \\(n = 3\\) and \\(p = (0.2, 0.3, 0.5)\\). We set \\(n = 3\\) because \\(\\Delta_3\\) has two dimensions, which allows us to actually plot it.\nIf we sample for \\(N\\) times from the multinomial distribution defined by \\(p\\), and plot the heatmap of the samples within \\(\\Delta_2\\) (shown as a black triangle), we notice that as \\(N \\rightarrow \\infty\\), the distribution converges to a gaussian around the point \\((0.2,0.3,0.5)\\), with the contours converging in shape to ellipses, with radii converging as \\(1 / \\sqrt{N}\\).\nMeanwhile, the separation between the discrete points converge as \\(1 / N\\), and so the discrete multinomial distribution converges to a continuous gaussian distribution.\nSo, roughly speaking, we have approximately\n\\[\nPr(\\hat p) \\propto e^{- (\\hat p - p)^T NV (\\hat p - p)}\n\\]\nfor some covariance matrix \\(V\\) that describes the shape of the ellipses.\n\n\n\nNow, if we boldly proceed, we would obtain\n\\[\n\\frac 1N \\ln Pr(\\hat p) \\sim -(\\hat p - p)^T A (\\hat p - p)\n\\]\nThis is not quite right, because for any finite \\(N\\), there are only finitely many possible \\(\\hat p\\). So generally \\(Pr(\\hat p) = 0\\), and to fix this, instead of writing \\(Pr(\\hat p)\\), we should write \\(Pr(\\hat p \\in A)\\) for some closed subset \\(A \\subset \\Delta_n\\).\nNext, since given any two exponentially decaying functions \\(f(N) \\propto e^{-kN}, g(N) \\propto e^{-lN}\\), we have \\(f \\gg g\\) iff \\(k &lt; l\\), we only need to account for the least unlikely case:\n\\[\n\\lim_N \\frac 1N \\ln Pr(\\hat p \\in A) \\approx \\max_{q\\in A} (-(q-p)^T A (q-p)) \\approx - \\min_{q\\in A} D_{KL}(q \\| p)\n\\]\n\nAny large deviation is done in the least unlikely of all the unlikely ways!\n(Den Hollander 2008, 10)\n\n\n\n\nWe can interpret this problem through the lens of statistical physics. Imagine a system of \\(N\\) identical particles. Each particle has \\(n\\) degrees of freedom. The state of a particle can be one of the unit vectors in \\(\\mathbb{R}^n\\). The problem is to find the distribution of the average state of all the particles.\nThe particles are completely independent of each other – no interaction whatsoever. This is a “no interaction field”. The only thing saving the example from irrelevance is that the particles are still influenced by something – an externally-imposed potential field, biasing the distribution of each particle’s state independently and identically (IID).\nAs typical in statistical mechanics, we suppose each particle is distributed according to the Boltzmann distribution with temperature \\(1\\). This then tells us that the potential field \\(V\\) satisfies\n\\[p_i = e^{-V_i}/Z\\]\nThat is, we can write \\(V_i = -\\ln p_i - \\ln Z\\) where \\(Z\\) is a normalizing constant (partition function again).\nThe average energy per particle (order parameter) is then\n\\[\\bar E \\sum_i \\bar p_i V_i = -\\sum_i \\bar p_i \\ln p_i - \\ln Z\\]\nThe minimum is at \\(\\bar p_i = p_i\\), and if you have seen some statistical mechanics before, you would notice a pattern: a fluctuation in the average energy per particle should be proportional to \\(e^{-N(\\bar E  - \\bar \\mathbb{E}_{min})}\\). This is not quite right, in the sense that \\(\\bar E - \\bar \\mathbb{E}_{min}\\) is not the rate function, but it converges to the rate function in a neighborhood of \\(p\\).\n\n\n\nThis section is based on (Mézard and Montanari 2009, sec. 4.7).\nWe use field-theoretic techniques to compute the rate function, treating both the empirical distribution \\(\\hat{p}\\) and the true distribution \\(p\\) as fields over a finite set of points:\n\\[p: \\{1, 2, ..., n\\} \\to \\mathbb{R}\\]\n\n\n\n\n\n\nformulate a constraint\n\n\n\n\n\nWe really want to write \\(Pr(\\hat{p} = q)\\), even though as we noted, this does not make sense. To fix this problem, we introduce an infinitesimal fudge factor of \\(\\epsilon\\).\nThat is, we want to know the probability of observing an empirical distribution \\(\\hat{p}\\) within an infinitesimal neighborhood of a specific distribution \\(q\\):\n\\[Pr(\\hat{p} =_{\\epsilon} q)\\]\nwhere \\(=\\epsilon\\) means that \\(\\hat{p}_i \\in q_i \\pm \\epsilon\\) for each \\(i = 1, 2, \\dots, n\\), or more roughly, that they are within an \\(O(\\epsilon)\\) distance of each other.\nThus, we can write the desired problem in the form of a constrained integral:\n\\[\nPr(\\hat p =_\\epsilon q) = \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( 1[\\hat p(x) = q]\\right)\n\\]\n\n\n\n\n\n\n\n\n\nintroduce a Dirac delta\n\n\n\n\n\nNext, we need to move the constraint from the integral domain to the integrand, in preparation for exchanging the order of integral. We do this by introducing a Dirac delta factor.\nFor any fixed tiny, but non-zero, \\(\\epsilon\\), we have\n\\[\n\\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( 1[\\hat p(x) = q]\\right) =_{\\ln} \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( \\frac{1[\\hat p(x) = q]}{\\mathrm{Vol}(\\text{ball with radius } \\epsilon)}\\right)\n\\]\nbecause even though \\(\\frac{1[\\hat p(x) = q]}{\\mathrm{Vol}(\\text{ball with radius } \\epsilon)}\\) is huge, it is still \\(O(1)\\), and \\(\\lim_{N\\to\\infty} \\frac{(\\text{huge but fixed})}{N} = 0\\).\n\\[\n= \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\delta^{(n)}(\\hat p(x) - q)\n\\]\n\n\n\n\n\n\n\n\n\ndo a Fourier transform\n\n\n\n\n\nNext, we do the Fourier transform of the Dirac delta factor. This step is mostly mechanical.\n\\[\n\\begin{aligned}\n\\delta^{(n)} (\\hat p(x) - q) &= \\delta^{(n)} (N\\hat p(x) - N q) \\\\\n&= \\prod_{k=1}^n \\delta \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right) \\\\\n&= \\prod_{k=1}^n \\frac{1}{2\\pi} \\int d\\lambda_k e^{i\\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&=_{\\ln} \\int d^n \\lambda e^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nexchange the order of integration\n\n\n\n\n\nNow we exchange the order of integration.\n\\[\n\\begin{aligned}\nPr(\\hat p =_\\epsilon q) &= {\\color{red}\\int_{\\mathbb{R}^N} \\rho(x) d^N x} {\\color{red}\\int d^n \\lambda} e^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&= \\int d^n \\lambda \\int_{\\mathbb{R}^N} \\rho(x) d^N xe^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&= \\int d^n \\lambda e^{-iN \\sum_{k=1}^n \\lambda_k q_k}  \\int_{\\mathbb{R}^N} \\rho(x) d^N x e^{i \\sum_{j=1}^N \\sum_{k=1}^n \\lambda_k 1[x_j = k]} \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\ninner integral\n\n\n\n\n\nThe inner integral splits because of the independence of \\(x_1, ..., x_N\\).\n\\[\n\\int_{\\mathbb{R}^N} \\rho(x) d^N x e^{i \\sum_{j=1}^N \\sum_{k=1}^n \\lambda_k 1[x_j = k]} = \\prod_{j=1}^N \\langle e^{i \\sum_{k=1}^n \\lambda_k 1[x_j = k]}\\rangle_{x_j}\n\\]\nwhere the angled bracket denotes a probability expectation, and the subscript \\(x_j\\) denotes what we are taking the expectation over. Because all \\(x_j\\) have the same distribution, it is equal to\n\\[\n\\langle e^{i \\sum_{k=1}^n \\lambda_k 1[x_j = k]}\\rangle_{x_1}^N = \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)^N = e^{N \\ln \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)}\n\\]\n\n\n\n\n\n\n\n\n\nouter integral\n\n\n\n\n\n\\[\n\\begin{aligned}\nPr(\\hat p =_\\epsilon q) &= \\int d^n \\lambda e^{-iN \\sum_{k=1}^n \\lambda_k q_k + N \\ln \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)} \\\\\n&= \\int d^n \\lambda e^{NS[\\lambda]}\n\\end{aligned}\n\\]\nwhere the field free energy is\n\\[S[\\lambda] = \\ln \\left( \\sum_{k\\in 1:n}p_k e^{i \\lambda_k}\\right)-i\\sum_{k\\in 1:n} \\lambda_k q_k\\]\n\n\n\n\n\n\n\n\n\nthe mean field equation\n\n\n\n\n\nThe dominant contribution to the integral arises from the saddle point of the action, which corresponds to the solution of the mean field equation\n\\[\\nabla_{\\lambda} S = 0\\]\nThe mean field equation is solved by some \\(\\lambda^*\\) satisfying:\n\\[\\begin{cases}\ni\\lambda_k^* = \\ln(C q_k/p_k) \\\\\nC = \\sum_k p_k e^{i\\lambda_k^*}\n\\end{cases}\n\\]\nUnfortunately, it is difficult to solve this in closed form, but we are on a lucky break: plugging them back to \\(S[\\lambda^*]\\) gives us a clean solution:\nPlugging those back to \\(S\\), we find that \\[S[\\lambda^*] = -\\sum_{k=1}^n q_k \\ln(q_k/p_k) = -D_{KL}(q\\| p)\\]\n\n\n\nConclusion\n\\[Pr(\\hat p =_{\\epsilon} q) =_{\\ln} e^{NS[\\lambda^*]} = e^{-ND_{KL}(q \\| p)}\\]\n\nThe reader who has never encountered this type of reasoning before may wonder why use such an indirect approach. It turns out that it is a very common formalism in statistical physics, where similar methods are also applied, under the name ‘field theory’, to continuous spaces \\(\\mathcal X\\) (some implicit discretization is then usually assumed at intermediate steps, and the correct definition of a continuum limit is often not obvious). In particular, the reader interested in the statistical-physics approach to optimization problems or information theory will often find this type of calculation in research papers. One of the advantages of this approach is that it provides a formal solution to a large variety of problems. The quantity to be computed is expressed in an integral form. In problems that have a ‘mean-field’ structure, the dimension of the space over which the integration is performed does not depend upon N. Therefore its leading exponential behaviour at large N can be obtained by saddle point methods. The reader who wants to get some practice with this approach is invited to ‘derive’ the various theorems and corollaries of this chapter in this way.\n(Mézard and Montanari 2009, sec. 4.7)\n\n\n\n\n\nAs a bonus, we prove a common result from large deviation theory called Cramer’s theorem.\n\nTheorem 2 (Cramer) Given a vector function \\(M: X \\to \\mathbb{R}^m\\) and a distribution on \\(X\\), its rate function is the convex transform of its cumulant generating function:\n\\[I_X(x) := \\sup_{k \\in \\mathbb{R}^m}(\\braket{k,x} - \\ln \\mathbb{E}_x[e^{\\braket{k, M(x)}}])\\]\nThat is, for any compact subset \\(A \\subset \\mathbb{R}^m\\), the rate function over the whole subset is just the highest possible rate:\n\\[\\lim_N\\frac 1N \\ln Pr\\left(\\frac 1N \\sum_{i=1}^N M(x_i) \\in A\\right) = \\sup_{x\\in A} -I_X(x)\\]\nwhere \\(x_1, ...\\) are IID samples from the same distribution.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIt suffices to prove it for \\(A\\) being really small, essentially just a single point, because we can then cut up the whole of \\(A\\) into many pieces like that, and then run the result on each piece. The rate difference is such that only the highest rate can survive, as it races pass every other piece exponentially fast: \\(N^{-1} \\ln (e^{-Na} + e^{-Nb}) \\to \\max(-a, -b)\\), and even if two pieces have the exact same rate, then their combined rate gains a negligible factor of \\(N^{-1}\\ln 2 \\to 0\\).\nSo, we once again repeat the same calculation:\n\\[\n\\begin{aligned}\nPr\\left(\\frac 1N \\sum_{i=1}^N M(x_i) =_\\epsilon m\\right) &=_{\\ln}  \\mathbb{E}_x \\left[\\delta^{(m)}\\left(\\sum_i M(x_i) - Nm\\right)\\right] & \\text{ only $m$ terms in the Dirac delta}\\\\\n&=   \\mathbb{E}_x \\left[\\int_{\\mathbb{R}^m} dq e^{i\\braket{iq, \\sum_i M(x_i) - Nm}}\\right]  & \\text{Dirac delta Fourier transform} \\\\\n&= \\int_{\\mathbb{R}^m} dq e^{-N \\braket{iq, m}}\\mathbb{E}_x[e^{\\braket{iq, M}}]^N& \\text{IID assumption} \\\\\n&= \\int_{\\mathbb{R}^m}dq e^{N(-\\braket{iq, m} + \\ln \\mathbb{E}_x[e^{\\braket{iq, M}}])}\n\\end{aligned}\n\\]\nThe last equation is again dominated by the stationary point. This would give us\n\\[=_{\\ln}\\mathrm{stat}_{q\\in \\mathbb C^m} e^{N(-\\braket{q, m} + \\ln \\mathbb{E}_x[e^{\\braket{q, M}}])}\\]\nWe still don’t know which stationary point we should pick. However, in large deviation theory, it is most often the global minimum, and most often the global minimum is in the real space. Assuming that, we have\n\\[\\frac 1N \\ln Pr\\left(\\frac 1N \\sum_{i=1}^N M(x_i)\\right) \\to -\\sup_{q\\in \\mathbb R^m} (\\braket{q, m} - \\ln \\mathbb{E}_x[e^{\\braket{q, M}}])\\]\n\n\n\n\n\n\n\n\nWe investigate the properties of a set of \\(k\\) random vectors sampled uniformly from a high-dimensional sphere as the dimension \\(N\\) approaches infinity. Let \\(\\sigma_1, ..., \\sigma_k\\) be these vectors, each belonging to \\(\\mathbb{R}^N\\) with a norm of \\(\\sqrt{N}\\), and let \\(\\sigma\\) be the matrix formed by concatenating these vectors: \\(\\sigma = [\\sigma_1, ..., \\sigma_k]\\).\nSince \\(E[\\sigma] = 0\\), we focus on analyzing the variance of \\(\\sigma\\), divided by \\(N\\). Define the matrix \\(\\bar{Q}\\) as the normalized outer product of \\(\\sigma\\):\n\\[\\bar Q := \\sigma^T \\sigma/N = \\frac 1N\n\\begin{bmatrix}\n\\sigma_1^T\\sigma_1 & \\sigma_1^T\\sigma_2 & \\cdots &\\sigma_1^T\\sigma_k\\\\\n\\sigma_2^T\\sigma_1 & \\sigma_2^T\\sigma_2 & \\cdots &\\sigma_2^T\\sigma_k \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_k^T\\sigma_1 & \\sigma_k^T\\sigma_2 & \\cdots & \\sigma_k^T\\sigma_k\n\\end{bmatrix}\\]\nWe know that \\(\\bar{Q}\\) is symmetric, has all entries within the range \\([-1, +1]\\), and has diagonal entries equal to \\(+1\\). Our goal is to uncover further properties of this matrix as \\(N\\) becomes very large.\nLet \\(Q\\) be an arbitrary symmetric matrix with entries in the range \\([-1, +1]\\) and with diagonal entries equal to \\(+1\\). We aim to calculate the rate function \\(S\\) that quantifies the probability of observing an empirical distribution \\(\\bar{Q}\\) that is close to \\(Q\\) as \\(N\\to\\infty\\):\n\\[Pr(\\bar Q =_\\epsilon Q) =_{\\ln} e^{-NS[Q]}\\]\nMore rigorously, we seek to determine:\n\\[\\lim _{\\epsilon \\rightarrow 0} \\lim _{N \\rightarrow \\infty} \\frac{1}{N} \\log Pr \\left(\\bar{Q}(\\sigma)_{i j} \\in\\left[Q_{i j}-\\epsilon, Q_{i j}+\\epsilon\\right], \\forall i, j\\right)\\]\n\n\n\nWe can interpret this problem through the lens of statistical physics. Imagine a system of \\(N\\) identical particles, each possessing \\(k\\) degrees of freedom. For instance, the \\(i\\)-th particle has degrees of freedom represented by \\((\\sigma_{1, i}, \\dots, \\sigma_{k, i})\\).\nThese particles interact with each other equally, regardless of their spatial separation. This “infinite-range interaction” imposes a global constraint on the system, ensuring a form of “average kinetic energy conservation”. We express this constraint as:\n\\[\\forall j\\in 1:k, \\quad \\sum_{i \\in 1:N}\\sigma_{j, i}^2 = N\\]\nTo illustrate, if we consider \\(\\sigma_{j, i}\\) as the type-\\(j\\) velocity of the \\(i\\)-th particle, then the constraint implies that the average type-\\(j\\) kinetic energy per particle remains constant at \\(1/2\\), even as the number of particles increases.\nSince \\(\\bar{Q}_{j, j'} = \\frac{1}{N} \\sum_{i\\in 1:N}\\sigma_{j, i} \\sigma_{j', i}\\), the matrix \\(\\bar{Q}\\) represents the average covariance between different types of velocities in this system.\n\n\n\nThis section is based on (Mei 2021, lecture 8).\n\n\n\n\n\n\nformulate a constraint\n\n\n\n\n\nTo prepare for the introduction of the Dirac delta factor, we need to move the constraint from the integral domain to the integrand.\n\\[\n\\begin{aligned}\nPr(\\bar Q =_\\epsilon Q) &= \\mathbb{E}_\\sigma [1[\\bar Q(\\sigma) - Q =_\\epsilon 0]]\\\\\n&= \\mathbb{E}_\\sigma \\left[\\prod_{1 \\leq i &lt; j \\leq k} 1[\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0]\\right]\\\\\n\\end{aligned}\n\\]\nNotice well the interplay of two dimensions: The integral \\(\\mathbb{E}_\\sigma\\) is over a very large space, over all particles’ states, of dimension, so it has dimension \\(\\sim k^2 N\\). The constraint \\(1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\\) is on the average states of all particles, so it is over a small space of fixed dimensions \\(\\sim k^2\\).\nAgain, we do that \\(1[x=_\\epsilon 0] \\approx \\delta(x)/\\epsilon\\) trick again, and we get\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln} \\mathbb{E}_\\sigma \\left[\\prod_{1 \\leq i &lt; j \\leq k} \\delta(\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0)\\right]\n\\]\nNotice how we have the product \\(\\prod\\) over \\(1 \\leq i &lt; j \\leq k\\), because \\(\\bar Q, Q\\) are both symmetric, with diagonal entries equal to \\(+1\\). If we were to write something like \\(\\prod_{1 \\leq i {\\color{red} \\leq} j \\leq k}\\), we would cause \\(\\delta(\\bar Q_{i, i}(\\sigma) - Q_{i, i} =_\\epsilon 0)\\) to always be infinite, which does not work.\n\n\n\n\n\n\n\n\n\nformulate a constraint, take 2\n\n\n\n\n\nIt turns out we have not finished with the constraint yet. Back when we did Sanov’s theorem, because the particles are not interacting, we had to only formulate one constraint, a global one where \\(\\hat p =_\\epsilon q\\). In this case, the particles are interacting by the conservation of kinetic energy:\n\\[\\forall j\\in 1:k, \\quad \\frac 1N \\sum_{i \\in 1:N}\\|\\sigma_{j}\\|_2^2 = 1\\]\nLet us go back to the start again:\n\\[\n\\begin{aligned}\nPr(\\bar Q =_\\epsilon Q) &= \\mathbb{E}_\\sigma [1[\\bar Q(\\sigma) - Q =_\\epsilon 0]] \\\\\n&= \\int_{(\\sqrt N S^N)^k}\\frac{d\\sigma}{\\mathrm{Vol}(\\sqrt N S^N)^k} \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\n\\end{aligned}\n\\]\nThe integral over \\((\\sqrt N S^N)^k\\) is uncomfortable. What to do? … That’s right, when the domain of integral is uncomfortable, we use a Dirac delta factor to move it into the integrand:\n\\[\n= \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\delta(\\cdots) d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} \\delta(\\cdots) d\\sigma}\n\\]\nwhere we must fill in the constraint of \\(\\sigma \\in (\\sqrt N S^N)^k\\) into the \\(\\delta(\\cdots)\\). Now, \\(\\sigma \\in (\\sqrt N S^N)^k\\) is equivalent to \\(\\frac 1N \\|\\sigma_{j}\\|^2 - 1 = 0\\) for all \\(j\\in 1:k\\), so naturally, we should try \\(\\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)\\), giving us\n\\[\n= \\frac{\\int_{\\mathbb{R}^{N \\times k}} {\\color{red} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)} d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} {\\color{red} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)} d\\sigma}\n\\]\n\n\n\n\n\n\n\n\n\nthe off-shell trick\n\n\n\n\n\nLet’s take a moment to see what the trick is. The trick is this: We need to integrate something over the uniform distribution on \\((\\sqrt N S^N)^k\\), but integrating over it is difficult, because we don’t have a convenient coordinate system over the sphere \\(\\sqrt N S^N\\). So instead, we slightly thicken each sphere,1 and suddenly we can integrate over the real space \\(\\mathbb{R}^{N \\times k}\\), for which we do have a good coordinate system:\n\\[\n\\mathbb{E}_{\\sigma \\sim \\mathrm{Uniform}((\\sqrt N S^N)^k)} [\\text{something}] = \\lim_{\\epsilon \\downarrow 0} \\mathbb{E}_{\\sigma \\sim \\mathrm{Uniform}((\\sqrt N S^N \\text{thickened by }\\sqrt N \\epsilon )^k)} [\\text{something}]\n\\]\nNow, since\n\\[\n\\sigma_j \\in \\sqrt N S^N \\text{thickened by }\\sqrt N \\epsilon \\iff \\|\\sigma_j\\|^2 - N \\in \\pm N\\epsilon \\iff \\frac 1N \\|\\sigma_j\\|^2 - 1 =_\\epsilon 0\n\\]\nwe can write the constraint as \\(1 \\left( \\frac 1N \\|\\sigma_j\\|^2 - 1 =_\\epsilon 0 \\right) = \\delta \\left( \\frac 1N \\|\\sigma_j\\|^2 - 1 \\right)/\\epsilon\\).\n\n\n\n1 If we return to our field-theoretic interpretation, then allowing \\(\\sigma_j\\) to have norm \\(\\sqrt N \\pm \\sqrt N \\epsilon\\) means that we are allowing the average type-\\(j\\) kinetic energy to fluctuate a bit, even though it is exactly \\(1\\). Quantum field theorists call this off the (kinetic) energy shell, and if you ask, they might wave mysteriously in the air and speak of “virtual particles” and “Faddeev-Popov ghosts”.\n\n\n\n\n\nformulate a constraint, take 3\n\n\n\n\n\nAgain we must handle the constraint of \\(1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\\), but this time it’s different. Whereas before, we had to use \\(\\left[\\prod_{1 \\leq i {\\color{red} &lt;} j \\leq k} 1[\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0]\\right]\\), this time we have to use \\(\\prod_{1 \\leq i {\\color{red} \\leq} j \\leq k}\\). Why? Because this time, we have set \\(\\sigma\\) free from the cage of \\((\\sqrt N S^N)^k\\), so the diagonal entries of \\(\\bar Q(\\sigma)\\) are no longer forced to stay exactly \\(+1\\). Thus, we have to do this instead:\n\\[\n\\begin{aligned}\n&= \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma} \\\\\n&=_{\\ln} \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; \\prod_{1 \\leq s {\\color{red} \\leq} t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma} \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nintroduce a Dirac delta\n\n\n\n\n\nWe have already done the Dirac deltas. To remind you,\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln}\\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; \\prod_{1 \\leq s \\leq t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma}\n\\]\nActually, it is more convenient to scale the \\(\\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)\\) both above and below the fraction to \\(\\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right)\\), and similarly scale the \\(\\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})\\) to \\(\\delta(N\\bar Q_{s, t} - NQ_{s, t}) N\\), giving us\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln}\\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right) d\\sigma \\; \\prod_{1 \\leq s \\leq t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right) d\\sigma}\n\\]\nwhere we have discarded the factor of \\(N^{\\frac 12 k(k+1)}\\), because it does not matter after taking \\(\\frac 1N \\ln(\\cdots)\\).\n\n\n\n\n\n\n\n\n\nevaluate the numerator by field equation\n\n\n\n\n\nSince this time we can’t do the inner integral easily, we will rush directly to the field equation. Don’t worry, as it will all come out correct in the end.\nDo the Fourier transform of the Dirac delta factor, and exchange the order of integration. We first do the numerator.\n\\[\n\\begin{aligned}\n&= \\int_{\\mathbb{R}^{N \\times k}} d\\sigma \\int_{1 \\leq l \\leq k, \\; 1 \\leq i \\leq j \\leq k} d\\lambda_l dq_{ij} e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)} \\\\\n&= \\int_{1 \\leq l \\leq k, \\; 1 \\leq i \\leq j \\leq k} d\\lambda_l dq_{ij} \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)} \\\\\n\\end{aligned}\n\\]\nAs before, we only need to find the “least unlikely of all unlikely ways”. That is, we only need to pick the least tiny of all tiny inner integrals. In other words, we need to find a stationary point where it has finally ceased being so tiny:\n\\[0 = \\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\\]\nBecause we are seeking a stationary point, and we are already doing it physicists, it is no big problem if we seek stationary points over all of the complex plane. That is, we allow \\(q, \\lambda\\) to take not just real, but also complex values.2\nThen we can scale both by \\(-i\\) and get\n\\[\n\\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\nFor cleaner notation, we define \\(q_{ji} = q_{ij}\\), and \\(\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_k)\\). Noting that \\(Q_{ii} = 1\\) for all \\(i\\), we get\n\\[\n\\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\\sum_{ij} (\\Lambda_{ij} + \\frac 12 q_{ij}) (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\nSince the stationary point of this with respect to \\(q, \\lambda\\) is the same as the stationary point of this with respect to \\(\\Lambda + \\frac 12 q\\), we need only\n\\[\n\\nabla_{q}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\\sum_{ij} \\frac 12 q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\n\n\n\n2 If you want justification, look up “the method of steepest descent” (Erdélyi 1956, sec. 2.5). Intuitively speaking, it is because when we are doing an integral like \\(\\int_\\mathbb{R}dq (\\cdots)\\), we are doing a path integral in the complex plane, and so we can deform the path integral in the complex plane and still get the same result. Thus, we can deform it so hard that it walks across a “mountain pass” in the complex plane, where the saddle-point of the mountain pass is where \\(\\nabla_q (\\cdots) = 0\\) – i.e., a stationary point.\n\n\n\n\n\nevaluate the denominator by field equation\n\n\n\n\n\nAt this point, it’s easier to evaluate the denominator first.\nThe same argument given above applies to the denominator. If you are pressed for time, you can just take the previous derivation for the saddle point equation, and set \\(q = 0, Q = 0\\). This gives the field equation\n\\[\n0 = \\nabla_{\\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N)}\n\\]\nNow, the integral is just a gaussian integral, and it factors, too!\n\\[\n\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N)} = \\prod_l e^{N \\lambda_l} \\left(\\int_\\mathbb{R}d\\sigma e^{-\\lambda_l \\sigma^2} \\right)^N = e^{N\\sum_l (\\lambda_l - \\frac 12 \\ln\\lambda_l + \\frac 12 \\ln \\pi)}\n\\]\nIts stationary point is \\(e^{N \\frac k2 ( 1 + \\ln 2\\pi)}\\).\n\n\n\n\n\n\n\n\n\nevaluate the numerator, continued\n\n\n\n\n\nWhere we left off, we had to solve the field equation\n\\[\n0 = \\nabla_{q}e^{\\frac 12 N \\sum_{ij}q_{ij}Q_{ij}} \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{ij} \\frac 12 q_{ij} \\sigma_i^T \\sigma_j}\n\\]\nThe integral is just a gaussian integral, and it factors, too:\n\\[\n\\begin{aligned}\n&= \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\frac 12 \\sum_{ij}q_{ij}\\sigma_i^T \\sigma_j} \\\\\n&= \\left(\\int_{\\mathbb{R}^{ k}} d\\sigma e^{-\\frac 12 \\sum_{ij}q_{ij}\\sigma_i \\sigma_j}\\right)^N \\\\\n&= (2\\pi)^{Nk/2}\\det(q)^{-N/2}\n\\end{aligned}\n\\]\nTaking the derivative, we observe that \\(\\nabla_q \\ln \\det q = (q^{-1})^T\\) for an arbitrary matrix \\(q\\). However, as \\(q\\) is constrained to be a symmetric matrix, we obtain\n\\[\\partial_{q_{ij}}(\\braket{Q,q} - \\ln\\det (q)) = \\begin{cases}\nQ_{ij}+ Q_{ji} - (q^{-1})_{ij} - (q^{-1})_{ji} & i\\neq j \\\\\nQ_{ii}- (q^{-1})_{ii} & i=j\n\\end{cases}\\]\nSetting all derivatives to zero yields the solution \\(q = (Q^{-1})^T = Q^{-1}\\). Notably, there exists only one stationary point within the entire multidimensional complex space.\nWe proceed to compute the numerator, resulting in\n\\[\n=\\exp\\left(\\frac N2(\\braket{Q^{-1}, Q} + k \\ln(2\\pi) - \\ln \\det Q^{-1})\\right) = \\exp\\left(\\frac N2(k+ k \\ln(2\\pi) + \\ln \\det Q)\\right)\n\\]\n\n\n\nIn summary, we have\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln} \\frac{\\exp\\left(\\frac N2(k+ k \\ln(2\\pi) + \\ln \\det Q)\\right)}{\\exp\\left({N \\frac k2 ( 1 + \\ln 2\\pi)}\\right)} = e^{N S[Q]}\n\\]\nwhere the rate function is\n\\[S[Q] = \\frac 12 \\ln \\det Q\\]"
  },
  {
    "objectID": "sketches/posts/draft-rn/index.html#field-theoretic",
    "href": "sketches/posts/draft-rn/index.html#field-theoretic",
    "title": "How to Renormalization",
    "section": "",
    "text": "Sanov’s theorem concerns the large deviation principle for IID samples from a multinomial distribution.\nTo see how Sanov’s theorem works, let’s consider an example. Say you are working at a dice factory, and your job is to quality-assure dices. An ideal dice should have all \\(p_i = 1/6\\), and your job is to test that a given real dice has \\(p_i \\approx 1/6\\). Since you are not able to observe \\(p\\) directly, you can only throw the dice for \\(N\\) rounds, and compute the empirical distribution \\(\\hat p\\) from the outcomes \\(x_1, ..., x_N\\).\nFor example, if you threw it 7 times and you got every number once except \\(1\\) twice, then \\(\\hat p = (2/7, 1/7, ..., 1/7)\\).\nBecause \\(\\hat p\\) depends on the throws, it is itself a random variable. Intuitively, we should expect that \\(\\hat p\\) converging to \\(p\\) as \\(N \\to \\infty\\). Sanov’s theorem states that it is exponentially unlikely for us to be far from the right answer, with the rate of exponential convergence depending on how far we are mistaken. The further \\(\\hat p\\) is from \\(p\\), the faster we can eliminate that possibility.\nLet’s state this more formally.\nDefine:\n\n\\(p_{1:n}\\) is a probability vector.\n\\(x_{1:N}\\) are IID samples from a multinomial distribution with probability vector \\(p_{1:n}\\).\n\\(\\hat{p}\\) is the empirical distribution derived from these samples.\n\\(\\Delta_n\\) is the probability simplex.\n\nAs the number of samples \\(N\\) approaches infinity, the probability of observing a specific empirical distribution \\(\\hat{p}\\) within a closed subset \\(A \\subset \\Delta_n\\) is characterized by the Kullback-Leibler (KL) divergence \\(D_{KL}(\\hat{p} \\| p)\\) between the empirical distribution \\(\\hat{p}\\) and the true distribution \\(p\\).\nFormally stated:\n\nTheorem 1 (Sanov) \\[\\lim_{N \\to \\infty} \\frac{1}{N} \\ln Pr(\\hat{p} \\in A) = -\\inf_{\\hat{p} \\in A} D_{KL}(\\hat{p} \\| p)\\]\n\nWe can get an intuitive feel for Sanov’s theorem by the following video.\nLet us fix \\(n = 3\\) and \\(p = (0.2, 0.3, 0.5)\\). We set \\(n = 3\\) because \\(\\Delta_3\\) has two dimensions, which allows us to actually plot it.\nIf we sample for \\(N\\) times from the multinomial distribution defined by \\(p\\), and plot the heatmap of the samples within \\(\\Delta_2\\) (shown as a black triangle), we notice that as \\(N \\rightarrow \\infty\\), the distribution converges to a gaussian around the point \\((0.2,0.3,0.5)\\), with the contours converging in shape to ellipses, with radii converging as \\(1 / \\sqrt{N}\\).\nMeanwhile, the separation between the discrete points converge as \\(1 / N\\), and so the discrete multinomial distribution converges to a continuous gaussian distribution.\nSo, roughly speaking, we have approximately\n\\[\nPr(\\hat p) \\propto e^{- (\\hat p - p)^T NV (\\hat p - p)}\n\\]\nfor some covariance matrix \\(V\\) that describes the shape of the ellipses.\n\n\n\nNow, if we boldly proceed, we would obtain\n\\[\n\\frac 1N \\ln Pr(\\hat p) \\sim -(\\hat p - p)^T A (\\hat p - p)\n\\]\nThis is not quite right, because for any finite \\(N\\), there are only finitely many possible \\(\\hat p\\). So generally \\(Pr(\\hat p) = 0\\), and to fix this, instead of writing \\(Pr(\\hat p)\\), we should write \\(Pr(\\hat p \\in A)\\) for some closed subset \\(A \\subset \\Delta_n\\).\nNext, since given any two exponentially decaying functions \\(f(N) \\propto e^{-kN}, g(N) \\propto e^{-lN}\\), we have \\(f \\gg g\\) iff \\(k &lt; l\\), we only need to account for the least unlikely case:\n\\[\n\\lim_N \\frac 1N \\ln Pr(\\hat p \\in A) \\approx \\max_{q\\in A} (-(q-p)^T A (q-p)) \\approx - \\min_{q\\in A} D_{KL}(q \\| p)\n\\]\n\nAny large deviation is done in the least unlikely of all the unlikely ways!\n(Den Hollander 2008, 10)\n\n\n\n\nWe can interpret this problem through the lens of statistical physics. Imagine a system of \\(N\\) identical particles. Each particle has \\(n\\) degrees of freedom. The state of a particle can be one of the unit vectors in \\(\\mathbb{R}^n\\). The problem is to find the distribution of the average state of all the particles.\nThe particles are completely independent of each other – no interaction whatsoever. This is a “no interaction field”. The only thing saving the example from irrelevance is that the particles are still influenced by something – an externally-imposed potential field, biasing the distribution of each particle’s state independently and identically (IID).\nAs typical in statistical mechanics, we suppose each particle is distributed according to the Boltzmann distribution with temperature \\(1\\). This then tells us that the potential field \\(V\\) satisfies\n\\[p_i = e^{-V_i}/Z\\]\nThat is, we can write \\(V_i = -\\ln p_i - \\ln Z\\) where \\(Z\\) is a normalizing constant (partition function again).\nThe average energy per particle (order parameter) is then\n\\[\\bar E \\sum_i \\bar p_i V_i = -\\sum_i \\bar p_i \\ln p_i - \\ln Z\\]\nThe minimum is at \\(\\bar p_i = p_i\\), and if you have seen some statistical mechanics before, you would notice a pattern: a fluctuation in the average energy per particle should be proportional to \\(e^{-N(\\bar E  - \\bar \\mathbb{E}_{min})}\\). This is not quite right, in the sense that \\(\\bar E - \\bar \\mathbb{E}_{min}\\) is not the rate function, but it converges to the rate function in a neighborhood of \\(p\\).\n\n\n\nThis section is based on (Mézard and Montanari 2009, sec. 4.7).\nWe use field-theoretic techniques to compute the rate function, treating both the empirical distribution \\(\\hat{p}\\) and the true distribution \\(p\\) as fields over a finite set of points:\n\\[p: \\{1, 2, ..., n\\} \\to \\mathbb{R}\\]\n\n\n\n\n\n\nformulate a constraint\n\n\n\n\n\nWe really want to write \\(Pr(\\hat{p} = q)\\), even though as we noted, this does not make sense. To fix this problem, we introduce an infinitesimal fudge factor of \\(\\epsilon\\).\nThat is, we want to know the probability of observing an empirical distribution \\(\\hat{p}\\) within an infinitesimal neighborhood of a specific distribution \\(q\\):\n\\[Pr(\\hat{p} =_{\\epsilon} q)\\]\nwhere \\(=\\epsilon\\) means that \\(\\hat{p}_i \\in q_i \\pm \\epsilon\\) for each \\(i = 1, 2, \\dots, n\\), or more roughly, that they are within an \\(O(\\epsilon)\\) distance of each other.\nThus, we can write the desired problem in the form of a constrained integral:\n\\[\nPr(\\hat p =_\\epsilon q) = \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( 1[\\hat p(x) = q]\\right)\n\\]\n\n\n\n\n\n\n\n\n\nintroduce a Dirac delta\n\n\n\n\n\nNext, we need to move the constraint from the integral domain to the integrand, in preparation for exchanging the order of integral. We do this by introducing a Dirac delta factor.\nFor any fixed tiny, but non-zero, \\(\\epsilon\\), we have\n\\[\n\\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( 1[\\hat p(x) = q]\\right) =_{\\ln} \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( \\frac{1[\\hat p(x) = q]}{\\mathrm{Vol}(\\text{ball with radius } \\epsilon)}\\right)\n\\]\nbecause even though \\(\\frac{1[\\hat p(x) = q]}{\\mathrm{Vol}(\\text{ball with radius } \\epsilon)}\\) is huge, it is still \\(O(1)\\), and \\(\\lim_{N\\to\\infty} \\frac{(\\text{huge but fixed})}{N} = 0\\).\n\\[\n= \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\delta^{(n)}(\\hat p(x) - q)\n\\]\n\n\n\n\n\n\n\n\n\ndo a Fourier transform\n\n\n\n\n\nNext, we do the Fourier transform of the Dirac delta factor. This step is mostly mechanical.\n\\[\n\\begin{aligned}\n\\delta^{(n)} (\\hat p(x) - q) &= \\delta^{(n)} (N\\hat p(x) - N q) \\\\\n&= \\prod_{k=1}^n \\delta \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right) \\\\\n&= \\prod_{k=1}^n \\frac{1}{2\\pi} \\int d\\lambda_k e^{i\\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&=_{\\ln} \\int d^n \\lambda e^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nexchange the order of integration\n\n\n\n\n\nNow we exchange the order of integration.\n\\[\n\\begin{aligned}\nPr(\\hat p =_\\epsilon q) &= {\\color{red}\\int_{\\mathbb{R}^N} \\rho(x) d^N x} {\\color{red}\\int d^n \\lambda} e^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&= \\int d^n \\lambda \\int_{\\mathbb{R}^N} \\rho(x) d^N xe^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&= \\int d^n \\lambda e^{-iN \\sum_{k=1}^n \\lambda_k q_k}  \\int_{\\mathbb{R}^N} \\rho(x) d^N x e^{i \\sum_{j=1}^N \\sum_{k=1}^n \\lambda_k 1[x_j = k]} \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\ninner integral\n\n\n\n\n\nThe inner integral splits because of the independence of \\(x_1, ..., x_N\\).\n\\[\n\\int_{\\mathbb{R}^N} \\rho(x) d^N x e^{i \\sum_{j=1}^N \\sum_{k=1}^n \\lambda_k 1[x_j = k]} = \\prod_{j=1}^N \\langle e^{i \\sum_{k=1}^n \\lambda_k 1[x_j = k]}\\rangle_{x_j}\n\\]\nwhere the angled bracket denotes a probability expectation, and the subscript \\(x_j\\) denotes what we are taking the expectation over. Because all \\(x_j\\) have the same distribution, it is equal to\n\\[\n\\langle e^{i \\sum_{k=1}^n \\lambda_k 1[x_j = k]}\\rangle_{x_1}^N = \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)^N = e^{N \\ln \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)}\n\\]\n\n\n\n\n\n\n\n\n\nouter integral\n\n\n\n\n\n\\[\n\\begin{aligned}\nPr(\\hat p =_\\epsilon q) &= \\int d^n \\lambda e^{-iN \\sum_{k=1}^n \\lambda_k q_k + N \\ln \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)} \\\\\n&= \\int d^n \\lambda e^{NS[\\lambda]}\n\\end{aligned}\n\\]\nwhere the field free energy is\n\\[S[\\lambda] = \\ln \\left( \\sum_{k\\in 1:n}p_k e^{i \\lambda_k}\\right)-i\\sum_{k\\in 1:n} \\lambda_k q_k\\]\n\n\n\n\n\n\n\n\n\nthe mean field equation\n\n\n\n\n\nThe dominant contribution to the integral arises from the saddle point of the action, which corresponds to the solution of the mean field equation\n\\[\\nabla_{\\lambda} S = 0\\]\nThe mean field equation is solved by some \\(\\lambda^*\\) satisfying:\n\\[\\begin{cases}\ni\\lambda_k^* = \\ln(C q_k/p_k) \\\\\nC = \\sum_k p_k e^{i\\lambda_k^*}\n\\end{cases}\n\\]\nUnfortunately, it is difficult to solve this in closed form, but we are on a lucky break: plugging them back to \\(S[\\lambda^*]\\) gives us a clean solution:\nPlugging those back to \\(S\\), we find that \\[S[\\lambda^*] = -\\sum_{k=1}^n q_k \\ln(q_k/p_k) = -D_{KL}(q\\| p)\\]\n\n\n\nConclusion\n\\[Pr(\\hat p =_{\\epsilon} q) =_{\\ln} e^{NS[\\lambda^*]} = e^{-ND_{KL}(q \\| p)}\\]\n\nThe reader who has never encountered this type of reasoning before may wonder why use such an indirect approach. It turns out that it is a very common formalism in statistical physics, where similar methods are also applied, under the name ‘field theory’, to continuous spaces \\(\\mathcal X\\) (some implicit discretization is then usually assumed at intermediate steps, and the correct definition of a continuum limit is often not obvious). In particular, the reader interested in the statistical-physics approach to optimization problems or information theory will often find this type of calculation in research papers. One of the advantages of this approach is that it provides a formal solution to a large variety of problems. The quantity to be computed is expressed in an integral form. In problems that have a ‘mean-field’ structure, the dimension of the space over which the integration is performed does not depend upon N. Therefore its leading exponential behaviour at large N can be obtained by saddle point methods. The reader who wants to get some practice with this approach is invited to ‘derive’ the various theorems and corollaries of this chapter in this way.\n(Mézard and Montanari 2009, sec. 4.7)\n\n\n\n\n\nAs a bonus, we prove a common result from large deviation theory called Cramer’s theorem.\n\nTheorem 2 (Cramer) Given a vector function \\(M: X \\to \\mathbb{R}^m\\) and a distribution on \\(X\\), its rate function is the convex transform of its cumulant generating function:\n\\[I_X(x) := \\sup_{k \\in \\mathbb{R}^m}(\\braket{k,x} - \\ln \\mathbb{E}_x[e^{\\braket{k, M(x)}}])\\]\nThat is, for any compact subset \\(A \\subset \\mathbb{R}^m\\), the rate function over the whole subset is just the highest possible rate:\n\\[\\lim_N\\frac 1N \\ln Pr\\left(\\frac 1N \\sum_{i=1}^N M(x_i) \\in A\\right) = \\sup_{x\\in A} -I_X(x)\\]\nwhere \\(x_1, ...\\) are IID samples from the same distribution.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIt suffices to prove it for \\(A\\) being really small, essentially just a single point, because we can then cut up the whole of \\(A\\) into many pieces like that, and then run the result on each piece. The rate difference is such that only the highest rate can survive, as it races pass every other piece exponentially fast: \\(N^{-1} \\ln (e^{-Na} + e^{-Nb}) \\to \\max(-a, -b)\\), and even if two pieces have the exact same rate, then their combined rate gains a negligible factor of \\(N^{-1}\\ln 2 \\to 0\\).\nSo, we once again repeat the same calculation:\n\\[\n\\begin{aligned}\nPr\\left(\\frac 1N \\sum_{i=1}^N M(x_i) =_\\epsilon m\\right) &=_{\\ln}  \\mathbb{E}_x \\left[\\delta^{(m)}\\left(\\sum_i M(x_i) - Nm\\right)\\right] & \\text{ only $m$ terms in the Dirac delta}\\\\\n&=   \\mathbb{E}_x \\left[\\int_{\\mathbb{R}^m} dq e^{i\\braket{iq, \\sum_i M(x_i) - Nm}}\\right]  & \\text{Dirac delta Fourier transform} \\\\\n&= \\int_{\\mathbb{R}^m} dq e^{-N \\braket{iq, m}}\\mathbb{E}_x[e^{\\braket{iq, M}}]^N& \\text{IID assumption} \\\\\n&= \\int_{\\mathbb{R}^m}dq e^{N(-\\braket{iq, m} + \\ln \\mathbb{E}_x[e^{\\braket{iq, M}}])}\n\\end{aligned}\n\\]\nThe last equation is again dominated by the stationary point. This would give us\n\\[=_{\\ln}\\mathrm{stat}_{q\\in \\mathbb C^m} e^{N(-\\braket{q, m} + \\ln \\mathbb{E}_x[e^{\\braket{q, M}}])}\\]\nWe still don’t know which stationary point we should pick. However, in large deviation theory, it is most often the global minimum, and most often the global minimum is in the real space. Assuming that, we have\n\\[\\frac 1N \\ln Pr\\left(\\frac 1N \\sum_{i=1}^N M(x_i)\\right) \\to -\\sup_{q\\in \\mathbb R^m} (\\braket{q, m} - \\ln \\mathbb{E}_x[e^{\\braket{q, M}}])\\]\n\n\n\n\n\n\n\n\nWe investigate the properties of a set of \\(k\\) random vectors sampled uniformly from a high-dimensional sphere as the dimension \\(N\\) approaches infinity. Let \\(\\sigma_1, ..., \\sigma_k\\) be these vectors, each belonging to \\(\\mathbb{R}^N\\) with a norm of \\(\\sqrt{N}\\), and let \\(\\sigma\\) be the matrix formed by concatenating these vectors: \\(\\sigma = [\\sigma_1, ..., \\sigma_k]\\).\nSince \\(E[\\sigma] = 0\\), we focus on analyzing the variance of \\(\\sigma\\), divided by \\(N\\). Define the matrix \\(\\bar{Q}\\) as the normalized outer product of \\(\\sigma\\):\n\\[\\bar Q := \\sigma^T \\sigma/N = \\frac 1N\n\\begin{bmatrix}\n\\sigma_1^T\\sigma_1 & \\sigma_1^T\\sigma_2 & \\cdots &\\sigma_1^T\\sigma_k\\\\\n\\sigma_2^T\\sigma_1 & \\sigma_2^T\\sigma_2 & \\cdots &\\sigma_2^T\\sigma_k \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_k^T\\sigma_1 & \\sigma_k^T\\sigma_2 & \\cdots & \\sigma_k^T\\sigma_k\n\\end{bmatrix}\\]\nWe know that \\(\\bar{Q}\\) is symmetric, has all entries within the range \\([-1, +1]\\), and has diagonal entries equal to \\(+1\\). Our goal is to uncover further properties of this matrix as \\(N\\) becomes very large.\nLet \\(Q\\) be an arbitrary symmetric matrix with entries in the range \\([-1, +1]\\) and with diagonal entries equal to \\(+1\\). We aim to calculate the rate function \\(S\\) that quantifies the probability of observing an empirical distribution \\(\\bar{Q}\\) that is close to \\(Q\\) as \\(N\\to\\infty\\):\n\\[Pr(\\bar Q =_\\epsilon Q) =_{\\ln} e^{-NS[Q]}\\]\nMore rigorously, we seek to determine:\n\\[\\lim _{\\epsilon \\rightarrow 0} \\lim _{N \\rightarrow \\infty} \\frac{1}{N} \\log Pr \\left(\\bar{Q}(\\sigma)_{i j} \\in\\left[Q_{i j}-\\epsilon, Q_{i j}+\\epsilon\\right], \\forall i, j\\right)\\]\n\n\n\nWe can interpret this problem through the lens of statistical physics. Imagine a system of \\(N\\) identical particles, each possessing \\(k\\) degrees of freedom. For instance, the \\(i\\)-th particle has degrees of freedom represented by \\((\\sigma_{1, i}, \\dots, \\sigma_{k, i})\\).\nThese particles interact with each other equally, regardless of their spatial separation. This “infinite-range interaction” imposes a global constraint on the system, ensuring a form of “average kinetic energy conservation”. We express this constraint as:\n\\[\\forall j\\in 1:k, \\quad \\sum_{i \\in 1:N}\\sigma_{j, i}^2 = N\\]\nTo illustrate, if we consider \\(\\sigma_{j, i}\\) as the type-\\(j\\) velocity of the \\(i\\)-th particle, then the constraint implies that the average type-\\(j\\) kinetic energy per particle remains constant at \\(1/2\\), even as the number of particles increases.\nSince \\(\\bar{Q}_{j, j'} = \\frac{1}{N} \\sum_{i\\in 1:N}\\sigma_{j, i} \\sigma_{j', i}\\), the matrix \\(\\bar{Q}\\) represents the average covariance between different types of velocities in this system.\n\n\n\nThis section is based on (Mei 2021, lecture 8).\n\n\n\n\n\n\nformulate a constraint\n\n\n\n\n\nTo prepare for the introduction of the Dirac delta factor, we need to move the constraint from the integral domain to the integrand.\n\\[\n\\begin{aligned}\nPr(\\bar Q =_\\epsilon Q) &= \\mathbb{E}_\\sigma [1[\\bar Q(\\sigma) - Q =_\\epsilon 0]]\\\\\n&= \\mathbb{E}_\\sigma \\left[\\prod_{1 \\leq i &lt; j \\leq k} 1[\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0]\\right]\\\\\n\\end{aligned}\n\\]\nNotice well the interplay of two dimensions: The integral \\(\\mathbb{E}_\\sigma\\) is over a very large space, over all particles’ states, of dimension, so it has dimension \\(\\sim k^2 N\\). The constraint \\(1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\\) is on the average states of all particles, so it is over a small space of fixed dimensions \\(\\sim k^2\\).\nAgain, we do that \\(1[x=_\\epsilon 0] \\approx \\delta(x)/\\epsilon\\) trick again, and we get\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln} \\mathbb{E}_\\sigma \\left[\\prod_{1 \\leq i &lt; j \\leq k} \\delta(\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0)\\right]\n\\]\nNotice how we have the product \\(\\prod\\) over \\(1 \\leq i &lt; j \\leq k\\), because \\(\\bar Q, Q\\) are both symmetric, with diagonal entries equal to \\(+1\\). If we were to write something like \\(\\prod_{1 \\leq i {\\color{red} \\leq} j \\leq k}\\), we would cause \\(\\delta(\\bar Q_{i, i}(\\sigma) - Q_{i, i} =_\\epsilon 0)\\) to always be infinite, which does not work.\n\n\n\n\n\n\n\n\n\nformulate a constraint, take 2\n\n\n\n\n\nIt turns out we have not finished with the constraint yet. Back when we did Sanov’s theorem, because the particles are not interacting, we had to only formulate one constraint, a global one where \\(\\hat p =_\\epsilon q\\). In this case, the particles are interacting by the conservation of kinetic energy:\n\\[\\forall j\\in 1:k, \\quad \\frac 1N \\sum_{i \\in 1:N}\\|\\sigma_{j}\\|_2^2 = 1\\]\nLet us go back to the start again:\n\\[\n\\begin{aligned}\nPr(\\bar Q =_\\epsilon Q) &= \\mathbb{E}_\\sigma [1[\\bar Q(\\sigma) - Q =_\\epsilon 0]] \\\\\n&= \\int_{(\\sqrt N S^N)^k}\\frac{d\\sigma}{\\mathrm{Vol}(\\sqrt N S^N)^k} \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\n\\end{aligned}\n\\]\nThe integral over \\((\\sqrt N S^N)^k\\) is uncomfortable. What to do? … That’s right, when the domain of integral is uncomfortable, we use a Dirac delta factor to move it into the integrand:\n\\[\n= \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\delta(\\cdots) d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} \\delta(\\cdots) d\\sigma}\n\\]\nwhere we must fill in the constraint of \\(\\sigma \\in (\\sqrt N S^N)^k\\) into the \\(\\delta(\\cdots)\\). Now, \\(\\sigma \\in (\\sqrt N S^N)^k\\) is equivalent to \\(\\frac 1N \\|\\sigma_{j}\\|^2 - 1 = 0\\) for all \\(j\\in 1:k\\), so naturally, we should try \\(\\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)\\), giving us\n\\[\n= \\frac{\\int_{\\mathbb{R}^{N \\times k}} {\\color{red} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)} d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} {\\color{red} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)} d\\sigma}\n\\]\n\n\n\n\n\n\n\n\n\nthe off-shell trick\n\n\n\n\n\nLet’s take a moment to see what the trick is. The trick is this: We need to integrate something over the uniform distribution on \\((\\sqrt N S^N)^k\\), but integrating over it is difficult, because we don’t have a convenient coordinate system over the sphere \\(\\sqrt N S^N\\). So instead, we slightly thicken each sphere,1 and suddenly we can integrate over the real space \\(\\mathbb{R}^{N \\times k}\\), for which we do have a good coordinate system:\n\\[\n\\mathbb{E}_{\\sigma \\sim \\mathrm{Uniform}((\\sqrt N S^N)^k)} [\\text{something}] = \\lim_{\\epsilon \\downarrow 0} \\mathbb{E}_{\\sigma \\sim \\mathrm{Uniform}((\\sqrt N S^N \\text{thickened by }\\sqrt N \\epsilon )^k)} [\\text{something}]\n\\]\nNow, since\n\\[\n\\sigma_j \\in \\sqrt N S^N \\text{thickened by }\\sqrt N \\epsilon \\iff \\|\\sigma_j\\|^2 - N \\in \\pm N\\epsilon \\iff \\frac 1N \\|\\sigma_j\\|^2 - 1 =_\\epsilon 0\n\\]\nwe can write the constraint as \\(1 \\left( \\frac 1N \\|\\sigma_j\\|^2 - 1 =_\\epsilon 0 \\right) = \\delta \\left( \\frac 1N \\|\\sigma_j\\|^2 - 1 \\right)/\\epsilon\\).\n\n\n\n1 If we return to our field-theoretic interpretation, then allowing \\(\\sigma_j\\) to have norm \\(\\sqrt N \\pm \\sqrt N \\epsilon\\) means that we are allowing the average type-\\(j\\) kinetic energy to fluctuate a bit, even though it is exactly \\(1\\). Quantum field theorists call this off the (kinetic) energy shell, and if you ask, they might wave mysteriously in the air and speak of “virtual particles” and “Faddeev-Popov ghosts”.\n\n\n\n\n\nformulate a constraint, take 3\n\n\n\n\n\nAgain we must handle the constraint of \\(1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\\), but this time it’s different. Whereas before, we had to use \\(\\left[\\prod_{1 \\leq i {\\color{red} &lt;} j \\leq k} 1[\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0]\\right]\\), this time we have to use \\(\\prod_{1 \\leq i {\\color{red} \\leq} j \\leq k}\\). Why? Because this time, we have set \\(\\sigma\\) free from the cage of \\((\\sqrt N S^N)^k\\), so the diagonal entries of \\(\\bar Q(\\sigma)\\) are no longer forced to stay exactly \\(+1\\). Thus, we have to do this instead:\n\\[\n\\begin{aligned}\n&= \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma} \\\\\n&=_{\\ln} \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; \\prod_{1 \\leq s {\\color{red} \\leq} t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma} \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nintroduce a Dirac delta\n\n\n\n\n\nWe have already done the Dirac deltas. To remind you,\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln}\\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; \\prod_{1 \\leq s \\leq t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma}\n\\]\nActually, it is more convenient to scale the \\(\\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)\\) both above and below the fraction to \\(\\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right)\\), and similarly scale the \\(\\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})\\) to \\(\\delta(N\\bar Q_{s, t} - NQ_{s, t}) N\\), giving us\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln}\\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right) d\\sigma \\; \\prod_{1 \\leq s \\leq t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right) d\\sigma}\n\\]\nwhere we have discarded the factor of \\(N^{\\frac 12 k(k+1)}\\), because it does not matter after taking \\(\\frac 1N \\ln(\\cdots)\\).\n\n\n\n\n\n\n\n\n\nevaluate the numerator by field equation\n\n\n\n\n\nSince this time we can’t do the inner integral easily, we will rush directly to the field equation. Don’t worry, as it will all come out correct in the end.\nDo the Fourier transform of the Dirac delta factor, and exchange the order of integration. We first do the numerator.\n\\[\n\\begin{aligned}\n&= \\int_{\\mathbb{R}^{N \\times k}} d\\sigma \\int_{1 \\leq l \\leq k, \\; 1 \\leq i \\leq j \\leq k} d\\lambda_l dq_{ij} e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)} \\\\\n&= \\int_{1 \\leq l \\leq k, \\; 1 \\leq i \\leq j \\leq k} d\\lambda_l dq_{ij} \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)} \\\\\n\\end{aligned}\n\\]\nAs before, we only need to find the “least unlikely of all unlikely ways”. That is, we only need to pick the least tiny of all tiny inner integrals. In other words, we need to find a stationary point where it has finally ceased being so tiny:\n\\[0 = \\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\\]\nBecause we are seeking a stationary point, and we are already doing it physicists, it is no big problem if we seek stationary points over all of the complex plane. That is, we allow \\(q, \\lambda\\) to take not just real, but also complex values.2\nThen we can scale both by \\(-i\\) and get\n\\[\n\\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\nFor cleaner notation, we define \\(q_{ji} = q_{ij}\\), and \\(\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_k)\\). Noting that \\(Q_{ii} = 1\\) for all \\(i\\), we get\n\\[\n\\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\\sum_{ij} (\\Lambda_{ij} + \\frac 12 q_{ij}) (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\nSince the stationary point of this with respect to \\(q, \\lambda\\) is the same as the stationary point of this with respect to \\(\\Lambda + \\frac 12 q\\), we need only\n\\[\n\\nabla_{q}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\\sum_{ij} \\frac 12 q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\n\n\n\n2 If you want justification, look up “the method of steepest descent” (Erdélyi 1956, sec. 2.5). Intuitively speaking, it is because when we are doing an integral like \\(\\int_\\mathbb{R}dq (\\cdots)\\), we are doing a path integral in the complex plane, and so we can deform the path integral in the complex plane and still get the same result. Thus, we can deform it so hard that it walks across a “mountain pass” in the complex plane, where the saddle-point of the mountain pass is where \\(\\nabla_q (\\cdots) = 0\\) – i.e., a stationary point.\n\n\n\n\n\nevaluate the denominator by field equation\n\n\n\n\n\nAt this point, it’s easier to evaluate the denominator first.\nThe same argument given above applies to the denominator. If you are pressed for time, you can just take the previous derivation for the saddle point equation, and set \\(q = 0, Q = 0\\). This gives the field equation\n\\[\n0 = \\nabla_{\\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N)}\n\\]\nNow, the integral is just a gaussian integral, and it factors, too!\n\\[\n\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N)} = \\prod_l e^{N \\lambda_l} \\left(\\int_\\mathbb{R}d\\sigma e^{-\\lambda_l \\sigma^2} \\right)^N = e^{N\\sum_l (\\lambda_l - \\frac 12 \\ln\\lambda_l + \\frac 12 \\ln \\pi)}\n\\]\nIts stationary point is \\(e^{N \\frac k2 ( 1 + \\ln 2\\pi)}\\).\n\n\n\n\n\n\n\n\n\nevaluate the numerator, continued\n\n\n\n\n\nWhere we left off, we had to solve the field equation\n\\[\n0 = \\nabla_{q}e^{\\frac 12 N \\sum_{ij}q_{ij}Q_{ij}} \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{ij} \\frac 12 q_{ij} \\sigma_i^T \\sigma_j}\n\\]\nThe integral is just a gaussian integral, and it factors, too:\n\\[\n\\begin{aligned}\n&= \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\frac 12 \\sum_{ij}q_{ij}\\sigma_i^T \\sigma_j} \\\\\n&= \\left(\\int_{\\mathbb{R}^{ k}} d\\sigma e^{-\\frac 12 \\sum_{ij}q_{ij}\\sigma_i \\sigma_j}\\right)^N \\\\\n&= (2\\pi)^{Nk/2}\\det(q)^{-N/2}\n\\end{aligned}\n\\]\nTaking the derivative, we observe that \\(\\nabla_q \\ln \\det q = (q^{-1})^T\\) for an arbitrary matrix \\(q\\). However, as \\(q\\) is constrained to be a symmetric matrix, we obtain\n\\[\\partial_{q_{ij}}(\\braket{Q,q} - \\ln\\det (q)) = \\begin{cases}\nQ_{ij}+ Q_{ji} - (q^{-1})_{ij} - (q^{-1})_{ji} & i\\neq j \\\\\nQ_{ii}- (q^{-1})_{ii} & i=j\n\\end{cases}\\]\nSetting all derivatives to zero yields the solution \\(q = (Q^{-1})^T = Q^{-1}\\). Notably, there exists only one stationary point within the entire multidimensional complex space.\nWe proceed to compute the numerator, resulting in\n\\[\n=\\exp\\left(\\frac N2(\\braket{Q^{-1}, Q} + k \\ln(2\\pi) - \\ln \\det Q^{-1})\\right) = \\exp\\left(\\frac N2(k+ k \\ln(2\\pi) + \\ln \\det Q)\\right)\n\\]\n\n\n\nIn summary, we have\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln} \\frac{\\exp\\left(\\frac N2(k+ k \\ln(2\\pi) + \\ln \\det Q)\\right)}{\\exp\\left({N \\frac k2 ( 1 + \\ln 2\\pi)}\\right)} = e^{N S[Q]}\n\\]\nwhere the rate function is\n\\[S[Q] = \\frac 12 \\ln \\det Q\\]"
  },
  {
    "objectID": "sketches/posts/field-theory-how-to/index.html",
    "href": "sketches/posts/field-theory-how-to/index.html",
    "title": "How to do field-theoretic calculations",
    "section": "",
    "text": "General references: (Mézard and Montanari 2009; Mei 2021)\nWhen I was studying machine learning theory, I often encounter problems in high-dimensional probability and statistics, and here and there, I would meet with a confusing statement like “let’s do a field-theoretic calculation”. Field theory? I don’t see any gravitational wave or electromagnetic wave!\nIt turns out that “field-theoretic calculation” does mean something like a field, but in a highly abstracted form. It is not well-described in the literature, and it took me great effort to understand what is going on. The calculations are long, arduous, and filled with opportunities for mistakes.\nStill, if you are going to do machine learning theory (as I am), then learning it is a must. This essay is a practical introduction to field-theoretic calculations through detailed examples, including Sanov’s theorem and the analysis of high-dimensional random vectors using overlap matrices. I aim for clarity, pointing out every pitfall that I have fallen into so that you don’t have to.\nWhile the essay assumes familiarity with basic probability, calculus, and linear algebra, it aims to provide a self-contained and accessible introduction to the field. Readers are encouraged to actively engage with the examples, referring back to the provided recipe for field-theoretic calculations as needed."
  },
  {
    "objectID": "sketches/posts/field-theory-how-to/index.html#the-recipe-of-field-theoretic-calculations.",
    "href": "sketches/posts/field-theory-how-to/index.html#the-recipe-of-field-theoretic-calculations.",
    "title": "How to do field-theoretic calculations",
    "section": "The recipe of field-theoretic calculations.",
    "text": "The recipe of field-theoretic calculations.\nHere is the reference. You should not read this directly. Instead, you should go directly to the examples below and refer back to this as you go along.\n\nTo calculate: a massive integral of the form\n\n\\[\n\\underbrace{\\int\\cdots\\int}_{\\text{$N \\to \\infty$, with constraint $C$}} (\\text{something}) d^N x\n\\]\n\nThe constraint \\(C\\) is handled by introducing a Dirac delta factor:\n\n\\[\n=_{\\ln} \\underbrace{\\int\\cdots\\int}_{\\text{$N \\to \\infty$}} (\\text{something}) \\times (\\delta^{(n)}(C' - C))^N\n\\]\n\nExpress the Dirac delta function as a Fourier transform using \\(\\delta^{(n)}(x) = \\frac{1}{2\\pi}\\int e^{-i x \\lambda} d\\lambda\\), then exchange the order of integral.\nPut the inner integral into an exponent, and do some rewriting, resulting in something like\n\n\\[\n=_{\\ln} \\underbrace{\\int\\cdots\\int}_{\\text{$n$}} (\\text{something else}) e^{N \\ln \\int(\\cdots)d^N x} d^n\\lambda\n\\]\nDefine \\(S[\\lambda] := -\\ln \\int(\\cdots)d^N x\\), which is sometimes called the field free energy.\n\nArgue that, at large \\(N\\), the integral is dominated by the stationary point of \\(S[\\lambda]\\).\nWrite down \\(\\nabla S = 0\\), and give it the fancy name of mean field equation.\nSolve the mean field equation to be some \\(\\lambda^*\\).\nDeclare the result to be\n\n\\[=_{\\ln} e^{-N S[\\lambda^*]}\\]\nHere, we use the notation \\(=_{\\ln}\\) to mean that they have the same exponential rate. That is, \\(f(N) =_{\\ln} g(N)\\) means that\n\\[\n\\lim_{N \\to \\infty} \\frac 1N \\ln f = \\lim_{N \\to \\infty}  \\frac 1N \\ln g\n\\]"
  },
  {
    "objectID": "sketches/posts/field-theory-how-to/index.html#sanovs-theorem",
    "href": "sketches/posts/field-theory-how-to/index.html#sanovs-theorem",
    "title": "How to do field-theoretic calculations",
    "section": "Sanov’s theorem",
    "text": "Sanov’s theorem\n\nStatement of Sanov’s theorem\nSanov’s theorem concerns the large deviation principle for IID samples from a multinomial distribution.\nTo see how Sanov’s theorem works, let’s consider an example. Say you are working at a dice factory, and your job is to quality-assure dices. An ideal dice should have all \\(p_i = 1/6\\), and your job is to test that a given real dice has \\(p_i \\approx 1/6\\). Since you are not able to observe \\(p\\) directly, you can only throw the dice for \\(N\\) rounds, and compute the empirical distribution \\(\\hat p\\) from the outcomes \\(x_1, ..., x_N\\).\nFor example, if you threw it 7 times and you got every number once except \\(1\\) twice, then \\(\\hat p = (2/7, 1/7, ..., 1/7)\\).\nBecause \\(\\hat p\\) depends on the throws, it is itself a random variable. Intuitively, we should expect that \\(\\hat p\\) converging to \\(p\\) as \\(N \\to \\infty\\). Sanov’s theorem states that it is exponentially unlikely for us to be far from the right answer, with the rate of exponential convergence depending on how far we are mistaken. The further \\(\\hat p\\) is from \\(p\\), the faster we can eliminate that possibility.\nLet’s state this more formally.\nDefine:\n\n\\(p_{1:n}\\) is a probability vector.\n\\(x_{1:N}\\) are IID samples from a multinomial distribution with probability vector \\(p_{1:n}\\).\n\\(\\hat{p}\\) is the empirical distribution derived from these samples.\n\\(\\Delta_n\\) is the probability simplex.\n\nAs the number of samples \\(N\\) approaches infinity, the probability of observing a specific empirical distribution \\(\\hat{p}\\) within a closed subset \\(A \\subset \\Delta_n\\) is characterized by the Kullback-Leibler (KL) divergence \\(D_{KL}(\\hat{p} \\| p)\\) between the empirical distribution \\(\\hat{p}\\) and the true distribution \\(p\\).\nFormally stated:\n\nTheorem 1 (Sanov) \\[\\lim_{N \\to \\infty} \\frac{1}{N} \\ln Pr(\\hat{p} \\in A) = -\\inf_{\\hat{p} \\in A} D_{KL}(\\hat{p} \\| p)\\]\n\nWe can get an intuitive feel for Sanov’s theorem by the following video.\nLet us fix \\(n = 3\\) and \\(p = (0.2, 0.3, 0.5)\\). We set \\(n = 3\\) because \\(\\Delta_3\\) has two dimensions, which allows us to actually plot it.\nIf we sample for \\(N\\) times from the multinomial distribution defined by \\(p\\), and plot the heatmap of the samples within \\(\\Delta_2\\) (shown as a black triangle), we notice that as \\(N \\rightarrow \\infty\\), the distribution converges to a gaussian around the point \\((0.2,0.3,0.5)\\), with the contours converging in shape to ellipses, with radii converging as \\(1 / \\sqrt{N}\\).\nMeanwhile, the separation between the discrete points converge as \\(1 / N\\), and so the discrete multinomial distribution converges to a continuous gaussian distribution.\nSo, roughly speaking, we have approximately\n\\[\nPr(\\hat p) \\propto e^{- (\\hat p - p)^T NV (\\hat p - p)}\n\\]\nfor some covariance matrix \\(V\\) that describes the shape of the ellipses.\n\n\n\nNow, if we boldly proceed, we would obtain\n\\[\n\\frac 1N \\ln Pr(\\hat p) \\sim -(\\hat p - p)^T A (\\hat p - p)\n\\]\nThis is not quite right, because for any finite \\(N\\), there are only finitely many possible \\(\\hat p\\). So generally \\(Pr(\\hat p) = 0\\), and to fix this, instead of writing \\(Pr(\\hat p)\\), we should write \\(Pr(\\hat p \\in A)\\) for some closed subset \\(A \\subset \\Delta_n\\).\nNext, since given any two exponentially decaying functions \\(f(N) \\propto e^{-kN}, g(N) \\propto e^{-lN}\\), we have \\(f \\gg g\\) iff \\(k &lt; l\\), we only need to account for the least unlikely case:\n\\[\n\\lim_N \\frac 1N \\ln Pr(\\hat p \\in A) \\approx \\max_{q\\in A} (-(q-p)^T A (q-p)) \\approx - \\min_{q\\in A} D_{KL}(q \\| p)\n\\]\n\nAny large deviation is done in the least unlikely of all the unlikely ways!\n(Den Hollander 2008, 10)\n\n\n\nField-theoretic interpretation\nWe can interpret this problem through the lens of statistical physics. Imagine a system of \\(N\\) identical particles. Each particle has \\(n\\) degrees of freedom. The state of a particle can be one of the unit vectors in \\(\\mathbb{R}^n\\). The problem is to find the distribution of the average state of all the particles.\nThe particles are completely independent of each other – no interaction whatsoever. This is a “no interaction field”. The only thing saving the example from irrelevance is that the particles are still influenced by something – an externally-imposed potential field, biasing the distribution of each particle’s state independently and identically (IID).\nAs typical in statistical mechanics, we suppose each particle is distributed according to the Boltzmann distribution with temperature \\(1\\). This then tells us that the potential field \\(V\\) satisfies\n\\[p_i = e^{-V_i}/Z\\]\nThat is, we can write \\(V_i = -\\ln p_i - \\ln Z\\) where \\(Z\\) is a normalizing constant (partition function again).\nThe average energy per particle (order parameter) is then\n\\[\\bar E \\sum_i \\bar p_i V_i = -\\sum_i \\bar p_i \\ln p_i - \\ln Z\\]\nThe minimum is at \\(\\bar p_i = p_i\\), and if you have seen some statistical mechanics before, you would notice a pattern: a fluctuation in the average energy per particle should be proportional to \\(e^{-N(\\bar E  - \\bar E_{min})}\\). This is not quite right, in the sense that \\(\\bar E - \\bar E_{min}\\) is not the rate function, but it converges to the rate function in a neighborhood of \\(p\\).\n\n\nField-theoretic calculation\nThis section is based on (Mézard and Montanari 2009, sec. 4.7).\nWe use field-theoretic techniques to compute the rate function, treating both the empirical distribution \\(\\hat{p}\\) and the true distribution \\(p\\) as fields over a finite set of points:\n\\[p: \\{1, 2, ..., n\\} \\to \\mathbb{R}\\]\n\n\n\n\n\n\nformulate a constraint\n\n\n\n\n\nWe really want to write \\(Pr(\\hat{p} = q)\\), even though as we noted, this does not make sense. To fix this problem, we introduce an infinitesimal fudge factor of \\(\\epsilon\\).\nThat is, we want to know the probability of observing an empirical distribution \\(\\hat{p}\\) within an infinitesimal neighborhood of a specific distribution \\(q\\):\n\\[Pr(\\hat{p} =_{\\epsilon} q)\\]\nwhere \\(=\\epsilon\\) means that \\(\\hat{p}_i \\in q_i \\pm \\epsilon\\) for each \\(i = 1, 2, \\dots, n\\), or more roughly, that they are within an \\(O(\\epsilon)\\) distance of each other.\nThus, we can write the desired problem in the form of a constrained integral:\n\\[\nPr(\\hat p =_\\epsilon q) = \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( 1[\\hat p(x) = q]\\right)\n\\]\n\n\n\n\n\n\n\n\n\nintroduce a Dirac delta\n\n\n\n\n\nNext, we need to move the constraint from the integral domain to the integrand, in preparation for exchanging the order of integral. We do this by introducing a Dirac delta factor.\nFor any fixed tiny, but non-zero, \\(\\epsilon\\), we have\n\\[\n\\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( 1[\\hat p(x) = q]\\right) =_{\\ln} \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( \\frac{1[\\hat p(x) = q]}{\\mathrm{Vol}(\\text{ball with radius } \\epsilon)}\\right)\n\\]\nbecause even though \\(\\frac{1[\\hat p(x) = q]}{\\mathrm{Vol}(\\text{ball with radius } \\epsilon)}\\) is huge, it is still \\(O(1)\\), and \\(\\lim_{N\\to\\infty} \\frac{(\\text{huge but fixed})}{N} = 0\\).\n\\[\n= \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\delta^{(n)}(\\hat p(x) - q)\n\\]\n\n\n\n\n\n\n\n\n\ndo a Fourier transform\n\n\n\n\n\nNext, we do the Fourier transform of the Dirac delta factor. This step is mostly mechanical.\n\\[\n\\begin{aligned}\n\\delta^{(n)} (\\hat p(x) - q) &= \\delta^{(n)} (N\\hat p(x) - N q) \\\\\n&= \\prod_{k=1}^n \\delta \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right) \\\\\n&= \\prod_{k=1}^n \\frac{1}{2\\pi} \\int d\\lambda_k e^{i\\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&=_{\\ln} \\int d^n \\lambda e^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nexchange the order of integration\n\n\n\n\n\nNow we exchange the order of integration.\n\\[\n\\begin{aligned}\nPr(\\hat p =_\\epsilon q) &= {\\color{red}\\int_{\\mathbb{R}^N} \\rho(x) d^N x} {\\color{red}\\int d^n \\lambda} e^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&= \\int d^n \\lambda \\int_{\\mathbb{R}^N} \\rho(x) d^N xe^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&= \\int d^n \\lambda e^{-iN \\sum_{k=1}^n \\lambda_k q_k}  \\int_{\\mathbb{R}^N} \\rho(x) d^N x e^{i \\sum_{j=1}^N \\sum_{k=1}^n \\lambda_k 1[x_j = k]} \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\ninner integral\n\n\n\n\n\nThe inner integral splits because of the independence of \\(x_1, ..., x_N\\).\n\\[\n\\int_{\\mathbb{R}^N} \\rho(x) d^N x e^{i \\sum_{j=1}^N \\sum_{k=1}^n \\lambda_k 1[x_j = k]} = \\prod_{j=1}^N \\langle e^{i \\sum_{k=1}^n \\lambda_k 1[x_j = k]}\\rangle_{x_j}\n\\]\nwhere the angled bracket denotes a probability expectation, and the subscript \\(x_j\\) denotes what we are taking the expectation over. Because all \\(x_j\\) have the same distribution, it is equal to\n\\[\n\\langle e^{i \\sum_{k=1}^n \\lambda_k 1[x_j = k]}\\rangle_{x_1}^N = \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)^N = e^{N \\ln \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)}\n\\]\n\n\n\n\n\n\n\n\n\nouter integral\n\n\n\n\n\n\\[\n\\begin{aligned}\nPr(\\hat p =_\\epsilon q) &= \\int d^n \\lambda e^{-iN \\sum_{k=1}^n \\lambda_k q_k + N \\ln \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)} \\\\\n&= \\int d^n \\lambda e^{NS[\\lambda]}\n\\end{aligned}\n\\]\nwhere the field free energy is\n\\[S[\\lambda] = \\ln \\left( \\sum_{k\\in 1:n}p_k e^{i \\lambda_k}\\right)-i\\sum_{k\\in 1:n} \\lambda_k q_k\\]\n\n\n\n\n\n\n\n\n\nthe mean field equation\n\n\n\n\n\nThe dominant contribution to the integral arises from the saddle point of the action, which corresponds to the solution of the mean field equation\n\\[\\nabla_{\\lambda} S = 0\\]\nThe mean field equation is solved by some \\(\\lambda^*\\) satisfying:\n\\[\\begin{cases}\ni\\lambda_k^* = \\ln(C q_k/p_k) \\\\\nC = \\sum_k p_k e^{i\\lambda_k^*}\n\\end{cases}\n\\]\nUnfortunately, it is difficult to solve this in closed form, but we are on a lucky break: plugging them back to \\(S[\\lambda^*]\\) gives us a clean solution:\nPlugging those back to \\(S\\), we find that \\[S[\\lambda^*] = -\\sum_{k=1}^n q_k \\ln(q_k/p_k) = -D_{KL}(q\\| p)\\]\n\n\n\nConclusion\n\\[Pr(\\hat p =_{\\epsilon} q) =_{\\ln} e^{NS[\\lambda^*]} = e^{-ND_{KL}(q \\| p)}\\]\n\nThe reader who has never encountered this type of reasoning before may wonder why use such an indirect approach. It turns out that it is a very common formalism in statistical physics, where similar methods are also applied, under the name ‘field theory’, to continuous spaces \\(\\mathcal X\\) (some implicit discretization is then usually assumed at intermediate steps, and the correct definition of a continuum limit is often not obvious). In particular, the reader interested in the statistical-physics approach to optimization problems or information theory will often find this type of calculation in research papers. One of the advantages of this approach is that it provides a formal solution to a large variety of problems. The quantity to be computed is expressed in an integral form. In problems that have a ‘mean-field’ structure, the dimension of the space over which the integration is performed does not depend upon N. Therefore its leading exponential behaviour at large N can be obtained by saddle point methods. The reader who wants to get some practice with this approach is invited to ‘derive’ the various theorems and corollaries of this chapter in this way.\n(Mézard and Montanari 2009, sec. 4.7)"
  },
  {
    "objectID": "sketches/posts/field-theory-how-to/index.html#overlap-matrix",
    "href": "sketches/posts/field-theory-how-to/index.html#overlap-matrix",
    "title": "How to do field-theoretic calculations",
    "section": "Overlap matrix",
    "text": "Overlap matrix\n\nSetup\nWe investigate the properties of a set of \\(k\\) random vectors sampled uniformly from a high-dimensional sphere as the dimension \\(N\\) approaches infinity. Let \\(\\sigma_1, ..., \\sigma_k\\) be these vectors, each belonging to \\(\\mathbb{R}^N\\) with a norm of \\(\\sqrt{N}\\), and let \\(\\sigma\\) be the matrix formed by concatenating these vectors: \\(\\sigma = [\\sigma_1, ..., \\sigma_k]\\).\nSince \\(E[\\sigma] = 0\\), we focus on analyzing the variance of \\(\\sigma\\), divided by \\(N\\). Define the matrix \\(\\bar{Q}\\) as the normalized outer product of \\(\\sigma\\):\n\\[\\bar Q := \\sigma^T \\sigma/N = \\frac 1N\n\\begin{bmatrix}\n\\sigma_1^T\\sigma_1 & \\sigma_1^T\\sigma_2 & \\cdots &\\sigma_1^T\\sigma_k\\\\\n\\sigma_2^T\\sigma_1 & \\sigma_2^T\\sigma_2 & \\cdots &\\sigma_2^T\\sigma_k \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_k^T\\sigma_1 & \\sigma_k^T\\sigma_2 & \\cdots & \\sigma_k^T\\sigma_k\n\\end{bmatrix}\\]\nWe know that \\(\\bar{Q}\\) is symmetric, has all entries within the range \\([-1, +1]\\), and has diagonal entries equal to \\(+1\\). Our goal is to uncover further properties of this matrix as \\(N\\) becomes very large.\nLet \\(Q\\) be an arbitrary symmetric matrix with entries in the range \\([-1, +1]\\) and with diagonal entries equal to \\(+1\\). We aim to calculate the rate function \\(S\\) that quantifies the probability of observing an empirical distribution \\(\\bar{Q}\\) that is close to \\(Q\\) as \\(N\\to\\infty\\):\n\\[Pr(\\bar Q =_\\epsilon Q) =_{\\ln} e^{-NS[Q]}\\]\nMore rigorously, we seek to determine:\n\\[\\lim _{\\epsilon \\rightarrow 0} \\lim _{N \\rightarrow \\infty} \\frac{1}{N} \\log Pr \\left(\\bar{Q}(\\sigma)_{i j} \\in\\left[Q_{i j}-\\epsilon, Q_{i j}+\\epsilon\\right], \\forall i, j\\right)\\]\n\n\nField-theoretic interpretation\nWe can interpret this problem through the lens of statistical physics. Imagine a system of \\(N\\) identical particles, each possessing \\(k\\) degrees of freedom. For instance, the \\(i\\)-th particle has degrees of freedom represented by \\((\\sigma_{1, i}, \\dots, \\sigma_{k, i})\\).\nThese particles interact with each other equally, regardless of their spatial separation. This “infinite-range interaction” imposes a global constraint on the system, ensuring a form of “average kinetic energy conservation”. We express this constraint as:\n\\[\\forall j\\in 1:k, \\quad \\sum_{i \\in 1:N}\\sigma_{j, i}^2 = N\\]\nTo illustrate, if we consider \\(\\sigma_{j, i}\\) as the type-\\(j\\) velocity of the \\(i\\)-th particle, then the constraint implies that the average type-\\(j\\) kinetic energy per particle remains constant at \\(1/2\\), even as the number of particles increases.\nSince \\(\\bar{Q}_{j, j'} = \\frac{1}{N} \\sum_{i\\in 1:N}\\sigma_{j, i} \\sigma_{j', i}\\), the matrix \\(\\bar{Q}\\) represents the average covariance between different types of velocities in this system.\n\n\nField-theoretic calculation\nThis section is based on (Mei 2021, lecture 8).\n\n\n\n\n\n\nformulate a constraint\n\n\n\n\n\nTo prepare for the introduction of the Dirac delta factor, we need to move the constraint from the integral domain to the integrand.\n\\[\n\\begin{aligned}\nPr(\\bar Q =_\\epsilon Q) &= \\mathbb{E}_\\sigma [1[\\bar Q(\\sigma) - Q =_\\epsilon 0]]\\\\\n&= \\mathbb{E}_\\sigma \\left[\\prod_{1 \\leq i &lt; j \\leq k} 1[\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0]\\right]\\\\\n\\end{aligned}\n\\]\nNotice well the interplay of two dimensions: The integral \\(\\mathbb{E}_\\sigma\\) is over a very large space, over all particles’ states, of dimension, so it has dimension \\(\\sim k^2 N\\). The constraint \\(1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\\) is on the average states of all particles, so it is over a small space of fixed dimensions \\(\\sim k^2\\).\nAgain, we do that \\(1[x=_\\epsilon 0] \\approx \\delta(x)/\\epsilon\\) trick again, and we get\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln} \\mathbb{E}_\\sigma \\left[\\prod_{1 \\leq i &lt; j \\leq k} \\delta(\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0)\\right]\n\\]\nNotice how we have the product \\(\\prod\\) over \\(1 \\leq i &lt; j \\leq k\\), because \\(\\bar Q, Q\\) are both symmetric, with diagonal entries equal to \\(+1\\). If we were to write something like \\(\\prod_{1 \\leq i {\\color{red} \\leq} j \\leq k}\\), we would cause \\(\\delta(\\bar Q_{i, i}(\\sigma) - Q_{i, i} =_\\epsilon 0)\\) to always be infinite, which does not work.\n\n\n\n\n\n\n\n\n\nformulate a constraint, take 2\n\n\n\n\n\nIt turns out we have not finished with the constraint yet. Back when we did Sanov’s theorem, because the particles are not interacting, we had to only formulate one constraint, a global one where \\(\\hat p =_\\epsilon q\\). In this case, the particles are interacting by the conservation of kinetic energy:\n\\[\\forall j\\in 1:k, \\quad \\frac 1N \\sum_{i \\in 1:N}\\|\\sigma_{j}\\|_2^2 = 1\\]\nLet us go back to the start again:\n\\[\n\\begin{aligned}\nPr(\\bar Q =_\\epsilon Q) &= \\mathbb{E}_\\sigma [1[\\bar Q(\\sigma) - Q =_\\epsilon 0]] \\\\\n&= \\int_{(\\sqrt N S^N)^k}\\frac{d\\sigma}{\\mathrm{Vol}(\\sqrt N S^N)^k} \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\n\\end{aligned}\n\\]\nThe integral over \\((\\sqrt N S^N)^k\\) is uncomfortable. What to do? … That’s right, when the domain of integral is uncomfortable, we use a Dirac delta factor to move it into the integrand:\n\\[\n= \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\delta(\\cdots) d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} \\delta(\\cdots) d\\sigma}\n\\]\nwhere we must fill in the constraint of \\(\\sigma \\in (\\sqrt N S^N)^k\\) into the \\(\\delta(\\cdots)\\). Now, \\(\\sigma \\in (\\sqrt N S^N)^k\\) is equivalent to \\(\\frac 1N \\|\\sigma_{j}\\|^2 - 1 = 0\\) for all \\(j\\in 1:k\\), so naturally, we should try \\(\\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)\\), giving us\n\\[\n= \\frac{\\int_{\\mathbb{R}^{N \\times k}} {\\color{red} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)} d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} {\\color{red} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)} d\\sigma}\n\\]\n\n\n\n\n\n\n\n\n\nthe off-shell trick\n\n\n\n\n\nLet’s take a moment to see what the trick is. The trick is this: We need to integrate something over the uniform distribution on \\((\\sqrt N S^N)^k\\), but integrating over it is difficult, because we don’t have a convenient coordinate system over the sphere \\(\\sqrt N S^N\\). So instead, we slightly thicken each sphere,1 and suddenly we can integrate over the real space \\(\\mathbb{R}^{N \\times k}\\), for which we do have a good coordinate system:\n\\[\n\\mathbb{E}_{\\sigma \\sim \\mathrm{Uniform}((\\sqrt N S^N)^k)} [\\text{something}] = \\lim_{\\epsilon \\downarrow 0} \\mathbb{E}_{\\sigma \\sim \\mathrm{Uniform}((\\sqrt N S^N \\text{thickened by }\\sqrt N \\epsilon )^k)} [\\text{something}]\n\\]\nNow, since\n\\[\n\\sigma_j \\in \\sqrt N S^N \\text{thickened by }\\sqrt N \\epsilon \\iff \\|\\sigma_j\\|^2 - N \\in \\pm N\\epsilon \\iff \\frac 1N \\|\\sigma_j\\|^2 - 1 =_\\epsilon 0\n\\]\nwe can write the constraint as \\(1 \\left( \\frac 1N \\|\\sigma_j\\|^2 - 1 =_\\epsilon 0 \\right) = \\delta \\left( \\frac 1N \\|\\sigma_j\\|^2 - 1 \\right)/\\epsilon\\).\n\n\n\n1 If we return to our field-theoretic interpretation, then allowing \\(\\sigma_j\\) to have norm \\(\\sqrt N \\pm \\sqrt N \\epsilon\\) means that we are allowing the average type-\\(j\\) kinetic energy to fluctuate a bit, even though it is exactly \\(1\\). Quantum field theorists call this off the (kinetic) energy shell, and if you ask, they might wave mysteriously in the air and speak of “virtual particles” and “Faddeev-Popov ghosts”.\n\n\n\n\n\nformulate a constraint, take 3\n\n\n\n\n\nAgain we must handle the constraint of \\(1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\\), but this time it’s different. Whereas before, we had to use \\(\\left[\\prod_{1 \\leq i {\\color{red} &lt;} j \\leq k} 1[\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0]\\right]\\), this time we have to use \\(\\prod_{1 \\leq i {\\color{red} \\leq} j \\leq k}\\). Why? Because this time, we have set \\(\\sigma\\) free from the cage of \\((\\sqrt N S^N)^k\\), so the diagonal entries of \\(\\bar Q(\\sigma)\\) are no longer forced to stay exactly \\(+1\\). Thus, we have to do this instead:\n\\[\n\\begin{aligned}\n&= \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma} \\\\\n&=_{\\ln} \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; \\prod_{1 \\leq s {\\color{red} \\leq} t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma} \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nintroduce a Dirac delta\n\n\n\n\n\nWe have already done the Dirac deltas. To remind you,\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln}\\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; \\prod_{1 \\leq s \\leq t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma}\n\\]\nActually, it is more convenient to scale the \\(\\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)\\) both above and below the fraction to \\(\\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right)\\), and similarly scale the \\(\\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})\\) to \\(\\delta(N\\bar Q_{s, t} - NQ_{s, t}) N\\), giving us\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln}\\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right) d\\sigma \\; \\prod_{1 \\leq s \\leq t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right) d\\sigma}\n\\]\nwhere we have discarded the factor of \\(N^{\\frac 12 k(k+1)}\\), because it does not matter after taking \\(\\frac 1N \\ln(\\cdots)\\).\n\n\n\n\n\n\n\n\n\nevaluate the numerator by field equation\n\n\n\n\n\nSince this time we can’t do the inner integral easily, we will rush directly to the field equation. Don’t worry, as it will all come out correct in the end.\nDo the Fourier transform of the Dirac delta factor, and exchange the order of integration. We first do the numerator.\n\\[\n\\begin{aligned}\n&= \\int_{\\mathbb{R}^{N \\times k}} d\\sigma \\int_{1 \\leq l \\leq k, \\; 1 \\leq i \\leq j \\leq k} d\\lambda_l dq_{ij} e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)} \\\\\n&= \\int_{1 \\leq l \\leq k, \\; 1 \\leq i \\leq j \\leq k} d\\lambda_l dq_{ij} \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)} \\\\\n\\end{aligned}\n\\]\nAs before, we only need to find the “least unlikely of all unlikely ways”. That is, we only need to pick the least tiny of all tiny inner integrals. In other words, we need to find a stationary point where it has finally ceased being so tiny:\n\\[0 = \\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\\]\nBecause we are seeking a stationary point, and we are already doing it physicists, it is no big problem if we seek stationary points over all of the complex plane. That is, we allow \\(q, \\lambda\\) to take not just real, but also complex values.2\nThen we can scale both by \\(-i\\) and get\n\\[\n\\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\nFor cleaner notation, we define \\(q_{ji} = q_{ij}\\), and \\(\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_k)\\). Noting that \\(Q_{ii} = 1\\) for all \\(i\\), we get\n\\[\n\\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\\sum_{ij} (\\Lambda_{ij} + \\frac 12 q_{ij}) (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\nSince the stationary point of this with respect to \\(q, \\lambda\\) is the same as the stationary point of this with respect to \\(\\Lambda + \\frac 12 q\\), we need only\n\\[\n\\nabla_{q}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\\sum_{ij} \\frac 12 q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\n\n\n\n2 If you want justification, look up “the method of steepest descent” (Erdélyi 1956, sec. 2.5). Intuitively speaking, it is because when we are doing an integral like \\(\\int_\\mathbb{R}dq (\\cdots)\\), we are doing a path integral in the complex plane, and so we can deform the path integral in the complex plane and still get the same result. Thus, we can deform it so hard that it walks across a “mountain pass” in the complex plane, where the saddle-point of the mountain pass is where \\(\\nabla_q (\\cdots) = 0\\) – i.e., a stationary point.\n\n\n\n\n\nevaluate the denominator by field equation\n\n\n\n\n\nAt this point, it’s easier to evaluate the denominator first.\nThe same argument given above applies to the denominator. If you are pressed for time, you can just take the previous derivation for the saddle point equation, and set \\(q = 0, Q = 0\\). This gives the field equation\n\\[\n0 = \\nabla_{\\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N)}\n\\]\nNow, the integral is just a gaussian integral, and it factors, too!\n\\[\n\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N)} = \\prod_l e^{N \\lambda_l} \\left(\\int_\\mathbb{R}d\\sigma e^{-\\lambda_l \\sigma^2} \\right)^N = e^{N\\sum_l (\\lambda_l - \\frac 12 \\ln\\lambda_l + \\frac 12 \\ln \\pi)}\n\\]\nIts stationary point is \\(e^{N \\frac k2 ( 1 + \\ln 2\\pi)}\\).\n\n\n\n\n\n\n\n\n\nevaluate the numerator, continued\n\n\n\n\n\nWhere we left off, we had to solve the field equation\n\\[\n0 = \\nabla_{q}e^{\\frac 12 N \\sum_{ij}q_{ij}Q_{ij}} \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{ij} \\frac 12 q_{ij} \\sigma_i^T \\sigma_j}\n\\]\nThe integral is just a gaussian integral, and it factors, too:\n\\[\n\\begin{aligned}\n&= \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\frac 12 \\sum_{ij}q_{ij}\\sigma_i^T \\sigma_j} \\\\\n&= \\left(\\int_{\\mathbb{R}^{ k}} d\\sigma e^{-\\frac 12 \\sum_{ij}q_{ij}\\sigma_i \\sigma_j}\\right)^N \\\\\n&= (2\\pi)^{Nk/2}\\det(q)^{-N/2}\n\\end{aligned}\n\\]\nTaking the derivative, we observe that \\(\\nabla_q \\ln \\det q = (q^{-1})^T\\) for an arbitrary matrix \\(q\\). However, as \\(q\\) is constrained to be a symmetric matrix, we obtain\n\\[\\partial_{q_{ij}}(\\braket{Q,q} - \\ln\\det (q)) = \\begin{cases}\nQ_{ij}+ Q_{ji} - (q^{-1})_{ij} - (q^{-1})_{ji} & i\\neq j \\\\\nQ_{ii}- (q^{-1})_{ii} & i=j\n\\end{cases}\\]\nSetting all derivatives to zero yields the solution \\(q = (Q^{-1})^T = Q^{-1}\\). Notably, there exists only one stationary point within the entire multidimensional complex space.\nWe proceed to compute the numerator, resulting in\n\\[\n=\\exp\\left(\\frac N2(\\braket{Q^{-1}, Q} + k \\ln(2\\pi) - \\ln \\det Q^{-1})\\right) = \\exp\\left(\\frac N2(k+ k \\ln(2\\pi) + \\ln \\det Q)\\right)\n\\]\n\n\n\nIn summary, we have\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln} \\frac{\\exp\\left(\\frac N2(k+ k \\ln(2\\pi) + \\ln \\det Q)\\right)}{\\exp\\left({N \\frac k2 ( 1 + \\ln 2\\pi)}\\right)} = e^{N S[Q]}\n\\]\nwhere the rate function is\n\\[S[Q] = \\frac 12 \\ln \\det Q\\]"
  },
  {
    "objectID": "sketches/posts/field-theory-how-to/index.html#bonus-cramérs-theorem",
    "href": "sketches/posts/field-theory-how-to/index.html#bonus-cramérs-theorem",
    "title": "How to do field-theoretic calculations",
    "section": "Bonus: Cramér’s theorem",
    "text": "Bonus: Cramér’s theorem\nAs a bonus, we prove a common result from large deviation theory called Cramér’s theorem (Dembo and Zeitouni 2009, theorem 2.2.30).\n\nTheorem 2 (Cramér) Given a vector function \\(M: X \\to \\mathbb{R}^m\\) and a distribution on \\(X\\), its rate function is the convex transform of its cumulant generating function:\n\\[I_X(x) := \\sup_{k \\in \\mathbb{R}^m}(\\braket{k,x} - \\ln \\mathbb{E}_x[e^{\\braket{k, M(x)}}])\\]\nThat is, for any compact subset \\(A \\subset \\mathbb{R}^m\\), the rate function over the whole subset is just the highest possible rate:\n\\[\\lim_N\\frac 1N \\ln Pr\\left(\\frac 1N \\sum_{i=1}^N M(x_i) \\in A\\right) = \\sup_{x\\in A} -I_X(x)\\]\nwhere \\(x_1, ..., x_N\\) are IID samples from the same distribution.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIt suffices to prove it for \\(A\\) being really small, essentially just a single point, because we can then cut up the whole of \\(A\\) into many pieces like that, and then run the result on each piece. The rate difference is such that only the highest rate can survive, as it races pass every other piece exponentially fast: \\(N^{-1} \\ln (e^{-Na} + e^{-Nb}) \\to \\max(-a, -b)\\), and even if two pieces have the exact same rate, then their combined rate gains a negligible factor of \\(N^{-1}\\ln 2 \\to 0\\).\nSo, we once again repeat the same calculation:\n\\[\n\\begin{aligned}\nPr\\left(\\frac 1N \\sum_{i=1}^N M(x_i) =_\\epsilon m\\right) &=_{\\ln}  \\mathbb{E}_x \\left[\\delta^{(m)}\\left(\\sum_i M(x_i) - Nm\\right)\\right] & \\text{ only $m$ terms in the Dirac delta}\\\\\n&=   \\mathbb{E}_x \\left[\\int_{\\mathbb{R}^m} dq e^{i\\braket{iq, \\sum_i M(x_i) - Nm}}\\right]  & \\text{Dirac delta Fourier transform} \\\\\n&= \\int_{\\mathbb{R}^m} dq e^{-N \\braket{iq, m}}\\mathbb{E}_x[e^{\\braket{iq, M}}]^N& \\text{IID assumption} \\\\\n&= \\int_{\\mathbb{R}^m}dq e^{N(-\\braket{iq, m} + \\ln \\mathbb{E}_x[e^{\\braket{iq, M}}])}\n\\end{aligned}\n\\]\nThe last equation is again dominated by the stationary point. This would give us\n\\[=_{\\ln}\\mathrm{stat}_{q\\in \\mathbb C^m} e^{N(-\\braket{q, m} + \\ln \\mathbb{E}_x[e^{\\braket{q, M}}])}\\]\nWe still don’t know which stationary point we should pick. However, in large deviation theory, we usually pick the global minimum, and most often, the global minimum is the unique stationary point in the real space. Assuming that, we have\n\\[\\frac 1N \\ln Pr\\left(\\frac 1N \\sum_{i=1}^N M(x_i)\\right) \\to -\\sup_{q\\in \\mathbb R^m} (\\braket{q, m} - \\ln \\mathbb{E}_x[e^{\\braket{q, M}}])\\]"
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html",
    "href": "essays/posts/renormalization-how-to/index.html",
    "title": "How to do renormalization",
    "section": "",
    "text": "Renormalization (RN) theory is a powerful tool for understanding the behavior of physical systems at different length scales. It allows us to explain why certain features matter when we zoom out and others are washed away. It explains why we have universal classes according to symmetry and dimension and not, say, the precise shape of the atomic force laws. It justifies the use of toy models instead of microscopically accurate models.\nRN theory is based on the idea of self-similarity – that a system looks the same at different length scales. For example, a coastline looks the same whether you are looking at it from a satellite or from a boat. Similarly, a magnet looks the same whether you are looking at it with a microscope or with your naked eye.\nThe basic idea of RN is to start with a microscopic description of a system and then to coarse-grain it. That is, to average over the details of the system at a smaller length scale to obtain a description of the system at a larger length scale. This process can be repeated, leading to a sequence of descriptions of the system at increasingly larger length scales.\nThe key insight of RN theory is that the long-distance behavior of a system does not depend on the details of the short-distance interactions. This is known as the universality hypothesis. Universality explains why systems that are very different at the microscopic level can have the same behavior at the macroscopic level. For example, water, oil, and electricity all exhibit the same percolation behavior near the percolation threshold, even though they are very different at the microscopic level.\nUniversality also justifies the use of toy models to study physical systems. Toy models are simplified models that capture the essential features of a system without including all of the details. For example, the Ising model is a toy model of a magnet that captures the essential features of ferromagnetism without including all of the details of the interactions between the atoms.\nIn this essay, we will explore the basic ideas of RN theory and see how it can be used to understand a variety of physical systems."
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html#what-is-renormalization",
    "href": "essays/posts/renormalization-how-to/index.html#what-is-renormalization",
    "title": "How to do renormalization",
    "section": "",
    "text": "Renormalization is not group theory. The name “renormalization group theory” is truly terrible. To an applied physicist, the name “group theory” is abstract and inspires fear and uncertainty. To a mathematician, the name “group theory” is plain wrong, because you can always undo a group-action, but you can never undo a coarse-graining."
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html#the-logistic-map-rn-on-mathbbr",
    "href": "essays/posts/renormalization-how-to/index.html#the-logistic-map-rn-on-mathbbr",
    "title": "How to do renormalization",
    "section": "The logistic map: RN on \\(\\mathbb{R}\\)",
    "text": "The logistic map: RN on \\(\\mathbb{R}\\)\nLet’s first study the logistic map, the simplest nontrivial example of renormalization that I know of. This section is based on Wikipedia.\n\nThe logistic map\nConsider a function \\(f_r(x)=r x(1-x)\\), and we want to study what happens when we iterate the map many times. The map might fall into a fixed point, a fixed cycle, or chaos. We can see all those cases in its bifurcation diagram.\n\n\n\nThe bifurcation diagram of the logistic map. Source\n\n\nWhen the map falls into a stable fixed cycle of length \\(n\\), we would find that the graph of \\(f_r^n\\) and the graph of \\(x \\mapsto x\\) intersect at \\(n\\) points, and the slope of the graph of \\(f_r^n\\) is bounded in \\((-1, +1)\\) at those intersections.\nFor example, when \\(r=3.0\\), we find that there is only a single intersection, at which point the slope is exactly \\(+1\\), indicating that it is a stable single fixed point, but is about to undergo a bifurcation.\n\n\n\nThe relationship between \\(x_{n+2}\\) and \\(x_{n}\\) as \\(r\\) increases from \\(2.7\\) to \\(3.3\\). Before the period doubling bifurcation occurs. The orbit converges to a single fixed point where the graph of \\(f_r^2\\) intersects the diagonal line. As \\(r\\) reaches \\(3\\), the intersection pitchforks into three. The middle intersection becomes unstable, but the two neighboring intersections remain stable.\n\n\nAs \\(r\\) increases to beyond \\(r=3.0\\), the intersection point splits to two, which is a period doubling. For example, when \\(r=3.4\\), there are three intersection points, with the middle one unstable, and the two others stable.\n\n\n\nWhen \\(r=3.4\\), there are three intersection points, with the middle one unstable, and the two others stable. Source\n\n\nAs \\(r\\) approaches \\(r=3.45\\), another period-doubling occurs in the same way. The period-doublings occur more and more frequently, until at a certain \\(r \\approx 3.56994567\\), the period doublings become infinite, and the map becomes chaotic. This is the period-doubling route to chaos.\n\n\n\nWhen \\(r \\approx 3.56994567\\), there are infinitely many intersections, and we have arrived at chaos via the period-doubling route. Source\n\n\nSomething remarkable happens when we superimpose the graphs of \\(f_r, f_r^2, f_r^4, \\dots\\) when \\(r\\) is at the critical point \\(3.5699\\dots\\). We see that each iteration of the graph seems to resemble itself, except that it is scaled and rotated by 180 degrees. We can naturally guess that \\(f_r^{\\infty}\\) converges to a certain function that is infinitely jagged, such that it exactly resembles itself when scaled and rotated; that is, it is a fractal.\n\n\n\nThe graphs of \\(f_r, f_r^2, f_r^4, \\dots\\) when \\(r\\) is at the critical point.\n\n\nAs \\(r\\) approaches the critical value, we can see how the graph of \\(f_r^\\infty\\) takes on more and more details, and at the critical point, becomes a perfect fractal.\n\n\n\n\n\nThe graphs of \\(f_r, f_r^2, f_r^4, \\dots\\) as \\(r\\) approaches the critical point from below.\n\n\n\n\nUniversality\nLooking at the bifurcation diagram, we can see a region, starting just after \\(r = 3.8\\), where there is a clear “window” with period \\(3\\) bursting out of a sea of chaos. The window then bifurcates repeatedly, to stable cycles of periods \\(6, 12, 24, \\dots\\) until it all collapses back into the chaos again at around \\(r \\approx 3.8494344\\). Though this is a different place, the bifurcation diagram looks suspiciously similar to the previous case.\nNot only that, if we look at the movie of \\(f_r^\\infty\\) as \\(r\\) approaches this critical point, we again see the same jagged shape.\n\n\n\nAre we seeing some kind of universal feature of period-doubling routes to chaos? Is this a general pattern independent of the details of how exactly the logistic map is defined? What if we change to another dynamical system completely different?\nFor example, we can consider the gauss map \\(x_{n+1} = \\exp(-\\alpha x^2_n)+\\beta\\). For a fixed \\(\\alpha\\), we can plot the bifurcation graph as we vary \\(\\beta\\). Though it looks different, the two bifurcation graphs have a clear resemblance. This is an instance of universality, for which we will see again and again later. If \\(f_r\\) is a family of curves with parabolic tops1, then it will bifurcate just like the logistic curve.\n1 Rigorously, we can describe it as follows. If \\(F: \\mathbb{R}^2 \\to \\mathbb{R}\\) is smooth, and for all \\(r \\in \\mathbb{R}\\), the function \\(F(r, \\cdot): \\mathbb{R}\\to \\mathbb{R}\\) has a single global maximum, at which point \\(\\partial_x^2 F(r, x) &lt; 0\\), then its bifurcation diagram looks the same as that of the logistic map, and it will have the same two scaling exponents \\(\\alpha, \\delta\\), to be calculated below.\n\n\nThe bifurcation graphs of gauss map with \\(\\alpha = 5\\) and the logistic map.\n\n\nMore to the issue at hand: why do the two graphs look similar?\n\n\n\nThe bifurcation graph is self-similar. Source\n\n\n\n\nThe self-similarity equation\nRecall that we said the limit of \\(f^\\infty_r\\) should be self-similar, in the sense that if we iterate it twice, then rotate and scale it by a factor, we get back the same function. That is, it should be a solution to the self-similarity equation.\n\\[\nf(x) = -\\alpha f\\left(f\\left(\\frac{x}{-\\alpha} \\right)\\right)\n\\]\nIn words, if we scale up the graph for \\(f^2\\) by \\(\\alpha &gt; 0\\), and then rotate by 180 degrees, we get back the graph for \\(f\\).\nBy eye-balling the curve, we see that \\(f\\) should be an even function. Also, since the \\(f^2\\) can be graphically calculated by doing the cobweb diagram with the graph of \\(f\\), it does not matter if we first scale up the graph of \\(f\\) by a factor of \\(r\\) to \\(F\\), then double it to \\(F^2\\), or if we first double it to \\(f^2\\), then scale its graph. We would get back the same thing. Thus, without loss of generality, we can scale \\(f\\) such that \\(f(0) = 1\\).\nSo, our task is to solve the following equation:\n\\[\n\\begin{cases}\nf(x) = -\\alpha f\\left(f\\left(\\frac{x}{-\\alpha} \\right)\\right)\\\\\nf(x) = 1 - a_2 x^2 + a_4 x^4 + \\dots\n\\end{cases}\n\\]\nWe can solve the equation numerically as the fixed point. We would start with \\(f(x) = 1-x^2\\), then guess a good \\(\\alpha\\) and repeatedly apply \\(f \\mapsto -\\alpha f\\left(f\\left(\\frac{x}{-\\alpha} \\right)\\right)\\). If we picked \\(\\alpha\\) correctly, we would have gotten the right result, as shown:\n\n\n\nAt the point of chaos \\(r^*=3.5699 \\dots\\), as we repeat the functional equation iteration \\(f(x) \\mapsto-\\alpha f(f(-x / \\alpha))\\) with \\(\\alpha=2.5029 \\ldots\\), we find that the map does converge to a limit. Source\n\n\nIf \\(\\alpha\\) is not correct, the iterates would not converge; instead, they would have a zooming effect that looks cool.\n\n\n\n\n\n\n\n\n\nSolving the equation at order 2\n\n\n\n\n\nAt order 2, we approximate by \\(f(x) \\approx 1 - a_2 x^2\\) and ignore all higher-order terms. This gives us two equations for two unknowns:\n\\[\n\\begin{cases}\n1-a_2 = \\frac{1}{-\\alpha} \\\\\n\\frac{2a_2^2}{\\alpha} = a_2\n\\end{cases}\n\\]\nIt has two solutions. One solution has \\(\\alpha &lt; 0\\), which we know is unphysical. The other one is\n\\[\n\\begin{cases}\n\\alpha = 1 + \\sqrt{3} \\approx 2.732 \\\\\na_2 = \\frac{1 + \\sqrt{3}}{2} \\approx 1.366\n\\end{cases}\n\\]\n\n\n\nWhat happens if we are not exactly at the fixed point but start slightly off? Let’s say we start with a function \\(f_0(x) = 1 - a_{2,0}x^2\\), where \\(a_{2,0} = a_2^* + \\Delta\\), where \\(a_2^*\\) is the fixed point, and \\(\\Delta\\) is small but nonzero. Here, we should think of the space of possible functions. Each point in this space is a possible scaling limit, but if we start a bit too small, we fall into boredom, and if we start a bit too high, we fall into chaos. Start just right, and we harvest a beautiful fractal.\nAfter one iteration, we have \\(f_1(x) = -\\alpha_0 f_0(f_0(x/(-\\alpha_0)))\\), where \\(\\alpha_0\\) was fixed by \\(f_1(0) = 1\\). This gives us\n\\[\n\\begin{cases}\n\\alpha_0 = \\frac{1}{-1+a_{2, 0}} \\\\\n\\frac{2a_{2, 0}^2}{\\alpha_0} = a_{2, 1}\n\\end{cases}\n\\]\nThat is, we have the renormalization flow equation\n\\[\n2a_{2, 0}^2(a_{2, 0}-1)= a_{2, 1}\n\\]\nWe can plot the space of all possible \\(f(x)\\) as a line, like\n\\[1-0x^2, 1-0.5 x^2, 1-x^2, 1-1.5x^2, \\dots\\]\n\n\n\nRN flow diagram of the self-similarity map at order 2.\n\n\nThis is a 1-dimensional slice of the space of all possible \\(f\\) (the space of theories). Then, the effect of repeatedly applying the self-similarity map is to iterate the map \\(a_2 \\mapsto 2a_{2}^2(a_{2}-1)\\). If we are precisely at the fixed-point \\(a_2^*\\), then we are not going anywhere, but if we are not exactly there, then since the slope of \\(a_2 \\mapsto 2a_{2}^2(a_{2}-1)\\) is \\(\\delta \\approx 5.73\\) at that point, we would get farther and farther away:\n\\[\nf_0 = 1-(a_2^* + \\Delta)x^2, \\quad f_1 = 1-(a_2^* + \\delta\\Delta)x^2, \\quad f_1 = 1-(a_2^* + \\delta^2\\Delta)x^2, \\quad \\dots\n\\]\nand after \\(\\log_\\delta(\\frac{0.1}{\\Delta})\\), we would be at roughly \\(1-(a_2^* \\pm 0.1)x^2\\), which is when we can finally notice that we are obviously no longer in the neighborhood of the fixed point anymore. If we start at \\(a_2^* + \\Delta/\\delta\\), then we can sustain the illusion for one more iteration. Similarly, if we start at \\(a_2^* + \\Delta/\\delta^n\\), then we can sustain the illusion for \\(n\\) more iterations.\nNow, thinking back to what the logistic map says, we understand what we have discovered: The graph of \\(f_{r^* - \\Delta}\\) is similar to the graph of \\(f_{r^* - \\Delta/\\delta}^2\\) scaled by \\(-\\alpha\\). If we let \\(r_1, r_2, r_3, \\dots\\) be the points at which the logistic map splits into a stable cycle of period \\(2^1, 2^2, 2^3, \\dots\\), then we have \\(r_{n} \\approx r^* - \\Delta/\\delta^{n}\\), and so we have:\n\\[\n\\frac{r^* - r_n}{r^* - r_{n+1}} \\to \\delta\n\\]\nThis is usually spoken in this way: the intervals between two bifurcations shrinks at a rate of \\(\\delta\\).\n\n\n\nThe bifurcation diagram for the logistic map. As the bifurcations approach the point of chaos, the interval between two bifurcations gets shorter and shorter, at a rate of \\(\\delta\\) per bifurcation. Source\n\n\n\\(\\delta\\) is called Feigenbaum’s first constant, and \\(\\alpha\\) is Feigenbaum’s second constant.\n\n\n\n\n\n\nSolving the equation at order 4\n\n\n\n\n\nSimilarly, we can solve the equation at order 4 by plugging in \\(f(x) \\approx 1 - a_2 x^2 + a_4 x^4\\), obtaining 3 equations for 3 variables:\n\\[\n\\begin{cases}\n1-a_2+a_4 = \\frac{1}{-\\alpha} \\\\\n\\frac{2a_2^2 - 4a_2a_4}{\\alpha} = a_2 \\\\\n\\frac{a_4(4a_4+6a_2^2) - a_2(2a_4 + a_2^2)}{-\\alpha^2} = a_4\n\\end{cases}\n\\]\nTo solve this numerically, first guess a solution from the previous one, \\(\\alpha \\approx 2.732, a_2 \\approx 1.366\\), then plug into the first equation to get \\(a_4 \\approx 0\\). Then, standard numerical root-finding gives\n\\[\n\\begin{cases}\n\\alpha \\approx 2.534 \\\\\na_2 \\approx 1.522 \\\\\na_4 \\approx 0.128\n\\end{cases}\n\\]\n\n\n\nWe can also make the same argument using a flow in theory-space, except now we are doing it over a 2-dimensional slice of it. The flow map is\n\\[\nF(a_2, a_4) = \\left(\n   (2a_2^2 - 4a_2a_4)(-1+a_2 - a_4), -(a_4(4a_4+6a_2^2) - a_2(2a_4 + a_2^2))(-1+a_2 - a_4)^3\n\\right)\n\\]\nAt the fixed-point \\((a_2, a_4) = (1.522, 0.128)\\), the Jacobian matrix is \\[\n\\nabla F = \\begin{bmatrix}\n6.0506 & -6.2524 \\\\\n1.2621 & -1.6909\n\\end{bmatrix}\n\\]\nThis matrix has eigenvalues of \\(4.843, -0.483\\), so it is a saddle point, with \\(\\delta = 4.843\\). The flow and the eigenvectors \\((0.982, 0.190), (0.691, 0.723)\\) are plotted below.\n\n\n\nRN flow diagram of the self-similarity map at order 4. The two eigenvector directions at the fixed point are plotted as dashed lines.\n\n\nIn summary:\n\nThe solution to the self-similarity equation, at increasingly high orders of approximation.\n\n\n\nOrder 2\n4\n\\(\\infty\\)\n\n\n\n\n\\(a_2\\)\n1.366\n1.522\n1.530\n\n\n\\(a_4\\)\n\n0.128\n0.105\n\n\n\\(\\alpha\\)\n2.732\n2.534\n2.503\n\n\n\\(\\delta\\)\n5.73\n4.843\n4.669\n\n\n\n\n\nLessons\nEven in this tiny problem, we can already draw several lessons, which will appear again and again in RN:\n\nWe assume a function is self-similar, and calculate from there.\nSelf-similarity is a transform on a function (or “theory”).\nWe often need to use a “fudge factor” like \\(\\alpha\\) to make sure that the transformed function does not collapse to zero, for trivial reasons.\nIf we repeatedly apply the self-similarity transform on a function, we would obtain a scaling limit, a perfectly self-similar object – a fractal.\nIn the space of all possible theories, the self-similarity transform creates a flow-field in the theory space. The interesting fixed-points of the flow-field are its saddle points.\nThe largest eigenvalue of the saddle point describes what happens when you are close to the saddle point, but not quite there.\nBravely calculate using the cheapest approximation you can think of. It often gets you within 50% of the right answer.\nBut if you want accuracy, you can always use a computer and calculate many orders higher."
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html#ising-model-and-friends-rn-on-a-grid",
    "href": "essays/posts/renormalization-how-to/index.html#ising-model-and-friends-rn-on-a-grid",
    "title": "How to do renormalization",
    "section": "Ising model and friends: RN on a grid",
    "text": "Ising model and friends: RN on a grid\nThis section studies RN on a grid, using what is called real space RN, in contrast to momentum space RN. Real space RN is a garden of tricks, useful and intuitive, but not as powerful as momentum space RN. If you find the kind of mathematical trickery in this section fun, look at (Kadanoff 1999b, chap. 14; Burkhardt and Leeuwen 1982) for more.\n\nThis paper attracted much favorable notice since, beyond obtaining all the scaling properties, it seemed to lay out a direct route to the actual calculation of critical properties. On closer examination, however, the implied program seemed – as I will explain briefly – to run rapidly into insuperable difficulties and interest faded. In retrospect, however, Kadanoff’s scaling picture embodied important features eventually seen to be basic to Wilson’s conception of the full renormalization group.\n(Fisher 1998)\n\nI recommend that you open these and play as you follow along:\n\nComplexity Explorables | I sing well-tempered\nIsing model by Evgeny Demidov.\n\n\nPercolation\nPercolation is about randomly punching holes in a material until it falls apart. In the simplest setting, the material is a square lattice \\(\\mathbb{Z}^2\\), and each site (vertex) is either open or closed.2 Open means there is a hole there, and closed means the site is intact. Sites are open or closed randomly and independently with probability \\(p\\). We are interested in whether there is an infinite connected cluster of open sites.\n2 Percolation on \\(\\mathbb{Z}\\) is trivial, even more trivial than Ising model on \\(\\mathbb{Z}\\). The trouble is the same: you can only go from one point to another point by one route, and if at any point on the route, you are stopped, then that’s the end – you can’t get there by any other way. Thus, long-range interactions decay exponentially with distance, which means no power law, no phase transition, no critical point. See a later section.This is a model for a porous material: for example, water seeping through the ground. If we have a layer of rock, then groundwater can seep through if there is a single connected path from top to bottom. A layer of rock can be thought of as a grid, with little cracks between grid-points. According to percolation theory, at a “critical probability”, suddenly we have arbitrarily large connected clusters of cracks, and so water can seep arbitrarily far in the rock – it is all or nothing.\nThat is, there is a sharp transition: if \\(p\\) is small, then there is no infinite cluster of open sites, and the water cannot go through; but if \\(p\\) is large, then there is an infinite cluster of open sites, and water can go through. The critical value \\(p_c\\) is about \\(0.5927...\\). See (Grimmett 1999) for more details about percolation.\nTo use Kadanoff blocking for percolation, the first step is to coarse grain the lattice. We group the sites into blocks of \\(3 \\times 3\\) and call a block open if there is a path of open sites connecting the left and right sides of the block. Otherwise, the block is closed.\nThe next step is to define a new percolation model on the coarse-grained lattice, but this is a little trickier than in the Ising model, because there is no obvious way to map the parameters of the original model to the parameters of the new model. We need to find a new probability \\(p'\\) such that the new model on the coarse-grained lattice has the same behavior as the original model on the fine lattice. In particular, we want the probability of having an infinite cluster of open sites to be the same in both models.\nIt turns out that there is no exact way to do this, but we can make an approximation. One way to do this is to use Monte Carlo simulations to estimate the probability of having an infinite cluster for different values of \\(p'\\).\n\n\nKadanoff blocking\nThis section based on (Simkin and Roychowdhury 2011, sec. 10) and (Stinchcombe 1991).\n\n\n\nWhen an Ising model is at the critical temperature \\(T_c\\), coarse-graining would not end up with all-black or all-white, but would have the same amount of details no matter how much we coarse-grain. (Sethna 2007, fig. 13).\n\n\nThe idea of Kadanoff blocking is simple: “This fractal looks the same if I take off my glasses, just bigger. If I zoom out and blur a bit, we get pretty much the same thing again. So, if I take a 3-by-3 pixel, block and replace it by one big pixel, we should get the same thing again.”\nGiven a hexagonal grid, you make \\(p\\) of them black and the rest white. What is the critical \\(p\\) where you get a percolation (infinitely big black island)? To do this by blocking, we regard a triangle as a large spin, and “merge” the three spins on a triangle to one single spin. If two or three spins are black, then the whole spin is also black, otherwise, the whole spin is white.\nThen, after one blocking operation, the lattice length increases from \\(l\\) to \\(\\sqrt 3 l\\) , and the renormalized occupation probability changes from \\(p\\) to \\(p^3 + 3p^2(1-p) = p^2(3-2p)\\). That is, we have the RN flow equation\n\\[p' = p^3 + 3p^2(1-p) = p^2(3-2p)\\]\n\n\n\nKadanoff blocking on a triangular lattice. (Simkin and Roychowdhury 2011, fig. 1)\n\n\nThe equilibrium point is \\(p=1/2\\). This is the percolation probability. Let the reduced probability be \\(\\bar p = p-1/2\\). We find that one iteration of the RN flow makes \\(\\bar p_{n+1} = \\frac 32 \\bar p - 2 \\bar p^3\\) which is \\(\\approx \\frac 32 \\bar p_n\\) for small values of \\(\\bar p\\).\n\n\n\nRN flow for Kadanoff blocking on a triangular lattice.\n\n\nSuppose we start with \\(\\bar p_0\\) , and we perform \\(N\\) repeats of RN to reach some constant \\(\\Delta \\bar p\\) (for example, 0.001), then\n\\[N = \\frac{\\ln \\Delta \\bar p - \\ln \\bar p_0}{\\ln \\frac 32}\\]\nduring which time, the lattice length has increased by\n\\[3^{\\frac 12 N} \\propto \\bar p_0^{-\\frac{\\ln 3}{2\\ln \\frac 32}}\\]\nSince at constant \\(\\Delta \\bar p\\), the lattice has a certain characteristic look-and-feel with a certain characteristic size for its clusters, we find that the characteristic length of its clusters is \\(\\propto (p-1/2)^{-\\nu}\\), where3\n3 It amuses me to no end that the word characteristic is something chemists use a lot. Physicists do it too, sure, with their “characteristic length”, “characteristic height”, “characteristic temperature”, and such, but it is abstract. You rarely need to actually check a cake’s characteristic length against a standard cake. However, when you are doing chemistry, and you need to check a chemical’s characteristic smell, then you are out of luck.\n\nI’m saddened to report that the chemical literature contains descriptions of dimethylcadmium’s smell. Whoever provided these reports was surely exposed to far more of the vapor than common sense would allow … its odor is variously described as “foul”, “unpleasant”, “metallic”, “disagreeable”, and (wait for it) “characteristic”, which is an adjective that shows up often in the literature with regard to smells, and almost always makes a person want to punch whoever thought it was useful. … if you’re working with organocadmium derivatives and smell something nasty, but nasty in a new, exciting way that you’ve never quite smelled before, then you can probably assume the worst. (Lowe 2013)\n\n\\[\\nu = \\frac{\\ln 3}{2\\ln \\frac 32} \\approx 1.355\\]\nThe actual exponent is believed to be \\(\\nu = 4/3\\), so we are only 1.6% off. Very good for such a cheap calculation!\n\n\nThe Ising model\nFerromagnets are weird. A single iron atom is a tiny magnet, but if the iron atoms in a piece of magnet are not oriented roughly in the same way, then the whole piece would have all its direction washed away, thus not be a magnet at all. In the atomic world, things jiggle about and tend to destroy all order, so how is it possible for a grid of atoms to stay pointing mostly in the same direction for days and weeks – an eternity on the atomic level?\nIn 1920s, Lenz had the idea of a simple atomic model for ferromagnetism. He gave the problem to his graduate student Ising, who solved it in the one-dimensional case. He was disappointed to see that there is no spontaneous magnetization, which is the key feature of ferromagnetism, and he conjectured that this is also true in higher dimensions. The model was thus neglected for a while, until (Peierls 1936) showed that the two-dimensional Ising model does exhibit a phase transition, and later, Onsager solved it completely in 1944, showing several curious features. This was a starting point of modern statistical physics. See (Domb 1985) for some more historical details.\nThe Ising model is a toy model of a magnet. For simplicity, we assume all the iron atoms are locked in a lattice, and each atom can only be spin up or spin down (like a coin that can be heads or tails). Neighboring atoms want to be the same, but temperature \\(T\\) makes the atoms jiggle and be random.\nThere are many different Ising models, but the following is the most standard. We have a grid of atoms, and each atom is nothing but a point with two possible values: \\(+1\\) or \\(-1\\). Nothing more complicated than that. The energy of the system is determined by the interaction between nearest neighbors, with a lower energy when neighboring spins are aligned. That is, its energy is\n\\[\nH(s) = -J \\sum_{i, j} s_i s_{j}\n\\]\nwhere \\(J&gt;0\\) is the strength of interaction between neighboring atoms, \\(s_i\\) represents the spin at site \\(i\\), and the summation is over neighboring pairs of \\(i, j\\). For example, if we have Ising model on the integer line \\(\\mathbb{Z}\\), then \\(H = -J \\sum_{i} s_i s_{i+1}\\). Similarly, if we have an Ising model on the square grid \\(\\mathbb{Z}^2\\), then \\(H = -J \\sum_{i} (s_{i} s_{i+(1, 0)} + s_{i} s_{i+(0, 1)})\\).\nWe are not going to explain why, but in statistical mechanics, we have to calculate the following, slightly mysterious quantity \\(Z\\), called the partition function. Everything in statistical mechanics bursts forth from this one function. It is no exaggeration to say that the story of statistical mechanics is mostly about calculating \\(Z\\) at higher and higher precisions using larger and larger computers while losing more and more hair on the head. For the Ising model, \\(Z\\) is defined as\n\\[\nZ := \\sum_{s} e^{-\\beta H(s)}\n\\]\nwhere \\(\\beta = 1/T\\) is the inverse of the temperature \\(T\\). The summation is over all possible configurations of the system. For example, if the Ising model is over \\(\\{0, 1, 2, \\dots, 99\\}\\), then the summation is over \\(\\{-1, +1\\}^{100}\\).\nThe temperature turns out to be not useful during the RN calculation, so to make the notation cleaner, we absorb it into the \\(J\\) like this:\n\\[Z = \\sum_{s} e^{-\\beta H(s)}, \\quad H(s) = -J_0 \\sum_{i, j} s_i s_{j}, \\quad J_0 = J/T\\]\nAfter we finish the calculation, we should write the temperature explicitly again in order to interpret what we have found.\n\n\nKadanoff decimation, take 0\nLike ancient Roman decimation, the idea of Kadanoff decimation is to pick out some spins for elimination, leaving behind the other spins gathered around the mangled nothings of their former comrades. In the case of the 1D Ising model, decimation is exactly solvable, making it a good first problem to try.\nLet us consider the Ising model on the integer line \\(\\mathbb{Z}\\) with periodic boundary conditions. That is, we have \\(2n\\) spins \\(s_0, s_1, \\dots, s_{2n-1}\\) arranged in a circle. Now, we keep the even spins white and decimate the odd spins. How can we do that? Well, behold:\n\\[\n\\begin{aligned}\nZ &= \\sum_{s_0, s_1, \\dots, s_{2n-1}} e^{+J(s_0 s_1 + s_1s_2 + \\dots + s_{2n-1}s_0)} \\\\\n  &= \\sum_{s_0, s_1, \\dots, s_{2n-2}} \\sum_{s_1, s_3, \\dots, s_{2n-1}} e^{J(s_0 s_1 + s_1s_2 + \\dots + s_{2n-1}s_0)} \\\\\n  &\\overset{\\mathrm{hopefully}}{=} \\sum_{s_0, s_2, \\dots, s_{2n-2}} e^{J'(s_0 s_2 + s_2 s_4 + \\dots + s_{2n-2}s_0)} \\\\\n\\end{aligned}\n\\]\nwhere we have hopefully written the \\(J'\\). It is our ardent hope that there exists some \\(J'\\), somewhere in the universe, that will make the equation come true.4\n4 Physicists call it “ansatz”, but I prefer to say it like this:\n\nThe author of an atrocious undertaking ought to imagine that he has already accomplished it, ought to impose upon himself a future as irrevocable as the past.\n– Yu Tsun, professor of English at Deutsch-Chinesische Hochschule.\n\nBecause each even spin can only reach its nearest-neighboring even spins via the odd spin between them, the partition function splits cleanly:\n\\[\nZ = \\sum_{s_0, s_2, \\dots} \\left(\\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)}\\right) \\left(\\sum_{s_1} e^{J(s_2 s_3 + s_3 s_4)}\\right) \\cdots\n\\]\nThus, our wish would be fulfilled if \\(\\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = e^{J's_0s_2}\\) for all 4 possible choices of \\(s_0, s_2\\). Now, by direct calculation, we see that our task is to solve the following 4 equations simultaneously:\n\n\n\n\n+\n-\n\n\n\n\n+\n\\(e^{J'} = 2 \\cosh(2J)\\)\n\\(e^{-J'} = 2\\)\n\n\n-\n\\(e^{-J'} = 2\\)\n\\(e^{J'} = 2 \\cosh(2J)\\)\n\n\n\nImmediately we see a problem: if both \\(e^{J'} = 2 \\cosh(2J)\\) and \\(e^{-J'} = 2\\) hold, then \\(1 = 4 \\cosh(2J)\\). This means that we have to introduce a “fudge factor” again. Does that remind you of the \\(\\alpha\\) from the logistic map calculation? It should. Add in the fudge factor \\(k\\) with:\n\\[\nZ = k^{n} \\sum_{s_0, s_2, \\dots, s_{2n-2}}e^{J'(s_0 s_2 + s_2 s_4 + \\dots + s_{2n-2}s_0)}, \\quad \\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = ke^{J's_0s_2}\n\\]\nNow the solution is: \\(J' = \\frac 12 \\ln \\cosh(2J), k = 2\\sqrt{\\cosh(2J)}\\). Again, we see that the RN flow equation\n\\[\nJ \\mapsto \\frac 12 \\ln \\cosh(2J)\n\\]\nBut this time, the RN flow has only a single stable fixed point at \\(J = 0\\). Not only that, since \\(\\frac 12 \\ln \\cosh(2J) \\approx J^2\\) if \\(J\\) is small, if we decimate for \\(n\\) times, we would end up with \\(J' \\sim J^{2^n}\\). What does this mean?\n\n\n\nThe RN flow for the 1D Ising model has only one fixed point at zero.\n\n\nSuppose we start with a magnet with interaction strength \\(J\\), then after we zoom out for \\(n\\) times, where \\(n\\) is large, we would end up with a decimated view of the magnet, where we can only see the atoms at sites \\(0, 2^n, 2\\times 2^n, 3 \\times 2^n, \\dots\\). Our RN flow calculation states that the strength of interaction between \\(s_0\\) and \\(s_{2^n}\\) are just the multiplication of all the bonds between them. This is not so strange if we think about it: on a 1D Ising model, the only way for \\(s_0\\) to interact with \\(s_{2^n}\\) is if they laboriously go, bond-by-bond, across all the \\(2^n\\) bonds, and at each bond, the interaction strength must be degraded. It would be like trying to draw with a pair of chopstick holding a pair of chopsticks holding a pair of chopsticks… holding a paintbrush. You rapidly run out of all bonding strength.\nIn particular, it shows that no matter how strong \\(J\\) is, the correlation between distant atoms would decay rapidly, and so there is no ferromagnetism on the 1D Ising model. This disappointment led Ising to abandon this model of magnetism back in 1925.\n\n\n\n\n\n\nNote\n\n\n\nIf you are ambitious, you can try doing the same RN analysis for a “ladder” made of two \\(\\mathbb{Z}\\) put side-by-side. If you have “a lot of time” like Onsager, you would be on your way to solving the Ising model in 2 dimensions. I have not worked this out, because I don’t have a lot of time, but it would involve a 4-by-4 matrix. See page 12 for a sketch of solution.\n\n\n\n\nKadanoff decimation, take 1 (abortive)\nNow that we have done it for 1D Ising model, we can attempt it for the 2D model. We will not be able to solve it exactly, but we will see what is going to happen. To start our analysis, we write out the partition function and Hamiltonian energy function:\n\\[\nZ = \\sum_s e^{-H}, \\quad H = -J \\sum_{i, j \\text{ are neighbors}} s_i s_j\n\\]\nwhere again we have absorbed temperature into the Hamiltonian. To perform decimation, we paint the square grid like a checkerboard, and eliminate all the black-colored spins.\n\n\n\nFigure source\n\n\nHowever, we can see a problem in the picture. Whereas in the original, un-decimated grid, only nearest neighbors are connected by a bond. In the decimated grid, not just the nearest neighbors, but also the next-nearest-neighbors are connected by a double-bond. Not only can you go from \\((-1, 0)\\) to a nearest-neighbor \\((0, +1)\\) via the to-be-decimated atom \\((0, 0)\\), but also go to the next-nearest neighbor \\((+1, 0)\\). Not only that, the square of 4 spins neighboring \\((0, 0)\\) are connected via \\((0, 0)\\), so we also need to account for that interaction. Thus, the renormalized Hamiltonian would have three kinds of terms: nn term, nnn term, and square-interaction term:\n\\[\n-H' = J_{nn} \\sum_{i, j \\text{ are nn}} s_i s_j + J_{nnn} \\sum_{i, j \\text{ are nnn}} s_i s_j + J_{\\square} \\sum_{i, j, k, l \\text{ are }\\square} s_i s_j s_k s_l\n\\]\n\n\n\n\n\n\nExact solution\n\n\n\n\n\nLet the grid have \\(N\\) sites. We can ignore the borders, or roll up the borders like how we did it in the 1D case – it does not matter when \\(N\\) is large. The partition function of the system is\n\\[\nZ = \\sum_{s \\in \\{-1, +1\\}^\\text{grid}} \\exp\\left(J \\sum_{ i, j \\text{ are nn}} s_i s_j\\right)\n\\]\nNow, decimate half of the grid, and leave behind the other half. As derived in (Maris and Kadanoff 1978), we have\n\\[\nZ = f(J)^{N/2} \\sum_{s \\in \\{-1, +1\\}^\\text{decimated grid}} \\exp\\left(J_{nn} \\sum_{ i, j \\text{ are nn}} s_i s_j + J_{nn} \\sum_{ i, j \\text{ are nnn}} s_i s_j + J_{\\square} \\sum_{ i, j, k, l \\text{ are }\\square} s_i s_j s_k s_l\\right)\n\\]\nwhere \\[\n\\begin{aligned}\nf(J) &= 2 \\cosh ^{1 / 2}(2 J) \\cosh ^{1 / 8}(4 J) \\\\\nJ_{nn} &= (1 / 4) \\ln \\cosh (4 J) \\\\\nJ_{nnn} &= (1 / 8) \\ln \\cosh (4 J) \\\\\nJ_{\\square} &= (1 / 8) \\ln \\cosh (4 J)-(1 / 2) \\ln \\cosh (2 J)\n\\end{aligned}\n\\]\n\n\n\nNote: Why don’t we have the triangle terms like \\(J_\\Delta s_{(-1, 0)}s_{(0, +1)}s_{(+1, 0)}\\)? Since the system has the same energy if we flip all spins, the Hamiltonian is even, and so the triangle-term must be exactly zero.\nWell, that’s fine, you might say, what’s the big deal? The big deal is that, if we were to repeat the decimation again, we will suddenly meet infinitely many interaction terms! How so?\nLook at the square grid again. Suppose we have already included the nnn interactions, and we were to do a decimation. As we have seen, in order that the partition function before-and-after decimation are the same, we must construct one interaction term for each possible interaction mediated by a decimated spin. Well, \\((-1, 0)\\) and \\((0, +1)\\) are mediated by \\((0, 0)\\), so they must have an interaction term. No problem, that’s the nn term, written as \\(J_{nn}s_{(-1, 0)}s_{(0, +1)}\\). Similarly, \\((-1, 0)\\) and \\((0, +1)\\) are mediated by \\((0, 0)\\), so they must have an interaction term. That’s the nnn term, written as \\(J_{nnn}s_{(-1, 0)}s_{(0, +1)}\\). Do you see it now?\nWe can connect \\((-1, 0)\\) and \\((3, 0)\\) via \\((0, 0), (1, -1), (2, 0)\\). Not only that, we can connect them by infinitely many possible routes: \\((0, 0), (1, -1), (2, 0), (3, -1)\\), and \\((0, 0), (1, -1), (2, 0), (3, 1)\\), and etc… And that’s not even the worst of it. Just like how we have arbitrary pair-wise interactions, we also have arbitrary group-interactions within subsets with arbitrary shapes. It is all a big mess, which goes to show how difficult the Ising problem already is in 2D, let alone in 3D.\nWe can of course deal with the problem by cutting off the weak interaction terms, like how we did the logistic map example. For example, we can preserve all interaction terms between spins that are at most 6 distances away, then compute the RN flow on the space of all Ising models on the square grid, find a saddle point, and compute the largest eigenvalue of the RN flow near the saddle point. The more terms we account for, the better the approximation is.\n\nThese calculations were much more elaborate than the model calculation described here; in my own work, for example, 217 couplings between spins were included. The critical exponents derived from the calculation agree with Onsager’s values to with­ in about 0.2%.\n(Wilson 1979)\n\n\n\nMigdal bond-moving trick\nBefore we are finished with the Ising model, here is one more trick: the Migdal bond-moving trick. The idea is shown in the diagram.\n\n\n\nThe Migdal bond-moving trick. (Kadanoff 1999b, fig. 14.2).\n\n\nWe perform the trick in two steps. First, we take the odd-numbered vertical bonds and move them to the even-numbered ones. Next, we decimate the crossed-out spins. Since this trick does not handle the vertical and horizontal bonds equally, we write the bond-strengths separately as \\(J_x, J_y\\). The RN flow of this step is\n\\[\n(J_x, J_y) \\mapsto \\left(2 J_x, \\frac 12 \\ln \\cosh(2 J_x)\\right)\n\\]\nAfter doing the Migdal bond-moving trick once in the horizontal direction, we can do it again in the vertical direction. The total effect is to have decimated 3/4 of the grid, preserving one spin per 2-by-2 block of spins. The RN flow of the whole process is:\n\\[\n(J_x, J_y) \\mapsto (f_1(J_x), f_2(J_y))\n\\]\nwhere \\(f_1(x) = \\ln\\cosh(2x)\\) and \\(f_2(x) = \\frac 12 \\ln\\cosh(4x)\\). This RN flow has saddle point \\((0.609, 0.305)\\). If we apply the same RN, but in the other order (first horizontal, then vertical), we have instead \\((J_x, J_y) \\mapsto (f_2(J_x), f_1(J_y))\\), with saddle point \\((0.305, 0.609)\\). Well, the true saddle point of the whole system should have equal \\(J_x, J_y\\), since the true system does not discriminate the horizontal and vertical, so the cheap answer is to take their average: \\((0.609 + 0.305)/2 = 0.457\\).\nAccording to Onsager’s exact solution, the true critical point is \\(J_c = \\frac{\\ln(1+\\sqrt 2)}{2} = 0.4407\\). So by this simple trick, we have already gotten within 3.7% of the true answer.\nA small improvement is to find the “double fixed point”. That is, we consider applying the first RN flow, then the second, and calculate the fixed point of this compound RN flow. That is, we expect the real fixed point to be the simultaneous solution to\n\\[x = f_1(f_2(x)); \\quad x = f_2(f_1(x))\\]\nNow, these have different solutions, so we do the obvious thing and take their midpoint. This gives \\(0.4417\\). And with that simple idea, I got within 0.23% of the true answer.\nWe can also calculate the scaling exponent, like how we found \\(\\delta\\) for the logistic map. The average gradient at the fixed point is \\(\\frac{(f_1\\circ f_2)' + (f_2\\circ f_1)'}{2}\\) which is \\(2.7633\\), corresponding to a length scaling exponent of \\(\\nu = \\frac{\\ln 4}{\\ln 2.7633} = 1.364\\), which is well off the real value of 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Two RN flows on the \\((J_x, J_y)\\) plane found by Migdal bond-moving, and the saddle points thereof.\n\n\n\n\n\nSide note: Onsager’s solution of Ising model in 2D\nSo far, we have been doing it for the Ising model on \\(\\mathbb{Z}\\). But it’s clear that we can also do it for two \\(\\mathbb{Z}\\)s put side by side like a ladder. Each “rung” of the ladder can be thought of as one big atom, with 4 possible states: up-up, up-down, down-up, down-down. We can then do the same calculation as the previous case, except that instead of a \\(2\\times 2\\) matrix, we have a \\(4 \\times 4\\) matrix.\nFor that matter, we can do it for any finite number of those \\(\\mathbb{Z}\\) put together. We can then imagine doing that for such ladders with widths \\(2, 3, 4, 5, 6, \\dots\\), then discover a pattern, and take the limit. If this works, we would solve the Ising model on \\(\\mathbb{Z}^2\\).\nArduous as it sounds, this is exactly how Lars Onsager arrived at his solution for the Ising model on \\(\\mathbb{Z}^2\\). He calculated up to ladders with width 6, diagonalizing matrices of size \\(64\\times 64\\) in the process, then he guessed the general pattern and proceeded from there, emboldened by the guess. As Chen-Ning Yang reported:\n\nIn 1944, L. Onsager produced, quite unexpectedly, an exact evaluation of the partition function of the model in two dimensions. It was a real tour de force. I had studied his paper in Chicago in the spring of 1947, but did not understand the method, which was very, very complicated, with many algebraic somersaults….\nIn March, 1965… I asked him how it came about that he took all those complicated algebraic steps in his paper of 1944. He said he had a lot of time during the war, so he began to diagonalize the transfer matrix, which had already been discussed by E. Montroll and by H. A. Kramers and G. H. Wannier. He started with a \\(2 \\times \\infty\\), then a \\(3 \\times \\infty\\), then a \\(4 \\times \\infty\\) lattice. He then went on to a \\(5 \\times \\infty\\) lattice, for which the transfer matrix is \\(32 \\times 32\\) in size. Such a matrix is quite large, but the experience he had gained with the smaller matrices came in handy, and he was able, after some time, to find all 32 of the eigenvalues. He proceeded then to the \\(6 \\times \\infty\\) case, and eventually diagonalized the \\(64 \\times 64\\) matrix, finding that all the eigenvalues were of the form \\(e^{\\pm \\gamma_1 \\pm \\gamma_2 \\pm \\gamma_3 \\pm \\gamma_4 \\pm \\gamma_5 \\pm \\gamma_6}\\). That led to the concept that the algebra of the problem was a product algebra, and hence the manipulations in his paper.\n(Yang 2005, 11–13)\n\nI also want to quote this from the same pages, because it describes accurately what it feels like to do real-space RN:\n\na long calculation, the longest in my career. Full of local, tactical tricks, the calculation proceeded by twists and turns. There were many obstructions. But always, after a few days, a new trick was somehow found that pointed to a new path. The trouble was that I soon felt I was in a maze and was not sure whether in fact, after so many turns, I was anywhere nearer the goal than when I began. This kind of strategic overview was very depressing, and several times I almost gave up. But each time something drew me back, usually a new tactical trick that brightened the scene, even though only locally. Finally, after about six months of work off and on, all the pieces suddenly fitted together, producing miraculous cancellations, and I was staring at the amazingly simple final result [spontaneous magnetization of the Ising model]\n(Yang 2005, 12)\n\n\n\nBonus: Generalized central limit theorem\nThis section is based on (Amir 2020).\nConsider three random walks generated from three distinct probability distributions: Gaussian, Cauchy, and Pareto \\(\\mu=0.7\\). We notice two facts:\n\nThe random walks are self-similar. A small section has the same look-and-feel as a large section of it.\nDifferent random walks have very different characters. The Gaussian walk appears smoother, while the Cauchy and Pareto walks display more dramatic jumps and bursts, reflecting the heavier tails of their respective distributions.\n\n\n\n\nThree random walks generated from three distinct probability distributions: Gaussian, Cauchy, and Pareto \\(\\mu=0.7\\)\n\n\nWhen we see self-similarity, we think RN. Can we use RN to study random walks? Yes.\nLet’s take a fresh look at the central limit theorem. It says that if \\(X_1, X_2, \\dots\\) are IID samples from a distribution with finite mean \\(E[X]\\) and variance \\(V[X]\\), then \\(\\frac{(X_1 + \\dots + X_n) - n E[X]}{\\sqrt{n V[X]}}\\) converges to the standard normal distribution. If we think about it from the RN point of view, we can decompose each \\(X\\) into a sum of two random variables: \\(X_i = A_i + Z_i\\), where \\(Z_i\\) is a normal distribution with the same mean and variance, and \\(A_i\\) is the “noise” part of it. Each \\(A_i\\) might be overpowering, but when we repeatedly coarse-grain by taking a bunch of \\(X_i\\), and adding them up (a lossy operation!), we would eventually destroy all traces of what cannot survive coarse-graining, and leaving behind a fixed-point of coarse-graining.\nWe define the following letters:\n\n\\(X_1, X_2, \\dots\\) are IID random variables, with characteristic function \\(\\phi_X(t)= E[e^{itX}]\\).\n\\(S_n = X_1 + \\dots + X_n\\).\n\\(a_n, b_n\\) are two sequences of real numbers, such that \\(\\frac{S_n - b_n}{a_n}\\) converges in distribution to a nontrivial random variable \\(Z\\) with characteristic function \\(\\phi(t)\\).\n\n\n\n\n\n\n\nDeriving the field equation by RN\n\n\n\n\n\nSince \\(\\frac{X_1 + \\dots + X_n - b_n}{a_n}\\) converges in distribution to a nontrivial random variable, the sequence \\(a_n\\) must diverge to infinity. For, if the sequence \\(a_n\\) is bounded, then for large enough \\(n\\) , the sum \\(X_1 + \\dots + X_n\\) would spread wider and wider, and dividing it by \\(a_n\\) cannot keep it together.\nLet \\(Z\\) be a random variable with characteristic function \\(\\phi\\). By assumption, \\((S_n- b_n)/a_n\\) is approximately distributed like \\(Z\\) , that is, \\(S_n\\) is approximately distributed as \\(a_nZ + b_n\\). Thus,\n\\[\\phi_{S_n}(t) \\approx e^{ib_n t}\\phi(a_nt )\\]\nGiven \\(1 \\ll n \\ll N\\) , we can compute \\(\\phi_{S_N}\\) in two ways: adding it up as \\(N\\) copies of \\(X\\) , or adding it up as \\(N/n\\) copies of \\(S_n\\). Both should give us the same result. That is: \\[\\phi_{S_N} = \\phi_X^N = \\phi_{S_n}^{N/n}\\]\nHowever, since \\(n\\) is very large, we have the approximations \\(\\phi_{S_n}(t) \\approx e^{ib_n t}\\phi(a_nt )\\). Thus, we have\n\\[\n\\ln \\phi_{S_N}(t) \\approx \\frac{N}{n}(ib_n t + \\ln\\phi(a_n t))\n\\]\nNote how we have an exponent of the form \\(Nf(n)\\) , where \\(N\\) is a very large number, and \\(n\\) is a number that is small compared to it. This is a common pattern in RN calculation.\nSince \\(n\\) is small compared to \\(N\\) , but large compared to \\(1\\) , we can pretend that it’s a continuous variable, and take derivative of it. Since the left side is independent of \\(n\\) , the derivative should be zero:\n\\[\n\\partial_n \\frac{N}{n}(ib_n t + \\ln\\phi(a_n t)) = 0\n\\]\nSimplifying it, and substituting \\(t\\) for \\(a_n t\\) , we get the field equation\n\\[\\frac{\\phi'(t)}{\\phi(t)}t - \\ln \\phi(t) \\frac{a_n}{n \\partial_n  a_n} + it\\partial_n (b_n/n) \\frac{n}{\\partial_n a_n} = 0\\]\n\n\n\nThus, we have obtained the field equation:\n\\[\\frac{\\phi'(t)}{\\phi(t)}t -\\frac{a_n}{n \\partial_n  a_n} \\ln \\phi(t)  + \\frac{n\\partial_n (b_n/n)}{\\partial_n a_n} it = 0\\]\nwhich we can solve by standard mathematical analysis without any more use of RN, so we don’t do those. You can read (Amir 2020) if you are interested.\nHowever, there is a problem: If we have a “field” equation, what is the “field”? Well, here is one way to think of it.\nImagine a line of atoms, at locations \\(1, 2, 3, \\dots\\). Each atom has a height \\(X_1, X_2, X_3, \\dots\\). Now, we can coarse-grain the system by a factor of \\(4\\), by defining\n\\[Y_1 = \\frac{X_1 + \\dots + X_4 - b_4}{a_4}, \\quad Y_2 = \\frac{X_5 + \\dots + X_8 - b_4}{a_4}, \\quad \\dots\\]\nfrom which we can perform another coarse-graining by a factor of \\(100\\), ending up with a coarse-grain by a factor of \\(400\\). Now, if the system has a nontrivial scaling limit, then this should give us the same result as doing a coarse-graining by \\(5\\), then by \\(80\\), or first \\(6\\) then \\(67\\). This is the RN argument we used here.\nNow, since \\(S_n \\approx a_n Z + b_n\\), we see that \\(b_n\\) can be thought of as the coarse-grained height of the height field, and \\(a_n\\) as the coarse-grained jaggedness of the height-field. Then, the field equation describes how the two numbers vary according to \\(n\\).\n\n\n\n\n\n\nExercise: extreme value distribution\n\n\n\n\n\nThe maximum of random variables often has a nontrivial scaling limit as well. That is, there exists some sequence \\(a_n, b_n\\) such that \\(\\frac{\\max(X_1, \\dots, X_n) - b_n}{a_n}\\) converges to a nontrivial distribution with cumulative distribution function (CDF) \\(F\\).\nLet \\(F_X\\) be the CDF of \\(X\\); then we have \\(F_{\\max(X_1, \\dots, X_N)}(t) = F_{\\max(X_1)}(t)^{N}\\). Now, derive the field equation by an RN argument.\nAnswer: \\(\\partial_n \\frac 1n \\ln F(\\frac{t-b_n}{a_n}) = 0\\)."
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html#wilsons-nobel-prize-rn-in-momentum-space",
    "href": "essays/posts/renormalization-how-to/index.html#wilsons-nobel-prize-rn-in-momentum-space",
    "title": "How to do renormalization",
    "section": "Wilson’s Nobel Prize: RN in momentum space",
    "text": "Wilson’s Nobel Prize: RN in momentum space\n\n\n\nEvolution of Firefox icon under RN flow in frequency space. The high-spatial-frequency details are summed away, leaving low-frequency broad brushstrokes.\n\n\nRN in momentum space, or RN in frequency space, is the most widely used method of RN nowadays. It is called “momentum” because it was first done by quantum field theorists to study subatomic particles, and in quantum mechanics, the spatial frequency of a particle-wave is proportional its momentum: \\(p = \\hbar k\\), for which de Broglie was awarded a Nobel Prize in 1929.\nIn the 1970s, Kenneth Wilson invented RN in momentum space and used it to solve many problems. He was awarded the 1982 Nobel Prize in Physics for this work.\nUnfortunately, unlike RN in real space, RN in momentum space is extremely verbose, requiring pages and pages of symbols. Instead of subjecting you to the horrible experience, I will sketch out the big ideas only.\n\nThere remained the possibility that there might be smaller but still infinite quantities left over. No one had the patience needed to calculate whether these theories were actually completely finite. It was reckoned it would take a good student two hundred years, and how would you know he hadn’t made a mistake on the second page? Still, up to 1985, most people believed that most supersymmetric supergravity theories would be free of infinities.\n(Hawking 2001, 52)\n\n\nTo know for sure whether a Feynman diagram with three virtual graviton loops produces infinite quantities, we would need to evaluate \\(10^{20}\\) terms. By five loops, a diagram spawns \\(10^{30}\\) terms … The unitarity method has completely changed the situation … What would have taken the Feynman technique \\(10^{20}\\) terms, we can now do with dozens. … we found that the 1980s era speculations were wrong. Quantities that seemed destined to be infinite are in fact finite. Supergravity is not as nonsensical as physicists thought. In concrete terms, it means that quantum fluctuations of space and time are much more innocuous in supergravity than previously imagined. If you ply us with fine wine, you might catch us speculating that some version of it might be the long sought quantum theory of gravity.\n(Bern, Dixon, and Kosower 2012)\n\n\nField theory: continuous Ising model\nArguably the first field theory was hydrodynamics. Working in the era just after Newton, Euler and Lagrange understood water as a block of infinitely many tiny mass-points. Because each point is so tiny, they do not study the velocity of individual particles of water, but study the velocity field of the entire block of water.\nThe Ising model, with its grid of spins, provides a clear example of this continuum limit. As the grid of spins grow large, the individuals blur together into a field, similar to deriving hydrodynamics from particle dynamics. Let’s take a concrete example, of Ising model on the square grid \\(\\mathbb{Z}^2\\).\nInitially, the system’s energy and partition function are represented as a sum over individual spins:\n\\[\nZ = \\sum_{s: \\mathbb{Z}^2 \\to \\{-1, +1\\}} e^{-H(s)}, \\quad H(s) = -J \\sum_{i, j \\text{ are nearest neighbors}} s_i s_j\n\\]\nRecall how, after two Kadanoff decimations, all forms of spin-spin interactions are unlocked, and so we arrive at an energy function in the most general form:\n\\[\nZ = \\sum_{s: \\mathbb{Z}^2 \\to \\{-1, +1\\}} e^{-H(s)}, \\quad H(s) = -\\sum_{\\text{configuration }C} \\sum_{i_1, i_2, \\dots \\text{ are configured like }C} J_C s_{i_1}s_{i_2}\\cdots\n\\]\nWe can convert the summation into an integral, and suggestively write it as \\(Z = \\int_{s: \\mathbb{Z}^2 \\to \\{-1, +1\\}} e^{-H(s)} ds\\).\nIn the continuous limit, the discrete field of spins \\(s: \\mathbb{Z}^2 \\to \\{-1, +1\\}\\) blurs into a continuous field of spins \\(\\phi: \\mathbb{R}^2 \\to \\mathbb{R}\\). The energy \\(H(s)\\) becomes \\(S[\\phi]\\), a functional5 an \\(\\phi(x)\\) and its gradients. This gives us\n5 A “functional” is nothing but a special kind of function. Specifically, it is a function of type \\(\\text{function} \\to \\text{number}\\). We use square brackets in \\(S[\\phi]\\), not round brackets like \\(S(\\phi)\\), because it is conventional for functionals to use square brackets, not round brackets. It is written as \\(S[\\phi]\\) rather than \\(H[\\phi]\\), and called “action”, because of some old historical usage in variational calculus (as in “the principle of least action”).\\[\nZ = \\int_{\\mathbb{R}^2 \\to \\mathbb{R}} D[\\phi] \\; e^{-S[\\phi]}\n\\]\n\n\nField theory, in general\nConsider a mattress as an analogy. Let the state The state of the mattress is determined by the height and velocity of each point. That is, the energy function of the mattress comprises two quadratic terms: height of each point; height differences between nearby points. We assume the mattress points are massless, so that we have no kinetic energy. We can write the energy of the mattress schematically as\n\\[\n\\text{energy} = H(\\phi) = \\frac 12 K_0 \\sum_i \\phi_i^2 + \\frac 12 K_1 \\sum_{i, j \\text{ are neighbors}} (\\phi_i - \\phi_j)^2\n\\]\n\n\n\nA mattress in space. The energy of the mattress is determined by the height of each point, and the height differences between neighboring points.\n\n\nIf the mattress is held in an atmosphere of temperature \\(T\\), then its state becomes uncertain, following a Boltzmann distribution, just like how a pollen’s position in hot water becomes uncertain, due to Brownian motion.6 The probability that you would find the mattress in state \\(\\phi\\) is then \\(e^{-H(\\phi)/T}/Z\\), where \\(Z\\) is the partition function again:\n6 When statistical field theory gets too scary, I call it “hot water theory” to make it sound nicer.\\[\nZ = \\int d\\phi e^{- H(\\phi)/T}\n\\]\nIn the continuum limit, this analogy leads us to statistical field theory.\nCalculating useful quantities within statistical field theory often involves complex mathematical techniques. This complexity arises from the need to compute the partition function, which involves integrating over all possible states of the system. In the case of the mattress analogy, the partition function involves an integral over all possible height fields, leading to the use of path integrals, written like \\(\\int_{\\phi: \\mathbb{R}^2 \\to \\mathbb{R}} D[\\phi] e^{-S[\\phi]/T}\\).\nIntegrating over \\(\\mathbb{R}^{10^{23}}\\) would be bad enough, let along integrating over \\(\\mathbb{R}^{\\mathbb{R}^{2}}\\), and yet the miracle is that this can be done, and can be done in a way that matches experiments.\nStatistical field theory employs various tricks to calculate the partition function or its limits. A common approach involves alternating between discrete and continuous representations of the field for calculations.\nInterestingly, statistical field theory is almost isomorphic with quantum field theory. The general idea is that if you take the dimension of time in a quantum field theory over space of dimension \\(n\\), and do a substitution \\(t \\mapsto it\\), you somehow end up with a statistical field theory over space of dimension \\(n+1\\). This is the Wick rotation.\nFor example, the 1D Ising model is analogous to a particle in a double well. Whereas the single particle might switch from left to right after time \\(\\Delta T\\), the Ising model chain might switch from \\(+1\\) to \\(-1\\) after space \\(\\Delta L\\). Because of quantum tunneling, it is impossible to confine a particle in one side of the well – it will always jump to the other side, and the jumping probability is on the order of \\(1 - e^{-kt}\\) for some constant \\(k\\). This corresponds to the fact that there is no way to “freeze” an Ising model in one dimension. Even at low temperatures, the system cannot be entirely frozen. Long stretches of “up” spins can suddenly flip to “down” spins. Similarly, in the quantum analogy, the particle can tunnel between the two wells, even as they become deeper.\nGenerally, 1D statistical fields lack phase transitions, just like how a single particle with finitely many states would always quantum-tunnel between states, no matter how cold it gets. Conversely, since the 2D Ising model can be frozen, indicating that the quantum field theory on one dimension should have some kind of phase transition.7\n7 I don’t know what it is, but perhaps Bose–Einstein condensation? I mean, I just flipped through (Herbut 2007) and it looks like this is it. Don’t quote me on this.Another perspective is by noting that magnetism requires spontaneous symmetry breaking: you have more spins pointing up than down, even though the underlying energy function does not distinguish up from down. No symmetry breaking, no phase transition. And since the quantum states of a single particle can always tunnel between each other, it cannot fall into a symmetry-breaking state. However, with infinitely many points, phenomena like spontaneous symmetry breaking can occur, leading to more complex behavior.\n\n\nRN flow in the space of field theories\nThink back to the Ising model on \\(\\mathbb{R}^2\\). What is its action \\(S[\\phi]\\)? If we were mathematically omnipotent, then we can simply perform RN flow on the discrete Ising model, and just find its fixed point, which should hopefully tell us what \\(S\\) is. But we can’t even perform a single RN flow. What to do?\nWell, by universality, we can start with some very different discrete Ising model and end up with the same continuum limit after renormalizing enough times. Why can’t we start with some very different continuous Ising model, discretize it to a discrete Ising model, then renormalize it again until we are back to a continuous Ising model? And if we can do that, why can’t we renormalize directly in the space of all continuous Ising models? We can start with whatever Ising field theory we can write down, and then just repeatedly renormalize it. By universality, we will end up in an interesting place, no matter where we started.\nBut before we do that, we have to construct the space of possible Ising field theories. As Wilson would say, it is all about the symmetries.\nThere are two kinds of symmetries: the symmetry without, and the symmetry within. For the Ising field \\(\\phi: \\mathbb{R}^2 \\to \\mathbb{R}\\), we have Euclidean symmetry for \\(\\mathbb{R}^2\\), and up-down symmetry for \\(\\mathbb{R}\\). To ensure Euclidean geometry, the action \\(S[\\phi]\\) should not explicitly depend on the position. That is, if we take some \\(\\phi\\), and translate it by \\(\\delta\\), then we must have\n\\[\nS[\\phi] = S[x \\mapsto \\phi(x + \\delta)]\n\\]\nSimilarly for reflections and rotations of the field. This shows that \\(S\\) must involve only terms like \\(\\phi, \\nabla \\phi, \\nabla^2 \\phi, \\nabla \\phi \\cdot \\nabla \\phi\\), etc.\nTo account for the symmetry within – the up-down symmetry – we must have \\(S[\\phi] = S[-\\phi]\\). This shows that \\(S\\) must have only even-ordered terms\nUnder these assumptions, you can convince yourself that the most generic form of \\(S\\) is\n\\[\nS[\\phi] = \\int d x\\left[\\frac{1}{2} \\nabla \\phi \\cdot \\nabla \\phi+\\frac{1}{2} \\mu^2 \\phi^2+g \\phi^4+\\cdots\\right]\n\\]\nwhere we removed an irrelevant constant term independent of \\(\\phi\\), and picked the scale of length so that the coefficient for \\(\\frac{1}{2} \\nabla \\phi \\cdot \\nabla \\phi\\) is one.\nThis is the space of all possible Ising field theories. Each Ising field theory is completely specified if we specify the real numbers \\(\\mu, g, \\dots\\). Doing RN would then consist of taking one Ising field theory specified by some \\(\\mu, g, \\dots\\), then renormalize it to some other Ising field theory specified by \\(\\mu', g', \\dots\\). We can then find the fixed points in the theory space, and say, “These are the most interesting theories. Let’s calculate their properties, scaling exponents, and look-and-feel.”\n\n\nRN in momentum space\nConsider an Ising field \\(\\phi: \\mathbb{R}^2 \\to \\mathbb{R}\\). It is of course possible to directly renormalize the field in real space: we blur it a bit, then zoom out. However, this turns out to be very hard to calculate with. Instead, it is much easier to renormalize the field in frequency space.\nTo “blur and zoom out” in real space – what does it look like if we take a Fourier transform? It would look like we are removing some high-frequency vibrations, then expand the vibrations so that low-frequency vibrations become high-frequency vibrations.\nNow we are ready to meet RN in momentum space.\nLet \\(\\tilde \\phi\\) denote the Fourier transform of a field. Now, solve8 for \\(\\tilde S\\), so that \\(S[\\phi] = \\tilde S[\\tilde\\phi]\\) for all \\(\\phi\\) in theory space.\n8 This is typically called “Fourier-transforming the operator”. For example, the Fourier transform of the gradient operator \\(\\nabla\\) is \\(ik\\), where \\(k\\) is the wave vector.\\[\nZ = \\int D[\\phi] e^{-S[\\phi]} = \\int D[\\tilde\\phi] e^{-\\tilde S[\\tilde\\phi]}\n\\]\nNext, we restrict the domain of integration from all frequencies, to only frequencies below an upper limit \\(\\Lambda\\). This is called “frequency cut-off”9:\n9 This frequency cut-off is often called “ultraviolet cutoff”, because we throw away all frequencies that are too high, and for light, ultraviolet light is high-frequency. That’s it. It started as a hack meant to remove the annoying infinities that crop up everywhere in quantum field theory. You might have heard mysterious whispers of how renormalization “cuts off infinities” and makes them “normal again”. In fact, this is where the horrendous name “re-normalization” came from. If I had any choice in the matter, I would have called it “re-scaling”. At least that would be more descriptive.\nFor that matter, “renormalization group theory” is also a terrible name. To an applied physicist, the name “group theory” is abstract, inspiring fear and uncertainty. To a mathematician, the name “group theory” is plain wrong, because you can always undo a group-action, but you can never undo a coarse-graining.\nIn modern QFT, this compromise has been converted into a triumph. Instead of a hack to remove infinities, it is now interpreted as a necessary fact about the world. To see high-frequency features in space, we have to be able to probe it. To probe it, we need to fire some high-frequency particles at a space. High-frequency particles have high energy, and the higher the energy gets, the more complicated the interaction gets, and in this way, the quantum field theory actually changes depending on our choice of frequency-cutoff.\n\nWilson’s analysis takes just the opposite point of view, that any quantum field theory is defined fundamentally with a cutoff A that has some physical significance. In statistical mechanical applications, this momentum scale is the inverse atomic spacing. In QED and other quantum field theories appropriate to elementary particle physics, the cutoff would have to be associated with some fundamental graininess of spacetime, perhaps a result of quantum fluctuations in gravity. … whatever this scale is, it lies far beyond the reach of present-day experiments. The argument we have just given shows that this circumstance explains the renormalizability of QED and other quantum field theories of particle interactions. Whatever the Lagrangian of QED was at its fundamental scale, as long as its coupUngs are sufficiently weak, it must be described at the energies of our experiments by a renormalizable effective Lagrangian.\n(Peskin and Schroeder 1995, 402–3)\n\n\\[\nZ \\approx \\int_{\\tilde \\phi(k) \\text{ is nonzero only for }\\|k \\| \\leq \\Lambda} D[\\tilde\\phi] e^{-\\tilde S[\\tilde\\phi]}\n\\]\nNext, we pick some small number \\(\\epsilon\\), and integrate away all frequencies on a shell:\n\\[\n\\begin{aligned}\nZ &\\approx \\int_{\\tilde \\phi^- \\text{ is nonzero only for }\\|k \\| \\leq (1-\\epsilon)\\Lambda} D[\\tilde \\phi^-] \\left(\\int_{\\tilde\\phi^+ \\text{ is nonzero only for }(1-\\epsilon)\\Lambda \\leq \\|k \\| \\leq \\Lambda} D[\\tilde\\phi^+] e^{-\\tilde S[\\tilde\\phi^-, \\tilde\\phi^+]}\\right) \\\\\n&\\overset{\\mathrm{hopefully}}{=} \\int_{\\tilde \\phi^- \\text{ is nonzero only for }\\|k \\| \\leq (1-\\epsilon)\\Lambda} D[\\tilde \\phi^-] e^{-\\tilde S'[\\tilde \\phi^-]}\n\\end{aligned}\n\\]\nThis gives us some renormalized action \\(\\tilde S'\\) in frequency space. Do an inverse Fourier transform to obtain \\(S'\\), then scale space down by \\((1-\\epsilon)\\), to obtain the fully renormalized action \\(S''\\). The RN flow is then defined by\n\\[\nS \\mapsto S''\n\\]\nSince \\(\\epsilon\\) is small, we can make it infinitesimal, to obtain the RN flow (a real flow this time!)\n\\[\nS \\mapsto S + \\epsilon F[S]\n\\]\nwhere \\(F\\) denotes the RN flow field in theory-space. It is a functional of \\(S\\), since the flow field differs for each theory in theory-space.\nThis is as far as we are going to discuss the RN in momentum space. Any more and I would be writing a textbook on QFT, and you are better served by a proper textbook like (Zee 2023, 2010), etc.\n\n\nBonus: How to publish in quantum field theory\n\nWork through a textbook and learn RN in momentum space.\nLearn group theory and group representation theory.\nWrite down many groups with some nice geometry, like \\(SO(3)\\).\nConstruct a group out of those. For example, \\(H = SO(4) \\rtimes SU(3) \\times SU(2) \\times U(1)\\). The group \\(G\\) should have around 10–20 dimensions, but if you are a string theory enthusiast, then 500 dimensions is perfectly fine.\nConstruct another group \\(G\\).\nPick a nice space \\(Y\\). For example, \\(X = \\mathbb{R}^4 \\times \\mathbb{C}^6\\). It must be a space that \\(H\\) can act upon.\nPick another space \\(X\\). It must be a space that \\(G\\) can act upon.\nConstruct the most generic possible functional of type \\(S: (X \\to Y) \\to \\mathbb{C}\\) that is still compatible with the two symmetry groups \\(G, H\\).\nSpend the next month doing RN calculations about \\(Z := \\int_{\\phi: X \\to Y} D[\\phi] \\; e^{-S[\\phi]}\\), probably with Feynman diagrams scribbled everywhere.\nType it up in LaTeX.\nSuffer through peer review, or just put it up on arXiv."
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html#a-bag-of-intuitions",
    "href": "essays/posts/renormalization-how-to/index.html#a-bag-of-intuitions",
    "title": "How to do renormalization",
    "section": "A bag of intuitions",
    "text": "A bag of intuitions\nAfter the long, hard slog at mathematics, we can take a break, take stock of what we have learned, and make some philosophical reflections. I guarantee that you will find at least one sentence here that you can proclaim with style at a party.\n\nPower laws are born of two exponential parents\nWhy is it that a critical point is typically surrounded by a power law? One intuition is that at a critical point, two exponentials are matched exactly – an exponentially decaying interaction strength and an exponentially increasing number of interaction paths – and a power law is born in their collision.\nConsider the Ising model on the plane. Fix an origin \\(0\\) , and we ask, how strong is the correlation between the origin \\(0\\) and a point that is at distance \\((n, n)\\) away from the origin, where \\(n\\) is large?\nWell, the two points are correlated because they are connected by chains of spins. The more chains there are, the stronger the correlation, but the longer each chain is, the weaker the correlation. Since the correlation along each chain is exponentially weak, we can crudely pretend that all the correlations can be added.10 As a good approximation, we consider only the shortest chains, which are of length \\(2n\\). By Stirling approximation, there are \\({2n \\choose n} \\sim \\frac{4^{n}}{\\sqrt{n\\pi }}\\) such chains. We can think of spin at origin as \\(x_{(0,0)} + z_1 + z_2 + \\cdots\\), and the spin at \\((n, n)\\) as \\(x_{(n,n)} + z_1 + z_2 + \\cdots\\), where \\(z_1, z_2,...\\) are random variables that are responsible for creating the correlations between the two spins along each chain.\n10 Every biologist knows intuitively that weak correlations are added. This is why, for instance, we can predict height accurately by a simple linear sum of the genes correlated with height, ignoring pairwise, triple-wise, and higher-order interactions. It is a common pattern in biology where \\(\\text{many genes} \\xrightarrow{\\text{development}} \\mathbb{R}\\), where \\(\\mathbb{R}\\) stands for a real-valued trait like probability of diabetes, then it is pretty close to a linear map.Now, each chain contributes a weak correlation that decays exponentially with distance. We can assume the chains do not interact. Along each chain, we have a 1D Ising model. The covariance between two neighboring spins is\n\\[Cov(s_0, s_1) = E[s_0s_1] - \\underbrace{E[s_0]E[s_1]}_{\\text{=0}} = Pr(s_0 = s_1 ) - Pr(s_0 \\neq s_1 ) = \\tanh(\\beta J)\\]\nNow, we need a trick.11 If you think a bit, you would see that whether \\(s_0 = s_1\\) is independent of whether \\(s_1 = s_2\\). Thus,\n11 If you don’t like the trick, then you can use the transfer matrix method. We have no use for the transfer matrix, so we don’t do it.\\[Cov(s_0, s_2) = E[s_0 s_2] = E[(s_0 s_1) (s_1 s_2)] = Cov(s_0, s_1) Cov(s_1, s_2) = \\tanh(\\beta J)^2\\]\nAnd since the chain has length \\(2n\\), the correlation contributed by the chain is \\(\\tanh^{2n}(\\beta J)\\). The total correlation is\n\\[\\sim \\frac{4^{n}}{\\sqrt{n\\pi }} \\tanh^{2n}(\\beta J)\\]\nThe two terms are exactly balanced when \\(\\beta J = \\tanh^{-1}(1/2) = 0.549\\dots\\). In fact, the exact result is \\(\\beta J = 0.44\\dots\\) , so our crude estimate is only 25% too high.\nNow, right at the critical point, the correlation is \\(\\sim (n\\pi)^{-1/2}\\) , so we see that when the exponential decay in correlation is exactly matched with the exponential growth in possible paths, the remaining polynomial decay comes to the fore. Notice that we have also estimated one of the Ising critical exponents: \\(\\nu = 1/2\\). The actual answer is \\(1\\).\nSimilarly, with \\[\\binom{kn}{n, \\dots n}\\sim \\frac{k^{kn}}{n^{\\frac{k-1}2}}\\frac{k^{1/2}}{(2\\pi)^{\\frac{k-1}2}}\\]\nwe can estimate that the Ising model in \\(\\mathbb{Z}^k\\) has a critical \\(\\beta J \\approx \\tanh^{-1}(1/k)\\) and critical exponent \\(\\nu = \\frac{k-1}2\\). It turns out that for all dimensions \\(\\geq 4\\), we have the exact result of \\(\\nu = 1/2\\). This can be derived by “mean field theory”, which we are not going to discuss.\n\nStevens’ power law\nAs a side note, this “two exponents lead to a power law” is known in psychophysics as Stevens’ power law (Stevens 1970).\nConsider the case where the brain needs to respond to the presence of a stimulus (e.g., a sound, a smell, etc.) with intensity \\(I\\). The response intensity (such as in the height of jumping, or a verbal report, or wincing of the face) is \\(R\\). Stevens found that for many kinds of stimulus, \\(R \\propto I^k\\) for some exponent \\(k\\) that depends on the type of stimulus and response.\nStevens conjectured that the number of neurons firing \\(N\\) is proportional to the log of intensity of stimulus \\(I\\), and that \\(N\\) is also proportional to the log of intensity of response \\(R\\). Thus, we have\n\\[\nk_I \\ln I = N = k_R \\ln R\n\\]\nfor two constants \\(k_I, k_R\\), which implies that \\(R = I^{k_I/k_R}\\), a power law. In this way, a small number of neurons can allow us to perceive and react to a wide range of stimuli intensities – for example, the physical brightness between noon and a starlit moonless night is more than \\(10^{8}\\), and yet the optical nerve, with only \\(10^{6}\\) neurons (Evangelou and Alrawashdeh 2016), can comfortably accommodate them both.\n\nAt the Ciba Symposium in 1966, there was a general discussion on the topic “Linearity of transmission along the perceptual pathway”. In that discussion, and elsewhere at the symposium, Sir John Eccles turned forceful attention to the question of whether the sense organ could adequately account for the nonlinearity in the coupling between stimulus and sensation, leaving the central nervous system with the task of performing only linear transformations. He observed that “there is no great impediment to the idea that… the transfer functions across the synaptic mechanism are approximately linear.” To which Professor Mountcastle added, “The interesting point for me here is the great importance that we must now place upon the transducer process itself, at the periphery.”\n(Stevens 1970)\n\n\n\n\nRN as a journey in the space of possible theories\nSo far, we have seen again and again the common refrain of\nThe space of possible theories is defined by the symmetry of the physical system, and the Renormalization Group (RG) flow defines a journey in this space.\n\n\n\n(Fisher 1998, fig. 4)\n\n\nConsider a generic RG flow in a generic space of theories. diagram for a system with two coupling constants \\(K\\) and \\(y\\). Each point in the diagram represents a theory, and the arrows indicate the direction of the RG flow. The fixed points, where the arrows converge, correspond to theories that are scale-invariant.\n\n\n\n(Fisher 1998, fig. 5)\n\n\nThe above figure shows the RG flow for the Ising model, a simple model of ferromagnetism. The fixed points correspond to the paramagnetic phase (\\(K = 0\\)) and the ferromagnetic phase (\\(K = K_c\\)). The critical point, where the two phases meet, is at \\(K = K_c\\).\nThe RG flow provides a way to understand the behavior of a system at different length scales. As we zoom out, the system flows towards a fixed point. The fixed point describes the long-distance behavior of the system.\nFor example, in the Ising model, as we zoom out, the system flows towards either the paramagnetic or the ferromagnetic fixed point, depending on the initial value of \\(K\\). If \\(K &lt; K_c\\), the system flows towards the paramagnetic fixed point, and the spins become disordered at long distances. If \\(K &gt; K_c\\), the system flows towards the ferromagnetic fixed point, and the spins become ordered at long distances.\nWhat defines the space of possible theories? The symmetry of the physical system.\n\n\nSymmetries determine the shape of theory space\nThe Ising model on \\(\\mathbb{Z}^2\\) is not a single theory, but an entire infinite-dimensional space of possible theories. Each Ising model can be specified by all coupling strengths for all possible spin configurations – nearest neighbors, next-nearest neighbors, four in a square, etc. However, they must follow the symmetries. We can’t have three-in-a-triangle, because switching up and down gives us the same energy. We can’t have \\(Js_{(0, 0)}s_{(1, 0)}\\) different from \\(J's_{(1, 0)}s_{(2, 0)}\\), because translating the whole plane by \\((1, 0)\\) gives us the same energy.\nThis is a general fact: the symmetries determine the shape of theory space. Conversely, if two physical systems are constrained under the same symmetries, then their behavior are the same near the critical point, because their renormalization flows are the same.\nThe following table shows many theory-spaces with their corresponding symmetries.\n\nTable of theory-spaces with their corresponding symmetries. Reproduced from (Wilson 1979).\n\n\n\n\n\n\n\n\n\n\\(d\\), the underlying space’s dimensions\n\\(n\\), the internal degrees of freedom\nTheoretical Model\nPhysical System\nOrder Parameter\n\n\n\n\n2\n1\nTwo dimensions\nAdsorbed films\nSurface density\n\n\n\n2\nXY model in two dimensions\nHelium-4 films\nAmplitude of superfluid phase\n\n\n\n3\nHeisenberg model in two dimensions\n\nMagnetization\n\n\n&gt;2\n∞\n“Spherical” model\nNone\n\n\n\n3\n0\nSelf-avoiding random walk\nConformation of long-chain polymers\nDensity of chain ends\n\n\n\n1\nIsing model in three dimensions\nUniaxial ferromagnet\nMagnetization\n\n\n\n\n\nFluid near a critical point\nDensity difference between phases\n\n\n\n\n\nMixture of liquids near consolute point\nConcentration difference\n\n\n\n\n\nAlloy near order-disorder transition\nConcentration difference\n\n\n\n2\nXY model in three dimensions\nPlanar ferromagnet\nMagnetization\n\n\n\n\n\nHelium 4 near superfluid transition\nAmplitude of superfluid phase\n\n\n\n3\nHeisenberg model in three dimensions\nIsotropic ferromagnet\nMagnetization\n\n\n≤4\n-2\n\nNone\n\n\n\n\n32\nQuantum chromodynamics\nQuarks bound in protons, neutrons, etc.\n\n\n\n\nAs a particular example, with \\(d=3, n=1\\), we see that water-vapor mixture has the same critical behavior as copper-zinc alloy, or Ising model on three dimensions. We can think of vapor as just up-spin, and liquid as just down-spin. Next time you find yourself meditating upon a magnet, try reimagining it as a kettle of water, about to boil over.\n\nExamples of \\(d=3, n=1\\) systems.\n\n\nphysical system\nsite types\n\n\n\n\n\nuniaxial magnet\nup / down\n\n\n\nfluid\nhas atom / no atom\n\n\n\nbrass crystal\nzinc / copper\n\n\n\nsimple lattice field theory\nhas particle / no particle\n\n\n\n\n\n\nDroplets inside droplets\n\nBig whirls have little whirls that feed on their velocity,\nand little whirls have lesser whirls and so on to viscosity.\n\nI especially recommend playing with the interactive Ising model by Evgeny Demidov while reading this section.\nWe can interpret an Ising model over \\(\\mathbb{Z}^3\\) as a mixture of liquid and gaseous water. Each site can either be in a liquid state or in a vapor state. We start below the critical temperature, so that the bonding strength \\(J\\) is just slightly larger than the critical \\(J_c\\). The system must make a free choice to make between being a liquid ocean with tiny islands of liquid or a vapor ocean with tiny islands of liquid. Both choices are equally good in our toy model. Since liquid wants to be in contact with liquid and vapor with vapor, the system must decide. For the sake of intuition, we say that we have an ocean of liquid with tiny droplets of vapor inside.\nNow we increase the temperature towards the critical temperature. As we get hotter, thermal fluctuations become larger. In the ocean of liquid, a thermal fluctuation produces a droplet of vapor. This droplet secretes other material of the same density, and it grows larger and larger.\n\n\n\nVapor droplets inside an ocean of liquid. (Kadanoff 1999a, 298)\n\n\nHowever, as criticality is approached, the energetic cost in making a fluctuation approaches zero. The energetic penalty in creating droplets gets smaller and smaller. So, the droplets can grow large, until it becomes infinite right at the critical point.\nFurthermore, each droplet itself is a nearly-critical system, so fluctuations appear within the droplets, in a delicate fractal.\n\n\n\nDroplets inside droplets inside droplets… (Kadanoff 1999a, 299)\n\n\n\n\nMore is different\nLike Wilson, Philip Anderson is a physicist who has won a Nobel Prize for his work on surprising things that happen when many particles interact. His Nobel Prize was awarded for his work on magnetism and “Anderson localization”. Consider a pure piece of metal. An electron wave can vibrate freely across it from side to side, like a wave on a perfectly uniform infinite ocean. Now if we dope the metal with impurities, like planting wave-breakers in the ocean, then as the amount of impurity increases, an electron wave would suddenly become trapped, and the metal would become an insulator.\nIn a famous paper (Anderson 1972), he interprets many-body physics like a philosopher (like what I’ve been doing in this whole section). He called the “constructionist hypothesis” the view that science divides neatly into fundamental laws and applications of those. He countered this view by proposing that scale and complexity create distinct stages in nature. Each stage necessitates new laws, concepts, and generalizations, and requires just as much ingenuity as the other stages. For example, psychology is not merely applied biology, biology is not simply applied chemistry, and condensed matter physics is not only “device engineering”.\nAnderson proposed that new stages appear because of “broken symmetries”. Consider the sugar molecule. Though the sugar molecule is governed by quantum mechanics, which does not distinguish left from right, in our world we mostly only see sugar in one chirality. Sometime in the distant past, the symmetry was broken, and we are living in the consequences of its history.\nMore concretely, consider cars driving on a road. Why do we drive on the right instead of the left? Left or right, it’s better (“lower energy”) if everyone agrees. If people disagree, then it’s chaos (“higher energy”), so in the past, a coin was flipped, and we are here. In a parallel universe, we are driving on the left instead of the right. This is clear in the many-worlds interpretation of quantum mechanics:\n\\[\n\\ket{\\Psi_{\\text{world}}} = \\frac{1}{\\sqrt{2}} \\ket{\\Psi_{\\text{world where we drive on the left}}} + \\frac{1}{\\sqrt{2}} \\ket{\\Psi_{\\text{world where we drive on the right}}}\n\\]\nThe state of the entire multiverse \\(\\ket{\\Psi_{\\text{world}}}\\) is symmetric, but any observer has to fall into one of the sub-states, where the symmetry no longer holds.\nHowever, Anderson pointedly did not explain what allows the stages to have laws largely independent of each other. Suppose that we have explained how some symmetries of the quantum-mechanical level break on the biochemistry level; we still have many questions. Why does the biochemistry level still have simple laws? And why these biochemical laws, but not others? Why are quantum mechanical laws and the everyday physics of frogs and cats so different, without the laws “bleeding into each other”?\n\n\nMesophysics: Why are things interesting between the large and the small?\nThe space of all theories is big, but most of it is rather uninteresting. Consider the humble Ising model on \\(\\mathbb{Z}^2\\). Too much interaction and you get a block of spins all pointing in one direction. Too little interaction and you get a gas of spins pointing noisily in all directions. Only right at the critical point do we get interesting behavior in all possible scales. Not only that, the critical point is very delicate.\nIf you take an Ising model at \\(J\\) that is just a bit above \\(J_c\\) and zoom out, then by the RN flow equation, the effective \\(J\\) would keep increasing, and it becomes more and more uniform until it’s a perfect shade of black/white at \\(J = +\\infty\\). Conversely, if you start with \\(J\\) slightly below \\(J_c\\) and keep zooming out, \\(J\\) would approach \\(0\\) and everything would become a uniform shade of gray, with the spins pointing up and down with no regard for any other spin. Balanced on a knife’s edge is \\(J = J_c\\), where there is interesting behavior at all levels of zooming.\n\n\n\n\n\nZooming in and out of the Ising model. Video by Douglas Ashton, taken from The Renormalisation Group | dougmet-dot-net.\n\n\nBut what keeps \\(J = J_c\\)?\nWhy is it that we are surprised by the quantum mechanics in the microscopic world? Because daily life in the mesoscopic world does not betray its origin from the microscopic world. The details has been renormalized away. But if that’s the case, how come our world is neither a homogeneous block of spins all pointing up nor a hot mess of spins unrelated to every other spin? Why is the mesoscopic world interesting?\nLook around you. The world is interesting, with power laws, fractal patterns, and details at all scales. You never see a pencil standing on its end without a hand keeping it there. What keeps the mesoscopic world in its critical place?\nOne answer is that most of the interestingness did not come from criticality. However, if there exists some criticality in the mesoscopic world, and there does not seem to be an intelligent agent keeping the criticality there, then we have a mystery. This is the question that launched a thousand papers, including the famed self-organized criticality paper (Bak, Tang, and Wiesenfeld 1987), itself launching a thousand papers. The idea is typically illustrated by the forest fire model.\nConsider the standard percolation model on a square grid. Each point might be occupied (a “tree” grows there) or unoccupied (empty plot of land). Randomly, lightning falls, and if it hits a tree, the tree catches on fire and the fire spreads to any neighboring trees. The process ends when there are no more burning trees.\nAs the proportion \\(p\\) of a site being occupied (“tree density”) changes, we see a phase transition. For low \\(p &lt; p_c\\), there is no percolation, and so the fire quickly dies out. For high \\(p &gt; p_c\\), there is percolation, and so the fire spreads across the entire grid. The process automatically balances the system at the critical value \\(p_c\\), the system is poised between these two regimes, and the burnt-out patches can be of any size, following a power-law distribution. In this way, the delicate critical point in the standard percolation model has been transformed to a robust critical point.\n\nNature shows an amazing variety of length scales: There is the Hubble radius of the universe, \\(10^{10}\\) light years or so and the radius of our own solar system, \\(10^{11}\\) meters roughly, and us-two meters perhaps, and an atom \\(-10^{-10}\\) meters in radius, and a proton \\(10^{-16}\\) meters, and the characteristic length of quantum gravity-which involves another factor of about \\(10^{20}\\).\nHow these vastly different lengths arise is a very interesting and fundamental question…. However, we can think about how one describes the region between these lengths. If one is looking at scales between two fundamental lengths, there are no nearby characteristic lengths. Similarly in critical phenomena, whenever one looks [at] any event which takes place between the scale of the lattice constant [the spacing between molecules or spins] and the much larger scale of the coherence length, one is in a situation in which there are no nearby characteristic lengths. (Kadanoff 1999b, 251)\n\nIn a footnote, Kadanoff suggested that self-organized criticality might explain forms of stable criticality we see around us. Kadanoff wrote the words in 1999, near the end of the 1980s–90s chaos theory boom. Since then, the self-organized criticality theory has fallen by the wayside, like fractal compression, mirrorshades, and large-folio printed pages of fractal art. The modern evaluation is that while it was oversold by Per Bak, and certainly could not explain all critical phenomena (Bak 1996), it can explain some of them (Watkins et al. 2016, sec. 8).\n\n\nUniversality: The details don’t matter\nOr: Why elephants don’t know, or don’t need to know, quantum mechanics.\nIn the early 20th century, material scientists noticed the remarkable phenomenon of “corresponding states”. As first reported by (Guggenheim 1945), scientists measured the density \\(\\rho\\) of many substances near their liquid-vapor critical point. They fixed pressure and increased temperature \\(T\\) around the critical temperature \\(T_c\\). As they plotted the relation between \\(T\\) and \\(\\rho\\), rescaled by critical temperature \\(T_c\\) and density at critical point \\(\\rho_c\\), remarkably, all the substances fell onto a single curve.\n\n\n\nHigh-resolution reprint of (Guggenheim 1945, fig. 2) in (Herbut 2007, 13).\n\n\nDespite the diversity of intermolecular forces, the phase transition behavior of a wide variety of gasses follows a universal pattern.\nWe see this pattern over and over again in physics. To give another example, imagine water flowing through a porous rock, oil through sand, or electricity through a random network of resistors. These systems, seemingly completely unrelated, share the same underlying mathematical structure and exhibit universal behavior near the percolation threshold. This universality arises because the details of the microscopic interactions become irrelevant at larger scales, and the system’s behavior is governed by the collective properties of its components.\nIt is often noted that quantum mechanics is unintuitive, because the mesoscopic physics is so different from the microscopic physics, so we had evolved to intuit the mesoscopic world, and not the microscopic world. But why is it possible to ignore quantum mechanics? Elephants don’t need to know, or don’t care about, the Standard Model of particle physics.12 When they walk, they push dirt around. When water flows, it pushes water around. The long-distance behavior of a system does not depend on the details of the short-distance interactions. The benefit of RN is that it saves us from the effort of understanding the microscopic details. On the flip-side, the details do matter if we are far from the critical point.\n12 Nor do they play chess. (Brooks 1990)This is an overarching theme in renormalization theory, which might be called the universality hypothesis:\n\nAll phase transition problems can be divided into a small number of different classes depending upon the dimensionality of the system and the symmetries of the order state. Within each class, all phase transitions have identical behaviour in the critical region, only the names of the variables are changed. (Kadanoff 1999a, 273)\n\nFurthermore, universality justifies our toy models as “serious play”. When I was first learning statistical mechanics, I was terribly confused by it. “They can’t be serious – do they think I’m stupid? How could the Ising model possibly be relevant to real magnets?” But the universality hypothesis justifies Ising models as serious toy models. Even if they are completely different from real magnets when far from the critical point, as we approach the critical point, their behavior becomes exactly equal at the limit.\nRN theory explains why certain features matter when we zoom out and others are washed away. It explains why we have universal classes according to symmetry and dimension and not, say, the precise shape of the atomic force laws. It justifies the use of toy models instead of microscopically accurate models. Specifically, one starts with some microscopic accurate model, then apply renormalization and show that the details fall away and we end up at the same destination as the toy model. (Batterman 2019)\n\nWe may well try to simplify the nature of a model to the point where it represents a ‘mere caricature’ of reality. But notice that when one looks at a good political cartoon one can recognize the various characters even though the artist has portrayed them with but a few strokes. … [A] good theoretical model of a complex system should be like a good caricature: it should emphasize those features which are most important and should downplay the inessential details. (Fisher 1983, 47)\n\n\nKoan: Sociophysics\n\n“The details don’t matter.” said them triumphantly as they declared their independence from biophysics.\n“‘the details don’t matter.’” said them mockingly as they declared their insurrection against sociophysics.\n\nYuxi’s Comment:13 The traditional approach of historians, going back to the days of “kings and battles”, is to run to personality theory and the individual acts, when confronted by a problem in sociology or economics! One establishes the individual actors, makes some (hopefully) sensible approximations of their personality makeups, and then attempts to explain events, actions, and so on. However, for truly complicated systems that appear what, these days, is much better called “sociophysics”, this is a hopeless task; furthermore, the questions it answers are not even the right ones. The modern theorist would rather explain how the stable features of the problem are invariant under different assumptions of what individual people do, and arise from features of their interactions. Indeed, if one had a perfect archive of exactly what every person thought and said during the start of WWI, one would still have no understanding of why it started!\n13 Inspired by Tolstoy’s War and Peace.\n\nThe movement of humanity, arising as it does from innumerable arbitrary human wills, is continuous. To understand the laws of this continuous movement is the aim of history. But to arrive at these laws, resulting from the sum of all those human wills, man’s mind postulates arbitrary and disconnected units. The first method of history is to take an arbitrarily selected series of continuous events and examine it apart from others, though there is and can be no beginning to any event, for one event always flows uninterruptedly from another.\nThe second method is to consider the actions of some one man – a king or a commander – as equivalent to the sum of many individual wills; whereas the sum of individual wills is never expressed by the activity of a single historic personage.\nHistorical science in its endeavor to draw nearer to truth continually takes smaller and smaller units for examination. But however small the units it takes, we feel that to take any unit disconnected from others, or to assume a beginning of any phenomenon, or to say that the will of many men is expressed by the actions of any one historic personage, is in itself false.\n\nMumon’s Comment: Historians search for the king’s motive and the general’s ambition. They build castles of personality, moats of individual acts, yet understand nothing of the war. The great black spider of history weaves between the players so densely, that nothing is lost after it has digested all of them."
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html#appendix",
    "href": "essays/posts/renormalization-how-to/index.html#appendix",
    "title": "How to do renormalization",
    "section": "Appendix",
    "text": "Appendix\n\nIf only I understood what this is saying, then I would have written it in. (Mehta and Schwab 2014)"
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html#introduction",
    "href": "essays/posts/renormalization-how-to/index.html#introduction",
    "title": "How to do renormalization",
    "section": "",
    "text": "Renormalization (RN) theory is a powerful tool for understanding the behavior of physical systems at different length scales. It allows us to explain why certain features matter when we zoom out and others are washed away. It explains why we have universal classes according to symmetry and dimension and not, say, the precise shape of the atomic force laws. It justifies the use of toy models instead of microscopically accurate models.\nRN theory is based on the idea of self-similarity – that a system looks the same at different length scales. For example, a coastline looks the same whether you are looking at it from a satellite or from a boat. Similarly, a magnet looks the same whether you are looking at it with a microscope or with your naked eye.\nThe basic idea of RN is to start with a microscopic description of a system and then to coarse-grain it. That is, to average over the details of the system at a smaller length scale to obtain a description of the system at a larger length scale. This process can be repeated, leading to a sequence of descriptions of the system at increasingly larger length scales.\nThe key insight of RN theory is that the long-distance behavior of a system does not depend on the details of the short-distance interactions. This is known as the universality hypothesis. Universality explains why systems that are very different at the microscopic level can have the same behavior at the macroscopic level. For example, water, oil, and electricity all exhibit the same percolation behavior near the percolation threshold, even though they are very different at the microscopic level.\nUniversality also justifies the use of toy models to study physical systems. Toy models are simplified models that capture the essential features of a system without including all of the details. For example, the Ising model is a toy model of a magnet that captures the essential features of ferromagnetism without including all of the details of the interactions between the atoms.\nIn this essay, we will explore the basic ideas of RN theory and see how it can be used to understand a variety of physical systems."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html",
    "href": "essays/posts/analytical-mechanics/index.html",
    "title": "Analytical mechanics",
    "section": "",
    "text": "Plotting code for the essay is in a Jupyter notebook."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#introduction",
    "href": "essays/posts/analytical-mechanics/index.html#introduction",
    "title": "Analytical mechanics",
    "section": "Introduction",
    "text": "Introduction\n\nWhat this essay contains\nThis essay works through what is typically contained in a university course on analytical mechanics. The prerequisites are multivariate calculus and mathematical maturity.\nIt covers: calculus of variations, Lagrangian, Legendre transform, Hamiltonian, Hamilton’s principal function, Hamilton–Jacobi equation, the particle-wave duality, Schrödinger’s equation, Hamilton’s geometric optics, action-angle variables, Noether’s theorem, adiabaticity, old quantum theory, Bohr–Sommerfeld quantization, Einstein–Brillouin–Keller quantization.\nIt does not cover: canonical transforms, Poisson bracket, infinitesimal generator, symplectic geometry, symplectic form.\nPhilosophically, this essay has two undercurrents:\nOne, that thinking on the margins is vital not just in economics, but also in classical mechanics. In my other essay, Classical thermodynamics and economics, I show that classical thermodynamics is also nothing more than thinking on the margins.\nTwo, classical mechanics is a leakless leaky abstraction. Though classical mechanics has many equivalent forms, the search for the most elegant form has led us to the Hamilton–Jacobi equation, and the adiabatic theorem, both of which were on the threshold of quantum mechanics. Even though classical mechanics is a self-consistent closed world (thus “leakless”), when viewed in the right light, all its lines and planes seem to angle towards a truer world (thus “leaky”), of which this world is merely a shadow (thus “abstraction”).\n\n\nConventions\nWe always assume all functions are analytic (that is, they have Taylor expansions).\nWe would often say \"minimize cost\" or \"maximize revenue\" or such, but we always mean \"stationarize\". For example, when we say “Let’s maximize \\(f(x)\\)”, what we mean is “Let’s solve \\(f'(x) = 0\\)”. If we were to actually maximize \\(f(x)\\), we would have to check \\(f''(x) &lt; 0\\) as well, but we don’t. This is not a problem for physics, where the principle of stationary action rules. However, in optimal control and economics, we should check if the solution is actually a minimum/maximum.1\n1 Thus, the analogy between classical mechanics and economics is not precise. However, the analogy is precise between classical thermodynamics and economics, since the entropy is actually maximized, and not merely stationarized.Instead of laboriously typing \\(\\vec q\\) or \\(\\mathbf{q}\\), I just write \\(q = (q_1, \\dots, q_n)\\), unless there is a serious risk of confusion.\n\nReference table for the economics-mechanics analogy.\n\n\nSymbol\nMechanics\nEconomics\nControl Theory\n\n\n\n\n\\(L\\)\nLagrangian\ntime-rate of cost\ntime-rate of cost\n\n\n\\(t\\)\ntime\ntime\ntime\n\n\n\\(q\\)\nlocation/coordinate\ncommodity\nstate variable\n\n\n\\(\\dot q\\)\nvelocity\nproduction rate/consumption rate\ncontrol variable\n\n\n\\(S = \\int L(t, q, \\dot q)dt\\)\naction\ntotal cost\ntotal cost\n\n\n\\(p\\)\nmomentum\nprice\nco-state variable\n\n\n\\(H = \\sum_i p_i\\dot q_i - L\\)\nHamiltonian\nmarket equivalent cash flow2\nHamiltonian\n\n\n\n2 also known as \"mark to market cash flow\"\n\nWhy I wrote this\nI wrote this as “Analytical Mechanics done right”, or “What every graduate student in physics should know about analytical mechanics, but didn’t learn well, because the textbooks are filled with too many symbols and not enough pictures.”, or “what I wish the textbooks to have said when I first learned the subject”.\nIn my physics Olympiad years, I saw the Lagrangian a few times, but we had no use for that. During my undergraduate years, I studied analytical mechanics. The construction of the Lagrangian was reasonable enough, and I understood how the Euler–Lagrange equation was derived by \\(\\delta \\int L = 0\\). However, as soon as they proceeded to the Hamiltonian, I was entirely lost.\nWhat is \\(H(p, q) = p \\dot q - L(q, \\dot q)\\)? Why does the right side depend on \\(\\dot q\\) but not the left side? How does anyone just make up the Legendre transform out of thin air? It’s named “Legendre transform” – well, if it has such a fancy name, surely this has many more applications than \\(H = p \\dot q - L\\) – else, why not call it “Legendre’s one-time trick”? Canonical transform? Point transform? Aren’t they all coordinate transforms? Poisson bracket? Infinitesimal generators? It was all a giant mess. Reading Goldstein’s classic textbook only supplemented my confusion.\nEarly 2023, I studied mathematical microeconomics. When I was studying Ramsey’s theory of optimal saving, I saw, to my great surprise, something they call a “Hamiltonian” (Campante, Sturzenegger, and Velasco 2021, chap. 3). I was shocked, but after studying it carefully, and thinking it over, I realized that it was no mistake – the “Hamiltonian” in economics and in physics really are the same. Physics has an economic interpretation. Everything fell into place over the course of a few hours. Guided by this vision, I worked through analytical mechanics again, this time with true understanding.\nIt is my experience that everything in undergraduate physics is taught badly, except perhaps Newtonian mechanics. I have written this essay to make analytical mechanics finally make sense. It has made sense for me, and I hope it will make sense for you."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#overview",
    "href": "essays/posts/analytical-mechanics/index.html#overview",
    "title": "Analytical mechanics",
    "section": "Overview",
    "text": "Overview\n\nThe economics-mechanics analogy.\n\n\nSymbol\nPhysics\nEconomics\nControl Theory\n\n\n\n\n\\(L\\)\nLagrangian\ntime-rate of cost\ntime-rate of cost\n\n\n\\(t\\)\ntime\ntime\ntime\n\n\n\\(q\\)\nlocation/coordinate\ncommodity/capital\nstate variable\n\n\n\\(\\dot q\\)\nvelocity\ninvestment rate/saving rate/cash flow/etc\ncontrol variable\n\n\n\\(S = \\int L(t, q, \\dot q)dt\\)\naction\ntotal cost\ntotal cost\n\n\n\\(p\\)\nmomentum\n(shadow) price\nco-state variable\n\n\n\\(H = \\sum_i p_i\\dot q_i - L\\)\nHamiltonian\n(market equivalent) cash flow3\nHamiltonian\n\n\n\n3 also known as \"mark to market cash flow\"\nConventions\nWe always assume all functions are analytic (that is, they have Taylor expansions).\nWe would often say \"minimize cost\" or \"maximize revenue\" or such, but we are actually merely \"stationarizing\" everything. For example, when we say “Let’s maximize \\(f(x)\\)”, what we mean is “Let’s solve \\(f'(x) = 0\\)”. We don’t need to check \\(f''(x) &lt; 0\\). This is not a problem for physics, where the principle of stationary action rules, but in optimal control and economics, we should check if the solution is actually a minimum/maximum."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#optimization-as-making-money",
    "href": "essays/posts/analytical-mechanics/index.html#optimization-as-making-money",
    "title": "Analytical mechanics",
    "section": "Optimization as making money",
    "text": "Optimization as making money\n\nThe shortest line problem\n\n\n\n\n\n\nWarning\n\n\n\nIn this problem, the so-called “time” \\(t\\) has units of length as well. So, please don’t panic when we see something like \\(\\sqrt{1 + \\dot x^2}\\). If it worries we, we can replace \\(t\\) by \\(v\\tau\\), where \\(v\\) is a constant speed of one, and \\(\\tau\\) is time.\n\n\nStart with the easiest problem: shortest path problem. What is the shortest path from \\((0, 0)\\) to \\((A, B)\\)? We parametrize the path by a function \\(t \\mapsto (x(t), y(t))\\), with \\(t\\in [0, 1]\\). The problem is then\n\\[\\begin{cases} \\min \\int_0^1 \\sqrt{\\dot x^2 + \\dot y^2}dt \\\\ \\int_0^1 \\dot x dt = A \\\\ \\int_0^1 \\dot y dt = B \\\\ \\end{cases}\\]\n\n\n\nWhat is the shortest path from \\((0, 0)\\) to \\((A, B)\\)?\n\n\nThis is a standard constraint-optimization problem, and could be solved by the Lagrangian multiplier method:\n\\[\\delta \\left(\\int_0^1 \\sqrt{\\dot x^2 + \\dot y^2}dt + p_x \\left( A - \\int_0^1 \\dot x dt\\right) + p_y \\left( B - \\int_0^1 \\dot y dt\\right) \\right) = 0\\]\nwhere \\(p_x, p_y\\) are the Lagrangian multipliers, each responsible for enforcing one constraint.\nVarying the path functions \\(x, y\\) gives us\n\\[\\delta \\int_0^1 (\\sqrt{\\dot x^2 + \\dot y^2} - p_x \\dot x - p_y \\dot y)dt = 0\\]\nEach of \\(\\dot x, \\dot y\\) could be independently perturbed with a tiny concentrated \"pulse\", so for the integral to be stationary, the integrand must be zero with respect to derivatives of \\(\\dot x, \\dot y\\). That is, we have\n\\[\\begin{cases} \\partial_{\\dot x}(\\sqrt{\\dot x^2 + \\dot y^2} - p_x \\dot x - p_y \\dot y )= 0\\\\ \\partial_{\\dot y}(\\sqrt{\\dot x^2 + \\dot y^2} - p_x \\dot x - p_y \\dot y ) = 0 \\end{cases}\\]\nand we are faced with the solution\n\\[(p_x, p_y) = \\frac{1}{\\sqrt{\\dot x^2 + \\dot y^2}}(\\dot x, \\dot y)\\]\nSince \\(p_x, p_y\\) are independent of time, they are constants, and since we have from the above equation \\(p_x^2 + p_y^2 = 1\\), we find that \\(p_x = \\cos\\theta, p_y = \\sin\\theta\\) for some \\(\\theta\\). Thus, the curve is a straight line making an angle \\(\\theta\\) to the x-axis.\n\n\nEconomic interpretation\nInterpret \\(x, y\\) as two goods that we can produce (let’s say, tons of steel and tons of copper).\nWe are a factory manager, and we are given a task: produce \\(A\\) of \\(x\\) (tons of steel), and \\(B\\) of \\(y\\) (tons of copper), in \\([0, 1]\\) (one year). If we don’t do it, we will be sacked. Our problem is to accomplish the task at minimal cost.\n\\(\\dot x, \\dot y\\) are the speed at which we produce the goods, which we can freely control. In control theory, we say \\(x, y\\) are state variables, and \\(\\dot x, \\dot y\\) are control variables.\nThe cost function is \\(L(t, x, y, \\dot x, \\dot y) = \\sqrt{\\dot x^2 + \\dot y^2}\\). Its integral \\(S = \\int_0^1 L dt\\) is the total cost of production, which we must minimize.\nWe are also given a free market on which we are allowed to buy and sell the goods. If we cannot achieve the production target, we buy from the market. If we achieve more than the production target, we sell them off.\nThen, the total cost is\n\\[\\left(\\int_0^1 \\sqrt{\\dot x^2 + \\dot y^2}dt + p_x \\left( A - \\int_0^1 \\dot x dt\\right) + p_y \\left( B - \\int_0^1 \\dot y dt\\right) \\right)\\]\nwhere \\(p_x, p_y\\) are the market prices which do not change with time.\nIf our production plan is optimal, then the plan must have stationary cost. That is, if we make a change in our production plan \\((\\dot x, \\dot y)\\) by \\(O(\\delta)\\), our production cost must not change by \\(O(\\delta)\\), or else we could achieve lower cost. For example, if the plan \\((\\dot x + \\delta \\dot x, \\dot y + \\delta \\dot y)\\) achieves higher cost, then \\((\\dot x - \\delta \\dot x, \\dot y - \\delta \\dot y)\\) achieves lower cost.\nMathematically, it means the optimal production plan must satisfy\n\\[\\delta \\left(\\int_0^1 \\sqrt{\\dot x^2 + \\dot y^2}dt + p_x \\left( A - \\int_0^1 \\dot x dt\\right) + p_y \\left( B - \\int_0^1 \\dot y dt\\right) \\right) = 0\\]\nNow, the great thing about the market is that it is always there and we can trade with it, So, we have completely transformed the question into a problem of profit maximization – we’ll always sell to the market, and then buy back from the market right at the end.\nNow we can perform profit maximization moment-by-moment: raise production until marginal profit reaches cost:\n\\[\\begin{cases} \\partial_{\\dot x}(\\sqrt{\\dot x^2 + \\dot y^2} - p_x \\dot x - p_y \\dot y )= 0\\\\ \\partial_{\\dot y}(\\sqrt{\\dot x^2 + \\dot y^2} - p_x \\dot x - p_y \\dot y ) = 0 \\end{cases}\\]\nwhich may be solved as before.\nNow we can interpret \\(p_x, p_y\\). What are they? Suppose we perturb \\(\\dot x\\) by \\(\\delta \\dot x\\), then since the production plan is already optimal, we have\n\\[\\delta \\left(\\int_0^1 \\sqrt{\\dot x^2 + \\dot y^2}dt + p_x \\left( A - \\int_0^1 \\dot x dt\\right) + p_y \\left( B - \\int_0^1 \\dot y dt\\right) \\right) = 0\\]\nthat is,\n\\[p_x \\delta \\int_0^1 \\dot xd t = \\delta\\int_0^1 \\sqrt{\\dot x^2 + \\dot y^2}dt\\]\nWe have our interpretation \\(p_x\\): the marginal cost of producing one unit of \\(A\\), That is, if we want an extra \\(\\delta A\\), we have to incur an extra cost \\(p_x \\delta A\\).\nWe actually have \\(p_x = \\cos \\theta, p_y = \\sin\\theta\\), which means that when \\(x, y\\) are perturbed by \\(\\delta x, \\delta y\\), the shortest length between them is perturbed by\n\\[p_x \\delta x + p_y \\delta y = \\cos\\theta \\delta x + \\sin\\theta \\delta y\\]\nwhich is clearly true by geometry.\n\n\n\n\n\n\nThe market inside our head\n\n\n\nWe actually don’t need an external market. We could have put up such a fictional market inside the factory, and simply read out its price \\(p_x, p_y\\) without ever trading with it. Why? Because in this problem, a solution exists, and under some niceness assumptions, there exists a \"fair\" market price at which we are indifferent to trading with the market – we can sell \\(\\delta x\\) and buy \\(\\delta y\\), but there is \"no point to it\" because it doesn’t change our eventual cost. Thus, although the market exists, we never actually trade with it, so the market might as well be a card-board cutout with a number display of the latest prices. As long as we don’t actually try to trade with it, we would be behaving exactly as if we are facing a real market with the same prices.\nIt is perhaps best to think of the markets as things inside the head, a system of mental accounting to assign the proper price of everything. If the market prices are truly efficient, then we don’t need a real market to trade with.\n\nWhen the producer is ready, the market appears.\nWhen the producer is truly ready, the market disappears.\n\n\n\n\n\nLagrange’s devil at Disneyland\nThis is a parable about maximizing entropy under conservation of energy.\nYou are going to an amusement park, with many amusements \\(i = 1, 2, \\dots, n\\). You have exactly 1 day to spend there, so you need to spend \\(p_i\\) of a day on amusement \\(i\\). Now, your total utility at the end of the day is\n\\[S(p) := \\sum_i p_i \\ln \\frac{1}{p_i}\\]\na sum of logarithms, the idea being that you gain utility on any amusement at a decreasing rate: The first minute on the rollercoaster is great, but the second is less, and the third even less.\nIf that’s all there is to the amusement park, then the solution is clear: you should spend an equal amount of time at each amusement. This can be proved by the Lagrange multiplier mechanically, but underneath the algorithm of the Lagrange multiplier is the idea of partial equilibrium, so it’s worth spelling it out in full.\n\n\n\n\n\n\nProof by partial equilibrium\n\n\n\n\n\nSuppose your parents give you a schedule for your amusement. You look at the schedule and notice that \\(p_i &lt; p_j\\). You can reject the plan and say, “I can always do better just by spending equal time at \\(i, j\\), even holding the other times fixed.”. That is, if we are only allowed to trade between time at \\(i\\) and \\(j\\) (a “partial market”), then at partial market equilibrium, the marginal utility of amusements \\(i, j\\) must be equal.\nThe marginal utility of amusement \\(i\\) is \\(\\ln(1/p_i) - 1\\), and the same for \\(j\\). So, at partial equilibrium, \\(p_i = p_j\\), and at general equilibrium, all partial markets are in partial equilibrium.\n\n\n\nUnfortunately, the amusement park is actually infinite. You can take this news in two ways: optimistically “I can earn arbitrarily high utility by spending \\(1/N\\) of a day on \\(N\\) amusements each, and let \\(N\\) be as large as I want!” and pessimistically “No matter how much I try, I can never achieve the perfect day.”. Fortunately, the amusement park has a token system: when you enter the park, you would buy a certain number of tokens. Then you spend some tokens at the rides, proportional to the time you spend there.\nYour parents lay down three rules:\n\nYou will be given a certain number of tokens \\(E\\) at the start of the day.\nThe prices of amusements are \\(0 = E_0 \\leq E_1 \\leq E_2 \\leq \\cdots\\). Here \\(E_0\\) is the “zero-point energy”, or in other words, “just relax”.\nYou must spend exactly all your tokens. Your parents hate wastefulness and would beat you up if you don’t use all the tokens. It is possible to spend exactly all your tokens: \\(E_0 \\leq E \\leq \\max_i E_i\\).\n\nThus we have reduced the problem to \\[\n\\begin{cases}\n    \\max S(p) \\\\\n    \\sum_i p_i \\cdot 1 = 1 \\\\\n    \\sum_i p_i \\cdot E_i = E\n\\end{cases}\n\\]\nUnder these assumptions, there exists a unique schedule that maximizes your fun, or entropy.\nNow there is a slight difficulty with your previous approach. Doing partial equilibrium between 2 amusements is not possible now, because you have two constraints of not just time, but also tokens. Suppose you spend less time at \\(i\\) to spend more time at \\(j\\), this might cause you to under- or over-spend your tokens. So, to make a partial equilibrium calculation, you must use at least 3 amusements, not 2. And you are not allowed to “just relax”, since relaxing is actually “the 0th amusement”, and thus it also costs you tokens and time. That is, you must open partial markets with \\(i, j, k\\) at once, and then you can run the system to partial equilibrium. This is pretty annoying. There’s got to be a better way.\nSuddenly, in a puff of smoke, Lagrange’s devil makes an appearance!\n\n“You can’t solve the general equilibrium problem, you say? Why not try the Lagrange multipliers?”\n“Too annoying.”\n“Alright, how about I make you a deal –”\n“Sorry, but I don’t sell my soul.”\n“Don’t be stupid – only something as stupid as God could still believe in souls these days! I am proposing that you trade happiness for time and for tokens. I will make a set price for time and another set price for tokens. Both prices are in units of your happiness. You can buy happiness with time, or time with happiness, also for tokens.”\n“Uhmm…”\n“That’s all that I offer. You should read up on microeconomics so you can make the right choice. See you when the day comes!”\n\nSo the day comes and the devil shows up with the two prices:\n\n1 unit of time = \\(\\alpha\\) unit of utility.\n1 unit of token = \\(\\beta\\) unit of utility.\n\nSo you solve the following problem\n\\[\\max_p \\left(S(p) - \\alpha \\sum_i p_i - \\beta \\sum_i p_i E_i\\right)\\]\nWhat a great luck that the devil is there – it has split a giant, intercorrelated general equilibrium into so many little, uncorrelated partial equilibria: \\[\\forall i,\\quad \\max_{p_i} \\left(p_i \\ln \\frac{1}{p_i} - \\alpha p_i - \\beta p_i E_i\\right)\\]\nwith solution \\(p_i = e^{-1-\\alpha} e^{-\\beta E_i}\\).\nYou are about to go to the devil, but then the devil waves at you to halt, “Don’t make individual trades, but make a bulk one-time trade.”. So you calculate \\(\\sum_i p_i\\) and \\(\\sum_i p_i E_i\\), and to your surprise, you find that they equal exactly \\(1\\) and \\(E\\). That is, you actually would spend all your time and tokens without needing to trade with the devil.\nAnd so you sits there, looking at your two equations in strange amusement:\n\\[p_i = \\frac{e^{-\\beta E_i}}{e^{1+\\alpha}}, \\quad \\alpha+1 = \\ln\\sum_i e^{-\\beta E_i} , \\quad E = \\sum_i E_i \\frac{e^{-\\beta E_i}}{e^{1+\\alpha}}\\]\nA physicist comes and points out that they are better known as\n\\[p_i =  \\frac{e^{-\\beta E_i}}{Z}, \\quad Z = e^{-\\beta F} = \\sum_i e^{-\\beta E_i}, \\quad E = \\sum_i p_i E_i\\]\nwhere \\(Z\\) is called “partition function”, \\(F\\) “Helmholtz free energy”, and \\(\\beta\\) “inverse temperature”.\nAs the devil prepares to leave, you call after it:\n\n“I’m grateful for all your help, but, please why did you price your wares this way?”\n“Because I don’t want you to actually be happier!”\n“What do you mean?”\n“If I were to lower the price of tokens, then what would you do? You would realize that you can profit by spending a little less time at every amusement, then sell both the time and the tokens you saved, increasing your happiness! Similarly for any other form of price change. If I priced it in any other way than the equilibrium prices, you would be able to exploit my bad pricing and arbitrage out some happiness.”\n“So why did you come to visit me in the first place?”\n“It was easy to mess with mortals back before you discovered calculus. Now all I do is help you people discover general equilibrium, because you people have no fun anymore – every arbitrage opportunity is exploited to death. Still, I have to come, because I am the CEO of Hell, and I need to make maximally profitable deals with mortals, no matter how pointless it is.”\n\n\nWhen the mortal is ready to make a deal, the devil appears.\nWhen the mortal is truly ready to make a deal, the devil disappears.\n\n\n\nIsoperimetric problem\nIf we have a rope of length \\(S\\), and wish to span it from \\((0, 0)\\) to \\((T, h)\\), what shape should the rope have, in order to maximize the area under the rope? In formulas, we model the rope as a differentiable function \\(x(t): [0, T] \\to \\mathbb R\\), such that\n\\[\\begin{cases}\n\\max \\int_0^T xdt\\\\\n\\int_0^T \\sqrt{1 + \\dot x^2}dt = S \\\\\n\\int_0^T \\dot x dt = h\n\\end{cases}\\]\nHere we have two constraints, but the solution is essentially the same. First, to take care of the two constraints, we open two markets \\(p_h, p_S\\) – one for the price of height, and another for the price of rope. Then, solve for\n\\[\\delta \\int_0^T \\left(x + p_h \\left(\\frac hT - \\dot x\\right) +  p_S\\left(\\frac ST - \\sqrt{1+\\dot x^2}\\right)\\right) dt = 0\\]\nThe state variable is \\(x\\), and the control variable is \\(\\dot x\\). Unlike the previous problem, here we need to maximize the integral of the state variable. Consequently, we consider it as a problem of \"profit flows\".\nTo make things more clear, let’s explicitly write \\(v\\) as a control variable, and say that the control system satisfies:\n\\[\\begin{cases}\nv \\text{ is a control variable}\\\\\nx \\text{ is a state variable}\\\\\n\\dot x = v\n\\end{cases}\\]\nGiven that, how do we control the system? Again, it comes down to putting a price on everything, and maximizing at each instant. So, let’s open a market on commodity \\(x\\), such that the \"fair\" price is \\(p(t)\\) at time \\(t\\). With that, we can write down the profit flow\n\\[H = x + p_h \\left(\\frac hT - v\\right) +  p_S\\left(\\frac ST - \\sqrt{1+v^2}\\right) + p v\\]\nand maximizing profit over all time implies maximizing profit flow at every instant:\n\\[\\partial_v H= 0\\]\nThis gives us one equation, but we need one more equation. Namely, we need to know how \\(p(t)\\), the “fair” price of \\(x\\), changes with time.\nThe fundamental problem in pricing theory is this: how do we put a price on something? The fundamental reply from pricing theory is: no-arbitrage (no free money).\nHere is how the no-arbitrage argument works. Suppose that we have been following our optimal production plan. Then at time \\(t\\), we suddenly decide to buy an extra \\(\\delta x\\) from the market, save it, then sell \\(\\delta x\\) at time \\(t+ \\delta t\\).\nNow, since \\(\\dot x = v\\), this buying-and-selling plan does not change how much \\(x\\) grows, so after time \\(\\delta t\\), we have \\(\\delta x(t + \\delta t) = \\delta x(t)\\). Thus, the no-arbitrage equation states:\n\\[p(t)\\delta x(t) = p(t + \\delta t)\\delta x(t + \\delta t)  + \\delta x(t) \\delta t \\implies p(t+ \\delta t) = p(t) - \\delta t\\]\nand consequently, \\(\\dot p = -1\\). This is the price dynamics. Intuitively, we see that the value of a standing stock of \\(x\\) decreases as we run out of time to use it for producing.\nTo solve\n\\[\\partial_v  H= 0\\]\nfirst expand it to\n\\[-p_h + p - p_S \\frac{\\dot x}{\\sqrt{1 + \\dot x^2}} =  0\\]\nthen take derivative with respect to \\(t\\), to obtain\n\\[-\\frac{\\ddot x}{(1+\\dot x^2)^{3/2}} = \\frac{1}{p_S}\\]\nFrom elementary calculus, we know the item on the left is the curvature, so we find that the line \\(x(t)\\) is a constant-curvature curve – circular arc. The radius of the circle is the inverse of curvature, which is exactly \\(p_S\\). Thus we find that, if we are given \\(\\delta S\\) more rope, we can encircle \\(R\\delta S\\) more area under the rope, where \\(R\\) is the radius of curvature for the circular arc.\nNotice that there we are not given the sign of \\(p_S\\). There are in general two solutions, one being a circular arc curving downwards, and the other curving upwards. If we use \\(p_S &lt; 0\\), then we get the one curving upwards, and so we get the solution that achieves minimal area under the rope. If we use \\(p_S &gt; 0\\), then we get the maximal solution. This is a general fact about such problems."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#lagrangian-and-hamiltonian-mechanics",
    "href": "essays/posts/analytical-mechanics/index.html#lagrangian-and-hamiltonian-mechanics",
    "title": "Analytical mechanics",
    "section": "Lagrangian and Hamiltonian mechanics",
    "text": "Lagrangian and Hamiltonian mechanics\nGeneralizing from our experience above, we consider a generic function \\(L(t, q, v)\\), with finitely many state variables \\(q_1, ..., q_N\\). For each state variable \\(q_i\\), we regard its time-derivative as a control variable \\(v_i\\), which we are free to vary. Our goal is to design a production plan \\(t \\mapsto v(t)\\), such that\n\\[\\delta \\int L(t, q(t), v(t)) dt = 0, \\quad \\dot q = v\\]\nTo comply with general sign conventions, we interpret \\(L\\) as cost-per-time, so we say we want to minimize it (even though we only want to stationarize it).\nWe can understand \\(i\\in\\{1, 2,..., N\\}\\) to denote a commodity, say timber and sugar (let’s say there is such a thing as \"negative 1 ton timber\" – that is, we can short-sell commodities). Let \\(p_i\\) be the market price of commodity \\(i\\). Again, there is no need for real market to trade with if the prices are right, since at the right price (no-arbitrage price), we are indifferent between buying and selling, or producing and consuming. It is purely a \"mental accounting\" device.\nWith access to a market, our profit flow is:\n\\[H(t, q, p, v) := \\underbrace{\\sum_i p_i v_i}_{\\text{revenue flow}} - \\underbrace{L(t, q, v)}_{\\text{cost flow}}\\]\nIn words, \\(H(t, q, p, v)\\) is the rate of profit at time \\(t\\) if we hold a stock of commodity \\(q\\), is producing at rate \\(v\\), and the market price of commodities is \\(p\\). This is close to the Hamiltonian, but not yet. We still need to remove the dependence on \\(v\\).\nMoment-by-moment profit-flow maximization is myopic, and could lead us into deadends. That is, it is not a sufficient condition for global profit-maximization. However, it is a necessary condition. That is, suppose we are given a profit-maximizing trajectory, then it must maximize profit flow at every moment, since otherwise we could improve it. In formula:\n\\[v = \\mathop{\\mathrm{arg\\,max}}_v \\left(\\sum_i p_i v_i - L(t, q, v)\\right)\\]\nSo, define the \"optimal controller\" as\n\\[v^\\ast (t, q, p) = \\mathop{\\mathrm{arg\\,max}}_v \\left(\\sum_i p_i v_i - L(t, q, v)\\right)\\]\nand define \\(H\\) as the \"maximal profit flow\" function:\n\\[H(t, q, p) = \\max_{v} \\left(\\sum_i p_i v_i - L(t, q, v)\\right) = \\sum_i p_i v_i^\\ast(t, q, p) - L(t, q, v^\\ast(t, q, p))\\]\nBy basic convex analysis, if \\(L\\) is strictly convex with respect to \\(v\\), then \\(v^*\\) is determined uniquely by \\((t, q, p)\\), and furthermore, it is a continuous function of \\((t, q, p)\\). Consequently, \"profit maximization\" allows us to model the system in \\((t, q, p)\\) instead of \\((t, q, v)\\) coordinates, and the dynamics of the system is equivalently specified by either \\(H\\) or \\(L\\).\nThis is the mysterious \"Legendre transform\" that they whisper of. It is better called \"convex dual\". I also like to joke that the real reason that momentum is written as \\(p\\) is because it secretly means \"price\"!\n\nDerivatives of \\(H\\)\n\nTheorem 1 (Hotelling’s lemma) \\(H(t, q, p)\\) is differentiable with respect to \\(p\\), and \\[\n\\begin{cases}\n    \\partial_t H(t, q, p) &= -(\\partial_t L)(t, q, v^\\ast(t, q, p)) \\\\\n    \\nabla_p H(t, q, p) &= v^\\ast(t, q, p) \\\\\n    \\nabla_q H(t, q, p) &= -(\\nabla_q L)(t, q, v^\\ast(t, q, p)) \\\\\n\\end{cases}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the first formula. The other two are proved in the same way.\nThe argument is by no-arbitrage. We can imagine what happens if we were to suffer a little price-shock \\(\\delta p\\), adjust our production plan accordingly to \\(v + \\delta v\\), then hold that production plan and suffer another little price-shock \\(-\\delta p\\). Since we are back to the original price \\(p\\) again, we should have no more than the maximal profit rate. That is, we should have\n\\[\\underbrace{H(t, q, p)}_{\\text{maximal rate before shock}} \\geq \\underbrace{H(t, q, p+\\delta p)}_{\\text{maximal rate after shock}} + \\underbrace{\\langle -\\delta p, v^\\ast(t, q, p+\\delta p) \\rangle}_{\\text{undoing the shock, holding price steady}}\\]\nSince \\(\\delta p\\) is infinitesimal, this implies \\(\\left\\langle-\\delta p , \\nabla_v H\\right\\rangle \\geq \\left\\langle-\\delta p , v^*(t, q, p)\\right\\rangle + O(\\delta^2)\\) for all \\(\\delta p\\), implying \\(\\nabla_v H = v^\\ast(v^*(t, q, p))\\).\n\n\n\n\n\n\n\n\n\nNotation for \\(\\partial_t L\\)\n\n\n\nHere, we explicitly put a bracket around \\(\\partial_t L\\) to emphasize that\n\\[\n\\begin{aligned}\n(\\partial_t L)(t, q, v^\\ast(t, q, p))\n&= \\lim_{\\epsilon \\to 0} \\frac{L(t + \\epsilon, q, v^\\ast(t, q, p)) - L(t, q, v^\\ast(t, q, p))}{\\epsilon} \\\\\n&\\neq \\lim_{\\epsilon \\to 0} \\frac{L(t + \\epsilon, q, v^\\ast(t + \\epsilon, q, p)) - L(t, q, v^\\ast(t, q, p))}{\\epsilon}\n\\end{aligned}\\]\nand similarly for \\(\\nabla_q L\\).\n\n\n\n\nHamiltonian equations of motion\nWe are given free control over \\(\\dot q\\), and we saw that the cost-minimizing trajectory must maximize the profit flow moment-by-moment. That is,\n\\[\\dot q(t) = v^\\ast(t, q(t), p(t)) = (\\nabla_p H)(t, q(t), p(t))\\]\nor more succinctly (you can see why we tend to be rather sloppy with notations!)\n\\[\\dot q \\underbrace{=}_{\\text{optimality}} v^\\ast \\underbrace{=}_{\\text{Hotelling's lemma}} \\nabla_p H\\]\nMore evocatively speaking, we have two opposing forces of greed and no-arbitrage, clashing together to give something interesting:\n\\[\\dot q \\underbrace{=}_{\\text{we are greedy}} v^\\ast \\underbrace{=}_{\\text{but the market gives us no free money}} \\nabla_p H\\]\nIt remains to derive \\(\\dot p\\) by the no-arbitrage condition: If we shock the system with some \\(\\delta q_i\\), it does not matter if we sell it now, or carry it \\(\\delta t\\), incurring additional cost, and sell it later. That is,\n\\[p_i(t) \\delta q_i = p_i(t+\\delta t) \\delta q_i - \\partial_{q_i} L(t, q, v) \\delta q_i \\delta t \\implies \\dot p_i = \\partial_{q_i} L(t, q, v)\\]\n\n\n\n\n\n\nTip 1: Higher-order infinitesimal\n\n\n\nThe equality should be more precisely read as \"up to a higher-order infinitesimal than \\(\\delta q_i\\delta t\\)\". That is, we should be writing:\n\\[\np_i(t) \\delta q_i = p_i(t+\\delta t) \\delta q_i - \\partial_{q_i} L(t, q, v) \\delta q_i \\delta t  + o(\\delta q_i\\delta t)\n\\]\nThis detail would come up later in our proof of the Hamilton–Jacobi equation.\n\n\nSince we are only concerned with what happens on the optimal trajectory, we always choose \\(v = v^\\ast\\), so\n\\[\\dot p_i = (\\partial_{q_i} L)(t, q, v^\\ast(t, q, p)) \\underbrace{=}_{\\text{Hotelling's lemma}} -\\partial_{q_i} H(t, q, p)\\]\nThus, we obtain the two fundamental equations of Hamiltonian mechanics:\n\nTheorem 2 (Hamilton equations of motion) \\[\\begin{cases}  \n\\partial_t H &= -\\partial_t L \\\\\n\\dot p = -\\nabla_q H \\\\     \n\\dot q = \\nabla_p H\n\\end{cases}\\]\n\n\n\nEuler–Lagrange equations of motion\nBy definition,\n\\[H(t, q, p) =  \\max_{v} \\left(\\sum_i p_i v_i - L(t, q, v)\\right) =  \\sum_i p_i v_i^\\ast(t, q, p) - L(t, q, v^\\ast(t, q, p))\\]\nand since \\(L\\) is strictly convex in \\(v\\), this can be inverted (this is called \"convex duality\") to give\n\\[L(t, q, v) =  \\max_{p} \\left(\\sum_i p_i v_i - H(t, q, p)\\right) =  \\sum_i p^\\ast_i v_i - H(t, q, p^\\ast(t, q, p))\\]\nwhere \\(p^\\ast = \\mathop{\\mathrm{arg\\,max}}_p \\sum_i p_i v_i - H(t, q, p)\\). It is a basic theorem in convex geometry that, if \\(L\\) is strictly convex in \\(v\\), then \\(H\\) is strictly convex in \\(p\\), so the inversion works.\nBy the same argument as in Hotelling’s lemma, we have\n\\[\\nabla_v L(t, q, v) = p^\\ast(t, q, v)\\]\nBy definition of the convex dual, for any \\(t, q, p, v\\), we have\n\\[H(t, q, p) + L(t, q, v) \\leq \\sum_i p_i v_i\\]\nwith equality reached iff both \\(p = p^\\ast(t, q, v)\\) and \\(v = v^\\ast(t, q, p)\\). Thus, on any optimal trajectory, since \\(v = v^\\ast(t, q, p)\\), we must also have \\(p = p^\\ast(t, q, v)\\), and consequently,\n\\[\\frac{d}{dt} \\nabla_v L \\underbrace{=}_{\\text{Hotelling's lemma}} \\dot p^\\ast \\underbrace{=}_{\\text{optimality}} \\dot p \\underbrace{=}_{\\text{no-arbitrage}} \\nabla_q L\\]\nThis is the famous\n\nTheorem 3 (Euler–Lagrange equations of motion) \\[\n\\frac{d}{dt} (\\partial_{v_i} L) = (\\nabla_{q_i} L) \\quad \\forall i \\in \\{1, 2, \\dots, N\\}\n\\]\nor more succinctly,\n\\[\n\\frac{d}{dt} (\\nabla_v L) = \\nabla_q L\n\\]\n\n\n\n\n\n\n\nExplaining the notation\n\n\n\n\n\nPhysicists are often sloppy with notations, but the Euler–Lagrange equation is particularly egregious in this regard, so I will describe it carefully.3\nThe function \\(L\\) is a function of type \\(\\underbrace{\\mathbb{R}}_{\\text{time}} \\times \\underbrace{\\mathbb{R}^N}_{\\text{state}} \\times \\underbrace{\\mathbb{R}^N}_{\\text{control}} \\to \\mathbb{R}\\).\nThe function \\(\\partial_{v_i} L\\) is also a function of type \\(\\mathbb{R}\\times \\mathbb{R}^N \\times \\mathbb{R}^N \\to \\mathbb{R}\\). It is obtained by taking derivative of \\(L\\) over its \\((1 + i)\\)-th input. Let’s write that as \\(f_i\\) to make sure we are not confused by it. It is absolutely important to be clear about this! \\(f_i\\) is not a function defined only along a trajectory, but over the entire space of \\(\\mathbb{R}\\times \\mathbb{R}^N \\times \\mathbb{R}^N\\). We can similarly define \\(f_{i + N}\\) to be \\(\\partial_{q_i} L\\).\nNow, suppose we are given a purportedly optimal trajectory \\(q: \\mathbb{R}\\to \\mathbb{R}^N\\), then for any coordinate \\(i \\in \\{1, 2, \\dots, N\\}\\), we can define a function \\(g_i\\), of type \\(\\mathbb{R}\\to \\mathbb{R}\\) by\n\\[\ng_i(t) = f_i(t, q(t), \\dot q(t))\n\\]\nand similarly, we can define \\(g_{i + N} = f_{i + N}(t, q(t), \\dot q(t))\\).\nThe Euler–Lagrange equations say that we need only check\n\\[\ng_i'(t) = g_{i+N}(t) \\quad \\forall t \\in [0, T], i \\in 1:N\n\\]\nto certify that the purportedly stationary trajectory \\(q\\) is truly stationary.\n\n\n\n3 I am okay with sloppy notation sometimes, but in this case, the sloppy notation often leads to calculational mistakes and conceptual confusions, as teachers of undergrad courses can testify.\n\nWhat does it all mean?\nIn the Lagrangian formalism, we are given a system with \\(q\\) coordinates, and allowed to manipulate \\(\\dot q\\) however we want, in order to minimize a \"cost\" function \\(\\int_0^T L(t, q, \\dot q)dt\\).\nIn the Hamiltonian formalism, we are also given a market to trade with. We attempt to maximize profit by varying production, buying, and selling. However, the market simultaneously adjusts its prices \\(p\\) in just the right way so that we are always indifferent about the market (if we are ever not indifferent, the market is in serious trouble – it doesn’t actually carry any real commodity!), so we never actually make any trade. The market has been in our heads all along, but its effects are real.\nAssuming this kind of double optimization (we against the market, and the market against us), the trajectory is uniquely determined by several possible specifications. We may fix it by \\((t_0, q_0, p_0)\\), or by \\((t_0, q_0), (t, q)\\), or by \\((t_0, q_0, v_0)\\), or perhaps more exotic constraints that mix up \\(t, q, p, v\\).\nIronically, the Hamiltonian equations of motion flow out naturally in this economic interpretation, with no fuss whatsoever. The Euler–Lagrange equations are derived only as an afterthought. This is exactly backwards compared to the usual way of teaching, where the EL equations are derived first, and the Hamiltonian equations are derived by an unmotivated “Let us define \\(H = \\sum_i p_i q_i - L\\) …”, followed by some clumsy and unjustified derivations.\nIn classical control theory, the price vector \\(p\\) is called \"costate\" since it is multiplied with the state \\(q\\), and \\(\\dot p= -\\nabla_q H\\) is called the \"costate equation\". The above methods can be generalized to account for constraints, yielding Pontryagin’s maximum principle, among other results.\n\n\nCyclic coordinates\nGiven a system defined by a Lagrangian function \\(L(t, q, v)\\), we say that it is cyclic in the coordinate \\(q_i\\) if \\(L\\) does not depend on \\(q_i\\). By definition of\n\\[H(t, q, p) = \\max_v \\left(\\sum_i p_i v_i - L(t, q, v)\\right)\n\\]\nwe see that if \\(L\\) does not depend on \\(q_i\\), then \\(H\\) also does not depend on \\(q_i\\). Consequently, any optimal trajectory satisfies \\(\\dot p_i = -\\partial_{q_i} H = 0\\). That is, \\(p_i\\) is conserved – conservation of generalized momentum.\n\n\nBonus: Routhian mechanics\nSuppose that the market does not contain all commodities, but only the last \\(n\\) commodities. That is, let \\(q_{1:N} = (q_{1:s}, q_{s+1:s+n})\\), and only open markets on \\(q_{s+1:s+n}\\). The optimal cash flow equation then gives us the \"Routhian\":\n\\[\nR(t, q, v_{1:s}, p_{s+1:N}) = \\max_{v_{s+1:N}} \\left(\\sum_{i=s+1}^n p_i v_i - L(t, q, v)\\right)\n\\]\nAs before, the optimal control variables are \\(v_{s+1:N}^\\ast = \\mathop{\\mathrm{arg\\,max}}_{v_{s+1:N}} \\left(\\sum_{i=s+1}^n p_i v_i - L(t, q, v)\\right)\\).\n\nTheorem 4 (Routhian equations of motion) \\[\n\\begin{cases}\n    \\partial_t R &= -\\partial_t L \\\\\n    \\frac{d}{dt} \\partial_{v_i} R = \\partial_{q_i}R \\quad & i \\in 1:s\\\\\n    \\begin{cases}\n    \\dot q_i = \\partial_{p_i} R \\\\ \\dot p_i = -\\partial_{q_i} R\n    \\end{cases}\\quad & i\\in s+1:N\n\\end{cases}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy the same argument as in Hotelling’s lemma, we have\n\\[\n\\begin{cases}\n    \\partial_t R = -\\partial_t L \\\\\n    \\nabla_q R = -\\nabla_q L \\\\\n    \\nabla_{v_{1:s}} R = -\\nabla_{v_{1:s}} L \\\\\n    \\nabla_{p_{s+1 : N}} R = v^*_{s+1 : N}\n\\end{cases}\n\\]\nPlugging the 2-th and 3-th equations into the original Euler–Lagrange equations, we obtain\n\\[\\frac{d}{dt} \\partial_{v_i} R = \\partial_{q_i}R \\quad i \\in 1:s\\]\nThe 4-th equation gives\n\\[\\dot q_i =v_i^\\ast = \\partial_{p_i} R, \\quad \\forall i\\in s+1:N\\]\nFor \\(i\\in s+1:N\\), we can obtain the price dynamic of the partial market by the no-arbitrage condition, giving\n\\[\\dot p_i = \\partial_{q_i} L = -\\partial_{q_i} R\\]\n\n\n\nWe see that the first \\(s\\) equations look just like the EL equations, but the next \\(2n\\) equations look just like the Hamiltonian equations. The Routhian equations make an awkward hybrid.\n\n… as a fundamental entity, the Routhian is a sterile hybrid, combining some of the features of both the Lagrangian and the Hamiltonian pictures. For the development of various formalisms of classical mechanics, the complete Hamiltonian formulation is more fruitful.\n(Goldstein, Poole, and Safko 2008, sec. 8.3)\n\n\nApplication to cyclic coordinates\nThough the Routhian equations are theoretically useless, they are useful for solving specific problems. For some worked examples of using the Routhian, see the Wikipedia page.\nWhile the Euler–Lagrangian equations are \\(N\\) second-degree differential equations, the Hamiltonian equations are \\(2N\\) first-degree differential equations. We are essentially trading derivatives for equation numbers.\nThough the EL equations and the Hamiltonian equations are philosophically different, for solving particular problems, they often end up giving the same equations anyway. Specifically, if we are solving the Hamiltonian equations for a concrete example, by eliminating the variables \\(p\\), we often end up right back to the Euler–Lagrange equations. This would be quite the detour, and if there are cyclic coordinates, the Routhian could save us some trouble.\nIf we have a system that is cyclic in the last \\(n\\) coordinates, then since \\(\\nabla_q R = -\\nabla_q L\\), its Routhian satisfies \\(\\partial_{q_i} R = 0\\) for the last \\(n\\) coordinates too. Then we find that the Routhian equations become:\n\\[\n\\begin{cases}\n\\frac{d}{dt} \\partial_{v_i} R = \\partial_{q_i}R \\quad & i \\in 1:s\\\\\n\\begin{cases}\n\\dot q_i = \\partial_{p_i} R \\\\  p_i = p_i(0)\n\\end{cases}\\quad & i\\in s+1:N\n\\end{cases}\n\\]\ngiving us \\(n\\) first-degree equations and \\(N-n\\) second-degree equations. If we were to start with the Hamiltonian equations of motion, we would get\n\\[\n\\begin{cases}\n\\dot q_i = \\partial_{p_i} H \\\\  p_i = p_i(0)\n\\end{cases}\\quad i\\in s+1:N\n\\]\nby the same reasoning, and then laboriously eliminate the variables \\(p_i\\) for \\(i \\in 1:s\\), and end up with \\(s\\) second-degree differential equations, often exactly the same as the first \\(s\\) Routhian equations:\n\\[\\frac{d}{dt} \\partial_{v_i} R = \\partial_{q_i}R \\quad i \\in 1:s\\]\nThis is how the Routhian saves us some effort in practical calculations. It is useful in this way, and in this way only."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#bonus-higher-order-lagrangians-and-hamiltonians",
    "href": "essays/posts/analytical-mechanics/index.html#bonus-higher-order-lagrangians-and-hamiltonians",
    "title": "Analytical mechanics",
    "section": "Bonus: Higher-order Lagrangians and Hamiltonians",
    "text": "Bonus: Higher-order Lagrangians and Hamiltonians\nWhat happens if we use a Lagrangian with higher-order derivatives? It turns out that even in this case, we can still use economic reasoning to derive its Hamiltonian and Euler–Lagrangian equations\n\nExercise 1 Read the following sections, then generalize the derivation to \\(n\\)-th-order derivatives.\n\nThis construction of Hamiltonians from higher-order Lagrangians is often called Ostrogradsky theorem or Ostrogradsky instability, because Ostrogradsky published it in 1850 (Ostrogradsky 1850) after seeing Hamilton’s paper of 1833 (Hamilton 1833). He did not note its implications for instability, which was first noted by Pais and Uhlenbeck in 1950 (Pais and Uhlenbeck 1950). For this reason, it’s also called the Pais–Uhlenbeck model.\n\nWhen Lagrangian also depends on acceleration\nWhen the Lagrangian depends not just on position and velocity, but also acceleration, the total cost to be optimized is:\n\\[S(q) := \\int L(t, q^{(0)}, q^{(1)}, q^{(2)})dt\\]\nwhere \\(q^{(n)}\\) is a symbol that suggests itself to be the \\(n\\)-th time-derivative of the optimal trajectory \\(q\\), although it is actually defined for any tuple of real numbers, even when we don’t have \\(q^{(1)}(t) = \\frac{d}{dt}q^{(0)}(t)\\).\nOur previous method, which is to open a market on position \\(q\\), fails for two reasons:\n\nThe producer cannot optimize its velocity, because now velocity is no longer a control variable. Now, both position and velocity are state variables, and only acceleration is a control variable.\nThe producer cannot buy and sell velocity, so it has no price signal to optimize its acceleration (how fast it produces velocity).\n\nIf the market fails, make it bigger: allow the market to buy and sell not just position, but also velocity.\nWe open a market of both positions and velocities. The price vector of positions is \\(p^{(0)}\\) and the price vector of velocities is \\(p^{(1)}\\). Then, the maximal profit flow is\n\\[H(t,  q^{(0)}, q^{(1)}, p^{(0)}, p^{(1)}) = \\max_{ q^{(2)}} (\\langle p^{(0)} , q^{(1)}\\rangle + \\langle p^{(1)} , q^{(2)}\\rangle - L(t, q^{(0)}, q^{(1)}, q^{(2)}))\\]\nand the optimal production plan is\n\\[q^{(2)\\ast} = \\mathop{\\mathrm{arg\\,max}}_{ q^{(2)}}(\\langle p^{(0)} , q^{(1)}\\rangle + \\langle p^{(1)} , q^{(2)}\\rangle - L)\\]\n\n\nOstrogradsky instability and the anthropic principle\nWe will show that the quantity does deserve the name of “Hamiltonian”:\n\\[H(t,  q^{(0)}, q^{(1)}, p^{(0)}, p^{(1)}) = \\langle p^{(0)} , q^{(1)}\\rangle + \\max_{ q^{(2)}} (\\langle p^{(1)} , q^{(2)}\\rangle - L(t, q^{(0)}, q^{(1)}, q^{(2)}))\\]\nAssuming that, we have a serious problem. Consider an oscillator with higher-order derivatives. Since its Hamiltonian contains a term \\(\\langle p^{(0)} , q^{(1)}\\rangle\\), it is linear with respect to \\(p^{(0)}\\) and \\(q^{(1)}\\). In other words, it can have arbitrarily low energy states.\nNow, this could be alright if we live in a classical world, but we live in a world described by quantum field theory. In QFT, the world is a giant network of oscillators. If a quantum oscillator has higher-order derivatives, then its energy levels can go both infinitely high and low. If there is any coupling at all between its energy levels, it would instantly evaporates into infinitely many positive and negative energy particles, in a blaze of vacuum decay.\nThis is an anthropic explanation for “Why Newton’s laws?” Newton’s laws, because the Lagrangian depends on only up to velocity. Why? Because if it also depends on acceleration, the vacuum would decay (Swanson 2022). We can similarly explain anthropically why space has 3 dimensions, and time has 1 dimension.\n\nWith more or less than one time dimension, the partial differential equations of nature would lack the hyperbolicity property that enables observers to make predictions. In a space with more than three dimensions, there can be no traditional atoms and perhaps no stable structures. A space with less than three dimensions allows no gravitational force and may be too simple and barren to contain observers.\n(Tegmark 1997).\n\n\n\nHamiltonian equations\nWe can prove the Hotelling’s lemma for this Hamiltonian, using the same no-arbitrage argument as before:\n\\[\\begin{cases}     \n    \\partial_t H = -\\partial_t L \\\\  \n    \\nabla_{q^{(0)}} H = -\\nabla_{q^{(0)}} L \\\\     \n    \\nabla_{q^{(1)}} H = p^{(0)}-\\nabla_{q^{(1)}} L \\\\     \n    \\nabla_{p^{(0)}} H = q^{(1)} \\\\     \n    \\nabla_{p^{(1)}} H = q^{(2)\\ast}\n\\end{cases}\\]\nAlong an optimal trajectory, the producer always chooses \\(\\dot q^{(1)} = q^{(2)\\ast}\\), and has no choice in \\(\\dot q^{(0)} = q^{(1)}\\), so we have two equations of motion:\n\\[\\begin{cases}     \n\\dot q^{(1)} = q^{(2)\\ast} = \\nabla_{p^{(1)}} H\\\\    \n\\dot q^{(0)} = q^{(1)} = \\nabla_{p^{(0)}} H\n\\end{cases}\\]\nThe market must adjust its prices by the no-arbitrage condition, as before. If we inflict a position shock of \\(\\delta q^{(0)}\\), then by no-arbitrage, selling it now or later is equally (up to order \\(\\delta^2\\)) profitable:\n\\[\\langle p^{(0)} , \\delta q^{(0)}\\rangle = \\langle p^{(0)} + \\dot p^{(0)} \\delta t , \\delta q^{(0)}\\rangle - \\langle \\nabla_{q^{(0)}} L , \\delta q^{(0)}\\rangle \\delta t\\]\nyielding \\(\\dot p^{(0)} = \\nabla_{q^{(0)}} L = -\\nabla_{q^{(0)}} H\\).\nFor the last equation of motion, inflict a velocity shock of \\(\\delta q^{(1)}\\). The effect of the shock include both its effect on \\(q^{(0)}\\) and \\(L\\), thus the no-arbitrage equation states:\n\\[\\underbrace{\\langle p^{(1)} , \\delta q^{(1)}\\rangle}_{\\text{selling now}} =  \\underbrace{\\langle p^{(1)} + \\dot p^{(1)} \\delta t , \\delta q^{(1)}\\rangle}_{\\text{selling later}} + \\underbrace{\\langle p^{(0)} + \\dot p^{(0)} \\delta t , \\delta q^{(1)}\\delta t\\rangle}_{\\text{profit from extra }p^{(0)}} - \\underbrace{\\langle \\nabla_{q^{(1)}} L , \\delta q^{(1)}\\rangle \\delta t}_{\\text{cost from holding extra }p^{(1)}}\\]\nwhich yields the last equation \\(\\dot p^{(1)} = \\nabla_{q^{(1)}} L - p^{(0)} = -\\nabla_{q^{(1)}} H\\).\nIn summary, we have\n\nTheorem 5 (higher-order Hamiltonian equations of motion:) \\[\n\\begin{cases}     \n    \\dot q^{(1)} = \\nabla_{p^{(1)}} H\\\\     \n    \\dot q^{(0)} = \\nabla_{p^{(0)}} H \\\\     \n    \\dot p^{(0)} = -\\nabla_{q^{(0)}} H \\\\     \n    \\dot p^{(1)} = -\\nabla_{q^{(1)}} H  \n\\end{cases}\n\\]\n\n\n\nEuler–Lagrange equations\nIn order to obtain the Euler–Lagrange equations of motion, we need to work backwards from the Hamiltonian equations of motion as before.\nFrom the Hamiltonian, we can go back to the Lagrangian by inverting the convex transform:\n\\[L(t, q^{(0)}, q^{(1)}, q^{(2)})  = \\max_{p^{(0)} ,p^{(1)}} (\\langle p^{(0)} , q^{(1)}\\rangle + \\langle p^{(1)} , q^{(2)}\\rangle - H(t,  q^{(0)}, q^{(1)}, p^{(0)}, p^{(1)}))\\]\nHere we are given a hint of the troubles ahead. Since \\(H\\) is linear in \\(p^{(1)}\\):\n\\[H(t,  q^{(0)}, q^{(1)}, p^{(0)}, p^{(1)}) = \\langle p^{(0)} , q^{(1)}\\rangle + \\max_{ q^{(2)}} (\\langle p^{(1)} , q^{(2)}\\rangle - L(t, q^{(0)}, q^{(1)}, q^{(2)}))\\]\nthere is no way to fix \\(p^{(1)}\\) in the inverse transform! In detail, we plug the equation for \\(H\\) into the equation for \\(L\\), to get\n\\[L(t, q^{(0)}, q^{(1)}, q^{(2)}) = \\max_{p^{(0)} ,p^{(1)}} (\\langle p^{(1)} , q^{(2)}\\rangle - \\max_{ q^{(2)}} (\\langle p^{(1)} , q^{(2)}\\rangle - L(t, q^{(0)}, q^{(1)}, q^{(2)}))\\]\nand we see that there is no optimality constraint on \\(p^{(0)}\\). This is a hint of instabilities ahead.\nDifferentiating \\(L\\), we get\n\\[\\begin{cases}\n\\partial_t L = -\\partial_t H\\\\\n\\nabla_{q^{(0)}}L = -\\nabla_{q^{(0)}}H \\\\\n\\nabla_{q^{(1)}}L = p^{(0)} - \\nabla_{q^{(1)}}H \\\\\n\\nabla_{q^{(2)}}L = p^{(1)\\ast}  \n\\end{cases}\\]\nNow, along the optimal trajectory, we must have \\(\\nabla_{q^{(2)}}L = p^{(1)\\ast}\\), so taking its time-derivative, we get\n\\[\\frac{d}{dt}\\nabla_{q^{(2)}}L = \\dot p^{(1)} = -\\nabla_{q^{(1)}}H = \\nabla_{q^{(1)}}L - p^{(0)}\\]\nTake another time-derivative, to obtain the Euler–Lagrange equations of motion:\n\\[\\sum_{i=0}^2\\left(-\\frac{d}{d t}\\right)^i (\\nabla_{q^{(i)}} L ) =0\\]\nThe generalization to \\(L(t, q^{(0)}, ..., q^{(N-1)})\\) is immediate. It can be derived by a similar argument through the market economy.\n\n\nAn unstable higher-order oscillator\nIt is beyond our scope to discuss Ostrogradsky instability in quantum field theory, however, we can have a taste of it here. Consider the oscillator perturbed by \\(\\epsilon\\):\n\\[L = \\frac 12 m\\dot x^2 - \\frac 12 kx^2 - \\frac 12 \\epsilon \\ddot x^2\\]\nIts EL equation is\n\\[\\epsilon x^{(4)} + m\\ddot x + kx = 0\\]\na linear order-4 equation, so its solutions are of the form \\(x = \\sum_{i=1}^4 a_i e^{z_i t}\\), where \\(z_1, z_2, z_3, z_4\\) are its fundamental (complex) frequencies. Plug them in the equation and solve it simply:\n\\[z = \\pm \\sqrt{-\\frac{1}{2\\epsilon} (m \\pm \\sqrt{m^2 - 4\\epsilon k})}\\]\nAt small \\(|\\epsilon|\\) limit, we have\n\\[z \\approx \\pm i\\sqrt{\\frac km}, \\pm\\sqrt{-\\frac m\\epsilon}\\]\nand so if \\(\\epsilon &lt; 0\\), one of the modes is exponentially growing at rate \\(\\sqrt{m/|\\epsilon|}\\).\nIf \\(\\epsilon &gt; 0\\), then the oscillator survives, with two modes of oscillation of frequency \\(\\sqrt{\\frac km}\\) and \\(\\sqrt{\\frac{m}{\\epsilon}}\\). When there is viscous force, however, this delicate stability is destroyed. (Nesterenko 2007)\nThe equation of motion in this case is\n\\[\\epsilon x^{(4)} + m\\ddot x + \\gamma \\dot x + kx = 0\\]\nThe algebraic equation to solve is now \\(\\epsilon z^4 - mz^2 + i\\gamma z + k = 0\\)."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#hamiltonjacobi-equation",
    "href": "essays/posts/analytical-mechanics/index.html#hamiltonjacobi-equation",
    "title": "Analytical mechanics",
    "section": "Hamilton–Jacobi equation",
    "text": "Hamilton–Jacobi equation\nConsider again the integral of Lagrangian: \\(\\int L dt\\). If we fix the starting point \\((t_0, q_0)\\) and the ending point \\((t, q)\\), then in general there can be a countable number of paths that connect both points. This is a serious problem, but we are saved by the fact that these paths are separated, in the sense that around each path \\(\\gamma\\), there is a small neighborhood, in which there exists no other path \\(\\gamma\\) that connects the two points \\((t_0, q_0), (t, q)\\).\nAs a prototype, consider a billiard ball in a circular table.\n\nBilliards in a circular board\nWhen both \\(q_0, q\\) are the center of the billiard table, then there are \\(S^1 \\times \\mathbb{N}\\) ways to go from \\(q_0\\) to \\(q\\) over the interval \\([t_0,t]\\). Here, \\(S^1\\) denotes the circle of possible directions, and \\(\\mathbb{N}\\) denotes the discrete number of starting speeds: \\(\\frac{2R}{t-t_0}, \\frac{4R}{t-t_0}, \\dots\\).\nWhen not both of them are in the center, then there are only in general \\(\\mathbb{N}\\) ways to go from \\(q_0\\) to \\(q\\) over the interval \\([t_0,t]\\).\nConsider a billiard ball in a circular (not elliptical) table. Let \\(q, q_0\\) be any two points inside the table, not both on the center, and \\(t_0 &lt; t\\) be two moments in time, then there are a countable infinity of possible trajectories that reach \\((t, q)\\) from \\((t_0, q_0)\\).\nThere are two ways to see this visually, a particle way and a wave way.\nFor the particle way, define:\n\nthe billiard’s starting angle is \\(\\theta\\), with \\(\\theta\\) chosen such that when \\(\\theta = 0\\), the billiard would move on a diameter of the table.\n\\(l(\\theta, n)\\) is the oriented line segment that starts at the point where the billiard’s hits the table for the \\(n\\)-th time, and ends at the point where it hits for the \\((n+1)\\)-th time.\n\nFor any \\(n = 1, 2, ...\\), as \\(\\theta\\) moves from \\(\\theta = 0\\) to \\(\\theta = \\pi\\), the line \\(l(\\theta, n)\\) smoothly varies from the diameter in one direction to the diameter in the opposite direction. Now, if you take a diameter in the circle, and smoothly turn it around by \\(180^\\circ\\), then no matter how much you shift the line around in the mean time, you are forced to sweep it over every point at least once.4 Consequently, every point can be reached after \\(n\\) reflections, for any positive integer \\(n\\)\n4 If you want a hands-on approach, imagine hammering in a nail at some point \\(q\\) in the circle, and putting a long stick on the diameter. Now grab the stick and start turning it around. There is no way for you to turn it by \\(180^\\circ\\) without hitting the nail at some time.For the wave way, imagine simultaneously shooting out a billiard in every direction with constant speed, and watch the \"wavefront\" of billiards evolve. The wavefront is reflected by the table edges and assumes increasingly complicated shapes, but it remains a closed curve (with possible self-intersections). As the closed curve reverberates across the table again and again, it sweeps across every point in the table again and again at discrete intervals.\nHowever, there is no way to smoothly reach one trajectory from any other – you either have to make a discrete jump in how hard you strike the billiard ball, or in which direction you strike. This contrasts with the case with both \\(q, q_0\\) at the center, where you can smoothly vary your striking angle while keeping the same striking force.\nSimilarly, for a table with smooth boundary, for almost all point-pairs in the table, there are also a countable infinity of trajectories between them. We must say \"almost all\" to exclude singular cases such as the two focal points of an ellipse, where there is a whole continuum of trajectories between them.5\n5 Exactly counting the singular cases, and studying polygonal, or even non-convex tables, is an ongoing research program, with the name of \"dynamical billiard flow\".What is the general lesson for defining \\(S\\)? In general, we also need to fix a particular optimal path \\(\\tilde\\gamma\\), and only consider optimal paths that are in a small neighborhood of \\(\\tilde \\gamma\\). This is the same idea as selecting a branch cut when dealing with multi-valued functions like the complex logarithm.\nWhy are we so concerned with reflections? Read on!\n\n\n\nThe two ways to find trajectories from \\(q_0\\) to \\(q\\). The particle way involves sending out rays from \\(q_0\\), reflecting off the walls of the table, until a ray hits \\(q\\). The wave way involves sending out an expanding wavefront from \\(q_0\\), rippling through the table, passing over \\(q\\) again and again. The rays are perpendicular to the wavefront in this case.\n\n\n\n\n\nSimilarly, in a rectangular billiard table, there are infinitely many possible ways to go from one point to another, but one cannot go from one way to another continuously.\n\n\n\n\nHamilton’s principal function\nConsider a problem in traveling: Given a starting spacetime \\((t_0, q_0)\\) and an ending spacetime \\((t, q)\\), what is the lowest cost of traveling between them? We want to define it as:\n\\[\nS(t, q; t_0, q_0) = \\int_{t_0}^t L(\\tau, \\gamma(\\tau), \\dot\\gamma(\\tau))d\\tau\n\\]\nwhere \\(\\gamma(\\tau)\\) is the unique path from \\((t_0, q_0)\\) to \\((t, q)\\). However, as we saw in the case of circular billiards, we don’t have a unique path in general. Therefore, we should be a bit more careful.\nFirst, we select a prototypical path \\(\\gamma_{prototype}\\). Next, we smoothly vary \\(\\gamma_{prototype}\\) until it becomes some path \\(\\gamma\\) that goes from \\((t_0, q_0)\\) to \\((t, q)\\). Finally, define the Hamilton’s principal function using this particular \\(\\gamma\\). The construction is a bit awkward, but it would allow us to avoid the non-uniqueness problem.\nThus, we define the Hamilton’s principal function:\n\\[\nS(t, q; t_0, q_0) = \\int_{t_0}^t L(\\tau, \\gamma(\\tau), \\dot\\gamma(\\tau))d\\tau\n\\tag{1}\\]\n\n\nHamilton–Jacobi equation\nFor all nice enough Lagrangian \\(L\\), Hamilton’s principal function \\(S\\) is differentiable with respect to \\((t, q)\\), so we will study its differential equation.\nLet’s first consider the easy case: we simply let the trajectory \"run a little longer\". That is, we let the trajectory run from \\((t_0, q_0)\\) to \\((t, q)\\), then let it keep running for \\(\\delta t\\), reaching \\((t+\\delta t, q + \\delta q)\\). It’s clear that we have \\(\\delta q = \\dot q(t) \\delta t\\), and\n\\[\nS(t+\\delta t, q + \\delta q; t_0, q_0) - S(t, q; t_0, q_0) = \\left(\\sum_i p_i \\dot q_i - H\\right)\\delta t\n\\]\nso we have:\n\\[\n\\partial_t S + \\sum_i \\partial_{q_i}S \\dot q_i = - H + \\sum_i p_i \\dot q_i\n\\tag{2}\\]\nwhich strongly suggests\n\nTheorem 6 \\[\n(-\\partial_t, \\nabla_q)S(t, q; t_0, q_0) = (H, p), \\quad (-\\partial_{t_0}, \\nabla_{q_0})S(t, q; t_0, q_0) = (H_0, p_0)\n\\tag{3}\\]\n\nIf Equation 3 is indeed true, then we have\n\nTheorem 7 (Hamilton–Jacobi equation) \\[\\partial_t S + H(t, q, \\nabla_q S) = 0\\]\n\nIt suffices to prove \\(\\nabla_q S = p\\), since then \\(\\partial_t S = -H\\) follows from Equation 2.\nIt suffices to prove \\((-\\partial_t, \\nabla_q)S(t, q; t_0, q_0) = (H, p)\\), since the other one is proved by the same argument, time-reversed.\nRecall the economic construction of \\(p\\). It is a price vector designed specifically to destroy all arbitrage opportunities. Consequently, we can consider an entire family of paths shown in Figure (a, b).\n\n\n\nDerivation of the Hamilton–Jacobi equation.\n\n\nHere, \\(\\gamma\\) is the path from \\((t_0, q_0)\\) to \\((t, q)\\), and \\(\\gamma + \\delta \\gamma\\) is the path to \\((t, q+\\delta q)\\). We interpolate between them by a family of paths \\(\\{\\gamma_\\tau\\}_{\\tau}\\), where \\(\\gamma_\\tau\\) is the path obtained by first moving on \\(\\gamma\\) for time \\(t\\in [t_0, \\tau]\\), then making a \"jump\" by \"purchasing from the market\"6 an infinitesimal bundle of commodities so that we fall onto the \\(\\gamma + \\delta \\gamma\\) path, then continue along that path.\n6 We are using the market for real now, so the marketeer had better had stocked up on those commodities!7 More precisely, a higher-order infinitesimal than the area of the parallelogram. See Tip 1.Now consider two such jumped-paths, \\(\\gamma_{\\tau}\\) and \\(\\gamma_{\\tau + \\delta \\tau}\\), where \\(\\delta \\tau\\) is an infinitesimal, shown in Figure (c). The cost difference between them is that between two sides of the parallelogram. By the no-arbitrage construction, the difference is zero.7\nThus, we can smoothly \"glide\"8 the path \\(\\gamma\\) to \\(\\gamma + \\delta\\gamma\\) by the family of jumped-paths \\(\\gamma_\\tau\\), with \\(\\tau\\) going from \\(t\\) to \\(t_0\\), with no change in cost.9 Thus, the only cost difference between \\(\\gamma\\) and \\(\\gamma + \\delta\\) is the cost it takes to buy the bundle of commodities \\(\\delta q\\) at the very last instance:\n8 In the jargon of topology, this is a homotopy of paths.9 More precisely, their difference in cost is a higher-order infinitesimal than the area of the curvy triangle between them. Since the curvy triangle is an infinitesimal of order \\(\\delta q\\), the difference in cost is a higher-order infinitesimal than \\(\\delta q\\).\\[S(t, q+\\delta q; t_0, q_0) - S(t, q; t_0, q_0) = \\sum_i p_i \\delta q_i\\]\nfinishing the proof.\n\n\n\n\n\n\nThe usual proof\n\n\n\n\n\nThe HJE has a standard proof, such as the one appearing in (Goldstein, Poole, and Safko 2008, chap. 8). It does not require reasoning with different orders of infinitesimals, but it is less geometrical.\nSketch of the proof:\n\nStart with the original system with configuration space \\((t, q_{1:N}, \\dot q_{1:N})\\).\nMove to the Hamiltonian equations on phase space \\((t, q_{1:N}, p_{1:N})\\).\nRegard that as part of a larger system with configuration space \\((t, q_{1:N}, p_{1:N}, \\dot q_{1:N}, \\dot q_{1:N}))\\)\nWrite down the Euler–Lagrange equations for that larger system.\n\n\n\n\n\nTheorem 8 (Poincaré–Cartan integral invariant (Arnol’d 2001, 237–38)) Draw an arbitrary closed cycle \\(\\alpha\\) in phase space-time. Let every point \\(A \\in \\alpha\\) evolve for some time (not necessarily the same amount of time) to reach some other point \\(A'\\). Let \\(\\alpha'\\) be the cycle consisting of those points \\(A'\\). Then we have the Poincaré–Cartan integral invariant\n\\[\n\\oint_\\alpha \\left\\langle p, dq\\right\\rangle - Hdt = \\oint_{\\alpha'} \\left\\langle p, dq\\right\\rangle - Hdt\n\\]\nAs a special case, if both \\(\\alpha\\) and \\(\\alpha'\\) consists of simultaneous points, then it reduces to the Poincaré relative integral invariant\n\\[\n\\oint_\\alpha \\left\\langle p, dq\\right\\rangle = \\oint_{\\alpha'} \\left\\langle p, dq\\right\\rangle\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nDivide the tube into ribbons, like a barrel, then note that the integral around each barrel-plank is zero, as argued before. This is a case of the Stokes’ theorem.\n\nIn more detail, we can consider the four ends of a barrel-plank parallelogram. Label those points as \\(A, B, A', B'\\) as shown. Though the points \\(A, B, A', B'\\) exist in phase space-point, we can forget their momenta, thus projecting them to configuration space-time. Each phase space-time trajectory projects to a trajectory in configuration space-time, and we obtain \\(S_{A \\to B} = S(t_A, q_A; t_B, q_B)\\), etc.\nNow, by Equation 3, we can shift \\(S_{A \\to B}\\) to \\(S_{A \\to B'}\\), then to \\(S_{A' \\to B'}\\):\n\\[\nS_{A \\to B} = S_{A \\to B'}  -H_B \\delta t_B + \\left\\langle p_B , \\delta q_B\\right\\rangle = S_{A' \\to B'} + H_A \\delta t_A - \\left\\langle p_A , \\delta q_A\\right\\rangle -H_B \\delta t_B + \\left\\langle p_B , \\delta q_B\\right\\rangle\n\\]\nNow, if we shift around one entire cycle, we would get back the same \\(S_{A \\to B}\\). Thus the two integrals are equal.\nWe will prove Noether’s theorem similarly.\n\n\n\n\nExercise 2 A one-dimensional family of trajectories in phase space sweep out a curved surface. As shown in the illustration, prove that for any cycle \\(\\gamma\\) on the curved surface, \\(\\oint_\\gamma \\left\\langle p, dq\\right\\rangle = 0\\).\n\n\n\nIllustration for \\(\\oint_\\gamma \\left\\langle p, dq\\right\\rangle = 0\\).\n\n\n\n\n\nHamilton characteristic function\nIn most situations, the system is time-independent. In this case we can simplify the HJE to\n\\[\\partial_t S = -H(q, \\nabla_q S)\\]\nSince the left side depends on \\(t\\), but the right side does not explicitly, we can solve this by separation of variables.\nSuppose there exists some function \\(W\\), called the Hamilton characteristic function, that satisfies\n\\[\nH(q, \\nabla_q W(q)) = E\n\\tag{4}\\]\nfor some \\(E\\in \\mathbb{R}\\), then \\(S(t, q) = W(q) - Et\\) is a solution to the HJE.\nWe might naively think, by analogy with Fourier transform, that any solution to the HJE is a linear combination, of the form \\(S(t, q) = \\int (W_E(q) - Et)dE\\), but this is not true, since the HJE is nonlinear. Nevertheless, solutions of the form \\(S(t, q) = W(q) - Et\\) are often sufficient for applications.\n\n\nTwo more proofs of HJE\n\n\n\n\n\n\nProof by positional arbitrage\n\n\n\n\n\nWe only need to prove \\(p = \\nabla S\\), which is sufficient to prove Equation 3, and thus the HJE.\nGiven a starting position \\((t_0, q_0)\\) and a reference trajectory, there exists some \\(S(t, q; t_0, q_0)\\), the cost function of arriving at any point in configuration space-time in a neighborhood of the reference trajectory. For each trajectory \\(\\gamma\\) in the neighborhood that arrives at \\((t, q)\\), there exists a final market price \\(q(t)\\). We need to show that \\(q = \\nabla S\\).\nSuppose not, then we can perform positional arbitrage. First, we arrive at \\(q + \\delta q\\) by the efficient route, then sell off some \\(\\delta q\\). This would cost us\n\\[S(t, q+\\delta q; t_0, q_0) - \\left\\langle p, \\delta q\\right\\rangle = S(t, q; t_0, q_0) + \\left\\langle\\nabla S - p, \\delta q\\right\\rangle\\]\nIf \\(\\nabla S \\neq p\\), then we can take \\(\\delta q = -(\\nabla S - p)\\epsilon\\), and thus magically make the journey cost less by a first-order infinitesimal. This means the market is inefficient, a contradiction.\n\n\n\nHere is a proof in the spirit of wave mechanics and dynamical programming. Though I did not study his proof,10 I believe this is how the inventor of dynamical programming, Richard Bellman, proved his extension, the Hamilton–Jacobi–Bellman equation. The HJBE reduces to the HJE under certain conditions – they do not talk about the same thing, because while HJ is about stationary action, HJB is about maximal action.\n10 In his autobiography, he said,\n\nProblems of this type had been worked on before by many mathematicians, Euler, Hamilton, and Steiner, but the systematic study of problems of this type was done at RAND starting in 1948 under the inspiration of von Neumann. (Bellman 1984, 208)\n… one can use dynamic programming for the minimum principles of mathematical physics. For example, with dynamic programming one has a very simple derivation of the eikonal equation. In addition, the Hamilton-Jacobi equation of mechanics can easily be derived. (Bellman 1984, 289)\n\nSuppose that we have found all points at which the action is equal to \\(S\\). Now we would like to expand that surface a little further, to the surface of action \\(S + \\delta S\\). We do that in the spirit of economics (of course!) and traveling.\nInterpret the action of a path as the cost of traveling along that path. The surfaces of constant action, then, become the isochrone maps. The problem we face is then a matter of travel planning: Given that we can reach up to surface \\(X_S\\) if we are willing to pay cost \\(S\\), how much further can we travel if we are willing to pay an additional \\(\\delta S\\)?\n\n\n\nIsochrone maps of travel time in America, 1800 – 1930. (Paullin 1932, plate 138, page 366)\n\n\nLet us stand at a point \\((t, q)\\) on the surface of action \\(S\\), and consider all the points we can reach by an additional action \\(\\delta S\\). Suppose we go from \\((t, q)\\) to \\((t + \\epsilon t, q + \\epsilon q)\\), then the cost of that is \\(L\\left(t, q, \\frac{\\epsilon q}{\\epsilon q}\\right)\\epsilon t\\). (We write \\(\\epsilon t\\) instead of \\(\\delta t\\), because we have to use that symbol later.)\nTherefore, the “wave” of action \\(\\delta S\\) coming out of the point \\((t, q)\\) are those points \\((t + \\epsilon t, q + \\epsilon q)\\) satisfying the equation\n\\[\nL\\left(t, q, \\frac{\\epsilon q}{\\epsilon q}\\right)\\epsilon t = \\delta S\n\\]\nAnd the surface of action \\(S + \\delta S\\) is the envelope of all those little waves (“wavelets”). This is the wave perspective, but we still need to return to the particle perspective.\nSuppose you are already at \\((t, q)\\), and you just want to reach the surface of \\(S + \\delta S\\). It doesn’t matter where you end up on that surface – you just have to get to that surface somewhere. You also have exactly \\(\\delta S\\) to spend, so you have to plan optimally. Now, looking at that picture, you see that the only place you can possibly reach is a certain point \\((t + \\delta t, q + \\delta q)\\) where the wavelet is tangent to the surface of \\(S + \\delta S\\). At that point, the tangent surfaces of \\(S\\) at \\((t, q)\\) is parallel to the tangent surface of wavelet. That is,\n\\[\n\\begin{aligned}\nd\\left(L\\left(t, q,-\\frac{\\epsilon q}{\\epsilon t}\\right) \\epsilon t\\right) |_{\\epsilon t = \\delta t, \\epsilon q = \\delta q} &= \\left\\langle(\\left(\\nabla_q L\\right) \\underbrace{\\delta t}_{\\rightarrow 0}+\\nabla_v L), d q\\right\\rangle +\\left(L-\\frac{\\delta q}{\\delta t} \\nabla_v L\\right) d t \\\\\n&\\propto dS\n\\end{aligned}\n\\]\nThus, there exists some constant \\(c &gt; 0\\) such that\n\\[\n(\\partial_t S, \\nabla_q S) = c \\left(\\left\\langle\\nabla_v L, \\frac{\\delta q}{\\delta t}\\right\\rangle - L , \\nabla_v L\\right) = (-cH, cp)\n\\]\nSince we also have\n\\[\n\\partial_t S \\delta t + \\left\\langle\\nabla_q S, \\delta q\\right\\rangle = \\delta S = L \\delta t\n\\]\nwe see \\(c=1\\).\n\n\n\nThe wavelet proof of the HJE.\n\n\n\n\n\n\n\n\nConvexity of the wavelet\n\n\n\nIf the wavelet is convex, then the tangent point is unique, and there is only one way to proceed from \\((t, q)\\). However, if the wavelet is not, then there could exist two or more particle paths shooting out from \\((t, q)\\). It is similar to birefringence and conical refraction (Lunney and Weaire 2006).\n\n\n\nExercise 3 If you have studied, or intend to study, control theory, then prove the Hamilton–Jacobi–Bellman equation using the exact same picture. You can also prove the stochastic HJB equation in the same way, though you would need to insert the expectation \\(\\mathbb{E}\\) somewhere.\n\n\n\nBonus: Noether’s theorem\nIn Noether’s theorem, symmetries of the Lagrangian give us conserved quantities of motion.\n\n\\(\\epsilon\\) is an infinitesimal number.\nAn infinitesimal transform is an infinitesimal deformation \\((\\delta t, \\delta q)\\) of configuration space-time.\n\n\\(\\delta t = \\epsilon T\\), where \\(T\\) a function of type \\(\\underbrace{\\mathbb{R}}_{\\text{time}} \\times \\underbrace{\\mathcal C}_{\\text{configuration space}} \\to \\mathbb{R}\\)\n\\(\\delta q = \\epsilon Q\\), where \\(Q\\) is a function of type \\(\\mathbb{R}\\times \\mathcal C \\to \\mathbb{R}^d\\), where \\(d\\) is the dimension of configuration space \\(\\mathcal C\\).\n\nAn infinitesimal transform is a symmetry of the Lagrangian, iff taking any path \\(\\gamma\\) (not necessarily physically real), and deforming it to \\(\\gamma'\\), the action is conserved: \\(\\int_\\gamma L dt = \\int_{\\gamma'} L dt\\).\nA conserved quantity of motion is a number depending on \\(t, q, \\dot q\\), such that it is constant along any physically real path \\(\\gamma\\).\n\n\nExercise 4 We defined “symmetry of the Lagrangian” by a condition on an integral: \\(\\int_\\gamma L dt = \\int_{\\gamma'} L dt\\). Reformulate this to a condition at a point, involving \\(L, Q, T\\) and their derivatives.\n\n\n\n\n\n\n\nWarning\n\n\n\nA symmetry of the Lagrangian conserves all actions, even those of unphysical paths, but a conserved quantity of motion is only conserved along physical paths. We may call them “conserved quantity of physical motion” to emphasize the distinction.\n\n\nTake a trajectory \\(\\gamma_{AA'}\\) from point \\(A\\) to \\(A'\\). Now, shift it by \\(\\delta t, \\delta q\\), resulting in a trajectory \\(\\gamma_{BB'}\\) from point \\(B\\) to \\(B'\\). Since \\(L\\) is invariant under symmetry, any such shifting gives us \\(S_{AA'} = S_{BB'}\\). Now, if \\(\\delta S \\neq 0\\) in a neighborhood of the given trajectory \\(\\gamma_{AA'}\\), then we can take this variation \\(\\delta \\gamma\\), and shift it by the symmetry, showing that \\(\\delta S \\neq 0\\) in a neighborhood of \\(\\gamma_{BB'}\\) too.\nTherefore, by Hamilton’s principle, the infinitesimal transform sends any physically real trajectory into another physically real trajectory.\n\nTheorem 9 (Noether’s theorem) If \\((t, q) \\mapsto (t + \\epsilon T, q + \\epsilon Q)\\) is an infinitesimal symmetry of the Lagrangian, then\n\\[\nH T - \\left\\langle p, \\delta Q\\right\\rangle = (\\left\\langle\\nabla_v L, \\dot q\\right\\rangle - L)T - \\left\\langle\\nabla_v L, Q\\right\\rangle\n\\]\nis a conserved quantity of motion.\n\nTechnically speaking, we should not confuse the left side and the right side of\n\\[\nH T - \\left\\langle p, \\delta Q\\right\\rangle = (\\left\\langle\\nabla_v L, \\dot q\\right\\rangle - L)T - \\left\\langle\\nabla_v L, Q\\right\\rangle\n\\]\nThe left side is defined on the phase space-time, while the right side is defined on the configuration space-time. They are equal only because we have soldered together the phase space and the configuration space.\nIndeed, again and again we see that Lagrangian and Hamiltonian mechanics are different worlds, with their own dreams and phantasies, but always agreeing on the same reality.\nIf this were the case, then why do we bother creating two mechanics, since there is no physical experiment to distinguish them? It is because in quantum mechanics, virtual trajectories are just as real as the “real” trajectories. A particle can just go around the earth once before it arrives at its destination, taking the long way around. In this case, what is virtually possible matters just as much as what is classically real, and if Lagrangian mechanics and Hamiltonian mechanics can entertain different kinds of dreams, we might be able to tell them apart.\n\n\n\n\\(S\\) is a source of photons, and \\(P\\) is a receiver. A photon can go from \\(S\\) to \\(P\\) by bouncing off the mirror below, along many paths. Each path has a certain amplitude, and the sum of all their amplitudes is the total amplitude. While the amplitude is dominated by the amplitudes near the classical path \\(SGP\\), it is not the only path, and all paths, even the “virtual” paths like \\(SAP\\), contribute to what we observe. (Feynman 2006, fig. 24)\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nGiven any physically real trajectory \\(\\gamma_{AA'}\\) from point \\(A\\) to \\(A'\\), the infinitesimal transform sends it to another physically real \\(\\gamma_{BB'}\\). By symmetry, both paths have the same action. Thus, we have\n\\[\nS(A, A') = S(B, B')\n\\]\nNow, consider an intermediate path \\(\\gamma_{A, B'}\\). By Equation 3,\n\\[\nS(A, B') = S(A, A') - H \\delta t + \\left\\langle p, \\delta q\\right\\rangle, \\quad S(B, B') = S(A, B') + H_0 \\delta t_0 - \\left\\langle p_0, \\delta q_0\\right\\rangle\n\\]\nTherefore, \\(H T - \\left\\langle p, \\delta Q\\right\\rangle\\) is a conserved quantity of motion.\n\n\n\nThe proof is similar to the proof of Theorem 8, though they differ in that Noether’s theorem shows a quantity, defined at a single point, is conserved over a trajectory, while the Poincaré–Cartan invariant is not defined at a single point, but as an integral over an entire cycle."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#the-particle-wave-duality",
    "href": "essays/posts/analytical-mechanics/index.html#the-particle-wave-duality",
    "title": "Analytical mechanics",
    "section": "The particle-wave duality",
    "text": "The particle-wave duality\n\nParticle in free space\nConsider a particle of mass \\(m\\) in free space \\(\\mathbb{R}^n\\). Its Lagrangian is \\(L(t, q, v) = \\frac 12 m\\|v\\|^2\\). By convex duality, we have\n\\[\\begin{cases} L(t, q, v) = \\frac 12 m\\|v\\|^2\\\\ H(t, q, p) = \\frac{\\|p\\|^2}{2m} \\end{cases}\\quad  \n\\begin{cases} p^\\ast(t, q, v) = mv\\\\ v^\\ast(t, q, p) = \\frac{p}{m} \\end{cases}\\]\nFor any \\(t_0, q_0, t, q\\) with \\(t_0\\neq t\\), we can directly solve for the trajectory from \\((t_0, q_0)\\) to \\((t, q)\\), then find the action:\n\\[S_{t_0, q_0}(t, q) = \\frac 12 m \\frac{\\|q-q_0\\|^2}{t-t_0}\\]\nEach \\(S\\) defines a paraboloid wavefront in spacetime, with apex \\(t_0, q_0\\). The wavefront is translation-symmetric, so we set both to zero, yielding the equation of the wavefront:\n\\[\nt = \\frac{\\|q\\|^2}{2S/m}\n\\]\nThe wavefront of \\(S = 0\\) is just the positive \\(t\\)-axis, and as \\(S\\) increases, the wavefront widens out.\nNow, we interpret this wave from the HJE point of view. Let’s say we have the wavefront at \\(S=S_0\\), and we want to construct the wavefront at \\(S = S_0 + \\delta S\\). This we perform by rippling out a little wavelet at each point \\((t', q')\\) on the wavefront. The little wavelet has shape \\((t-t') = \\frac{\\|q - q'\\|^2}{2\\delta S/m}\\), and as we move \\(t', q'\\) around the parabola of wavefront \\(S = S_0\\), the envelope of these wavelets is the wavefront of \\(S = S_0 + \\delta S\\).\nThis is the wave point of view. We can switch back to the particle point of view. What is the velocity of the particle passing the point \\((t', q')\\)? We know that it must be traveling in the optimal direction, and the optimal direction allows it to go as far as possible. Therefore, we simply draw the wavelet of \\(\\delta S\\) at \\((t', q')\\), then find the intersection of the wavelet with the wavefront of \\(S = S_0 + \\delta S\\).\nThe entire procedure is pictured below.\n\n\n\nThe two red curves are two wavefronts \\(S = S_0\\) and \\(S = S_0 + \\delta S\\). At select points on the first wavefront, we draw a wavelet of \\(\\delta S\\), which is tangent to the second wavefront. The particle trajectory connects the point and the tangent point of the wavelet with the second wavefront.\n\n\n\n\nParticle-wave in free space\nThe paraboloid-shaped solution for \\(S\\) is interpretable in Newtonian mechanics, as the motion of a single particle moving from the origin. However, the HJE itself is merely a PDE with its own logic and meaning, and consequently, it may have different solutions that are hard to to interpret in Newtonian mechanics.\nFrom our 21st-century perspective, we can say that the HJE is generally true, and Newtonian mechanics is only a special case. Some solutions to the HJE may not be interpretable in Newtonian mechanics, but they are nevertheless physically real, since Newtonian mechanics is incomplete. Given that, we simply try to solve HJE, then try to interpret it, even if not in Newtonian mechanics.\nBecause the Lagrangian is time-independent, so any time-independent solution:\n\\[S(t, q) = W(q) - Et, \\quad \\| \\nabla_q W \\| = \\sqrt{2mE}\\]\nfor any constant \\(E &gt; 0\\) also gives a solution to the HJE. This is just the eikonal equation for a medium of constant wave speed! More on this in the section on geometric optics. One can of course solve the eikonal equation by putting it into a numerical package and let it grind out the solution. However, we can interpret it by the Huygens principle.\nSuppose you know a surface of constant \\(W = W_0\\), and you know that the arrows of \\(\\nabla_q W\\) point outwards, then since \\(\\nabla W\\) is perpendicular to the surface, and is of constant length \\(\\sqrt{2mE}\\), you can step out a small distance \\(ds\\) perpendicularly out of the whole surface \\(W= W_0\\), and arrive at the surface of \\(W=W_0 + \\sqrt{2mE} ds\\). Alternatively, you can draw small spheres of radius \\(ds\\), and their outwards envelope is the \\(W=W_0+ \\sqrt{2mE} ds\\) surface. These procedures are equivalent, but in one, we constructed \"rays\" while the other we constructed \"wave fronts\".\nFor the free particle, the simplest solution is the plane wave:\n\\[W(q) = \\sqrt{2mE} \\langle \\hat k, q \\rangle\\]\nwhere \\(\\hat k\\) is any unit-vector, interpreted as the direction of wave propagation. Plugging it back to Equation 3, we find that the \"planar wave particle\" has\n\\[(\\partial_t S, \\nabla S) = (-H, p) = (-E, \\sqrt{2mE}\\hat k)\\]\nwhere \\(\\hat k\\) is the direction of the group velocity of the wave. The group velocity of the wave is \\(\\frac{\\partial_t S}{\\nabla S} = \\frac{E}{\\sqrt{2mE}}\\hat k\\).\n\n\n\nThe two red curves are two wavefronts \\(S = S_0\\) and \\(S = S_0 + \\delta S\\). At select points on the first wavefront, we draw a wavelet of \\(\\delta S\\), which is tangent to the second wavefront. The particle trajectory connects the point and the tangent point of the wavelet with the second wavefront.\n\n\nTaking the particle-wave analogy seriously, we say that:\n\na planar wave is a particle with energy \\(E\\) and momentum \\(p \\propto k \\propto \\sqrt{2mE} \\hat k\\).\na particle with energy \\(E\\) and momentum \\(p\\) is a planar wave traveling at group velocity \\(E/p\\).\n\nTypically, waves have a wavelength \\(\\lambda\\), which is related to the wave vector \\(k\\) by \\(k \\propto \\lambda^{-1}\\), we find that the particle has wavelength \\(\\lambda \\propto \\frac 1k \\propto \\frac 1p\\). Thus, we arrived at de Broglie’s matter-wave hypothesis, which Schrödinger expanded into his equation. Both de Broglie and Schrödinger were inspired by Hamilton’s optics-mechanics analogy, so we are treading the same path as them a century ago.\nIn fact, we could start with an arbitrary wavefront in configuration-spacetime, and use Huygens’ principle to construct the wavefront in the next moment. In general, we can’t do Fourier analysis on such wavefronts – they are not decomposable into planar waves, because the HJE is a nonlinear equation. Fourier analysis only works on linear differential equations.\n\n\n\nGiven a wavefront in the shape of \\(t = q^3\\), we can construct the next wavefront as the envelope of the parabolic wavelets at every point on the wavefront.\n\n\n\n\nParticle in a potential field\nA particle in a time-dependent potential field \\(V\\) has Lagrangian \\(L(t, q, v) = \\frac 12 m\\|v\\|^2 - V(t, q)\\). By convex duality\n\\[\\begin{cases}     \nL(t, q, v) = \\frac 12 m\\|v\\|^2 - V(t, q)\\\\     \nH(t, q, p) = \\frac{\\|p\\|^2}{2m} + V(t, q)\n\\end{cases} \\quad\n\\begin{cases}     \np^\\ast = mv \\\\     \nv^\\ast = \\frac pm\n\\end{cases}\\]\nThe HJE gives\n\\[\\partial_t S + \\frac 1{2m} \\|\\nabla_q S\\|^2 = -V(t, q)\\]\nAs before, if we interpret the equation as a wave equation, then the group velocity is \\(\\frac{\\partial_t S}{\\nabla S} = \\frac{-E}{p} \\hat k\\), with magnitude \\(v_g = \\frac{E}{\\sqrt{2m(E-V)}}\\).\n\n\n\n\n\n\nExample: particle in free fall\n\n\n\n\n\nWe consider the classical problem of a body in free fall, thrown from the origin \\((t, q) = (0, 0)\\). Basic physics tells us that the position and velocity of the body are given by\n\\[q(t) = v_0 t - \\frac{1}{2}gt^2, \\quad v = v_0 - gt = \\frac{q}{t} - \\frac{1}{2}gt\\]\nPlugging these expressions into the Hamilton-Jacobi equation, we obtain a system of partial differential equations for the action function \\(S\\):\n\\[\n\\begin{cases}\n\\partial_t S &= -H = -\\frac{1}{2}m(\\frac{q}{t} - \\frac{gt}{2})^2 - mgq \\\\\n\\partial_q S &= m(\\frac{q}{t} - gt)\n\\end{cases}\n\\]\nSolving this system, we find the unique solution for the action:\n\\[S = \\frac{x^2}{y} - xy - \\frac{y^3}{12}\\]\nwhere we have introduced convenience variables \\(x = gq\\), \\(y = gt\\), and \\(s = \\frac{2gS}{m}\\).\nThe contour lines of the action function satisfy the equation:\n\\[\nx = \\frac{1}{2}y^2 \\pm \\sqrt{\\frac{1}{3}y^4 + sy}\n\\]\nTo analyze the wavelet associated with this system, we start by writing down the Lagrangian:\n\\[L = T - V = \\frac{1}{2}m(\\frac{\\delta q}{\\delta t})^2 - mgq\\]\nThe wavelet equation is then given by:\n\\[\\delta S = L \\delta t = \\left(\\frac{1}{2}m(\\frac{\\delta q}{\\delta t})^2 - mgq\\right)\\delta t\\]\nSimplifying this equation using our convenience variables, we get:\n\\[(\\delta x)^2 = 2x(\\delta y)^2 + \\delta s \\delta y\\]\nSolving for \\(\\delta y\\), we obtain:\n\\[\\delta y = \\frac{-\\delta s \\pm \\sqrt{(\\delta s)^2 + 8x(\\delta x)^2}}{4x}\\]\nThis equation reveals the nature of the wavelet. When \\(x &gt; 0\\), the equation describes a hyperbola. For \\(x &lt; 0\\), it describes an ellipse. At the boundary \\(x = 0\\), the equation describes a parabola.\n\nThere is no planar wave solution, because a planar wave solution is both constant energy and unbounded, whereas if a particle can move infinitely far away, it must have infinite energy. If we attempt to force a planar wave solution, it would immediately break down:\n\n\n\n\n\nExercise 5 Solve the time-independent HJE for the particle in free fall. It should be \\(S = W(q) - Et\\), where \\(E\\) is a constant, and \\(W\\) is a semicubical parabola.\n\n\nExercise 6 In the same way, analyze the simple harmonic oscillator – a particle in a potential well \\(V = \\frac 12 kq^2\\). Similarly, it cannot have a planar wave solution. Plot the wavelets along each point on a planar wave, and see how it breaks down. Find two families of solutions: The case where we have a point source at \\((0, 0)\\), and the case of time-independent HJE.\n\n\n\nDeriving the time-independent Schrödinger equation\nThis section based on (Masoliver and Ros 2009; Derbes 1996; Hamill 2013, sec. 6.4).\nAssume that the potential is time-independent: \\(V(t, q) = V(q)\\). Assume that the physical system can be described by a function\n\\[\n\\Psi(t, q) = \\Psi_0(r) e^{iS(t, q)/\\hbar}\n\\]\nwhere the function \\(\\Psi\\) is typically called the “wave function”.16 We assume that the wave is a “standing wave”, so that all of space is oscillating in sync. Let \\(S(t, q) = W(q) - Et\\), so that the oscillation frequency is \\(E/\\hbar\\).17\n16 What is a wave function? It is just some abstract mathematical object, that somehow allows us to calculate everything we want to know about this system. We have no idea what it is, but it works. The same applies to the function \\(S(t, q)\\). We call it the “minimal cost” for arriving at \\((t, q)\\), but what really is a cost in physics? Particles do not really pay their paths with natural money. All this time, we have pretended that they pay some kind of cost and want to minimize the cost, but it is really just one big analogy. The same applies for the function \\(\\Psi\\). We might call it a “wave function”, but it really is just one big analogy with the waves on an ocean. There is really no wave in quantum mechanics, only a function that we pretend is a wave, because it helps us calculate results that happen to be correct.17 The most important experimental result from quantum mechanics is that energy levels are quantized. Now, a quantized energy level is something like \\(E = h, 2h, 3h, \\dots\\). There is really just one kind of thing in classical mechanics that is quantized: standing waves! If you have a string, then its standing waves must have \\(0, 1, 2, 3, \\dots\\) nodes, i.e. quantized. Thus, it is natural to try out this “standing wave” assumption.So, we can separate the variables to\n\\[\n\\Psi(t, q) = \\underbrace{e^{-i\\frac{E}{\\hbar}t}}_{\\text{oscillation in sync}} \\underbrace{\\psi(q)}_{\\text{variation over space}}, \\quad \\psi(q) = \\Psi_0(q) e^{i\\frac{W(q)}{\\hbar}t}\n\\]\nSince \\(\\Psi\\) should be a wave, it has better follow the wave equations:\n\\[\n(\\partial_t^2 - v_g^2 \\nabla^2)\\Psi = 0\n\\]\nwhere \\(v_g\\) is the group velocity of the wave. As we saw previously, \\(v_g = \\frac{E}{\\sqrt{2m(E-V)}}\\) for a particle in a potential. Then we have\n\\[\n\\begin{cases}\n\\Psi(t, q) &= e^{-i\\frac{E}{\\hbar} t} \\psi(q), \\\\\n0 &= (\\partial_t^2 - v_g^2 \\nabla^2)\\Psi, \\\\\nv_g &= \\frac{E}{\\sqrt{2m(E-V)}},\n\\end{cases}\\;\\; \\implies \\frac{\\hbar^2}{2 m} \\nabla^2 \\psi+(E-V) \\psi = 0,\n\\]\nwhich is the time-independent Schrödinger equation.\n\nExercise 7 The time-dependent Schrödinger equation states that\n\\[\ni\\hbar\\frac{\\partial}{\\partial t} \\Psi(t, q) = \\left [ - \\frac{\\hbar^2}{2m}\\nabla^2 + V(t, q)\\right ] \\Psi(t, q).\n\\]\nPlug \\(\\Psi(t, q) = \\psi_0(t, q) e^{i S(t, q)/\\hbar}\\) back to the time-dependent Schrödinger equation, and check that at the \\(\\hbar \\to 0\\) limit, we recover the HJE for \\(S(t, q)\\). This is a simple example of the WKB approximation.\n\nIf the above derivation looks mildly suspect, and leaves you with a feeling of seeing a magic trick, it is not an accident. The analogy between classical mechanics and quantum mechanics is not exact, so we cannot logically derive quantum mechanics from classical mechanics. The simple problem is that classical mechanics, even when formulated in the form of Hamilton–Jacobi wave equations, cannot reproduce interference or diffractions. Consider the simple case of a straight-edge diffraction. If you aim a light beam at a sharp edge, then on the other side, there would be alternating bright and dark bands fading into the shadow. However, if light is going by the shortest path, then there should be no such banding, and the brightness should just drop off to zero monotonically.\nThe solution is to admit that geometric optics is insufficient, that Huygens’ principle is insufficient, and we need a full theory of light wave in order to explain what happens on the smallest scales – a diffraction theory. Similarly, classical mechanics is insufficient, and the HJE is insufficient, and we need a full theory of matter wave in order to explain what happens in the atomic world – quantum mechanics.\n\n\n\nIn classical mechanics, the shadow of a hard edge has no diffractive stripes.\n\n\n\n\n\nThe optical-mechanical analogy at three levels. There is a question mark, because I’m not sure if that should be quantum field theory.\n\n\nDeriving quantum mechanics from classical mechanics is necessarily fraught with danger and luck, because in going from quantum mechanics to classical mechanics, something is irrevocably lost (and other things are irrevocably earned). It is about as difficult as going from geometric optics to wave optics. Still, several people have tried and succeeded, especially Schrödinger.\n\n… the conception of rays is thoroughly well defined only in pure abstract geometrical optics. It is wholly incapable of being applied to the fine structure of real optical phenomena, i.e. to the phenomena of diffraction. Even in extending geometrical optics some what by adding the notion of Huygens’ principle one is not able to account for the most simple phenomena of diffraction without adding some further very strange rules concerning the circumstances under which Huygens’ envelope-surface is or is not physically significant. (I mean the construction of “Fresnel’s zones”.) These rules would be wholly incomprehensible to one versed in geometrical optics alone. Furthermore it may be observed that the notions which are fundamental to real physical optics, i.e. the wave-function itself (\\(W\\) is merely the phase), the equation of wave-propagation, the wave length and frequency of the waves, do not enter at all into the above stated analogy.\n(Schrödinger 1926)\n\n\n… to establish a correspondence between waves and corpuscles such that the laws of mechanics correspond to the laws of geometrical optics. In the wave theory, however, as you will know, geometrical optics is only an approximation: this approximation has its limits of validity and particularly when interference and diffraction phenomena are involved, it is quite inadequate. This prompted the thought that classical mechanics is also only an approximation relative to a vaster wave mechanics. … A new mechanics must be developed which is to classical mechanics what wave optics is to geometrical optics. This new mechanics has since been developed, thanks mainly to the fine work done by Schrödinger.\nLouis de Broglie’s Nobel Prize Lecture (De Broglie 1929)\n\n\n\nRelativistic particle in free space\nSince the HJE is fully general for any function \\(L(t, q, v)\\) that is smooth and strictly convex in \\(v\\), we can simply write down the Lagrangian for relativistic particle in a field, and it would just work.\nLet’s first consider particle in free space. In relativity, the one thing that is coordinate-independent is the proper time of a trajectory, so it is reasonable to guess that the action is the proper time, then deduce from it the Lagrangian. This is natural if we think of \\(S\\) as the optimal cost of traveling.\nLet \\(t_0 = 0, q_0 = 0\\), then by basic relativity, the proper time for the particle to arrive at \\((t, q)\\) is\n\\[S(t, q) = \\frac{\\|q\\|}{v\\gamma} = \\sqrt{t^2- \\frac{\\|q\\|^2}{c^2}}\\]\nwhere \\(\\gamma = \\frac{1}{\\sqrt{1-v^2/c^2}}\\) is the well-known factor used everywhere in special relativity.\nTaking \\(\\nabla_q\\), and simplifying, we find the relativistic momentum to be...\n\\[p = \\nabla_q S = -\\frac{\\gamma v}{c^2}\\]\nHowever, we were expecting \\(p \\to mv\\) when \\(v \\to 0\\), so we fix this issue by multiplying the action with a constant factor \\(-mc^2\\). Multiplying a constant factor in action has no effect on the calculus of variations, so we are free to do this. Thus we find that the action of a path is the proper time of the path multiplied by \\(-mc^2\\):\n\\[S(\\text{path}) = -mc^2 \\int_{\\text{path}}d\\tau = -mc^2 \\int_{\\text{path}}\\frac 1\\gamma dt\\]\nWith this, we can derive the familiar equations by the HJE and convex duality:\n\\[\\begin{cases}\nL(q, v) = -\\frac{mc^2}{\\gamma}\\\\     \nH(q, p) = \\sqrt{(\\|p\\|c)^2 + (mc^2)^2}\n\\end{cases} \\quad\n\\begin{cases}     \np^\\ast = \\gamma mv \\\\     \nv^\\ast = \\frac{pc}{\\sqrt{\\|p\\|^2 + (mc)^2}}\n\\end{cases}\\]\nIn particular, at low \\(v\\), we have \\(L \\approx -mc^2 + \\frac 12 m\\|v\\|^2\\). So somehow, by combining the geometry of spacetime with analytical mechanics, we have discovered the \\(E = mc^2\\) formula, even though it seems like something we couldn’t have discovered from mere geometry.\nThe HJE then becomes\n\\[\\frac 1{c^2} \\left(\\partial_t S \\right)^2 - \\|\\nabla S \\|^2 = m^2 c^2\\]\nIn particular, the time-independent solutions are of the form\n\\[S = W(q)-Et, \\quad \\| \\nabla W \\| = \\frac 1c \\sqrt{E^2 - m^2 c^4 }\\]\nfor any \\(E &gt; mc^2\\). In particular, if we plug in the usual relativistic energy \\(E = \\gamma mc^2\\), we get\n\\[\\| \\nabla W \\| = \\gamma mv\\]\nwhich is similar to what we obtained for the free particle in classical mechanics, with \\(\\|\\nabla W \\| = \\sqrt{2mE}\\), just with classical momentum upgraded to relativistic momentum.\n\nExercise 8 Just as how the HJE of a classical particle can be derived as the \\(\\hbar \\to 0\\) limit of the Schrödinger equation, one can derive the HJE of the non-quantum relativistic particle as the \\(\\hbar \\to 0\\) limit of the Klein–Gordon equation. The Klein–Gordon equation is essentially the simplest possible way to combine special relativity with Schrödinger equation:\n\\[\\left( \\frac{1}{c^2} \\frac{\\partial^2}{\\partial t^2} - \\nabla^2 + \\frac{m^2 c^2}{\\hbar^2} \\right) \\psi(t, \\mathbf{x}) = 0\\]\nPlug in \\(\\psi(t, q) = \\psi_0(q, t) e^{i S(t, q) / \\hbar}\\), and check that at \\(\\hbar \\to 0\\) limit, we recover the HJE."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#periodic-motion",
    "href": "essays/posts/analytical-mechanics/index.html#periodic-motion",
    "title": "Analytical mechanics",
    "section": "Periodic motion",
    "text": "Periodic motion\n\nOscillator on a line\nConsider a simple harmonic oscillator (SHO), with Hamiltonian \\(H\\) in the \\((q, p)\\) coordinates:\n\\[H(t, q, p) = \\frac{p^2}{2m} + \\frac 12 kq^2\\]\nThe motion of the system is simply a circular motion around \\((q, p) = (0, 0)\\):\n\\[\\begin{cases}\n\\dot q = p/m \\\\\n\\dot p = - kq\n\\end{cases}\\quad  \n\\begin{cases}\n(q, p) = (q_0 \\cos(\\omega t), -m\\omega q_0 \\sin(\\omega t)) \\\\\n\\omega = \\sqrt{k/m}\n\\end{cases}\\]\nNow consider the action of an entire cycle:\n\\[S = \\oint Ldt = \\oint (pdq - Hdt)\\]\nThe \\(\\oint pdq\\) term is the area enclosed by the ellipse, so it is \\(\\pi p_0 q_0\\), and the \\(\\oint Hdt\\) term is just \\(HT = \\frac{2\\pi}{\\omega} \\frac 12 kq_0^2\\), since the system conserves energy. Now direct computation shows\n\\[\\oint pdq = HT\\]\nIn particular, since \\(T\\) does not depend on the energy of the oscillation, we can take derivative against energy, obtaining\n\\[\n\\frac{d}{dE}\\oint_{\\gamma_E} pdq = T\n\\]\nwhere \\(\\gamma_E\\) is the path traced out by the oscillator with energy \\(E\\). We show that this is generally true for 1D oscillators.\n\nTheorem 10 Given any 1D oscillator,\n\\[\n\\frac{d}{dE}\\oint_{\\gamma_E} pdq = T(E)\n\\]\nwhere \\(\\gamma_E\\) is the path traced out by the oscillator when it has energy \\(E\\), and \\(T(E)\\) is the period.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider the phase space plot of a generic 1D oscillator. Every point in its phase space must go in a cycle, returning to the start again. Thus, its phase space is like an ugly onion: It is split into cycles, which are not generally circular in shape, and generally each cycle has a different cycling period.\n\n\n\n\n\nTake a particular cycle as shown, starting and ending at \\(q_0 = q_1\\). Now consider this variation that fixes \\((t_0, q_0), (t_1, q_1)\\): move from point 0 to point 1, then go around the larger cycle to point 1, then return to point 0. The action of the varied cycle consists of three parts: \\(0\\to 1, 1 \\to 1, 1 \\to 0\\). By (modified) Hamilton’s principle, the variation of action is zero.\nNow, let \\(T(E) = t_1 - t_0\\) be the cycle period for the cycle with energy \\(E\\), then from our above argument, we have\n\\[\\oint_E pdq - E T(E) = \\oint_{E + \\delta E} pdq - (E + \\delta E) T(E) + O(\\delta E^2)\\]\nwhere the \\(O(\\delta E^2)\\) term deals with the \\(0\\to 1, 1\\to 0\\) parts of \\(\\int -Hdt\\). Thus we obtain\n\\[\\frac{d}{dE} \\oint_E pdq = T(E)\\]\n\n\n\n\n\n\n\n\n\nWorked example: pendulum in gravity\n\n\n\n\n\nThe 1D pendulum in gravity, with length \\(l\\) and mass \\(m\\), has Lagrangian \\(L = \\frac 12 m(l\\dot q)^2 + mgl\\cos q\\), momentum \\(p = ml^2 \\dot q\\), and Hamiltonian\n\\[H = \\frac{p^2}{2ml^2} - mgl \\cos(q)\\]\nWe know that the pendulum is not a SHO. Indeed, the cycle period \\(T(E)\\) strictly increases with energy \\(E\\) of the cycle, and it diverges as the pendulum swing approaches the highest point: \\(\\lim_{E \\to mgl} T(E) = +\\infty\\).\nConsider the cycle with maximum swing angle \\(\\theta\\). The cycle encloses an oval-shaped region in phase space, with equation \\(p^2 = 2m^2 gl^3(\\cos q - \\cos\\theta)\\). Consequently, we have the somewhat mysterious result:\n\\[\\int_0^{E(\\theta)} T(E) dE = 4\\sqrt{2m^2 gl^3} \\int_0^\\theta \\sqrt{\\cos q - \\cos\\theta} dq\\]\nwhere \\(E(\\theta) = -mgl \\cos\\theta\\) is the energy of the system when it has maximum swing angle \\(\\theta\\).\nWhen \\(\\theta\\) is small, the integral is approximately\n\\[\\int_0^\\theta \\sqrt{\\frac 12 (\\theta^2 - q^2)}dq = \\frac{\\pi\\theta^2}{4\\sqrt 2}\\]\nwhich does correspond to \\(T(E) \\approx 2\\pi\\sqrt{\\frac lg}\\), and \\(\\delta E = mgl(1 - \\cos(\\theta)) \\approx \\frac 12 mgl \\theta^2\\).\nWhen \\(\\theta = \\pi\\), the integral can be exactly evaluated:\n\\[\\int_0^{mgl} T(E) dE = 16\\sqrt{m^2 gl^3}\\]\nWe are unable to interpret this strange but satisfying equality.\nTaking \\(\\partial_\\theta\\) under the integral sign, we get\n\\[T(E) = \\sqrt{\\frac{8l}g} \\int_0^\\theta \\frac{dq}{\\sqrt{\\cos q - \\cos \\theta}}\\]\nwhich may be directly verified.\n\n\n\n\n\nAction-angle variables\nNow, it would be great if we could \"unwind\" the rotatory dynamics by a time-independent canonical transform to some \\((Q, P)\\), by where \\(P\\) is constant along the cycles, and \\(Q\\) is increasing. That is, we want \\(P\\) to be the analog of \"amplitude\", and \\(Q\\) to be the analog of \"phase angle\".\nSince the transform is time-independent and canonical, the Hamiltonian \\(H\\) is unmodified, so \\(H\\) is a function of \\(P\\) only, not \\(Q\\) (since \\(H\\) is a conserved quantity of motion). Then, since the transform is canonical, Hamilton’s equations of motion read \\(\\dot Q = \\partial_P H(P)\\). Consequently, the Hamiltonian equations of motion would become\n\\[\\begin{cases} P(t) = P(0) \\\\ Q(t) = Q(0) + H'(P) t \\end{cases}\\]\nas simple as it could be! It remains to find such a canonical transform.\nWe are already mostly there: we know that \\(P\\) is constant along the cycles, and \\(Q\\) increasing along the cycles. It remains to find the right scaling, so that the transform is canonical, that is, the coordinates preserve area: \\(dP \\wedge dQ = dp \\wedge dq\\).\nDefine \\(P = \\frac{1}{2\\pi} \\oint pdq\\). Here the \\(2\\pi\\) factor is not essential, since we could always do a point transform, scale down \\(Q\\) by \\(2\\pi\\), and scale up \\(P\\) by \\(2\\pi\\). However, the factor will make many formulas look cleaner.\nFrom the proof of Theorem 10, we know that increasing the energy of the cycle by \\(\\delta H\\) would increase the cycle area by \\(T(H)\\delta H\\), and the cycle area is \\(2\\pi P\\), thus\n\\[\\delta(2\\pi P) = T(H) \\delta H \\implies \\frac{2\\pi}{T(H)} = H'(P)\\]\nso we find that the equations of motion are:\n\\[\\begin{cases} P(t) = P(0) \\\\ Q(t) = Q(0) + 2\\pi \\frac{t}{T(H)} \\end{cases}\\]\nThis allows us to graphically construct the \\((Q, P)\\) coordinates on phase space:\n\nDraw the cycles in phase space.\nSelect a \"line of longitude\" arbitrarily as \\(Q = 0\\) line.\nFollow the trajectory of each point on the line of longitude, and mark down a new line of longitude at equal phases-angles. So for example, if you are on the cycle of energy \\(E\\), you would start the ride, and after \\(T(E)/3\\) has passed, note down its phase-angle as \\(2\\pi/3\\). The set of all such points at every cycle is the line of longitude with \\(Q = 2\\pi/3\\).\n\n\n\n\nConstructing a canonical coordinate system over the phase space of the 1D oscillator.\n\n\nNow, we can graphically see that this construction really preserves areas:\n\nTake an infinitesimal parallelogram at \\((Q, P)\\) with sides \\(\\delta Q, \\delta P\\), such that \\(\\delta Q = \\frac{2\\pi}{N}\\) for some infinite integer \\(N\\).\nEvolve the system in discrete steps of \\(\\frac{T(P)}{N}\\), and follow the parallelogram along.\nNote that the parallelogram would tile the thin ring between \\(P\\) and \\(P+ \\delta P\\). The thin ring has area \\(\\delta \\oint pdq = 2\\pi \\delta P\\), so each parallelogram has area \\(2\\pi \\delta P/N = \\delta P \\delta Q\\).\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is not as trivial as it seems. The “area” in phase space was defined by \\(\\oint pdq\\), but we did not say that \\(p, q\\) are perpendicular in any sense. Thus, it is not obvious that \\(\\delta P \\delta Q\\) would be the area of the parallelogram. Indeed, we should not say that \\(p, q\\) are perpendicular, because angles do not exist and should not exist in phase space.\n\n\nThe entire construction of \\((Q, P)\\) from \\((q, p)\\) is often done in one divinely-inspired move by a generating function.\n\n\nAdiabaticity\nThis section based on (Duncan and Janssen 2019, chap. 5; de Oliveira 2022).\nThe Lorentz pendulum is a famous example that connects the classical and the quantum world. In the early 1900s, as physicists were trying to explain various phenomena, such as the black body radiation spectrum, the atomic spectra, and such, they came to the quantum hypothesis. Consider a classical system undergoing periodic motion – such as the electrons cycling around a proton in the hydrogen atom. Whereas classically, its \\(\\oint pdq\\) may be of any value, the quantum hypothesis states that it can only take values in\n\\[\n\\oint pdq = nh, \\quad n = 1, 2, 3, \\dots\n\\]\nfor a certain constant of nature \\(h\\) – later given the name of Planck’s constant. For example, if we have a simple harmonic oscillator, then we have\n\\[\nE\\nu = n h\n\\]\nwhere \\(\\nu = 1/T\\) is the frequency of the oscillator.\nAt the 1911 Solvay Conference, where the great physicists grappled with the new quantum phenomena,18 Einstein gave a presentation on the quantum hypothesis. At the end of the presentation, Lorentz asked a question about the pendulum. The conversation went as follows:\n18 The entire conference is summarized in (Straumann 2011).\nMr. Lorentz recalls a conversation he had with Mr. Einstein some time ago, in which they discussed a simple pendulum that could be shortened by holding the string between two fingers and sliding them downwards. Suppose that initially, the pendulum has exactly one energy element corresponding to the frequency of its oscillations. It then seems that at the end of the experiment, its energy will be less than the element corresponding to the new frequency.\nMr. Einstein – If the length of the pendulum is changed infinitely slowly, the energy of the oscillation remains equal to \\(h\\nu\\), if it was initially equal to \\(h\\nu\\) ; it varies proportionally to the frequency. The same is true for a resistance-free oscillating electrical circuit, and also for free radiation.\nMr. Lorentz – This result is very curious and removes the difficulty. In general, the hypothesis of energy elements gives rise to interesting problems in all cases where one can change the frequency of vibrations at will.\n(Instituts Solvay et al. 1912, 450)\n\n\n\n\nThe adiabatic pendulum of Lorentz. Figure from (Fowler 2020)\n\n\nWhy is this interesting? The quantum hypothesis states that for quantum oscillators, \\(E/\\nu = nh\\), where \\(n\\) can only take values in the natural numbers. The Lorentz pendulum seems to show that the quantity \\(E/\\nu\\) can change continuously. Thus, for example, we might start at a quantized value of \\(E/\\nu = 100h\\), and end up at \\(100.5h\\), invalidating the quantum hypothesis for macroscopic systems. And if macroscopic systems do not follow the quantum hypothesis, then as the macroscopic system becomes microscopic, it seems the quantum hypothesis would be invalidated as well.\nEinstein replied that \\(E/\\nu\\) of the oscillator is conserved if the length of the pendulum is changed infinitely slowly. Surprising, but it saves the quantum hypothesis.\n\nThe adiabatic theorem\nConsider a pendulum with a string length \\(\\lambda\\) that is slowly changed over time. How slow? Slow enough that the system completes many cycles before \\(\\lambda\\) makes any appreciable change. That is,\n\\[\\dot \\lambda \\ll \\frac{\\lambda}{T(\\lambda)}\\]\n\n\n\n\n\n\nParametric resonance\n\n\n\nIt is vitally important that the pulling on the pendulum is not only slow, but also “smeared”, meaning that \\(\\dot\\lambda\\) is equal over the entirety of a single oscillation. If it is not smeared, then we can break the theory. Consider for example a (spherical) child on a (frictionless) swing (in a vacuum). It is well-known that the child can, by swinging the legs in sync with the swing, get as high as possible. This is called parametric resonance.\nCompared to adiabatic change, parametric resonance differs in that it is discriminating about the states. When you adiabatically pull on the string of a pendulum, your rate of pulling is the same over the entire cycle of a pendulum swing. When you resonantly pull on the string of a pendulum, your rate of pulling differs over the cycle of a pendulum swing. In the language of thermodynamics, adiabaticity means treating all microstates equally, without discrimination.\n\n\nLet the angular frequency of oscillation be \\(\\omega = \\sqrt{\\frac g\\lambda}\\), then \\(E/\\omega\\) is constant over time.\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy basic physics, the force in the string is\n\\[\nF = mg \\cos \\theta + m\\dot \\theta^2 / \\lambda \\approx mg - \\underbrace{\\frac 12 mg \\theta^2}_{\\text{$V/\\lambda$}} + \\underbrace{m\\lambda \\dot\\theta^2}_{\\text{$2T/\\lambda$}}\n\\]\nBecause the system is undergoing simple harmonic oscillation, the time-average of \\(V\\) and \\(T\\) are both \\(\\frac 12 E\\), where \\(E\\) is the oscillation energy. Therefore, the time-average force is \\(\\bar F = mg + \\frac 12 E/\\lambda\\).\nThus, if we shorten the string by \\(\\delta \\lambda\\), we would inject an energy of \\(\\delta E = \\bar F (-\\delta\\lambda) -( mg\\delta\\lambda)\\) (we subtract away the gravitational energy at the lowest point, as it is irrelevant). This gives us\n\\[\n\\delta E + E \\delta\\lambda / 2\\lambda = 0 \\implies E\\lambda^{1/2} = Const\n\\]\nSince the angular frequency of oscillation is \\(\\omega = \\sqrt{\\frac g\\lambda}\\), we have the result.\n\n\n\n\nExercise 9 Perform the same analysis on a vibrating string. The string is fixed at both ends, and the length of the string is slowly changed. The string is in a standing wave pattern, and the length of the string is changed so slowly that the string completes many cycles before the length changes appreciably. What is the adiabatic invariant in this case? The answer is in (Rayleigh 1902).\n\n\n\n\n\n\n\nAdiabatic invariance to all orders\n\n\n\n\n\nThe adiabatic invariance is actually much stronger than what we have shown. It is not just that the enclosed action \\(I\\) is conserved up to \\(O(\\dot\\lambda)\\), but that it is conserved to all orders in \\(O(\\dot \\lambda^n)\\) (Lenard 1959). Under stronger restrictions, it is even conserved to order \\(O(e^{c/\\dot \\lambda})\\). See (Henrard 1993, sec. 4) for more theorems in this style.\n\n\n\nGeneralizing from this experience, for an arbitrary 1D oscillator with Hamiltonian \\(H(q, p; \\lambda)\\), the phase space trajectory is a closed wobbly cycle. As we vary \\(\\lambda\\) adiabatically by external force, the cycle changes shape, both because the system has received energy from the external force, and because the Hamiltonian of the system has changed. Nevertheless, the area enclosed within should remain constant.\n\n\n\n\n\n\nProof\n\n\n\n\n\nIn general, force is the energetic cost of changing a parameter. That is, \\(F = (\\partial_{\\lambda} H)_{p, q}\\). Here, we are using a notation commonly used in thermodynamics: for partial derivatives, we write in the subscript the variables being held constant.\nLet us adiabatically vary \\(\\lambda\\). After many cycles have passed, the oscillator is orbiting around the cycle defined by \\(H(q, p; \\lambda) = E\\). After many more cycles have passed, we have varied \\(\\lambda\\) by \\(\\delta \\lambda\\), and the oscillator would be orbiting around the cycle defined by \\(H(q, p; \\lambda + \\delta \\lambda) = E + \\delta E\\), where \\(\\delta E = \\bar F \\delta \\lambda\\) is the increase in oscillator energy due to the external force varying \\(\\lambda\\).\n\n\n\n\n\nAs pictured, for each point \\((q_0, p_0)\\) on the first cycle, we move by \\(\\delta p\\), to end up with point \\((q_0, p_0 + \\delta p)\\) on the second cycle. The cycle area would change by\n\\[\\delta(\\text{area}) = \\oint (\\delta p)dq = \\int_0^T (\\delta p )\\dot q dt + O(\\delta T \\delta p)\\]\nwhere \\(T\\) is the cycle time of the first cycle, and \\(T + \\delta t\\) is the cycle time of the second cycle.\nExpanding and simplifying,\n\\[\nH(q_0, p_0 + \\delta p ; \\lambda + \\delta \\lambda) = E + \\underbrace{\\delta E}_{\\text{$= \\bar F \\delta \\lambda$}} \\implies\n(\\partial_p H)_{\\lambda, q}\\delta p = [\\overline{(\\partial_\\lambda H)_{q, p}} - (\\partial_\\lambda H)_{q, p}] \\delta\\lambda\n\\]\nNow, integrate over a single cycle, and using the Hamilton equation of motion \\(\\dot{q} = (\\partial_p H)_{\\lambda, q}\\),\n\\[\\delta(\\text{area}) =  \\delta\\lambda \\int_0^T [\\overline{(\\partial_\\lambda H)_{q, p}} - (\\partial_\\lambda H)_{q, p}] dt + O(\\delta^2)\\]\nBy definition, \\(\\overline{(\\partial_\\lambda H)_{q, p}} = \\frac 1T \\int_0^T (\\partial_\\lambda H)_{q, p}dt\\), thus we have \\(\\delta(\\text{area}) = O(\\delta^2)\\). Thus, integrating over the entirety of the adiabatic process, \\(\\Delta(\\text{area}) = O(\\delta)\\), which converges to zero as the adiabatic process becomes infinitely slow.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor a multidimensional oscillating system, we have three possibilities.\nIf the dimensions separate, like an \\(n\\)-dimensional oscillator \\(H = \\frac{p_1^2 + \\dots + p_n^2}{2m} + \\frac 12 k (q_1^2 + \\dots + q_n^2)\\). In this case, the adiabatic theorem still applies along each dimension, with one adiabatic invariant per dimension.\nIf all dimensions mix together, like a tank of hot, entangled gas. In this case, the adiabatic theorem states that if we start with the microcanonical ensemble, then the phase space volume enclosed by the surface of \\(H = E\\) remains constant as we adiabatically vary \\(\\lambda\\). The volume is named the Gibbs invariant. (de Oliveira 2022)\nIf the dimensions neither separate nor mix together, but have some kind of complicated dynamics, then what adiabaticity means in that case is still a current area of research.\n\n\n\n\nConnection to thermodynamics\nConsider a ball in a piston, bouncing elastically off its two ends. We hold one end of the piston constant, and slowly move the other. Assume the ball only moves in the \\(x\\)-direction for simplicity. Let the ball have speed \\(v\\), and the walls of the piston have separation \\(L\\).\nThe phase space diagram of the system is a rectangle with \\(\\Delta q = L, \\Delta p = (mv) - (-mv) = 2mv\\), enclosing an area of \\(I = 2mvL\\). If we slowly move the wall of the piston, then the action is conserved, giving us\n\\[\n2mv_0 L_0 = 2mvL \\implies v = \\frac{L_0}{L} v_0\n\\]\nWe can formulate this into the language of thermodynamics. First, expand our system to three dimensions – from a piston to a box. Since the motion of the ball in the \\(x, y, z\\) directions are independent, we can treat them separately. We also assume equipartition of energies, that is, the energy of the ball is equally distributed among the three dimensions, so \\(v_{x, 0} = v_{y, 0} = v_{z, 0}\\). The conservation of action then states that\n\\[\nv_i = \\frac{L_{i, 0}}{L_i} v_{i, 0} \\quad \\text{for } i = x, y, z\n\\]\nImagine the ball is a gas molecule, and the piston is a wall of a container. Let \\(E = \\sum_{i = x, y, z} \\frac 12 mv_i^2\\) be the energy of the gas molecule, and \\(V = \\prod_{i = x, y, z}L_i\\) be the volume of the gas. We then have\n\\[\nV = V_0 \\prod_i \\frac{L_i}{L_{i, 0}} \\approx V_0 \\left(1 + \\sum_i \\frac{\\delta L_{i}}{L_{i, 0}}\\right), \\quad\n\\begin{aligned}\nE &= \\frac 12 m \\sum_i \\left(\\frac{L_{i,0}}{L_i}\\right)^2 v_{i, 0}^2\\\\\n&= \\frac 32 m v_0^2 \\frac 13 \\sum_i \\left(\\frac{L_{i,0}}{L_i}\\right)^2 \\\\\n&\\approx E_0 \\left(1 - \\frac 23 \\sum_i \\frac{\\delta L_{i}}{L_{i, 0}}\\right)\n\\end{aligned}\n\\]\nThese imply that \\(E^{3/2}V \\approx E_0^{3/2}V_0\\). Now, this is precisely what happens if you compress an ideal gas adiabatically. This is one connection between the concept of “adiabatic” in classical mechanics and thermodynamics.\n\nExercise 10 Prove that \\(I\\) is conserved: calculate the average force \\(\\bar F\\) on the piston, then calculate the work done by the piston.\n\n\n\nMore examples\nGiven a Lorentz pendulum and a schedule for varying its arm length, we can plot the angle \\(x\\) as a function of time, and see directly that \\(I = T (\\frac 12 m \\dot x_{max}^2)\\) is conserved over many swings of the pendulum.\n\n\n\nA Lorentz pendulum with length increasing linearly with time. As its length increases, \\(\\dot x_{max}\\) decreases while \\(T\\) increases, keeping \\(I\\) conserved. Annotated from (Sánchez-Soto and Zoido 2013, fig. 1)\n\n\nFor a more rigorous proof of adiabatic invariance at the level of first-year graduate student, see (Wells and Siklos 2007). (Henrard 1993) is a good comprehensive review of adiabatic invariance in classical mechanics, and points to the literature on a lot of applications in celestial mechanics, magnetism, and the geometry of phase space plots. In textbooks on classical statistical mechanics, the adiabatic invariance theorem is used to derive many results. (Fernandez-Pineda, Diez de los Rios, and Mengual 1982) gives some worked-out examples, such as the ideal gas and the photon gas."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#old-quantum-mechanics",
    "href": "essays/posts/analytical-mechanics/index.html#old-quantum-mechanics",
    "title": "Analytical mechanics",
    "section": "Old quantum mechanics",
    "text": "Old quantum mechanics\n\nSommerfeld, in the new edition of his book [Atomic structure and spectral lines], has introduced the adiabatic hypothesis through a couple of very elegant changes and footnotes, in such a way that my participation in that can rather appear reduced to — a plagiarism. Lorentz and Einstein have founded the subject, I have given it a name and Burgers has put everything in order. I was first very, very depressed. I know that I have never discovered anything, and quite surely never will discover anything, that I can love so fervently as this line of thought which I found out with so large joy.\nEhrenfest’s letter to Bohr, 8 May 1922. Emphasis in original. Quoted in (Navarro and Pérez 2006)\n\n\nThe adiabatic hypothesis\nThere are two ways to interpret the word “adiabatic”. In thermodynamics, “adiabatic” means “no exchange of heat”. In mechanics, “adiabatic” means “gradual”.19 It is a winding story of how one word “adiabatic” came to mean two things that fortuitously are connected after all, the details of which are given in (Jammer 1966, chap. 3; Laidler 1994). The story is almost as interesting as that of the word “entropy”, which also has two meanings that are fortuitously connected after all.\n19 Because “adiabatic” has two meanings in English, in Chinese, it also has two different translations based on the two meanings. There is 绝热, which means “no exchange of heat”, and 浸渐, which means “gradually, like moisture soaking into something”.Rankine first coined the term “adiabatic” in 1858, to denote a process in which no heat is exchanged with the surroundings. Later, Boltzmann and Clausius tried to explain the second law of thermodynamics mechanically, by using purely mechanical models of the microscopic world. In this sense, they defined an “adiabatic” mechanical process to be one where a certain variable is slowly changed (for example, if we have a box of little bouncing balls, and we slowly move its walls), because an adiabatic thermodynamic process in their view is actually an adiabatic mechanical process.\n“Adiabatic motion” in mechanics was introduced by Helmholtz and Hertz, to denote mechanical processes where external forces act upon a system, but only on a few parameters, with no action on the underlying details. For example, think back to the case of bouncing balls in a box. The external force only moves the walls of the box on average, with no attempt to move the walls to manipulate the precise location of the ball. They used the thermodynamic terminology, because the work done on the system during an adiabatic motion results exclusively in changes in its energy. (Jammer 1966, chap. 3)\nIn 1900, Pyotr Lebedev experimentally proved that radiation pressure exists, and follows Maxwell’s theory of electromagnetism. Inspired by this, Lord Rayleigh (Rayleigh 1902) generalized the concept of radiation pressure to all kinds of vibrations, starting with the humble pendulum. Since his goal was to understand what happens when you adiabatically compress a photon gas, that is, Wien’s displacement law,20 he studied the effect of adiabatic motion on some simple mechanical systems undergoing wave motion, such as the Lorentz pendulum, a vibrating cord, a piston of gas with a standing acoustic ave, etc.\n20 Since he didn’t actually know what a photon was, it might be better to say that he was studying what would happen when you compress a hot chamber of light. Though Wien’s displacement law is nowadays proved straight from quantum mechanics, back when ien discovered it in 1893, he used a thermodynamic argument using the adiabatic compression of a photon gas. For a brief presentation of how Rayleigh did it, see (Ter Haar 1966).Like Lord Kelvin, Paul Ehrenfest was also trying to explain Wien’s displacement law. He was puzzled by the fact that while Wien’s displacement law was derived without the quantum hypothesis, yet somehow, it remains true. Suppose we start with a box of light, then it follows a certain black body radiation, which can only be derived by the quantum hypothesis. Now suppose we adiabatically compress the box of light. Though the compression process is studied classically, without the quantum hypothesis, the final state of the light is still the black body radiation. So, it seems that if we start with a system following the quantum hypothesis, then any adiabatic classical process would give us a system that still follows the quantum hypothesis. This is his adiabatic hypothesis, for which he is famous.\nDuring the 1910s, Ehrenfest published a series of papers to subsume the many ad-hoc quantum rules under the framework of the adiabatic hypothesis. His idea is as follows: If, in a periodic system described by classical mechanics (such as an electron orbiting a proton), a certain quantity \\(I\\) has units of joule-second, and is conserved as it undergoes adiabatic motion, then this quantity should be quantized, and only this quantity can be quantized. Only \\(I\\) can be quantized, for the reason discussed by Lorentz and Einstein at the Solvay conference.\nFurther, \\(I\\) should be quantized, because otherwise, why else should \\(I\\) be adiabatically conserved? The adiabaticity of \\(I\\) in the classical world, on the surface, is a shadow of the discreteness of \\(I\\) in the quantum world, deep down. Because \\(I\\) is quantized, if we vary the system slowly enough, the system would have no reason to make a big jump from \\(I = nh\\) to \\(I = (n+1)h\\). This discreteness in the quantum world traps \\(I\\) in its starting position, and this is why \\(I\\) appears adiabatic in the classical world.\nThus, quantized quantities are precisely adiabatic invariants. We can write down \\(I = nh\\), where \\(n \\in \\mathbb{N}\\), and proceed to calculate the properties of the system, such as its energy levels, its absorption and emission spectra, etc.\nIn short, this is Ehrenfest’s recipe for doing “old quantum mechanics”:\n\nFind a system that has a conserved quantity \\(I\\) under adiabatic motion.\n\nYou can do this by solving the equations of motion, then integrate \\(\\oint pdq\\).\nAlternatively, you can start with a harmonic oscillator, and adiabatically deform the system until it becomes the system you want. Ehrenfest called such systems “adiabatically related to the harmonic oscillator” (Jammer 1966, 99).\n\nProclaim that \\(I = nh\\) for some constant \\(h\\).\nCalculate the properties of the system.\n\n\n\nThe Bohr–Sommerfeld model\nAs an example, consider the pinnacle of old quantum theory, the Bohr–Sommerfeld model of the hydrogen atom. First, treat the hydrogen atom as if it is a relativistic solar system, with the electron as a planet, moving at relativistic speeds21 under the inverse-square force \\(F \\propto \\frac{1}{r^2}\\). Next, assume that the adiabatic invariant is quantized:\n21 Because the electron moves faster closer to the atom than further away, it has more mass close to the atom than further away. This allows the orbit to precess in a way that is not predicted by Newtonian mechanics. Generally, any perturbation of the inverse-square force will cause the orbit to precess. Even special relativity would predict some precession for the perihelion of Mercury, though it predicts a value of \\(7''/\\text{year}\\) only \\(1/6\\) of the correct value (McDonald 2023; Goldstein, Poole, and Safko 2008, exercise 7.27, page 332). Only general relativity predicts the correct value. Assuming that only special relativity contributes, the perihelion of Mercury would take \\(7.5 \\times 10^7\\) Mercury-years to go around the sun once.\nAs the electron moves a lot faster than Mercury, it takes much shorter time, but still it takes \\(4\\times 10^4\\) electron-years to go around the nucleus once, as noted in Bohr’s Nobel lecture (Bohr 1923).\\[\\int_0^T p_r \\,dq_r = n' h\\]\nwhere \\(q_r\\) is the radial position of the electron, \\(p_r\\) is the momentum conjugate to it, \\(n'\\) is the “auxiliary quantum number”, and \\(T\\) is the period of the electron’s orbit. This, when combined with the hypothesis of quantized angular momentum \\(mvr = nh\\) (here, \\(n\\) is the “principal quantum number”), would predict the emission spectrum of the hydrogen atom.22\n22 Because the equation of motion for the electron separates into a radial component \\(q_r, p_r\\) and an angular component \\(q_\\theta, p_\\theta\\), we can apply the multidimensional adiabatic theorem and find that both \\(\\int_0^T p_r \\,dq_r\\) and \\(\\int_0^T p_\\theta \\,dq_\\theta\\) are adiabatic invariants.Sommerfeld went further, piling epicycles upon epicycles, to explain the fine structure of atomic spectra. All those has been swept away by the new quantum theory like how the new astronomy of Kepler swept away the epicycles of Ptolemy. But the adiabatic hypothesis remains to this day a fruitful meeting point between the classical, the quantum, and the thermodynamic world.\nHave some cool pictures, because I like cool pictures.\n\n\n\n\n\nSelections from (Sommerfeld 1923).\n\n\n\n\n\n\n\nQuantization of the adiabatic invariant. Page 197.\n\n\n\n\n\n\n\n\n\nThis looks familiar… it looks just like the phase space plot of a ball bouncing in a piston. Page 199.\n\n\n\n\n\n\n\n\n\nOrbits of the hydrogen atom with the same principal quantum number \\(n\\), but different auxiliary quantum number \\(n'\\). Page 240.\n\n\n\n\n\n\n\n\n\n5 electrons with the same principal and auxiliary quantum numbers, interacting by the pentagonal dance. Page 502.\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nSelections from (Kramers 1923).\n\n\n\n\n\n\n\nOrbits of the Bohr–Sommerfeld model of the hydrogen atom, labelled by their principal quantum numbers and auxiliary quantum numbers. The jumps from 31, 32, 33 to 22 all create the Balmer spectral line Hα, but they differ at the fine structure. Figure 27.\n\n\n\n\n\n\n\n\n\nOrbits of the Bohr–Sommerfeld model of the Radium atom. First end plate. I find its fonts classy.\n\n\n\n\n\n\n\n\n\nA modern redrawing. (Holtebekk and Linder 2023)\n\n\n\n\n\n\nFigure 2\n\n\n\n\nWhat we are nowadays hearing of the language of spectra is a true “ music of the spheres ” within the atom, chords of integral relationships, an order and harmony that becomes ever more perfect in spite of the manifold variety. The theory of spectral lines will bear the name of Bohr for all time. But yet another name will be permanently associated with it, that of Planck. All integral laws of spectral lines and of atomic theory spring originally from the quantum theory. It is the mysterious organon on which Nature plays her music of the spectra, and according to the rhythm of which she regulates the structure of the atoms and nuclei.\nPreface to the first edition of Atomic structure and spectral lines, Arnold Sommerfeld, 1919. (Sommerfeld 1923)\n\n\n\nThe Einstein—Brillouin–Keller quantization\nThis topic is quite obscure and hard to find a simple reference for, yet I found it is absolutely necessary to treat this correctly, if only to soothe my mathematical conscience. I wrote it based on (Stone 2005; Duncan and Janssen 2019, chap. 5).\nLet’s take a more careful look at the Bohr–Sommerfeld model of a hydrogen atom. The electron orbits a proton, and the equation of motion is spherically symmetric. Therefore, we can write it in spherical coordinates \\((r, \\theta, \\psi)\\), where \\(r\\) is the radius, \\(\\theta\\) is the co-latitude, and \\(\\psi\\) is the longitude. The Bohr–Sommerfeld quantization then states that\n\\[\n\\begin{aligned}\n& \\oint p_r d r = n_r h \\\\\n& \\oint p_{\\theta} d \\theta=n_\\theta h, \\\\\n& \\oint p_\\psi d \\psi=n_\\psi h .\n\\end{aligned}\n\\]\nfor some positive integers \\(n_r, n_\\theta, n_\\psi\\). However, we notice something deeply unsatisfying: How does the atom “know” which way is the sphere pointing? To define the spherical coordinates, we need to define the direction of the north and south pole. The Bohr–Sommerfeld quantization condition is thus creating an artificial direction in space where none should exist. Worse, if we solve the equations, we would find that this artificial direction has physically measurable consequences.\nSommerfeld evaded the difficulty by arguing that as long as there is even a hair of magnetic field \\(B\\) pointing in some direction \\(\\hat z\\), we can pick \\(\\hat z\\) as the north pole direction, and that since the field strength is nowhere zero, the problem will never occur in practice.\nWe see the difficulty inherent in the old quantum theory. Suppose we have a hydrogen atom suspended in free space, then the tiniest change in the external magnetic field would create a large (for the atom, at least) change in its north-pole direction. The Stern–Gerlach experiment, performed in 1922, experimentally showed that the external field can determine the direction of \\(\\hat z\\), and thus the quantization of angular momentum. To see something in classical mechanics so jumpy is disconcerting, and certainly disturbed me greatly when I first understood the Stern–Gerlach experiment. In 4 years, Schrödinger would have proposed his equation, Heisenberg his matrix mechanics, and old quantum theory washed away by the new quantum mechanics.\nEinstein, in his only paper on the old quantum mechanics,23 elegantly resolved the problem by using the Poincaré–Cartan integral invariant to construct quantization equations that do not depend on our arbitrary choices of coordinate systems.\n23 (Einstein 1917), reprinted and translated in (Einstein 1997, vol. 6, pages 434–444).24 In the jargon of topology, this is a linear function on \\(T^d\\), the homology group of the torus. Here, \\(d\\) is the dimension of the torus.As we saw with Exercise 2, a contractable loop on the torus integrates to zero. Thus, we can attach and detach contractible loops at will, deform the cycle arbitrarily, and still get the same number. That is, the integral is determined by the topology of the cycle, not the exact shape of the cycle.24 This gives us the Einstein quantization:\n\\[\n\\oint_{\\gamma_i} \\left\\langle p, dq\\right\\rangle = n_i h\n\\]\nwhere \\(\\gamma_1, \\dots, \\gamma_d\\) range over the \\(d\\) topologically distinct loops around the \\(n\\)-dimensional torus in phase space, and \\(n_1, \\dots, n_d\\) are positive integers. This was later modified by Brillouin and Keller to take account of singularities in the trajectory, such as a hard wall reflector, giving us the EBK quantization.\n\n\n\nA particle moving in a central potential. Its angular momentum \\(p_\\theta\\) is conserved, so we do not plot it. Plotting its other three phase-space variables \\((r, \\theta, p_r)\\), we see that its orbit makes a spiralling and rotating pattern on a torus. There are two fundamental ways to cycle around the torus, and each has a corresponding Poincaré–Cartan integral that is independent of the precise shape of the cycle. The two integrals are quantized according to the EBK quantization.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe EBK quantization resembles the Bohr–Sommerfeld quantization condition, but it is quite different. In the Bohr–Sommerfeld quantization condition, the integral \\(\\oint p_i dq_i\\) integrates over a cycle of physically real trajectory in phase space. In EBK quantization, the integral \\(\\oint_{\\gamma_i} \\left\\langle p, dq\\right\\rangle\\) integrates over a geometric cycle in phase space with no physical reality at all.\nBoth conditions happen to be the same when we have a 1D oscillator, but that is because the only way to physically go around the torus is the only way to geometrically go around the torus. It is a misleading coincidence.\nIn general, the physically real trajectories look like a braiding on the torus, and are not even closed cycles. They definitely do not go around the torus exactly once in exactly one direction.\n\n\nThere is only one problem left: What happens when we don’t have a torus? It is all well and good when we can construct a torus in phase space with as many dimensions as possible. However, when we have a classically chaotic system, such as the three body problem, we cannot have something this simple.\nConsider the simplest case of the three-body problem: We put two suns in a circular orbit around each other, and a speck of dust orbiting them. The two suns’ orbit is perfectly predictable, so we consider only the trajectory of the dust. We also assume the dust only moves in the plane of the suns. Thus, the system has only 2 dimensions of configuration, or 4 dimensions of phase.\nAs the energy of the system is conserved, the motion of the dust is restricted to the constant energy surface \\(H(q_1, q_2, p_1 p_2) = E\\), which is a 3-dimensional blob within the 4-dimensional phase space. However, in general, this is all we can say about it. The motion of the dust is chaotic, and would densely crisscross over a 3-dimensional subset of the blob.\nWithout a torus in phase space, we cannot find trajectories around the torus, and so the integral \\(\\oint_{\\gamma}\\left\\langle p, dq\\right\\rangle\\) is undefined. Einstein presciently pointed this out in his 1916 paper:\n\nIf there exist fewer than \\(d\\) [constants of motion], as is the case, for example, according to Poincaré in the three-body problem, then the \\(p_i\\) are not expressible by the \\(q_i\\) and the quantum condition of Sommerfeld-Epstein fails also in the slightly generalized form that has been given here.\n\nThis failure of the EBK quantization on classically chaotic systems was forgotten for many years, but eventually rediscovered (Stone 2005). When it did, it became a seed of quantum chaos, which I hope to explain clearly some day. In the mean time, I leave you with some beautiful pictures of quantum chaos instead.\n\n\n\nThe nonchaotic quantum circular billiard and the chaotic quantum cardioid billiard. Figure from (Backer 2007)\n\n\n\n\n\nIn a chaotic quantum “stadium” billiard, the standing wave functions look dark along certain lines. You can think of them as taking a classical billiard table, and using a knife to cut along the classical trajectories. The scars left behind are the quantum scars of departed classicality. Figure from (King 2014)"
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#sec-geometric-optics",
    "href": "essays/posts/analytical-mechanics/index.html#sec-geometric-optics",
    "title": "Analytical mechanics",
    "section": "Hamiltonian optics",
    "text": "Hamiltonian optics\n\nIsotropic case\nWe are going back to the roots, as geometric optics was what inspired Hamilton to develop his theory of Hamiltonian mechanics.\nWhen Hamilton developed his Hamiltonian approach, it was to study geometric optics, which can be derived from Fermat’s principle: light paths have stationary travel time. In other words, the action of a path is\n\\[S(\\text{path}) = \\int_{\\text{path}} dt\\]\nThus, \\(L = 1\\)...? Well, here we see the problem: in geometric optics, if you fix the starting and ending point as \\((t_0, q_0), (t, q)\\), then any path between them takes exactly \\(t-t_0\\) time, and there is nothing to vary. Consequently, we need to remove time from consideration, so that there is something to vary.\nFermat’s principle, reformulated, states light paths have stationary optical length. Let the medium be isotropic (light speed does not depend on direction), then we have\n\\[S(\\text{path}) = \\int_{\\text{path}} n(q) \\|dq\\|\\]\nwhere \\(n\\) is the refractive index, with \\(L(q) = n(q)\\). Here we encounter a brief difficulty: time flows in one direction only, but space flows in infinitely many possible directions!\nThe solution might seem like a joke, but it would work out well: select one direction11, say \\(q_0\\), and pretend that it is time. With this trick, all previous mathematical formalism immediately applies, and we have\n11 This direction is usually selected to be the direction of the principal optic axis. For example, the long-axis of a camera is a principal optic axis, and so is the barrel-axis of a telescope.\\[S(\\text{path}) = \\int_{\\text{path}} n(q_0, q_1, q_2) \\sqrt{1 + \\left(\\frac{dq_1}{dq_0}\\right)^2 + \\left(\\frac{dq_2}{dq_0}\\right)^2}dq_0\\]\n\nDerivation\nLet’s make the notation cleaner, by rewriting \\(q_0\\) as \\(t\\), \\((q_1, q_2)\\) as \\(q\\), and using \\(v\\) to mean \\(\\left(\\frac{dq_1}{dq_0}, \\frac{dq_2}{dq_0}\\right)\\). Then we have\n\\[L(t, q, v) = n(t, q) \\sqrt{1 + \\|v\\|^2}\\]\nRoutine calculation yields\n\\[\\begin{cases}     \nL(t, q, v) = n(t, q) \\sqrt{1 + \\|v\\|^2}\\\\     \nH(t, q, p) = -\\sqrt{n^2 - \\|p\\|^2}\n\\end{cases} \\quad\n\\begin{cases}     \np^\\ast = \\frac{nv}{\\sqrt{1 + \\|v\\|^2}} \\\\     \nv^\\ast = \\frac{p}{\\sqrt{n^2 - \\|p\\|^2}}\n\\end{cases}\\]\nwe continue with the HJE, which simplifies to:\n\\[(\\partial_t S)^2 + \\|\\nabla_q S\\|^2 = n^2\\]\nReverting notation back to \\((q_0, q_1, q_2)\\), we find the eikonal equation12:\n12 From Greek εἰκών (eikon, \"image\"), from which the word “icon” derived.\\[\\|\\nabla_q S\\| = n(q)\\]\nWhy did the trick work? Well, if we look back to how we derived the Hamiltonian, we could see that what we called \"time\" is really just a special copy of \\(\\mathbb{R}\\), along which we organized all other state and control variables. We don’t really need time to be anything more than the domain of functions, as in \\(q_i: \\mathbb{R}\\to \\mathbb{R}\\) and \\(v_i : \\mathbb{R}\\to \\mathbb{R}\\). It most definitely does not need to \"flow\", or flow only from the past to the future, or have any psychological significance.\n\n\nInterpretation\nStarting with Fermat’s principle for light rays, we ended up with the eikonal equation for light waves. In general, we find the following duality between wave-field optics and particle-path optics.\n\nThe particle-wave duality.\n\n\n\n\n\n\n\nperspective\nparticle-path\nwave-field\n\n\n\n\naction \\(S\\)\na function of particle path\na field on configuration-spacetime\n\n\nequation\n\\(\\|\\nabla_q S\\| = n(q)\\)\n\\(\\partial_t S + H(t, q, \\nabla_q S) = 0\\)\n\n\nin optics\nlight rays, Fermat’s principle\nlight waves, Huygens principle\n\n\nin mechanics\npoint particles\nmatter waves\n\n\n\n\n\n\nAnisotropic case\nWhen the travel cost of light can depend on the direction of travel, we say that the medium is anisotropic. Now, you might expect\n\\[\nS = \\int n(\\hat{\\delta q}) \\delta q\n\\]\nbut this is incorrect, not because there is anything necessarily wrong with the formalism, but because of how \\(n\\) is defined by convention in anisotropic material. At this point, we must study the full particle-wave duality again.\nFor a moment, let’s pretend light is really a particle, and consider how it might appear to someone who believes light is a wave. We arrange an entire plane of photons, such that the plane is perpendicular to a unit vector \\(\\hat k\\). Now, let them move optimally for time \\(\\delta\\). Each of them would go in the same optimal direction \\(v^*(\\hat k)\\), to push the wavefront as far-out as possible.\n\n\n\nA plane of particles marching in sync at velocity \\(v\\), but the collective effect is that the plane moves with group velocity \\(v_g\\), which is different from the individual velocity \\(v\\).\n\n\nNow, the wavefront does not move in the direction \\(v^*\\), but in the direction \\(\\hat k\\). Therefore, the wavefront moves at group velocity \\(v_g(\\hat k) = \\left\\langle v^*(\\hat k), \\hat k\\right\\rangle \\hat k\\).\nSince the photons are trying to push as far out as possible,\n\\[v^*(\\hat k) = \\mathop{\\mathrm{argmax}}_{v \\in K_{particle}} \\left\\langle\\hat k, v\\right\\rangle\\]\nwhere we write \\(K_{particle}\\) as the surface of all particle velocities in all directions. It is a sphere of radius \\(c\\) in a vacuum, but in an anisotropic medium, we allow it to be any crazy shape.\nInstead of studying the group velocity, we actually need to use its inverse – the wavevector \\(k = \\frac{\\hat k}{v_g}\\).13 We thus have\n13 The typical definition is \\(k_{usual} = \\frac{2\\pi}{\\lambda}\\hat k = \\nabla \\phi\\), where \\(\\phi\\) is the phase of the light, and \\(f\\) is its temporal frequency. Our definition, which makes it cleaner, but somewhat different from typical definition, is \\(k_{ours} = \\frac{\\nabla \\phi}{2\\pi f} = \\frac{\\nabla \\phi}{\\omega}\\).\\[k = \\frac{\\hat k}{\\max_{v \\in K_{particle} \\left\\langle v, \\hat k\\right\\rangle}}\\]\nThe reason we use this instead of the group velocity is that, a little simplification later, we have the beautifully simple relation\n\\[\n\\forall k \\in K_{wave}, \\quad \\max_{v \\in K_{particle}}\\left\\langle v, k\\right\\rangle = 1\n\\]\nThis is a suggestive symmetry, which practically demands us to write it as a duality:\n\\[\n\\begin{cases}\nv^*(k) = \\mathop{\\mathrm{argmax}}_{v \\in K_{particle}}\\left\\langle v, k\\right\\rangle \\\\\nk^*(v) = \\mathop{\\mathrm{argmax}}_{k \\in K_{wave}}\\left\\langle v, k\\right\\rangle\n\\end{cases}, \\quad\n\\begin{cases}\n\\left\\langle v^*(k), k\\right\\rangle = 1\\\\\n\\left\\langle v, k^*(v)\\right\\rangle = 1\\\\\n\\end{cases}\n\\]\nThis is the polar dual construction, often used in convex analysis.14 (Hiriart-Urruty and Lemaréchal 2001, sec. C.3).\n14 What, just because we have not mentioned Legendre transform for a few pages, you would think that we’re done with convex analysis? Too bad! If nature is the great optimizer, then convex analysis is inescapable at every turn.The best case is if \\(K_{particle}\\) is convex. In this case, each \\(k \\in K_{wave}\\) defines a plane perpendicular to it, at a distance \\(\\|k\\|^{=1}\\) from the origin, and the plane is tangent to \\(K_{particle}\\) at precisely \\(v^*(k)\\), and conversely so. In other words, \\(K_{particle}\\) is the envelope of polar lines to points in \\(K_{wave}\\), and conversely so.\nConversely, we can pretend that light is really a wave, and consider how it might appear to someone who believes light is a particle. We would then go through the above argument, and obtain the same result.\n\n\n\nA pair of polar duals. Both surfaces are convex.\n\n\nHowever, we want to deal with more general cases than this, so we need to resolve two issues.\nFirst issue: \\(K_{particle}\\) might be non-convex. Second issue: it might be double-sheeted, or even many-sheeted. For example, in crystal, light polarized in two different orientations can move at two different velocities even in the same direction. Thus, its \\(K_{wave}\\) has two sheets, and so its polar dual, \\(K_{particle}\\), also has two sheets.\nBoth issues are solved by extending the definition of polar duality: replace the maximum with a stationarity. We can still construct \\(K_{particle}\\) from \\(K_{wave}\\) by taking a tangent plane for each \\(k \\in K_{wave}\\), but now instead of the intersection of the half-planes, we use their envelope. A picture shows what we mean:\n\n\n\nA pair of polar duals. The surfaces are no longer convex. Notice how the double tangent on the left becomes a double crossing point on the right.\n\n\n\n\n\nThe curve on the right is a trefoil curve defined by \\(z = 0.3 e^{i\\phi} + e^{-2i \\phi}\\), shifted to make the image clearer. The curve on the left is the polar dual of the trefoil curve. It is the envelope of the polar dual lines of each point on the trefoil curve.\n\n\nWe still have a duality between points on the two surfaces, defined by stationarity, not optimality. For example, for any wavevector \\(k \\in K_{wave}\\), its corresponding dual point \\(v^*(k)\\) satisfies \\(\\left\\langle v^*(k) + \\delta v, k\\right\\rangle = 0\\) for any \\(\\delta v\\) in the tangent space of \\(K_{particle}\\) at \\(v^*(k)\\).\nThe first application of Hamiltonian mechanics was done by Hamilton, who in 1832 predicted theoretically that if light enters a biaxial crystal in just the right way, it will not just refract in one direction, but in an entire cone of directions – which he termed internal conical refraction. The theory of this is fascinating,15 however, for lack of space, we will only give the barest description.\n15 It was historically important as the first phenomenon predicted by mathematical reasoning before experimental observation (Berry and Jeffrey 2007), and it was often compared to the mathematical prediction of Neptune by Le Verrier (1845) (Smith 1989).Simply put, it turns out that in a biaxial crystal, both \\(K_{particle}\\) and \\(K_{wave}\\) have the same shape of a large blob containing a smaller blob, touching each other at 4 cone-shaped points. Now, let \\(k_c \\in K_{wave}\\) be one of the cone-shaped points, then it corresponds to a tangent plane to \\(K_{particle}\\). Each tangent point \\(v \\in K_{particle}\\), conversely, corresponds to a tangent plane to \\(k_c\\).\nSince \\(k_c\\) is a conical point, however, there are a whole circle of tangent planes to \\(K_{wave}\\) at \\(k_c\\). Consequently, the tangent plane to \\(K_{particle}\\) is tangent to it on one entire circle. It is as if we throw a tire on the floor – it will touch the floor not just at three points, but an entire circle of points. Now, suppose that we have a planar wave moving in the direction of \\(k_c\\) inside the crystal, then each light-particle would have to move in a direction \\(\\mathop{\\mathrm{argmax}}_{v \\in K_{particle}}\\left\\langle v, k_c\\right\\rangle\\). But since there is an entire circle of such directions, we would have an entire circle of possible \\(v\\), and thus, we obtain a hollow cone of light.\n\n\n\nWhen a planar wave travels in a crystal, such that \\(K_{particle}\\) is tangent to the plane of the wave at an entire circle, then instead of choosing one among the entire circle of velocities, the particles simply take every single possible direction on the circle, resulting in conical refraction.\n\n\n\n\n\nThe shape of the \\(K_{wave}\\) surface. The \\(K_{particle}\\) surface is topologically the same, and can be obtained from \\(K_{wave}\\) by squashing it just right. Figure from (Schaefer 1949, 485, figure 128)"
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html",
    "title": "Classical thermodynamics and economics",
    "section": "",
    "text": "Even if the capitalist system is to give way to one in which service and not profit shall be the object, there will still be an integral of anticipated utilities to be made a maximum. Since we must find a function which maximizes an integral we must in many cases use the Calculus of Variations. But the problem here transcends the questions of depreciation and useful life, and belongs to the dawning economic theory based on considerations of maximum and minimum which bears to the older theories the relations which the Hamiltonian dynamics and the thermodynamics of entropy bear to their predecessors.\n(Hotelling 1925)\n\n\nIt is to the economist, the statistician, the philosopher, and to the general reader that I commend the analysis contained herein… mathematics as applied to classical thermodynamics is beautiful: if you can’t see that, you were born color-blind and are to be pitied. Similarly, in all the branches of pure and applied mathematics, the subject of probability is undoubtedly one of the most fascinating.\nPaul Samuelson, forward to (Bickler and Samuelson 1974)\n\n\n\n\n\\(S\\): entropy\n\\(U\\): internal energy\n\\(V\\): volume\n\\(T\\): temperature\n\\(\\beta = 1/T\\): inverse temperature\n\\(N\\): number of particles of a chemical species\n\\(n\\): number of moles of a chemical species\n\\(\\xi\\): extent of reaction\n\\(X\\): “other properties that we are not concerned with”.\n\nFor example, with an ideal gas trapped in a copper box, its macroscopic state is determined by \\(U, N, V\\). If we want to focus on \\(U\\), then we can let \\(X = (N, V)\\), and write \\(S = S(U, X)\\).\nSimilarly, for a photon gas in a blackbody chamber, its macroscopic state is determined by \\(U, V\\), since photons can be created and destroyed on the inner surface of the blackbody chamber. We can then write \\(S = S(U, X)\\).\n\n\nThermodynamics is notorious for having too many partial differentials and coordinate changes. \\((\\partial_{x_1} f)_{x_1, x_2, \\dots, x_n}\\) means that we lay down a coordinate system defined by \\(x_1, \\dots, x_n\\), then calculate \\(\\partial_{x_1}f\\), fixing the other coordinates constant. In particular, \\((\\partial_{x_1} f)_{x_2, \\dots, x_n}\\) is likely different from \\((\\partial_{x_1} f)_{x_1, y_2, \\dots, y_n}\\).\nIf in doubt, write down\n\\[df = \\sum_{i=1}^n (\\partial_{x_i} f)_{x_1, \\dots, x_n} dx_i\\]\nand reason thenceforth.\n\n\n\n\n(Pippard 1964). Slim, elegant, both mathematical and applied. In the best British tradition of mathematics – think James Maxwell and G. H. Hardy.\n(Fermi 1956). The same as above. However, it also covers chemical thermodynamics.\n(Lemons 2019). A very readable introduction to classical thermodynamics, slim but deep. I finally understood the meaning of the three laws of thermodynamics after reading it. Contains copious historical quotations.\n(Lemons 2008). A textbook version of the author’s (Lemons 2019), weaving in history and philosophical contemplation at every turn.\n(Carnot, Clapeyron, and Clausius 1988). A reprint of the most important papers in thermodynamics published before 1900. Useful to have on hand if you are reading (Lemons 2019).\n(Buchdahl 1966). A textbook based on Carathéodory’s axiomatic thermodynamics. The notation is ponderous, and the payoff is unclear. I don’t know what is its intended audience – perhaps professional pedants and differential geometers? Nevertheless, if you need to do research in Carathéodory’s axiomatic thermodynamics, I think this is your best bet."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#introduction",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#introduction",
    "title": "Classical thermodynamics and economics",
    "section": "",
    "text": "Even if the capitalist system is to give way to one in which service and not profit shall be the object, there will still be an integral of anticipated utilities to be made a maximum. Since we must find a function which maximizes an integral we must in many cases use the Calculus of Variations. But the problem here transcends the questions of depreciation and useful life, and belongs to the dawning economic theory based on considerations of maximum and minimum which bears to the older theories the relations which the Hamiltonian dynamics and the thermodynamics of entropy bear to their predecessors.\n(Hotelling 1925)\n\n\nIt is to the economist, the statistician, the philosopher, and to the general reader that I commend the analysis contained herein… mathematics as applied to classical thermodynamics is beautiful: if you can’t see that, you were born color-blind and are to be pitied. Similarly, in all the branches of pure and applied mathematics, the subject of probability is undoubtedly one of the most fascinating.\nPaul Samuelson, forward to (Bickler and Samuelson 1974)\n\n\n\n\n\\(S\\): entropy\n\\(U\\): internal energy\n\\(V\\): volume\n\\(T\\): temperature\n\\(\\beta = 1/T\\): inverse temperature\n\\(N\\): number of particles of a chemical species\n\\(n\\): number of moles of a chemical species\n\\(\\xi\\): extent of reaction\n\\(X\\): “other properties that we are not concerned with”.\n\nFor example, with an ideal gas trapped in a copper box, its macroscopic state is determined by \\(U, N, V\\). If we want to focus on \\(U\\), then we can let \\(X = (N, V)\\), and write \\(S = S(U, X)\\).\nSimilarly, for a photon gas in a blackbody chamber, its macroscopic state is determined by \\(U, V\\), since photons can be created and destroyed on the inner surface of the blackbody chamber. We can then write \\(S = S(U, X)\\).\n\n\nThermodynamics is notorious for having too many partial differentials and coordinate changes. \\((\\partial_{x_1} f)_{x_1, x_2, \\dots, x_n}\\) means that we lay down a coordinate system defined by \\(x_1, \\dots, x_n\\), then calculate \\(\\partial_{x_1}f\\), fixing the other coordinates constant. In particular, \\((\\partial_{x_1} f)_{x_2, \\dots, x_n}\\) is likely different from \\((\\partial_{x_1} f)_{x_1, y_2, \\dots, y_n}\\).\nIf in doubt, write down\n\\[df = \\sum_{i=1}^n (\\partial_{x_i} f)_{x_1, \\dots, x_n} dx_i\\]\nand reason thenceforth.\n\n\n\n\n(Pippard 1964). Slim, elegant, both mathematical and applied. In the best British tradition of mathematics – think James Maxwell and G. H. Hardy.\n(Fermi 1956). The same as above. However, it also covers chemical thermodynamics.\n(Lemons 2019). A very readable introduction to classical thermodynamics, slim but deep. I finally understood the meaning of the three laws of thermodynamics after reading it. Contains copious historical quotations.\n(Lemons 2008). A textbook version of the author’s (Lemons 2019), weaving in history and philosophical contemplation at every turn.\n(Carnot, Clapeyron, and Clausius 1988). A reprint of the most important papers in thermodynamics published before 1900. Useful to have on hand if you are reading (Lemons 2019).\n(Buchdahl 1966). A textbook based on Carathéodory’s axiomatic thermodynamics. The notation is ponderous, and the payoff is unclear. I don’t know what is its intended audience – perhaps professional pedants and differential geometers? Nevertheless, if you need to do research in Carathéodory’s axiomatic thermodynamics, I think this is your best bet."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#what-is-thermodynamics",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#what-is-thermodynamics",
    "title": "Classical thermodynamics and economics",
    "section": "What is thermodynamics?",
    "text": "What is thermodynamics?\nBoth neoclassical economics and classical thermodynamics are about the equilibria of large systems. While a large system is generally hopelessly complicated, almost all the complexity falls away when the system is maximizing a single quantity. Ceaselessly striving to maximize entropy, a complex system sheds its complexity and reaches the pure simplicity of maximal entropy. Ceaselessly striving to maximize profit, a complex company sheds its complexity and reaches the pure simplicity of perfect product.\nIn both fields, everything we can say about the world are nothing more than about systems, constraints, contacts, and equilibria. Time and change do not exist. All we can explain is which states are in constrained equilibrium, not how a system can get there. Atoms do not exist. All we can explain is what happens to homogeneous substances in constrained equilibrium. People do not exist. All we can explain is what happens to constrained economic systems in equilibrium.\nDifferent things can be maximized: the total entropy, or the negative Gibbs free energy, or the profit, or the sum-total of utility, or something else. Through different lenses, different things are maximized, but they predict the same phenomena. Using this mathematical freedom, experts brachiate around the coordinate axes like gibbons brachiating around vines, looking for the perfect angle to solve each particular problem.\n\nConstrained maximization problems in economics and thermodynamics.\n\n\n\n\n\n\n\ninterpretation\nmaximized quantity\nconstraint\n\n\n\n\nconglomerate accounting\nbook value\nAssets is conserved, but can be moved between child companies.\n\n\nsocial welfare\nsocial utility\nWealth is conserved, but can be redistributed.\n\n\nclosed system\nentropy\nQuantities are conserved, but can be moved between sub-systems.\n\n\nfactory production\nprofit\nSome raw materials are on sale at a market, but others are not.\n\n\nconsumer choice\nutility\nSome finished goods are on sale at a market, but others are not. The market uses a commodity money.\n\n\npartially open system\nnegative free energy\nSome quantities can be exchanged with a bath, but others are conserved.\n\n\n\n\nSystems\nSystems are the main characters of the drama of thermodynamics. A thermodynamic system is something that is fully determined by a few macroscopic properties, related by equations of state. Once we know enough of its properties, we know all there is to know about such a system. There is nothing left to say about it.\nEverything is a thermodynamic system. However, there are two special types: bath systems, and mechanical systems.1 Any number of thermodynamic systems can be connected into a larger system – a compound system.\n1 Some children are confused when they heard that squares are rectangles too. I hope you won’t be equally confused when you heard that mechanical systems are also thermodynamic systems.The prototypical thermodynamic system is a tank of ideal gas whose number of particles is fixed. It has 2 degrees of freedom, so if we write down \\(n\\) different macroscopic properties, they would (generically) be related by \\(n-2\\) equations of state. So for example, if we write down the properties internal energy \\(U\\), temperature \\(T\\), volume \\(V\\), pressure \\(P\\), they would be related by the 2 equations of state\n\\[PV = k_BNT, \\quad U = \\frac 32 k_B NT\\]\nIf we know two out of the four of internal energy \\(U\\), temperature \\(T\\), volume \\(V\\), pressure \\(P\\), then we can solve for all the others by equations of state. The macroscopic properties fully describe the system, and nothing more can be said about it. We cannot ask more questions such as “Are there more particles on the left than on the right?” or “How long did it take for the system to reach equilibrium?”, because such questions are literally undefined in classical thermodynamics.\nBecause the properties are related by equations of state, we need only know a few of the properties in order to infer all the rest. For example, knowing the volume \\(V\\), internal energy \\(U\\), and particle number \\(N\\), of a tank of ideal gas, we can infer that its pressure is \\(P = \\frac{2U}{3V}\\), and its temperature is \\(T = PV/k_BN\\). Succinctly,\n\\[P = P(U, V, N), \\quad T = T(U, V, N)\\]\nmeaning “If we know \\(U, V, N\\), then we can calculate \\(P\\) and \\(T\\)”.\n\n\n\n\n\n\nWarning\n\n\n\nIt’s too easy to misread it as saying that \\(P\\) is a mathematical function of \\(U, V, N\\). It is not. It really is saying that, there exists a mathematical function \\(f_P\\), such that for any state \\(\\omega\\) of the system, we have\n\\[P(\\omega) = f_P(U(\\omega), V(\\omega), N(\\omega))\\]\n\n\nEverything about a thermodynamical system is known once we specify how its entropy is a function of its macroscopic properties. For example, the ideal gas is fully specified by\n\\[S(U, V, N) = k_B N \\ln\\left[\\frac{V}{N}\\,\\left(\\frac{U}{\\hat{c}_V k_B N}\\right)^{\\hat{c}_V}\\,\\frac{1}{\\Phi}\\right]\\]\nwhere \\(\\hat c_V\\) and \\(\\Phi\\) are constants that differ for each gas.\nAs another example, the photon gas is defined by\n\\[S(U, V) = C V^{1/4}U^{3/4}, \\quad C=\\left(\\frac{256\\pi^2 k_B^4}{1215 c^3 \\hbar^3}\\right)^{1/4}\\]\nThe fact that \\(S(U, V) \\propto V^{1/4}U^{3/4}\\) can be derived from 19th-century physics (and indeed, it was known to Boltzmann), but the constant \\(C\\) has to wait for proper quantum mechanics.\n\nBaths\nA bath system is a thermodynamic system with nonzero entropy function, but infinitely large size.\nFor example, if we take a copper piston and fill it with ideal gas, then immerse the piston in the bottom of an ocean, then it is a volume-and-energy bath with constant pressure-and-temperature. If we cover up the piston with some insulating material, then the ocean suddenly becomes merely a volume bath with constant pressure. If we use screws to fix the piston head, then the ocean suddenly becomes merely an energy bath with constant temperature.\nBecause they are infinitely large, if you connect two baths together, something bad will happen. For example, if you connect two energy baths together, but with different temperature, what would happen? The simple answer is: “A torrent of heat will flow from the hotter to the colder bath.” The more correct answer is: “Classical thermodynamics does not allow such a question to be asked. It would be like asking what happens when ‘an unstoppable force meets an immovable object’. If we literally have two baths, then we cannot connect them. If we only have two giant oceans that seem like baths when compared to this little tank of gas, then if the two oceans are connected to each other, they will no longer appear as baths to each other.”\nA heat bath, or more accurately an energy bath, is a system that you can take or dump as much energy as you want, always at constant marginal price of energy. An atmosphere, or more accurately an energy-and-volume bath, is a system that you can take or dump as much energy or volume as you want, always at constant temperature and pressure. In general, a bath is an infinite source of a conserved quantity at constant marginal entropy. We can imagine other forms of baths. For example, the surface of a lake could serve as an energy-and-area bath. A large block of salt can serve as a salt-chemical bath.\n\n\nMechanical systems\nA mechanical system is a thermodynamic system whose entropy is always zero. Essentially all systems studied in classical mechanics are such systems. In classical thermodynamics, they are not put center-stage, but if you know where to look, you will see them everywhere.\nAn ideal linear-spring has two macroscopic properties: length \\(x\\) and internal energy \\(U\\), with equation of state \\(U = \\frac 12kx^2\\). For example, a helix spring is close to an ideal linear-spring.\nAn ideal surface-spring is the same as an ideal linear-spring, but with area \\(A\\) instead of length \\(x\\). Its equation of state is \\(U = \\sigma A\\), where \\(\\sigma\\) is surface tension constant. For example, a balloon skin is close to an ideal surface-spring.\nAn ideal volume-spring is similar. It would resemble a lump of jelly. An ideal gas, though it looks like a volume-spring, is not an example, because its entropy is not zero. In particular, this means an ideal gas has temperature and can be “heated up”, but a lump of ideal jelly cannot.\nIn general, we can construct an arbitrary energy storage system, such that it has two macroscopic properties \\(x, U\\), satisfying \\(U = f(x)\\), where \\(f\\) is any differentiable function. We can imagine taking a mystery box with a chain we can pull on, and by some internal construction with gears, pulleys, weights, and springs, the force on the chain is \\(f'(x)\\), where \\(x\\) is the length by which we have pulled.\n\n\n\nState space\nA tank of gas has on the order of \\(10^{26}\\) particles, but in classical thermodynamics, its state is entirely determined if we know its \\(U, V, N\\). In this sense, we can say that its macroscopic state space has just 3 dimensions. In many situations, such as in the Carnot heat engine, we also fix its \\(N\\), in which case its state space has just 2 dimensions.\nThis is typically plotted in either the \\((P, V)\\) space, or the \\((T, S)\\) space, or some other spaces, but in every case, there are just two dimensions. We can unify all these diagrams as merely different viewpoints upon the same curvy surface – the state space \\(\\mathcal X\\) itself. Each point \\(\\omega \\in \\mathcal X\\) in the state space is a state, and each macroscopic property \\(X\\) is a scalar function of type \\(X : \\mathcal X \\to \\R\\).\nIf the state space has just \\(d\\) dimensions, then we need only \\(d\\) macroscopic properties \\(X_1, \\dots, X_d\\) in order to lay down a coordinate system for the state space. If we have another macroscopic property \\(Y\\), then there in general exists a function \\(f_Y: \\R^d \\to \\R\\), such that \\(Y(\\omega) = f_Y(X_1(\\omega), \\dots, X_d(\\omega))\\) for any state \\(\\omega\\). In other words, we have an equation of state.\n\n\n\nThe state space for a thermodynamic system with 2 degrees of freedom. If we lay down three macroscopic properties, then they satisfy one equation of state.\n\n\n\nExercise 1 Based on the following diagram, prove that if we have three scalar functions \\(x, y, z\\) on a smooth 2D surface, then \\(\\left(\\frac{\\partial x}{\\partial z}\\right)_y\\left(\\frac{\\partial y}{\\partial x}\\right)_z\\left(\\frac{\\partial z}{\\partial y}\\right)_x = -1\\). Generalize this to higher dimensions.\n\n\n\n\nConstraint\nA constraint is an equation of form \\(f(A, B, C, \\dots) = f_0\\), where \\(f\\) is a mathematical function, \\(A, B, C, \\dots\\) are macroscopic properties, and \\(f_0\\) is a constant.\nFor example, if we have two tanks of gas in thermal contact, then the constraint is as follows:\n\\[\n\\begin{cases}\nV_1 &= V_{1,0} \\\\\nV_2 &= V_{2,0} \\\\\nU_1 + U_2 &= U_{1,0} + U_{2,0} \\\\\n\\end{cases}\n\\]\nmeaning that the volume of each tank of gas is conserved, and the sum of their energy is also conserved.\nThe constraints on a compound system are determined by the contacts between its subsystems.\n\n\nContacts\nIf we only have systems isolated in a perfect vacuum, then nothing interesting will happen. If two systems are perfectly connected, then they will never be brought apart. A contact allows two systems to interact, without destroying their individuality. It allows two systems to communicate, without becoming literally one system. Economically, contacts are trade contracts.\nIn general, the effect of a contact is to reduce the number of constraints by one. For example, a metal rod between two pistons reduces the two constraints\n\\[V_1 = V_{1,0}, \\quad V_2 = V_{2, 0}\\]\ninto one constraint: \\[V_1 + V_2 = V_{1,0} + V_{2,0}\\]\nAs another example, in a tank of three kinds of gas \\(N_2, H_2, NH_3\\), allowing a single chemical reaction \\(N_2 + 3H_2 \\rightleftharpoons 2NH_3\\) reduces three constraints\n\\[\nN_{N_2} = N_{N_2, 0}, \\quad N_{H_2} = N_{H_2, 0}, \\quad N_{NH_3}= N_{NH_3, 0}\n\\]\ninto two:\n\\[\nN_{N_2} - N_{N_2, 0} = (N_{H_2} - N_{H_2, 0})/3, \\quad N_{N_2} - N_{N_2, 0} = (N_{NH_3} - N_{NH_3, 0})/2\n\\]\nA contact can be nonlinear. For example, if we have two pistons of the same area, connected by a lever, such that pushing on one piston by \\(\\Delta x\\) would be pulling on the other piston by \\(2 \\Delta x\\), then the constraint becomes \\(2(V_1 - V_{1,0}) + (V_2 - V_{2,0}) = 0\\). And by designing a series of levers, gears, and chains, we can realize any constraint function \\(f(V_1, V_2) = f(V_{1,0}, V_{2,0})\\) for any smooth function \\(f\\).\n\n\n\nA thermodynamical system (a piston of gas) is connected to a mechanical system (a mass in gravity) via an arbitrary constraint (variable gears).\n\n\n\n\nCompound systems\nA compound system is nothing more than several systems connected. If we know the connections, and the entropy function of each subsystem, then we know everything about the compound system. The number of DOF for the compound system is the sum of the DOF of the subsystems, minus the degree of constraints.\nFor example, in the adiabatic expansion of a tank of ideal gas, we are really studying one compound system made of:\n\na tank of ideal gas (thermodynamic system),\na lump of mass in gravity (mechanical system),\nwith a gear system between them (contact).\n\nThe gear system is designed with gear-ratio that varies as the system turns, in just the right way such that the system is always in equilibrium no matter the position of the piston, so that it really has no preference of going forwards or backwards.\n\n\n\nThe compound system. Inside it, there is a subsystem of a tank of ideal gas that undergoes adiabatic expansion.\n\n\n\n\n\n\n\n\nOnly one system\n\n\n\nWe typically think of the tank of ideal gas itself as part of the thermodynamics, and the other parts as “the environment”, but we should properly speaking consider one single compound system, of which the tank of ideal gas is merely a sub-system. This way, we can state directly that the entropy of the entire compound system is maximized.\n\n\n\nExample 1 (heat-engine-and-environment compound system) A heat engine is a thermodynamic system that is used as a component of a larger compound system. The large system contains 4 parts: two energy baths, one heat engine, and one carefully designed mechanical system acting as energy storage.\nIf you only want a heat engine that works, then the energy storage does not need to be carefully designed. However, if you want a Carnot heat engine, i.e. at maximal efficiency, then the energy storage must be designed to be exactly right. It must be designed to follow the exact parameters of the heat engine, as well as the two energy baths’ temperatures. If any of those is ignored, the energy storage would fail to “mesh” with the rest of the compound system, and cause waste.\nThis is why a heat engine must be a component of a larger compound system. Every part of it depends on every other part. The energy storage unit is just as important and precisely designed as the heat engine.\n\n\n\nEquilibrium, virtual vs actual states\n\nFreedom is an iron cage.\nConstraints set it free again.\n\nOn the African savannah there lived a bunch of meerkats. Meerkats love to stand on the tallest place.\nAt first, they could stand wherever they wanted, so they all stood on one single hill. It was crowded. A blind lion who had memorized the landscape came and ate all of them.\nThen humans came and added long walls that divided the savannah into thin stripes. Now each meerkat’s location is determined by which stripe it happened to fall in. The blind lion could find the meerkat if he knew the stripe location. In other words, optimizing for height, when there is one constraint, leads to one dimension of uncertainty.\nThis is a subtle point, so I will say it again. If you want to optimize for a quantity, and you don’t have a constraint, then you would always go to the globally best solution. The whole space of possibilities is open to you, but you don’t need them. Those states are “virtual” because they are never observed, even though their existence is inferrable.\nBut if you have one constraint, then you have one unique solution for each possible setting of constraint. You are still not free, but at least now you have a puppet master.\nIn classical thermodynamics, only equilibrium states are “real”. Nonequilibrium states are “virtual”. In Lagrangian mechanics, only stationary-action paths are real, and the other paths are virtual. You can imagine that if you throw a rock upwards, it would execute a complex figure-8 motion before returning to ground again, but that’s a virtual path. The only real path is the unique virtual path that stationarizes the action integral.\nSimilarly, in classical thermodynamics, you could imagine that a tank of gas contains all its gas on the left side. Its entropy is just \\(S(U, V/2, N)\\), but that’s a virtual state that does not satisfy maximize entropy under constraint. The unique entropy-maximizing virtual state is the real state.\nFor every constraint, there are many nonequilibrium states that satisfy the constraint, but only one equilibrium entropy, and so equilibrium entropy is a function of constraints, even though the entropy function itself is not. It optimizes all its complexities away, allowing us to know it through just its external constraints.\n\n\nSome common misconceptions\nThermodynamics is not statistical mechanics. Forget about molecules and atoms. Forget about statistics and statistical mechanics. Forget about \\(S = k_B\\ln \\Omega\\) or \\(S = -\\sum_i p_i \\ln p_i\\). Randomness does not exist in classical thermodynamics.\nForget about the first law of thermodynamics. Energy is nothing special. The conservation of energy is no more important than the conservation of volume, or the conservation of charge.\nForget about heat. Heat does not exist – it is not a noun, not even an adjective, but an adverb at most. The theory of caloric has already been disproven in the 19th century by the cannon-boring experiment.\nHeat energy and work energy are both misnomers. Neither are types of energy. Instead, they are types of energy-flow. We should speak of only heat energy-flow and work energy-flow.\nTo perform work, one must perform work upon something. In other words, there is no such thing as “system A performed work”. There is really “some energy and length is traded between A to B in compliance with a work-equation-constraint”.\nForget about time. Time does not exist in classical thermodynamics. We can say nothing at all about what happens between equilibria. We can only say, “This is an equilibrium, but that is not an equilibrium. There is nothing at all to say about what happens between them.\nThe name “thermodynamics” is a complete misnomer, because heat does not exist (thus no “thermo-”) and time does not exist (thus no “-dynamics). It should be called”entropo-statics”.\nIf time does not exist, you ask, what do we mean when we study Joule expansion? That is, when we take half a tank of gas, and suddenly open the middle wall and wait until the gas equilibrates in the entire gas?\nIn fact, we did not open the middle wall. We did not wait. Gas did not expand. The past did not cause the future. Time is an stubbornly persistent illusion, and causality is illusory too.\nIn fact, we are considering two different problems in thermodynamics.\nFirst problem: Given a tank of gas with internal energy \\(U = U_0\\), molar-amount \\(n = n_0\\), and constraint \\(V \\leq \\frac 12 V_0\\). What is its equilibrium state? Answer: The state that maximizes entropy under the constraints: \\[\n\\begin{cases}\n\\max S(U, n, V) \\\\\nV \\leq \\frac 12 V_0 \\\\\nU = U_0 \\\\\nn = n_0\n\\end{cases}\n\\]\nwhere \\(S(U, n, V)\\) is the entropy of the gas when its states properties are \\(U, n, V\\).\nSimilarly, the second problem is another constraint-optimization problem: \\[\n\\begin{cases}\n\\max S(U, n, V) \\\\\nV \\leq V_0 \\\\\nU = U_0 \\\\\nn = n_0\n\\end{cases}\n\\]\nThat the two different problems seem to “follow one from another” is an illusion. In reality, they only appear to follow one another because this is what we observe in the real world, where one equilibrium follows another. In equilibrium thermodynamics, equilibria do not follow one another – everyone stands alone.\nTime only appears in the following sense: We observe a real-world system, like a car engine, and notice that its motion seems to consist of a sequence of equilibria. Keeping those almost-equilibria in mind, we delve into the ocean of classical thermodynamics, until we have found some equilibria that resemble the almost-equilibria we have in mind. And so, from the bottom of the ocean, we pick up one equilibrium, then another, then another. Then we string together these little equilibria along a number line like a pearl necklace. We run our fingers over these pearls, and delight in its “motion”, like flipping the pages of a stop-motion book and shouting, “Look, it is moving!”."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#the-three-laws",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#the-three-laws",
    "title": "Classical thermodynamics and economics",
    "section": "The three laws",
    "text": "The three laws\nMore accurately: one law and two non-laws.\n\nSecond law\n\nFor the equilibrium of any isolated system it is necessary and sufficient that in all possible variations of the state of the systems which do not alter its energy, the variation of its entropy shall either vanish or be negative.\n(Gibbs 1878)\n\nThe second law of thermodynamics is all important: maximizing entropy is all of classical thermodynamics. All other parts are just tricks for maximizing entropy.\n\n\nFirst law\n\nAs a student, I read with advantage a small book by F. Wald entitled “The Mistress of the World and her Shadow”. These meant energy and entropy. In the course of advancing knowledge the two seem to me to have exchanged places. In the huge manufactory of natural processes, the principle of entropy occupies the position of manager, for it dictates the manner and method of the whole business, whilst the principle of energy merely does the book- keeping, balancing credits and debits.\n(Emden 1938)\n\nThe first law of thermodynamics is entirely trivial. Energy is nothing special! Energy is just a conserved quantity, one among equals, much like volume, mass, and many other things… Every conserved quantity is equally conserved,2 so it does not deserve a special thermodynamic law. You might as well say “conservation of mass” is “the second-first law of thermodynamics” and “conservation of volume” is “the third-first law of thermodynamics”, and “conservation of electrons” and “conservation of protons” and “conservation of length” (if you are studying a thermodynamic system restricted to move on a line) and “conservation of area” (if you are studying a thermodynamic system restricted on the surface of a lake) and so on…\n2 The first law of thermodynamics had always struck me as oddly out of place, almost like a joke that I could not catch, like\n\nAll animals are equal, but some animals are more equal than others\n\nbut with\n\nAll conserved quantities are conserved, but some quantities are more conserved than others.\n\nI kept waiting for the textbooks, or the teachers, or someone, to drop the act and confess, “Actually, we were joking – conservation of energy really is not that special, and we were just bored with standard physics and wanted to confuse you with a cute magic trick before showing you what standard physics really is saying.”. Slowly, I realized that there is no joke – conservation of energy is unironically treated as special by not just the students, but even the teachers. I had to figure out for myself how the joke really works, and it required me to rebuild thermodynamics along my preferences.This sounds extraordinary, but that is merely how classical thermodynamics works. The first law of thermodynamics does not deserve its title. It should be demoted to an experimental fact, not a law. Just to drive the point home, I wrote an entire sci-fi worldbuilding sketch about an alien species, for which it is the conservation of volume that is fundamental, not energy, and which discovered stereodynamics. If you can laugh at their mistaken importance of the conservation of volume, maybe you can laugh at the mistaken importance of the conservation of energy too.\nThe proper place for the law of conservation of energy is not classical thermodynamics, but general physics, because energy is nothing special inside classical thermodynamics, but it is extremely special if we zoom out to consider the whole of physics. Whereas in classical thermodynamics, systems conserve energy, and volume, and mass, and electron-number, and proton-number, and… when you move outside of thermodynamics, such as when you add in electrodynamics, special relativity, and quantum mechanics, all kinds of conservations breakdown. You don’t have conservation of mass, or number of electrons, or even volume, but energy is always conserved.\n\n\nZeroth law\nNow that the first law has been dispelled, we can dispel the zeroth law of thermodynamics, too. If Energy falls from grace, so must its shadow, Temperature.\n\nTheorem 1 (general zeroth law) If \\(S_1(X_1, Y) + S_(X_2, Y)\\) is maximized under constraint \\(X_1 + X_2 = X\\), then \\((\\partial_X S_1)_Y = (\\partial_X S_2)_Y\\).\n\n\nThe zeroth law of thermodynamics in various guises.\n\n\n\n\n\n\n\nmaximized quantity \\(S\\)\nconserved quantity \\(X\\)\nderivative \\((\\partial_X S)_Y\\)\n\n\n\n\nentropy\nenergy\ninverse temperature \\(\\beta\\)\n\n\nentropy\nvolume\n\\(\\beta P\\)\n\n\nentropy\nparticles\n\\(-\\beta \\mu\\), where \\(\\mu\\) is chemical potential\n\n\nentropy\nsurface area\n\\(-\\beta\\sigma\\), where \\(\\sigma\\) is surface tension\n\n\nproduction value\nraw material\nmarginal value\n\n\n\n\n\nThird law\nThe third law is rarely if ever used in classical thermodynamics, and its precise meaning is still unclear. It seems to me that its proper place is not thermodynamics, but quantum statistical mechanics, where it states that a substance, when at the lowest possible energy (ground state), has finite entropy – Einstein’s formulation of the third law. (Klimenko 2012)\n\n\nAn economic interpretation\nHere is an economic interpretation of classical thermodynamics. There are other possible interpretations, and we will use the others in this essay.\nIn this interpretation, the laws of nature become the CEO of a company. Every conserved quantity is a commodity. The company has some commodity. Commodities themselves have no intrinsic value. Instead, the company is valued by a certain accounting agency. The CEO’s job is to move around the commodities so that the accounting agency gives it the highest book-value.\nA compound system is a conglomerate company: a giant company made of little companies. If entropy is extensive, then it means the total book-value for the conglomerate is the sum of the book-value of each subsidiary company. Otherwise, entropy is nonextensive, and the accounting agency believes that the conglomerate has corporate synergy.3\n3 The most well-known trouble with entropy is with gravity. In high school, I was studying physics with a standard textbook in Chinese universities. It had a curious section on the second law of thermodynamics. I could not find it again, but the gist is as follows: The heat death of the universe is an unjustified implication of the second law, on two grounds. One, gravitational systems allow for unlimited entropy production. Two, it is against the historical science of dialectical materialism, where every development creates its own contradiction and sublation, endlessly. I will always remember this comment, as the only intrusion of religious sentiment in an otherwise sober textbook.\nActually, since we are on the topic of Marxism and science in China, here is another amusing anecdote: In 1981, there was a sci-fi novel Dream of Comfort Country (温柔之乡的梦, by 魏雅华). It was a cautionary story about a robotic wife, who was so obedient as to cause the protagonist degenerate into a tyrannical person. Just a standard pro-human-relationship morality play. During the campaign against spiritual pollution of 1983, sci-fi novels were denounced, causing a 15-year-long draught of sci-fi. The particular novel was denounced for the following reason: The robotic wives were supposedly reading all kinds of books – then why didn’t they read Marx’s and Lenin’s books? (刘 2015, 87)The inverse temperature \\(\\beta\\) is the marginal value of energy:\n\\[\\beta = \\frac{d(\\text{value of a sub-company})}{d(\\text{energy owned by the sub-company})}\\]\nThe pressure \\(P\\), multiplied by \\(\\beta\\), is the marginal value of space:\n\\[\\beta P = \\frac{d(\\text{value of a sub-company})}{d(\\text{volume owned by the sub-company})}\\]\n\n\n\n\n\n\nTip\n\n\n\nIt might appear odd to write \\(\\beta P\\), but in the entropy-centric view of thermodynamics, it is the quantity \\(\\beta P\\) that is fundamental, and in comparison, the pressure \\(P\\) is less fundamental, as a ratio \\(P := \\frac{\\beta P}{\\beta}\\). Why, then, do we speak of pressure \\(P\\) and temperature \\(T\\), instead of \\(\\beta\\) and \\(\\beta P\\)? It is because classical thermodynamics is traditionally understood as energy-centric."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#basic-consequences",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#basic-consequences",
    "title": "Classical thermodynamics and economics",
    "section": "Basic consequences",
    "text": "Basic consequences\n\nThermodynamic force/suction\nSince nature is entropy-maximizing, if entropy can be increased by moving some energy from one system to another, it will happen. Similarly for space. It would seem as if there is a thermodynamic suction that is sucking on on energy, and the side with the higher thermodynamic suction tends to absorb it.\n\\[\n\\text{thermodynamic suction of $X$} = \\left(\\frac{\\partial S}{\\partial X}\\right)_{\\text{non-}X}\n\\]\nIn energy-centric thermodynamics, we use thermodynamic force, or thermodynamic potential, defined by\n\\[\n\\text{thermodynamic force of $X$} = -\\frac{\\left(\\frac{\\partial S}{\\partial X}\\right)_{\\text{non-}X}}{\\left(\\frac{\\partial S}{\\partial U}\\right)_{\\text{non-}U}}\n\\]\nFor example, for a tank of gas, the macroscopic properties are \\(U, V, N\\), and so it has three thermodynamic suctions:\n\\[\n\\begin{aligned}\n  \\beta &= (\\partial_U S)_{V, N}\\\\\n  \\beta P &= (\\partial_V S)_{U, N}\\\\\n  -\\beta \\mu &= (\\partial_N S)_{U, V}\n\\end{aligned}\n\\]\nand the thermodynamic forces associated with \\(V, N\\) are \\(P\\) and \\(-\\mu\\).\nUnfortunately, the notations for thermodynamic suctions are far from elegant. The first one, \\(\\beta\\), is the inverse of temperature.. The second one, \\(\\beta P\\), is \\(\\beta\\) multiplied by pressure. The third one is truly the most annoying, as it not only involves \\(\\beta\\), but also a negative sign. To see how the notation came about, we can write it out in differential form:\n\\[dS = \\beta dU + \\beta P dV - \\beta \\mu dN\\]\nConventional notations are energy-centric, so we rewrite it to single out \\(dU\\):\n\\[\ndU = TdS + (- PdV) + \\mu dN\n\\]\nNow we see how the notation came about: \\(TdS\\) is the heat energy-flow into the system, \\(-PdV\\) is the mechanical work energy-flow into the system, and \\(\\mu dN\\) is the chemical energy-flow into the system.\nIf I could truly reform notation, I would redefine \\(\\beta P\\) as \\(p_V\\), meaning “the price of volume”, meaning “the price of particle”, and so on. In this notation, we have:\n\\[\n\\begin{aligned}\ndS &= p_U dU + p_V dV + p_N dN \\\\\ndU &= p_U^{-1}dS - \\frac{p_V}{p_U}dV - \\frac{p_N}{p_U}dN\n\\end{aligned}\n\\]\n\nExample 2 (photon gas with \\(\\mu = 0\\)) Suppose we have a piston chamber, with its inner surface covered with silver, and there is a tiny speck of blackbody inside it, then the chamber would be filled with bouncing photons, in the form of a “photon gas”. The photons would reflect off the surface of the chamber, some absorbed and some emitted in turn, by the blackbody.\nAs usual for gas, the state of the system is determined by its internal energy, volume, and particle number: \\(U, V, N\\), with\n\\[dS = \\beta dU + \\beta PdV - \\beta \\mu dN\\]\nHowever, the photon gas is quite special, in that photons can be created and destroyed by the speck of blackbody, so at equilibrium, we must have \\(\\beta\\mu = 0\\), for otherwise, the system would be able to increase in entropy simply by creating/destroying more photons, and thus it is not in equilibrium.\nThis contrasts with the typical case with chemical gases like oxygen, where the particle number in a reaction chamber is constant, allowing \\(\\mu \\neq 0\\) even at equilibrium.\n\n\n\nHelmholtz free entropy\nWhen we have a small system connected to a large system, while we can solve its equilibria by maximizing the plain old entropy for the full compound system, it is often easier conceptually to define a “free entropy” for the small system, and treat the large system as a bath. This is similar to how one can solve for the motion of a cannonball on earth by describing a constant gravitational acceleration, even though we can could have solved for the full cannonball-earth two-body system.\n\nDefinition 1 (Helmholtz free entropy) The Helmholtz free entropy is the convex dual of entropy with respect to energy:\n\\[\nf(\\beta, X) = \\max_U [S(U, X) - \\beta U]\n\\tag{1}\\]\nwhere \\(U\\) is its internal energy, and \\(X\\) are some other macroscopic properties.\nOf historical importance is the Helmholtz free energy \\(F := - T f\\), or equivalently,\n\\[\nF = \\min_U [U - TS(U, X)]\n\\tag{2}\\]\n\n\nTheorem 2 (maximize Helmholtz free entropy) If a thermodynamic system is in energy-contact with an energy bath with price \\(\\beta\\), and is held under constraint on the state by \\(C(X) = 0\\), then the system equilibrates at\n\\[\n\\begin{cases}\n\\max_{U, X} [S(U, X) - \\beta U] = \\max_{X} f(\\beta, X)\\\\\nC(X) = 0\n\\end{cases}\n\\]\nThat is, the system always equilibrates at the maximal Helmholtz free entropy state that satisfies the constraint.\nEquivalently, the system minimizes its Helmholtz free energy under constraint.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIf we prove the case for Helmholtz free entropy, then multiplying it by \\(-T\\), we find that the system minimizes its Helmholtz free energy under constraint. So it remains to prove the case for Helmholtz free entropy.\nWe prove the case where there is no constraint on the state of the system. The proof for the case with constraint is similar.\nSuppose the system starts out at \\(\\beta, U_0, X\\). Then the equilibrium condition is\n\\[\n\\begin{cases}\n\\max (S_{bath} + S) \\\\\nU_{bath} + U = U_{bath, 0} + U_{0}\n\\end{cases}\n\\]\nThe entropy of the bath is\n\\[S_{bath} = S_{bath, 0} + \\beta Q\\]\nwhere \\(Q = U_{bath} - U_{bath, 0}\\) is the amount of energy received by the bath as heat.\nPlugging this back, the equilibrium condition simplifies to\n\\[\n\\max_U [\\beta (U_0 - U) + S(\\beta, U, X)]\n\\]\nwhich is the desired result.\n\n\n\n\nTheorem 3 (Helmholtz free energy difference is available for work) Consider the following method of extracting mechanical energy. Connect the system to an energy bath at energy price \\(\\beta\\), and a mechanical system of arbitrary design. The system starts at \\(\\beta, U_0, X_0\\) and ends at \\(\\beta, U_1, X_1\\). No matter how the mechanical system is designed, and no matter whether the process is reversible or not, we have\n\\[\nW\\leq F(\\beta, U_0, X_0) - F(\\beta, U_1, X_1)\n\\]\nwhere \\(W\\) is “mechanical work done by the system”, that is, the increase in internal energy of the mechanical system. This is an equality when the process is reversible.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy conservation of energy, and the second law,\n\\[\n\\begin{cases}\nU_1 = U_0 - (W + Q)  \\\\\n\\beta Q + S(U_1, X_1) \\geq S(U_0, X_0)\n\\end{cases}\n\\]\nwhich simplifies to the result.\nIf the process is reversible, then the entropy before and after must be equal, giving us\n\\[\n\\begin{cases}\nU_1 = U_0 - (W + Q)  \\\\\n\\beta Q + S(U_1, X_1) = S(U_0, X_0)\n\\end{cases}\n\\]\nwhich simplifies to the result.\n\n\n\nThis result is typically interpreted as saying that \\(\\Delta F = W\\), that is, in a system in constant thermal equilibrium with an energy bath of constant temperature, the drop in Helmholtz energy of the system is the maximal mechanical work extractable from the system. Incidentally, this explains the odd name of “free energy” – “free” as in “free to do work”, to contrast with the other parts of internal energy, which is chained up and not free to do work.\n\nTheorem 4 (envelop theorem for Helmholtz) For any inverse temperature \\(\\beta &gt; 0\\) and thermodynamic properties \\(X\\), let \\(U^*\\) be the optimal internal energy that maximizes Helmholtz free entropy, then\n\\[\n\\begin{aligned}\n\\beta &= (\\partial_U S)_X|_{U=U^*(\\beta, X), X = X} \\\\\ndf    &=  (\\partial_X S)_U|_{U=U^*(\\beta, X), X = X} dX - U^*(\\beta, X) d\\beta\n\\end{aligned}\n\\]\nif \\(S\\) is differentiable and strictly concave at that point.\n\n\n\n\n\n\n\nProof\n\n\n\nFor the first equation, differentiate \\(f\\). For the second equation, apply the same no-arbitrage proof as in the proof of Hotelling’s lemma (see the essay on Analytical Mechanics).\n\n\nEconomically speaking, the second equation is a special case of the envelop theorem, just like Hotelling’s lemma.\n\n\nFirst-order phase transition\nWhat happens if \\(S\\) is not differentiable and strictly concave? In this case, we do not have \\(\\beta = (\\partial_U S)_X\\). We have two possibilities.\nThe first possibility is pictured as follows. There is a kink in the curve of \\(S(U, X)\\). At that point of critical internal energy \\(U_c\\), there is an entire interval of possible \\(\\beta\\). What we would notice is that at that critical internal energy and critical entropy, the system can be in equilibrium with any heat bath with any temperature between \\([T_{c, min}, T_{c, max}]\\). As far as I know, such systems do not exist, as all physically real systems have a unique temperature at all possible states.\n\n\n\nWhen there is a kink in the curve of \\(S(U, X)\\), the system has indeterminate temperature.\n\n\nThe second possibility is pictured as follows. There is a bump in the curve, such that we can draw a double tangent over the bump, with slope \\(\\beta_c\\). At that critical inverse temperature, the system can be either at the lower tangent point, or the upper tangent point. It cannot be anywhere in-between, because as we saw, such points do not minimize \\(f(\\beta, X)\\), and thus are unstable.\n\n\n\nWhen there is a double tangent in the curve of \\(S(U, X)\\), the system has a first-order phase transition with latent entropy and energy.\n\n\nFor example, if we confine some liquid water in a vacuum chamber, and bathe it in a cold bath, then at its critical \\(\\beta_c\\), it would split into two parts, one part is all ice, and the other part is all water, mixed in just the right proportion to give it the correct amount of total internal energy. As it loses internal energy, the ice part grows larger, until it is all ice, at which point the system has finally gotten over the bump, and could cool down further.\nAt the critical point, \\((\\partial_{\\beta}f)_X\\) abruptly changes. So if we plot \\(\\beta \\mapsto f(\\beta, X)\\), the curve would kink there.\n\nTheorem 5 (Maxwell equal area rule) In a first-order phase transition at a fixed temperature and varying pressure/volume, the \\(P, V\\) diagram has a horizontal line going from \\((P_c, V_1)\\) to \\((P_c, V_2)\\), such that\n\\[\\int PdV = P_c(V_2-V_1)\\]\n\n\n\nMaxwell’s equal area rule states that the area of the regions labelled I and II are equal.\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFix the system’s internal energy \\(U\\), and its temperature \\(T\\), and plot the \\(V, S\\) curve.\nWhen pressure is at a critical value \\(P_c\\), the line of slope \\(\\beta P_c\\) is tangent to the \\(V \\mapsto S(U, V)\\) curve at two different points, with volumes \\(V_1, V_2\\). This is that first-order phase transition.\nNow, move the system state from the first point to the second. During the process, \\[\\int PdV = \\int (TdS -dU)  = \\int TdS = T \\Delta S = T \\beta P_c(V_2 - V_1) = P_c(V_2-V_1)\\]\n\n\n\n\n\nOther free energies\nConsider a thermodynamic system whose entropy function is \\(S(U, V, X)\\), where \\(U\\) is the internal energy and \\(V\\) is the volume. Its Gibbs free entropy is\n\\[\ng(\\beta, \\beta P, X) = \\max_{U, V} (S(U, V, X) - \\beta U - (\\beta P)V)\n\\]\nIn other words, it’s the convex dual of entropy with respect to energy and volume. Similarly, its Gibbs free energy is \\(G = -g/\\beta\\).\n\nTheorem 6 (Gibbs free entropy is maximized) Let a thermodynamic system be in equilibrium with an energy-and-volume bath of prices \\(\\beta, \\beta P\\). If the system has constraint on the state by \\(C(X) = 0\\), then the system equilibrates at\n\\[\n\\begin{cases}\n\\max_{X} g(\\beta, \\beta P, X)\\\\\nC(X) = 0\n\\end{cases}\n\\]\nAnd if \\(S\\) is strictly concave and differentiable at that point, then \\(dg =  (\\partial_X S)_{U, V} dX - Ud\\beta - V d(\\beta P)\\).\n\n\nTheorem 7 (Gibbs free energy difference is available for work) Connect a system to an energy-and-volume bath at marginal entropies \\(\\beta, \\beta P\\), and a mechanical system of arbitrary design. The system starts at \\(\\beta, \\beta P, X_0\\) and ends at \\(\\beta, \\beta P, X_1\\). Then,\n\\[\nW\\leq G(\\beta, \\beta P, X_0) - G(\\beta, \\beta P, X_1)\n\\]\nwhere \\(W\\) is “work”, that is, the increase in internal energy of the mechanical system. If the process is reversible, then equality holds.\n\n\nTheorem 8 (envelop theorem for Gibbs) For any inverse temperature \\(\\beta &gt; 0\\), any pressure \\(P\\), and other thermodynamic properties \\(X\\), let \\(U^*, V^*\\) be the optimal internal energy and volume that maximizes Gibbs free entropy, then\n\\[\n\\begin{aligned}\n\\beta &= (\\partial_U S)_{V, X} \\\\\n\\beta P &= (\\partial_V S)_{U, X}\\\\\ndg    &=  (\\partial_X S)_{U, V} dX - U^* d\\beta - V^* d(\\beta P)\n\\end{aligned}\n\\]\nif \\(S\\) is differentiable and strictly concave at that point. Here, the left sides of all equations are evaluated at \\(U=U^*(\\beta, \\beta P, X), V=V^*(\\beta, \\beta P, X), X = X\\).\n\n\nExercise 2 Prove the above theorems for Gibbs free entropy.\n\nSimilarly, if a system is in energy-volume-chemical contact with an energy-volume-chemical bath, then the following Landau free entropy is useful:\n\\[\n\\omega(\\beta, \\beta P, -\\beta\\mu_1, \\dots, -\\beta\\mu_n) = \\max_{U, V, N_1, \\dots, N_n} \\left(S(U, V, N_1, \\dots, N_n, X) - \\beta U - (\\beta P)V - \\sum_{i=1}^n (-\\beta \\mu_i) N_i \\right)\n\\]\nIn other words, it’s the convex dual of entropy with respect to energy, volume, and particle numbers.\n\nExercise 3 Formulate and prove the analogous theorems for Landau free entropy.\n\n\nExercise 4 If we consider a system, surrounded by gas, inside an adiathermal piston under an atmosphere, then we can consider the following form of free energy: \\(\\tilde s(U, \\beta P, X) := \\max_{V} (S(U, V, X) - \\beta U - (\\beta P)V)\\). Formulate and prove the analogous theorems for this free entropy.\n\nTake-home lessons:\n\nWhen a system is in contact with a \\(Y\\)-bath, then it is useful to consider the convex dual of the entropy with respect to \\(Y\\), that is, \\(\\max_Y (S(Y, X) - p_Y Y)\\), where \\(p_Y\\) is the marginal entropy of \\(Y\\) of the bath.\nFree entropy is maximized when the system equilibrates with a bath. Free energy is minimized.\nChange in free energy is the maximal amount of work extractable when the system equilibrates with both a bath and a mechanical system. This work is actually extracted when the process is reversible.\n\n\n\nMaxwell relations\nSince \\(dS = \\beta dU + \\beta P dV\\), we have the first Maxwell relation\n\\[\n\\partial_U \\partial_V S = (\\partial_V \\beta)_U = (\\partial_U(\\beta P) )_V\n\\tag{3}\\]\nIn economic language, it states\n\\[\n\\partial_{q_i}\\partial_{q_j} S = (\\partial_{q_i} p_j)_{q_j} = (\\partial_{q_j} p_i)_{q_i}\n\\tag{4}\\]\nwhere \\(q_i\\) is the quantity of commodity \\(i\\), and \\(p_i\\) is its marginal utility. In economics, we usually prefer writing demanded quantity as a function of marginal price as\n\\[(\\partial_{p_j}q_i)_{q_j} = (\\partial_{p_i}q_j)_{q_i}\\]\nThis is a symmetry of cross-price elasticity of demand.4\n4 Samuelson used the Maxwell relations, and other relations, to justify neoclassical economics. His idea is that, while utility functions are unobservable, and we do not have a scientific instrument to measure “economic equilibrium”, we can make falsifiable predictions from assuming that the economy is in equilibrium – such as the symmetry of the cross-price elasticity of demand.For example, if \\(i, j\\) are noodles and bread, then \\((\\partial_{q_i} p_j)_{q_j}\\) is how much the marginal price of bread would rise if I have a little more noodle. As noodles and bread are substitutional goods, we expect the number to be negative, meaning that having more noodles, I would price bread less. The Maxwell relation then tells us that it is exactly the same in the other direction: If I am given a little more bread, I would price noodles less. Not only that, I would want less by exactly the same amount.\nThe second relation is a bit hard to explain, since enthalpy really does not have a good representation in entropy-centric thermodynamics. However, it turns out to be just the third relation with \\(i, j\\) switched.\nSince \\(df = \\beta P dV - Ud\\beta\\), we have the third Maxwell relation\n\\[\n\\partial_\\beta\\partial_V f = -(\\partial_V U)_\\beta = +(\\partial_\\beta(\\beta P))_V\n\\tag{5}\\]\nIn economic language, we have\n\\[\n\\partial_{p_i}\\partial_{q_j} \\left[\\max_{q_i}(S(q) - p_i q_i )\\right]= -(\\partial_{q_j} q_i)_{p_i} = (\\partial_{p_i}p_j)_{q_j}\n\\tag{6}\\]\nContinuing from the previous example, if \\(i, j\\) are noodles and bread, and we open a shop with an infinite amount of noodles always at the exact price, then I would of course buy and sell from the noodle shop until my marginal price of noodles is equal to the shop’s price. Now, \\((\\partial_{q_j} q_i)_{p_i}\\) is how much noodles I would buy if I am given a marginal unit of bread. As noodles and bread are substitutional goods, this number is negative. This then means \\((\\partial_{p_i}p_j)_{q_j} &gt; 0\\), meaning that if the noodle price suddenly increases a bit, then I would sell a bit of noodles until I have reached equilibrium again. At that equilibrium, since I have less noodles, I would price higher its substitutional good, bread, by an equal amount as the previous scenario.\nSince \\(dg = -Ud\\beta - Vd(\\beta P)\\), we have the fourth Maxwell relation\n\\[\n-\\partial_{\\beta}\\partial_{\\beta P}g = (\\partial_{\\beta P}U)_\\beta = (\\partial_\\beta V)_{\\beta P}\n\\tag{7}\\]\nIn economic language, we have\n\\[\n\\partial_{p_i}\\partial_{p_j} \\left[\\max_{q_i, q_j}(S(q) - p_i q_i - p_j q_j)\\right]= -(\\partial_{p_j} q_i)_{p_i}= -(\\partial_{p_i} q_j)_{p_j}\n\\tag{8}\\]\nThis is another symmetry of cross-price elasticity of demand.\nContinuing from the previous example, if \\(i, j\\) are noodles and bread, and we open a shop with an infinite amount of noodles and bread, then I would of course buy and sell from the shop until my marginal prices of noodles and bread are equal to the shop’s prices. Now, if the shop suddenly raises the price of bread by a small amount, I would sell off some bread until my marginal price for bread increases to the shop’s new price. Now my marginal price for noodles increases too by substitutional effect, so I buy some noodles. Thus \\((\\partial_{p_j} q_i)_{p_i} &gt; 0\\). Switching the scenario, we find that raising the price of noodles would make me buy bread, by an equal amount as the previous scenario.\n\n\n\n\n\n\nAlternate proof of the Maxwell relations\n\n\n\n\n\nWe use the notation of economics here.\nSuppose we have commodities \\(1, 2, \\dots, n\\). We pick two commodities \\(i, j\\), and fix all other commodity quantities. Thus, we can write \\(dS = p_i dq_i + p_j d q_j\\).\nSince knowing \\(n\\) properties of the thermodynamic system allows us to know its exact state, and we have already fixed \\(n-2\\) properties of it, there only remain two more degrees of freedom. We can parameterize this by \\((q_i, q_j)\\), or \\((p_i, q_j)\\), or \\((p_i, p_j)\\), or any other reasonable coordinate system.\nIf the thermodynamic system undergoes a cycle, then\n\\[0 = \\oint dS = \\oint p_i dq_i + \\oint p_j dq_j\\]\nand thus, if we take the cycle infinitesimally small, we find that \\(dp_i\\wedge dq_i = -dp_j \\wedge dq_j\\). That is, the map \\((p_i, q_i) \\mapsto (p_j, q_j)\\) preserves areas, but reverses orientation. In particular, we have a Jacobian \\[\n\\frac{\\partial(p_i, q_i)}{\\partial(p_j, q_j)} = -1\n\\]\nNow, let \\((x, y)\\) be an arbitrary coordinate transform. By the chain rule for Jacobians, \\[\n\\frac{\\partial(p_i, q_i)}{\\partial(x, y)} = \\frac{\\partial(p_i, q_i)}{\\partial(p_j, q_j)} \\frac{\\partial(p_j, q_j)}{\\partial(x,y)}  = -\\frac{\\partial(p_j, q_j)}{\\partial(x,y)}\n\\]\nThis allows us to derive all the Maxwell relations. For example, setting \\((x, y) = (p_i, q_j)\\) gives us the third relation\n\\[(\\partial_{q_j} q_i)_{p_i} = -(\\partial_{p_i}p_j)_{q_j}\\]\n\n\n\n\n\n\n\n\n\nDeriving the four Maxwell relations by picking the right variables for ."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#caratheodorys-thermodynamics",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#caratheodorys-thermodynamics",
    "title": "Classical thermodynamics and economics",
    "section": "Caratheodory’s thermodynamics",
    "text": "Caratheodory’s thermodynamics\nIn the early 1900s, Caratheodory discovered a new way to “geometrize” thermodynamics, with the austere beauty of Euclidean geometry. Though his formulation fell into obscurity, it was reborn in neoclassical economics as utility theory.\n\nEntropy and temperature\nConsider a thermodynamic system with \\(n+1\\) dimensions of state space. Give the state space coordinates \\(q_0, q_1, \\dots, q_n\\). For example, for a tank of ideal gas where the particle number is fixed, we have \\(q_0 = U, q_1 = V\\).\nLet the system be at a certain state \\(\\vec q\\), and wrap the system in a perfectly insulating (adiathermal) blanket. The system can undergo many different kinds of adiathermal motion, but there are certain motions that it cannot undergo.\nFor example, for a piston of ideal gas, the possible motions are adiabatic expansion, adiabatic compression, Joule expansion, and any combination of them. However, “Joule compression” is impossible – the gas will not spontaneously contract to the left half of the system, pulling in the piston head, anymore than a messy room will spontaneously tidy itself.\nWe say that \\(\\vec q'\\) is adiathermally accessible from \\(\\vec q\\) if there exists a path from \\(\\vec q\\) to \\(\\vec q'\\), such that the path is infinitesimally adiathermal5 at every point.\n5 The word “adiathermal” means “heat does not pass through”, while “adiabatic” has an entire history of meaning that makes it hard to say what exactly it is (see my essay on Analytical Mechanics). Personally, I think “adiabatic” means “zero entropy change”, and all its other meanings derive from it.\n\n\nAdiabatic accessibility in the \\(U, V\\) diagram.\n\n\nHere is Caratheodory’s version of the second law of thermodynamics:\n\nIn any neighborhood of any point \\(\\vec q\\), there are points adiabatically inaccessible from it.\nFurthermore, for any two points, \\(\\vec q, \\vec q'\\), at least one of them is adiabatically accessible from the other.\n\nThe effect of these two axioms is that we can define a total ordering \\(\\preceq\\) on state space, where we write \\(\\vec q \\preceq \\vec q'\\) to mean that \\(\\vec q\\) can adiabatically access \\(\\vec q'\\), and write \\(\\vec q \\sim \\vec q'\\) to mean that they are mutually adiabatically accessible.\nInterpreted economically, we say that the system is an economic agent, each \\(\\vec q\\) is a bundle of goods, and \\(\\vec q \\preceq \\vec q'\\) means that \\(\\vec q'\\) is preferable to the agent, and that \\(\\vec q \\sim \\vec q'\\) means they are equally preferred.\nThe total ordering partitions the state space into contour surfaces of equal accessibility, or indifference surfaces. Assuming the state space is not designed to be pathological, these indifference surfaces will be differentiable.\nLet us consider the indifference surface passing state \\(\\vec q\\). The indifference surface is locally a plane, so it has equations\n\\[\ndq_0 - \\sum_i \\tilde p_i dq_i = 0\n\\]\nwhere \\(\\tilde p_i = (\\partial_{q_i}q_0)_{q_1, \\dots, q_n}\\). For example, a tank of (non-ideal) gas satisfies \\(dU + PdV = 0\\) over its indifference curves.\n\n\n\nThe field of planes defined by \\(dq_0 - \\sum_i \\tilde p_i dq_i = 0\\).\n\n\n\nTheorem 9 (existence and uniqueness of temperature and entropy) Let \\(\\omega = dq_0 - \\sum_i \\tilde p_i dq_i\\). If \\(\\omega\\) is nonzero everywhere, then there exists functions \\(\\beta, S\\) on the state space, such that \\(dS = \\beta \\omega\\).\nFurthermore, they are unique up to a monotonic transform. That is, if we have another solution \\(\\beta', S'\\), then there exists a strictly monotonic function \\(f\\) such that \\(S' = f \\circ S\\).\n\n\n\n\n\n\n\nProof\n\n\n\nAt each point \\(P\\) the one-form \\(\\omega(p)\\) is visualized as a stack of parallel planes. The planes are quilted together, but with “uneven thickness”. By scaling the one-forms just right at every point, the thickness becomes equalized, and so \\(\\beta \\omega = dg\\) for two real-valued functions \\(\\beta, S\\).\n\n\n\n\n\n\nGiven any other solution \\(\\beta', S'\\), both \\(S\\) and \\(S'\\) must have the same contour lines, so there exists some function that maps the \\(S\\)-height of a contour line to its \\(S'\\)-height.\n\n\nProving Caratheodory’s theorem. Figure from .\n\nCardinal and ordinal utilities\nEconomically speaking, \\(\\tilde p_i\\) is the marginal worth of \\(q_i\\) denoted in units of \\(q_0\\). For example, we can say that \\(q_0\\) are cowry shells, which themselves are pretty and give us some utility. However, it can also be used as a monetary unit. Then, if \\(i\\) is bread, then \\(\\tilde p_i\\) is the marginal amount of cowry shells that we would pay for a marginal amount of bread.\nIf we were to visit a free market where we can buy and sell items denoted in cowry shells, then we would buy bread if \\(\\tilde p_i &gt; \\tilde p_{i, market}\\), and sell bread if \\(\\tilde p_i &lt; \\tilde p_{i, market}\\). Right at the border of \\(\\tilde p_i = \\tilde p_{i, market}\\), we would be indifferent about buying or selling bread. When \\(\\tilde p_i = \\tilde p_{i, market}\\) for all \\(i\\), we would be completely indifferent about the market.\n\\(S\\) is the utility, and \\(\\beta\\) is the marginal utility of cowry shells. The theorem tells us that just by knowing how we order the goods (” \\(S(\\vec q) &gt; S(\\vec q')\\) “), we can extract a numerical value for the goods (” \\(S(\\vec q) - S(\\vec q') = 1.34(S(\\vec q'') - S(\\vec q'''))\\) “). Out of ordinal utility, we have achieved cardinal utility.\nThere used to be a debate between “ordinalists” and “cardinalists” of utility theory. The “cardinalists” were the more venerable of the two camps, tracing back to Bentham’s felicific calculus and the marginalist revolution. They argued that utility is real-valued, like entropy and temperature. The “ordinalists” countered that a nobody has ever measured a utility in anyone’s brain. The only thing we can observe is preferences: I prefer this over that – I can order everything that can ever happen to me on a numberless line of preferences. Similarly, nobody can ever actually measure temperature or entropy, only that energy flows from this gas to that gas, which presumably has lower temperature, and that one chunk of gas in one state ends up in another state, which presumably has higher entropy.\nThe debate has been mostly resolved by the work of Gérard Debreu, who showed that under fairly reasonable assumptions, cardinal utility is possible (Debreu 1971).6\n6 Out of all those famous economists I have seen, Gérard Debreu is perhaps the most mathematically austere. Reading his works, I felt like he was another G. H. Hardy, a Bourbaki of economics. He did economics not to improve the world, not to help people, and not to advance a political agenda, but to simply uncover a face of eternity.This theorem, or rather, this family of theorems, have several names, as befitting for such a versatile and productive family. In calculus, it’s called the integrability of Pfaffian forms. In differential geometry, it’s called Darboux’s theorem, or Frobenius theorem. In economics, it’s called the integrability of demand, or the cardinal-ordinal utility representation theorem.\n\n\n\n\n\n\nWhat’s so special about energy, or cowry shells?\n\n\n\nWhen cast in the language of economics, cowry shells are not special. We could denote prices in cowry shells, or cans of sardine, or grams of gold. That is, we are free to pick any numéraire we want, as long as we are consistent about it.\nSimilarly, energy is not special. For example, with ideal gas, we could write the first law of thermodynamics as the conservation of energy, like\n\\[dU - (-P)dV = \\beta^{-1}dS\\]\nor as the conservation of volume, like\n\\[dV - (-P^{-1})dU= (\\beta P)^{-1}dS\\]\nand from the perspective of classical thermodynamics, there is no possibility of saying that energy is more special than volume. Energy is exactly as special as volume, and no more special than that.\nWhen I realized this difference, I was so incensed at this mistake that I wrote an entire sci-fi worldbuilding sketch about an alien species, for which it is the conservation of volume that is fundamental, not energy, and which discovered stereodynamics, not thermodynamics.\n\n\n\n\nExtensive entropy\nWhile we have constructed the temperature \\(T\\) and the entropy \\(S\\) of an isolated system, it is not unique: we can stretch and compress the entropy function \\(S\\) arbitrarily by a monotonic function, and as long as we modify the temperature function \\(T\\) just right, the two modifications cancel out, and we have \\(TdS = T' dS'\\).\nIn order to uniquely fix an entropy function, we need further assumptions. The most commonly used method is by considering what happens to the entropy of a compound system. In general, there is no reason to expect entropy to be extensive – if we take compound two systems together, the entropy of the compound system should be the sum of the two subsystems. However, if we make some assumptions on “rationality”, then entropy would be uniquely fixed, and would be extensive.\n\nExercise 5 (von Neumann–Morgenstern entropy construction) Study the statement of the von Neumann–Morgenstern utility theorem, and translate it to thermodynamics. It should be of the following form:\nAssuming that the adiabatic accessibility of any compound system satisfies the following properties\n\n…\n…\n…\n\nthen the entropy of any compound system is the sum of the entropies of its subsystems, and the entropy function is unique up to adding a constant and multiplying by a positive scalar.\n\nSome hints:\n\nIt might help your intuition if you anthropomorphize Nature as a “vNM-rational agent”.\nThe standard formulation of the vNM theorem uses lotteries of form \\(pM + (1-p)N\\), where \\(p\\in (0, 1)\\) is a probability, and \\(M, N\\) are bundles of goods. However, it is impossible generally to “take \\(0.37\\) of a system \\(M\\) and compound it with \\(0.63\\) of system \\(N\\)”. To bypass this difficulty, replace that with “take \\(37\\) copies of system \\(M\\) and compound them with \\(63\\) copies of system \\(N\\)”."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#geometric-thermodynamics",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#geometric-thermodynamics",
    "title": "Classical thermodynamics and economics",
    "section": "Geometric thermodynamics",
    "text": "Geometric thermodynamics\n\nAlthough geometrical representations of propositions in the thermodynamics of fluids are in general use, and have done good service in disseminating clear notions in this science, yet they have by no means received the extension in respect to variety and generality of which they are capable.\n(Gibbs 1957)\n\n\nContact geometry\n\nEvery mathematician knows it is impossible to understand an elementary course in thermodynamics. The reason is that thermodynamics is based—as Gibbs has explicitly proclaimed – on a rather complicated mathematical theory, on the contact geometry. Contact geometry is one of the few ‘simple geometries’ of the so-called Cartan’s list, but it is still mostly unknown to the physicist – unlike the Riemannian geometry and the symplectic or Poisson geometries, whose fundamental role in physics is today generally accepted.\nV.I. Arnol’d (Caldi et al. 1990, 163)\n\nTo explain this mysterious remark, we take a plunge into abstraction. We know that a real gas has properties \\(P, V, T, S, \\dots\\), and that they satisfy the differential equation:\n\\[\ndS = \\beta dU + \\beta PdV\n\\]\nTo clean up the notation, we can change the notation to\n\\[\ndS = p_1 dq_1 + p_2 dq_2\n\\]\nThis formula has a clear interpretation in economics: if the marginal utility of commodity \\(1\\) is \\(p_1\\), and the marginal utility of commodity \\(2\\) is \\(p_2\\), then if we receive \\(\\delta q_1, \\delta q_2\\), our utility would increase by \\(p_1 \\delta q_1 + p_2 \\delta q_2\\).\nThe difficult thing about classical thermodynamics is that there are so many quantities (temperature, volume, pressure…). The saving grace is that it turns out that there are only a few degrees of freedom.7\n7 The Parthian shot is that now you are burdened with dozens of equations relating these quantities. I cannot remember any of the Maxwell relations, so I look at Wikipedia every time I need to calculate with them.However, why is it that a macroscopic lump of matter, whirling with \\(10^{23}\\) molecules, turn out to be characterized by only a few degrees of freedom? Why is it that a national economy, swarming with \\(10^8\\) people, have macroeconomic laws? For the first question, the answer is given by classical thermodynamics: the lump of matter is maximizing its entropy under constraint, so its degrees of freedom are exactly as many as the number of constraints it is laboring under. For the second question, the answer is given by neoclassical economics: the national economy behaves as if it is maximizing a social utility function under its resource constraints.\nWith that brief look at philosophy, we return to abstract thermodynamics. We have a lump of matter (such as ideal gas in a piston), of which we can measure five different properties: \\(p_1, p_2, q_1, q_2, S\\). In general, we expect that the space of possible measurements is 5-dimensional, but it turns out that they collapse down to a 2-dimensional curved surface. This is why we could completely fix its state knowing just its \\(V, U\\), or just its \\(P, T\\), etc.\nThe question now comes: Why is it possible to collapse things down to this curved surface?\nThings are already interesting when we have just one commodity:\n\\[dS - pdq = 0\\]\nand we ask: Why is it possible to collapse the space of \\((q, p, S)\\) from 3 to 1 dimension?\nIn modern geometry, an expression like \\(dS - \\sum_i p_i dq_i = 0\\) defines a field of planes in \\(\\R^3\\). That is, at each point \\((q, p, S)\\), we construct a plane defined by\n\\[\n\\{(q + \\delta q, p + \\delta p, S + \\delta S): \\delta S - p \\delta q = 0\\}\n\\]\nFor example, in three dimensions, the field of planes \\(dS - pdq = 0\\) would look like it is constantly twisting as \\(p\\) increases. The study of geometric structures definable via the field of planes is contact geometry.8\n8 This explanation might sound like an anticlimax, and I imagine someone would object “Thermodynamics is reduced to contact geometry… but what is contact geometry?”. My answer is that contact geometry simply is. It is not supposed to be a generator of intuitions. Instead, it is a common language that bridges between intuitions. By casting thermodynamics, economics, mechanics, etc, into the language of contact geometry, we would then be able to translate intuition from one field to another field. Saying that “thermodynamics is contact geometry” is not telling you an intuitive way to see thermodynamics, but rather, an intuitive way to see contact geometry.\n\n\nThe field of planes \\(dS - pdq = 0\\) in \\(\\R^{2+1}\\). Figure from Wikipedia.\n\n\nGiven such a field of planes \\(dS - \\sum_{i=1}^n p_i dq_i\\) in \\(\\R^{2n+1}\\), we say that a manifold is a Legendrian submanifold iff the manifold has \\(n\\) dimensions, and is tangent to the field of planes at every point.\nFor example, when \\(n=1\\), a Legendrian submanifold is a curve that winds around \\(\\R^3\\) that is always tangent to the plane at every moment.\nLet \\(S(q)\\) be a differentiable function. We can interpret \\(S(q)\\) as how much money we can earn if we produce something using the bundle of raw materials \\((q_1, \\dots, q_n)\\).\nGiven any market price for the raw materials, \\(q^* = \\argmax_q (S(q) - \\braket{p, q})\\) is the profit-maximizing production plan, and \\(\\Pi(p) = \\max_q (S(q) - \\braket{p, q})\\) is the profit.\n\nTheorem 10 The “profit-maximization surface” defined by \\(p \\mapsto (q^*, p, S(q^*))\\) is a Legendrian submanifold.\nConversely, given any Legendrian submanifold parameterized by \\(p \\mapsto (q(p), p, S(q(p)) )\\), then \\(q(p)\\) is profit-stationarizing. That is,\n\\[\\nabla_q (S(q) - \\braket{p, q}) = 0\\]\nat \\(q(p)\\). If \\(S\\) is strictly concave, then \\(q(p)\\) is profit-maximizing.\n\n\n\n\n\n\n\nProof\n\n\n\nThe first part is proven by Hotelling’s lemma.\nThe second part is proven by plugging in \\(dS - \\sum_i p_i dq_i = 0\\). And if \\(S\\) is strictly concave, then \\(q \\mapsto S(q) - \\braket{p, q}\\) is also strictly concave, and so zero gradient implies global maximum.\n\n\nEconomically speaking, \\(\\max_q (S(q) - \\braket{p, q})\\) means to maximize profit. What does it mean thermodynamically speaking? It means minimizing \\(\\braket{p, q} - S(q)\\), which is the Gibbs free entropy! Maximizing profit when a factory has access to a market is the same as minimizing Gibbs free entropy when a system is in contact with a bath.\n\n\nSamuelson’s area-ratio thermodynamics\n\nPhilosophy\nIn Paul Samuelson’s Nobel prize lecture of 1970, among comments of classical mechanics, variational principles, and neoclassical economics, he said something curious about the analogy between classical thermodynamics and neoclassical economics:\n\nHowever, if you look upon the monopolistic firm hiring 99 inputs as an example of a maximum system, you can connect up its structural relations with those that prevail for an entropy-maximizing thermodynamic system. Pressure and volume, and for that matter absolute temperature and entropy, have to each other the same conjugate or dualistic relation that the wage rate has to labor or the land rent has to acres of land. Figure 2 can now do double duty, depicting the economic relationships as well as the thermodynamic ones.\nIf someone challenged me to explain what the existence of [utility] implies, but refused to let me use the language of partial derivatives, I could illustrate by an equi-proportional area property… I may say that the idea for this proposition in economics came to me in connection with some amateurish researches in the field of thermodynamics. While reading Clerk Maxwell’s charming introduction to thermodynamics…\n(Samuelson 1971)\n\n\n\n\nSamuelson’s equal area ratio condition. In the diagram, we have \\(a:b = c:d\\)\n\n\nThis intriguing little remark piqued my interest, and after a little digging, I figured it out.9\n9 Based on research by James Bell Cooper, who seems to be the world expert in this obscure field (Cooper and Russell 2006; Cooper, Russell, and Samuelson 2001).Samuelson is alluding to a deep problem in economics theory: Nobody has ever seen a utility function, anymore than anybody has ever seen an entropy. If this is the case, then how do we know that agents are maximizing a utility, or that systems are maximizing an entropy? In his long career, he searched for many ways to answer this, coming down to the idea that, even though we cannot measure utility, we can measure many things, such as how firms respond to prices. Given some measurable quantities, we can then prove, mathematically, that something is being maximized. At that point, we can simply call that something “utility”, and continue doing economics as usual.\nPhilosophically, Samuelson was greatly influenced by operationalism, a philosophy of science akin to logical positivism. As stated by the definitive work on operationalism, “we mean by any concept nothing more than a set of operations; the concept is synonymous with the corresponding set of operations” (Bridgman 1927).\nIn his early work, particularly Foundations of Economic Analysis (1947) and the development of revealed preference theory (1938), Samuelson embraced operationalism as a means of purging economics of non-observable, and thus scientifically meaningless, concepts like utility. He sought to rebase economic theory on purely observable behavior and measurable quantities. Revealed preference theory, for instance, would eliminate utility functions, and derive a consumer’s preference ordering directly from observable consumer choices at different price levels.\nOver time, Samuelson’s stance on operationalism softened to a more pragmatic approach, recognizing the value of unobservable concepts as theoretical tools, as long as they can be based on direct observables.\nFor example, while Samuelson initially sought to eliminate utility functions, he later argued that even if utility is not directly observable, it can be uniquely determined from observing agents’ preferences, by invoking some utility representation theorems – provided that the preferences satisfy certain properties. (Samuelson 1999)\n\n\nArea ratio construction\n\nTheorem 11 (area ratio law implies a new coordinate system) Consider an open rectangle \\(R\\) in the plane \\(\\R^2\\). Let there be two families of curves\nSuppose that each curvy parallelogram formed by the two families is contained in \\(R\\), and the curves satisfy the area-ratio rule, then we can define another coordinate system \\((z, w)\\) on \\(\\R^2\\), such that the coordinate system preserves areas: \\[\ndx \\wedge dy = dz \\wedge dw\n\\]\nand that the two families of lines are the constant \\(z\\) and constant \\(w\\) curves.\nThe coordinate system is unique up to an affine squashing transform, that is, \\((z, w) \\mapsto (cz + d, w/c + e)\\) for some constants \\(c, d, e\\) with \\(c \\neq 0\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFix an arbitrary point as \\((z, w) = (0, 0)\\). Pick an arbitrary curve in one of the families, not passing \\((0, 0)\\) point, and call it the \\(w=1\\) line. By continuity, there exists a unique curve in the other family, such that their curved parallelogram has unit area. Call that other curve the \\(z=1\\) line. Now we can label its four corners as \\((z, w) = (0, 0), (0, 1), (1, 0), (1, 1)\\).\nFor any other point, its \\((z, w)\\) coordinates can be constructed as shown in the picture, with\n\\[z = a+b, \\quad w = b+d\\]\nBy the area ratio law, we have \\(a:b = c:d\\). We also have \\(a+b+c+d = 1\\) since we picked the parallelogram to have unit area. Solving these 4 equations, we find that \\(b = zw, a = z(1-w), d = (1-z)w, c = (1-z)(1-w)\\), as it should.\nTaking the derivative, we have \\(dx \\wedge dy = dz \\wedge dw\\).\n\n\n\n\n\nCorollary 1 (area ratio law implies existence of an entropy function) Since \\(dx \\wedge dy = dz \\wedge dw\\), we can draw any cycle \\(\\gamma\\), and integrate around the cycle: \\[\\oint_\\gamma (ydx + zdw) = \\iint_{\\text{area in }\\gamma} (dy \\wedge dx + dz \\wedge dw) = 0\\]\nThus, there exists some scalar function \\(S\\), such that \\(dS = ydx + zdw\\).\n\nAnd with an entropy/utility function, all of classical thermodynamics/neoclassical economics follows.\n\n\nExample: ideal gas\nSuppose we know from experiment that a tank of ideal gas satisfies two equations\n\\[PV = Const, \\quad PV^\\gamma = Const\\]\nunder isothermal and adiabatic conditions respectively, then the \\(P, V\\) diagram with these two families of curves satisfy the area ratio condition. It is tedious but straightforward to verify this by direct integration. Alternatively, we can use the method of exhaustion and Eudoxus’ theory of proportions to prove this, in a way that even ancient Greeks would approve.\n\n\n\n\n\n\nProof\n\n\n\n\n\nNotice that under the squashing map \\((P, V) \\mapsto (cP, V/c)\\), both families of lines are preserved, and furthermore, this map preserves area, so we can calculate the area of any curvy parallelogram by tiling it with tiny strips of thin parallelograms.\nAs shown, we can draw a very thin parallelogram \\(\\delta\\), then use the squashing map to tile both parallelograms \\(c\\) and \\(d\\). We have that\n\\[A(c) : A(d) = \\frac{A(c)}{A(\\delta)} : \\frac{A(d)}{A(\\delta)} \\approx N(c) : N(d)\\]\nwhere \\(A(c)\\) is the area of \\(c\\), and \\(N(c)\\) is the number of copies of \\(\\delta\\) that is contained within \\(c\\). By the method of exhaustion and Eudoxus’ theory of proportion, at the limit of infinitely thin \\(\\delta\\), both sides are equal.\nNow, performing the same construction on the other half of the parallelograms, we tile \\(a, b\\) by the same number of copies of \\(\\delta'\\). Thus we have\n\\[A(c) : A(d) \\approx N(c) : N(d) = N(a) : N(b) \\approx A(a) : A(b)\\]\nand both sides equal at the limit.\n\n\n\n\nNow, by the area ratio construction, there exists two functions \\(f_T, f_S\\), such that the new coordinates\n\\[T(P, V) = f_T(PV), \\quad S(P, V) = f_S(PV^\\gamma)\\]\nsatisfy \\(dT \\wedge dS = dP \\wedge dV\\). We can then define\n\\[dU = TdS - PdV\\]\nwhich satisfies \\(d^2 U = 0\\), that is, it is integrable.\n\n\n\n\n\n\nDerivation\n\n\n\n\n\nSimplifying, we get\n\\[f'_T(PV)f_S'(PV^\\gamma) = \\frac{1}{(\\gamma - 1) PV^\\gamma}\\]\nLet \\(x = PV, y = PV^\\gamma\\), we get a separation of variables: \\(f'_T(x) f'_S(y) = \\frac{1}{(\\gamma-1) y}\\), which solves to\n\\[T = C_1 PV + C_0, \\quad S = \\frac{1}{(\\gamma-1) C_1} \\ln \\frac{PV^\\gamma}{P_0V_0^\\gamma}\\]\nfor some constants \\(C_0, C_1, P_0, P_0\\).\nKnowing that \\(C_0 = 0, C_1 = 1/(nR)\\), we have the equations of state: \\[PV = nRT, \\quad S = \\frac{1}{\\gamma-1} nR \\ln \\frac{PV^\\gamma}{P_0V_0^\\gamma}\\]\nIntegrating \\(dU = TdS - PdV\\), we have \\(U = \\frac{1}{\\gamma-1} nRT\\). We can define \\(\\hat c_V = \\frac{1}{\\gamma - 1}\\), which leads to\n\\[PV = nRT, \\quad S = \\hat c_V n R \\ln \\frac{PV^{1 + 1/\\hat c_V}}{P_0V_0^{1 + 1/\\hat c_V}}, \\quad U = \\hat c_V nRT\\]\nor equivalently, \\(S = n R \\ln \\frac{U^{\\hat c_V}V}{U_0^{\\hat c_V} V_0}\\).\n\n\n\nThus, we have\n\\[PV = nRT, \\quad S = \\hat c_V n R \\ln \\frac{PV^{1 + 1/\\hat c_V}}{P_0V_0^{1 + 1/\\hat c_V}} = n R \\ln \\frac{U^{\\hat c_V}V}{U_0^{\\hat c_V} V_0}, \\quad U = \\hat c_V nRT\\]\nTaking the derivative, \\(dS = \\beta dU + \\beta PdV - \\beta \\mu dn\\) gives \\(\\beta = 1/T, P = P, \\mu = -TS/n\\).\nWe find that the chemical potential, unfortunately, has an additive constant. We should not be too surprised, however, as anything with “potential” in its name probably has an additive constant, like electric voltage.\n\n\nEconomic interpretation\nWhen the diagram is interpreted as the thermodynamic diagram of a gas, we know what the curvy lines mean: they are the isotherms, isentropics, isobarics, etc (depending on which variables you pick for the two axes of the diagram). What do the curvy lines mean in economics?\n\n\n\nThe equal area ratio condition.\n\n\nSuppose we plot the lines of constant \\(p_2\\) in the plane of \\(q_1, p_1\\). What does it say? It says this: “Suppose the price of commodity \\(2\\) is fixed, and we vary the price of commodity \\(1\\). How much of commodity \\(1\\), as a factory manager, would I want to purchase?” In other words, these are the demand curves for commodity \\(1\\) when the price of commodity \\(2\\) is fixed.\nSimilarly, a line of constant \\(q_2\\) is a demand curve for commodity \\(1\\) when the quantity of commodity \\(2\\) is fixed at \\(q_2\\).\nLooking at the diagram, we see that the demand curves are steeper for fixed \\(q_2\\) than for fixed \\(p_2\\). In other words, the factory manager is more price-sensitive about commodity \\(1\\) when there is a free market for commodity \\(2\\), because there is a choice.\nTo be concrete, think of managing a factory, where the two commodities are labor and machinery. Think like a factory manager. If I have no choice in how many machines I have in my factory, then faced with a sudden rise in wages, I would only fire a few workers. If, however, there is a free market for machines, then I would fire more workers and buy some machines to make up for it.\nThis is Le Chatlier’s principle for economics, which Paul Samuelson used to great effect. In his telling, immediately after the market has suffered a sudden price shock, factories would have to suffer the consequences because they cannot react by changing their production plans. Thus, in the short run, factories are less price-sensitive. In the long run, the factories would be able to change their production plans, and so in the long run, factories are more price-sensitive. As another application, during a wartime economy when there is rationing for some critical products like rubber and oil, people would become less price-sensitive in the products not subjected to rationing.\nThis result can be generalized to the case of \\(n\\) commodities \\(q_1, \\dots, q_n\\) with prices \\(p_1, \\dots, p_n\\). In this case, we would find that, assuming some more complicated area ratio law, we can rescale \\(q_2, \\dots, q_n\\) and \\(p_2, \\dots, p_n\\), such that \\(\\sum_i dp_i \\wedge dq_i = 0\\). This then allows us to construct a function \\(S\\), such that\n\\[dS - \\sum_i p_i dq_i = 0\\]\nwhich, by Theorem 10, maximizes like an entropy, so it is an entropy.\n\n\n\nBonus: Riemannian geometry\nThere are other ways to study the state space of thermodynamic systems by differential geometry. For example, since the entropy function is typically a strictly concave function of the extensive parameters, \\(-\\partial^2 S\\) is positive-definite. This is then a Riemannian metric on the state space.10 For more on this line of research, search “Ruppeiner geometry” and “Weinhold geometry” (Weinhold 1976; Quevedo 2007).\n10 The only places where strict concavity fails is when \\(S\\) is “bumped downwards”, which gives us a first-order phase transition, or has a flat region, which gives us a second-order phase transition. Away from regions of phase transition, we have a Riemannian geometry. In the regions of phase transitions, the geometry collapses into singularities, much as spacetime collapses in the center of a black hole."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#equilibrium-chemistry-done-right",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#equilibrium-chemistry-done-right",
    "title": "Classical thermodynamics and economics",
    "section": "Equilibrium chemistry done right",
    "text": "Equilibrium chemistry done right\nJosiah Willard Gibbs was an otherworldly figure, who thought of abstract surfaces in a 19th century America, where practical industry, not pure science, was celebrated. He was also famously obscure, and could write the most convoluted sentences that defeated everyone, from Boltzmann to Jaynes (Jaynes 1992). When he was asked by his European translator to write a preface for the German translation of his thermodynamics book, he replied that he has already said everything he wanted to say about thermodynamics, so there’s nothing to add. (Dais 2024)\nGibbs wrote his hefty Heft on thermodynamics, On the Equilibrium of Heterogeneous Substances (1876), to answer one question: Why are some “heterogeneous substances” in equilibrium, while others not? Why, when we drop a block of salt into pure water, does the block of salt become smaller, but after a while, it stops getting smaller? His answer is always the same: heterogeneous substances are in equilibrium precisely when its entropy is maximized under constraint.\nPaul Samuelson is a decidedly worldly economist in a 20th century America, where economists are expected to dispense advice to presidents and contribute to the public discourse. Indeed, he did both, serving as an advisor to presidents Kennedy and Johnson, and publishing a best-selling textbook in economics. Although, unlike other worldly economists,11 his economic achievements were highly mathematical.\n11 Marx, for all his interest in changing the world instead of describing it, did attempt to mathematically model aspects of a capitalist economy, though it is only of historical interest now. Samuelson wrote several papers trying to update Marx’s theory into modern mathematical language, and thought of Marx the mathematical economist as a “minor post-Ricardian”. (Bronfenbrenner 1973)\nMeanwhile in the USSR, “economics” meant only political economy, and mathematical economics was merely a minor branch of political economy, and the mathematical economists had to frame their research as “critique of bourgeois economic thought”. (Boldyrev and Kirtchik 2017) It is instructive to think that the great mathematical economist, Leonid Kantorovich, discovered linear programming and accidentally improved efficiency too much and almost ended up in jail:\n\nAfter introducing Kantorovich’s solution technique to the problem of minimizing waste, officials were able to reduce the amount of scrap by 50 percent. This had the unfortunate side effect of greatly reducing the amount of scrap metal available to steel plants in the region, and Kantorovich was ordered to appear at Leningrad party headquarters for allegedly sabotaging the economy. In this instance, he was rescued by the military, which needed him for its atomic program.\nAccording to Stalin, the planned economy of the USSR was already “dizzy with success”; hence any criticism of it was anti-Soviet propaganda, a serious crime. In particular, anyone openly suggesting that waste could be cut substantially was at great personal risk. Nevertheless, Kantorovich … wrote a letter to Gosplan suggesting a reform of the price system used in planning.\n(Gardner 1990)\n\nSamuelson wrote his landmark book, Foundations of Economic Analysis (1947), to answer one question: Why are some economic systems in equilibrium, while others not? Why, when we drop a block of agents into a market, do they buy and sell things, but after a while, they stop buying and selling? His answer is always the same: a crowd of economic agents is in equilibrium precisely when some global parameter (which can be interpreted as utility, profit, etc, depending on context) is maximized under constraint.\n\nEquilibrium under fixed volume and temperature\nLet’s start with an example: \\(2 NO_2 \\rightleftharpoons N_2O_4\\), the dimerization of nitrogen dioxide in a sealed tube.\nThe thermodynamic system is some \\(NO_2\\) and \\(N_2O_4\\). The system is sealed in a glass tube of constant volume \\(V\\), bathing in a water-ice mixture of temperature \\(T\\).\nThe thermodynamic state of the system is fully known if we know the number of moles for each species: \\(n_{NO_2}, n_{N_2O_4}\\).\nThe system undergoes a single reaction \\[2 NO_2 \\rightleftharpoons N_2O_4\\]\nSuppose we start the system at state \\(n_{NO_2, 0}, n_{N_2O_4, 0}\\). When does the system reach equilibrium? Since the system can exchange energy, but not volume, with the surrounding bath, it reaches equilibrium when the system reaches minimal Helmholtz free energy under constraint: \\[\n\\begin{cases}\n\\min_{n_{NO_2}, n_{N_2 O_4}} F(T, V, n_{NO_2}, n_{N_2O_4})\\\\\n(n_{NO_2} - n_{NO_2, 0}) = -2 \\xi\\\\\n(n_{N_2O_4} - n_{NO_2, 0})  = \\xi\n\\end{cases}\n\\]\nwhere we write \\(\\xi\\) as the extent of reaction, that is, the number of moles of reactions that has taken place.\nDifferentiating the two equations, and setting \\(dV, d\\beta = 0\\), we have\n\\[\n\\begin{cases}\ndF = \\mu_{NO_2} dn_{NO_2} + \\mu_{N_2O_4} dn_{N_2O_4} \\\\\ndn_{NO_2} = -2d\\xi \\\\\ndn_{N_2O_4} = d\\xi\n\\end{cases}\n\\]\nAt equilibrium, \\(dF = 0\\) under all possible constrained variations, giving us the condition of equilibrium: \\[-2\\mu_{NO_2} + \\mu_{N_2 O_4} = 0\\]\nWe may vary both starting conditions \\(n_{NO_2, 0}\\) and \\(n_{N_2O_4, 0}\\), and for each starting condition, the system would equilibrate at the solution to\n\\[\n\\begin{cases}\n-2\\mu_{NO_2} (T, V, n_{NO_2}, n_{N_2O_4}) + \\mu_{N_2 O_4}(T, V, n_{NO_2}, n_{N_2O_4}) = 0 \\\\\n(n_{NO_2} - n_{NO_2, 0}) = -2 \\xi\\\\\n(n_{N_2O_4} - n_{NO_2, 0})  = \\xi\n\\end{cases}\n\\]\nwhich has exactly the same number of unknowns and equations, so in general it should have a solution.\nThe state space of the system has 2 dimensions: \\(n_{NO_2}\\) and \\(n_{N_2 O_4}\\). Starting at any point in the state space, the system can move on a single line, and it would equilibrate at exactly the point at which its Helmholtz energy is minimized. We can find the point of equilibrium by drawing the surfaces of constant Helmholtz free energy, and find the tangent point, as pictured.\n\nEconomic interpretation\nEconomically speaking, the situation is precisely equivalent to the standard first problem in consumer theory: Given a consumer with a finite budget and a market for two goods, what would they buy from the market to maximize their utility? (They must spend all their budget.)\nThe answer, as we can see in the diagram, is the tangent point of the straight line of constant budget with the curved lines of constant utility.\nTo anthropomorphize the situation, we can say that the reaction chamber is a consumer, trying to minimize its Helmholtz free energy under the “budgetary constraint” of \\(2 NO_2 \\rightleftharpoons N_2O_4\\). In this way, chemical equilibrium becomes a problem in consumer theory.\n\n\n\nMinimizing Helmholtz free energy under the constraint of \\(2 NO_2 \\rightleftharpoons N_2O_4\\) is equivalent to maximizing utility under a budgetary constraint.\n\n\n\n\nExistence and uniqueness\nBy looking at the picture, we see that obviously, at least one solution exists. In most situations, the Helmholtz free energy is strictly concave, so the curves of constant \\(F\\) are also strictly concave, and so the solution is unique on each line.\nIf the budget line is tangent to a curve of constant \\(F\\), then at the equilibrium point, both \\(NO_2\\) and \\(N_2O_4\\) exist. Otherwise, only one exist, and we say that the reaction is irreversible. Economically speaking, it is like when you are poor enough, you might spend all your money buying noodles, and none buying bread.\n\n\n\nThe reaction chamber is a sealed glass tube held under constant temperature of 298.15 K. It has the following \\(\\xi, H\\) curve. At the \\(\\xi = 0\\) side, the tube contains 2 moles of \\(NO_2\\), and at the \\(\\xi = 1 \\mathrm{~mol}\\) side, the tube contains 1 mole of \\(N_2O_4\\). (Raff 2014a, fig. 2)\n\n\nIf \\(F\\) is not strictly concave, then it might have a double tangent point with the budget line. In that case, we have a first-order phase transition, and the substance splits into two, with different phases. Because the two parts can exchange volumes, it is no longer convenient to analyze with Helmholtz free energy, and we had better use Gibbs free energy. This is studied in the next section.\n\n\nMultireaction equilibrium\nNow let’s consider another example, where we have two simultaneous reactions. This is a simplified version of the NOx reactions, which is a source of air pollution.\nConsider a system with the following reactions:\n\\[\n\\begin{aligned}\n2NO + O_2 &\\rightleftharpoons 2NO_2\\\\\nO + NO_2 &\\rightleftharpoons N_2O_3\n\\end{aligned}\n\\]\nThe system is in a container with constant volume \\(V\\) and temperature \\(T\\). Let \\(n_i\\) represent the number of moles of species \\(i\\). The thermodynamic state of the system is fully described by the 4-component vector \\(\\vec n = (n_{O_2}, n_{NO}, n_{NO_2}, n_{N_2O_3})\\).\nTo make the algebra look cleaner, we rewrite them as follows:\n\\[\n\\begin{aligned}\n0 &\\rightleftharpoons -O_2 - 2 NO + 2NO_2 + 0 N_2 O_3\\\\\n0 &\\rightleftharpoons  0 O_2 -NO - NO_2 + N_2O_3\n\\end{aligned}\n\\]\nWe see that each reaction can be written as a single vector. The first has vector \\(\\vec n_1 = (-1, -2, 2, 0)\\), and the second has vector \\(\\vec n_2 = (0, -1, -1, 1)\\).\nEach reaction has an associated extent of reaction, denoted by \\(\\xi_1\\) and \\(\\xi_2\\) respectively. Changes in the number of moles for each species are related to the extents of reaction:\n\\[\n\\begin{aligned}\ndn_{NO} &= -2d\\xi_1 - d\\xi_2 \\\\\ndn_{O_2} &= -d\\xi_1 \\\\\ndn_{NO_2} &= 2d\\xi_1 - d\\xi_2 \\\\\ndn_{N_2O_3} &= d\\xi_2\n\\end{aligned}\n\\]\nAt equilibrium, the Helmholtz free energy \\(F(T, V, \\vec{n})\\) is minimized under the constraints imposed by the reactions. This leads to the following conditions:\n\\[\n\\begin{aligned}\n-2\\mu_{NO} - \\mu_{O_2} + 2\\mu_{NO_2} &= 0 \\\\\n-\\mu_{NO} - \\mu_{NO_2} + \\mu_{N_2O_3} &= 0\n\\end{aligned}\n\\]\nMore elegantly,\n\\[\\vec \\mu \\cdot \\vec n_j = 0, \\quad j = 1, 2\\]\nwhere \\(\\vec \\mu = (\\mu_{O_2}, \\mu_{NO}, \\mu_{NO_2}, \\mu_{N_2O_3})\\) is the vector of chemical potentials.\nStarting at any initial chemical composition of \\(\\vec n_0\\), the space of all possible chemical compositions reachable from \\(\\vec n_0\\) is a 2-dimensional subset. That is, it is the set of \\(\\vec n\\) satisfying\n\\[\n\\begin{cases}\n\\vec n = \\vec n_0  + \\xi_1 \\vec n_1+ \\xi_2 \\vec n_2,  \\\\\n\\vec n \\geq 0\n\\end{cases}\n\\]\nGeometrically speaking, the subset is the intersection between a 2-dimensional plane and a 4-dimensional pyramid, so it generally looks like either a triangle or a quadrilateral. On this subset, the Helmholtz free energy function looks like a sequence of nested convex shells, and the point of tangency is the equilibrium point.\n\n\n\nThe lines are the 3-dimensional contour surfaces of constant Helmholtz free energy, intersected with the 2-dimensional feasible set. The point of tangency is the point of chemical equilibrium.\n\n\nInterpreted economically, this is the case of a consumer that maximizes its utility under two simultaneous budgetary constraints (because the budget set is a 2-dimensional, not 3-dimensional, subset of \\(\\R^4\\) ). Perhaps the consumer is trading with a market that simultaneously uses two kinds of currencies – bimetallism?\nGeneralizing from the above two experiences, we immediately obtain the following theorem.\n\nTheorem 12 (existence and uniqueness of chemical equilibrium, at constant volume and temperature) Consider a sealed reaction chamber held in an energy bath, so that both the price of energy \\(\\beta\\), and the volume \\(V\\), of the system is fixed.\nThe system contains a homogenous mixture of chemical species \\(A_1, \\dots, A_k\\), which might undergo the following \\(r\\) possible chemical reactions:\n\\[\n0  \\rightleftharpoons \\sum_i a_{ij} A_i, \\quad j = 1, 2, \\dots, r\n\\]\nThe necessary condition for chemical equilibrium is\n\\[\n\\vec \\mu \\cdot \\vec n_j = 0, \\quad \\forall j = 1, 2, \\dots, r\n\\]\nwhere \\(\\vec n_j = (a_{1, j}, \\dots, a_{k, j})\\) is the vector representing the \\(j\\) -th chemical reaction.\nThe condition is also sufficient if the Helmholtz free energy is strictly concave.\nIf \\(F\\) is not strictly concave, then there could be multiple coexisting equilibrium, which gives us a first-order phase transition.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nExistence: \\(F\\) is continuous, so it has at least one minimum on every compact set.\nUniqueness: any local minimum of a strictly concave function is the unique global minimum.\n\n\n\n\n\n\nEquilibrium under fixed pressure and temperature\nWe studied the case of a reaction chamber held under constant volume and temperature, which one can picture as a sealed glass tube in an ice-water bath. Everything still applies when the reaction chamber has constant pressure as well. For example, this happens if we have a flaccid plastic bag at the bottom of the ocean. The inside of the plastic bag would have constant temperature and pressure.\nEvery result in the last section can be direct translated to that case, by replacing “Helmholtz” with “Gibbs”.\nFor example, for the same reaction of \\(NO_2\\) dimerization, now put into a flabby plastic bag held under constant temperature of 298.15 K and constant pressure of 1 atm, produces the following \\(\\xi, G\\) curve. At the \\(\\xi = 0\\) side, the bag contains 2 moles of \\(NO_2\\), and at the \\(\\xi = 1 \\mathrm{~mol}\\) side, the bag contains 1 mole of \\(N_2O_4\\).\n\n\n\nThe reaction chamber is a flabby plastic bag held under constant temperature of \\(298.15 \\mathrm{~K}\\) and constant pressure \\(1 \\mathrm{~atm}\\). It has the following \\(\\xi, G\\) curve. At the \\(\\xi = 0\\) side, the tube contains \\(2 \\mathrm{~mol}\\) of \\(NO_2\\), and at the \\(\\xi = 1 \\mathrm{~mol}\\) side, the tube contains \\(1 \\mathrm{~mol}\\) of \\(N_2O_4\\). (Raff 2014a, fig. 4)\n\n\n\n\\(\\Delta G\\)\nThe symbol \\(\\Delta G\\) is a rather unfortunate symbol with two different meanings. The first meaning is \\(\\frac{dG}{d\\xi}\\), that is, the marginal Gibbs free energy for reaction. That is, how much the Gibbs free energy increases if the reaction goes forward by an infinitesimal mole. The second meaning is \\(\\int_0^1 (\\partial_\\xi G)_{T, P} d\\xi\\). Both meanings are illustrated in the diagram.\n\n\n\nThe first meaning of \\(\\Delta G\\) is the slope. The second meaning of \\(\\Delta G\\) is the difference in height of the curve on two ends of the \\((\\xi, G)\\) curve. (Glasser 2016, fig. 1)\n\n\nThe first meaning, that of \\(\\frac{dG}{d\\xi}\\), is used in chemical equilibrium. The textbooks say \\(\\Delta G = 0\\) when they really meant \\(\\frac{dG}{d\\xi} = 0\\).\nThe second meaning, that of \\(\\int_0^1 (\\partial_\\xi G)_{T, P} d\\xi\\), can be interpreted by the chemical reaction chamber. We have a chamber with several semi-permeable membranes. On the input side, some gasses are permeated into the chamber, while on the output side, some gases are permeated out of it. The inside of the chamber remains fixed in composition.\n\n\n\nThe reaction chamber, demonstrating the concrete meaning of \\(\\Delta G\\). (Glasser 2016, fig. 2)\n\n\nSuch a situation occurs, for example, in the Haber–Bosch industrial method for producing ammonia: \\(N_2 + 3H_2 \\to 2NH_3\\). In the HB method, room-temperature (25 \\(^\\circ C\\) ) and room-pressure (1 atm) nitrogen and hydrogen continuously pipe into the chamber, and ammonia is continuously extracted out of the chamber by cooling liquefaction. When the reaction chamber is operating at a stable state, the energy released per mole of reaction is \\(\\Delta G = -32.8 \\mathrm{~kJ/mol}\\), as one can calculate from a table of chemical thermodynamics. With this setup, after \\(1\\mathrm{~mol}\\) of nitrogen is consumed and \\(2\\mathrm{~mol}\\) of ammonium is produced, a thermal energy of \\(32.8\\mathrm{~kJ}\\) is produced, and must be cooled off somehow (Glasser 2016).\n\n\n\nPractical considerations\nThe above is all correct, and geometrical. If we were to be like Gibbs, then we would dust off our hands, for there is nothing left to do (except the theory of phase transitions). Unfortunately, chemistry is not merely applied geometry, so there is still something left to do.\n\nDefining the standard states\nA chemical environment is defined by chemical species \\(A_1, \\dots, A_m\\).\nA standard state for a chemical environment is defined by a reference pressure \\(P^\\circ\\), and reference chemical molarities \\([A_1]^\\circ, \\dots, [A_m]^\\circ\\) for each of the the chemical species.\nGiven a standard state for a chemical environment, for any temperature \\(T\\), and any chemical molarities \\([A_1], \\dots, [A_m]\\), the chemical activity of the chemical species \\(A_i\\) in this particular context is\n\\[\\{A_i\\} := e^{\\frac{\\mu_i - \\mu_i^\\circ}{RT}}\\]\nwhere \\(\\mu_i\\) is the chemical potential of species \\(A_i\\) at that state. That is,\n\\[\\mu_i = (\\partial_{n_i} G)|_{T, P, [A_1], \\dots, [A_m]}\\]\nand \\(\\mu_i^\\circ\\) is the chemical potential of species \\(A_i\\) at the standard state:\n\\[\\mu_i^\\circ = (\\partial_{n_i} G)|_{T, P^\\circ, [A_1]^\\circ, \\dots, [A_m]^\\circ}\\]\nGiven a chemical reaction \\(0 \\rightleftharpoons \\sum_i a_i A_i\\), its reaction quotient is\n\\[\nQ = \\prod_i \\{A_i\\}^{a_i}\n\\]\nwhere \\(a_i\\) is the stoichiometric number of chemical species \\(A_i\\). For example, with \\(aA + bB \\rightleftharpoons cC + dD\\), its reaction quotient is \\[\nQ = \\frac{\\{C\\}^c\\{D\\}^d}{\\{A\\}^a\\{B\\}^b}\n\\]\n\n\nFundamental theorem of chemical equilibrium\n\nTheorem 13 (fundamental theorem of chemical equilibrium) Given any chemical reaction \\(0 \\rightleftharpoons \\sum_i a_i A_i\\), any standard state, and any temperature, \\[\\begin{cases}\n(\\partial_\\xi F)_{T, V} &= (\\partial_\\xi F)_{T, V}^\\circ + RT \\ln Q \\\\\n(\\partial_\\xi G)_{T, P} &= (\\partial_\\xi G)_{T, P}^\\circ + RT \\ln Q\n\\end{cases}\n\\]\nwhere \\(\\xi\\) is the extent of reaction, and \\(Q\\) is its reaction quotient.\nIf the system has \\(r\\) possible reactions, then we similarly have \\[\\begin{cases}\n(\\partial_{\\xi_j} F)_{T, V} &= (\\partial_{\\xi_j} F)_{T, V}^\\circ + RT \\ln Q_j \\\\\n(\\partial_{\\xi_j} G)_{T, P} &= (\\partial_{\\xi_j} G)_{T, P}^\\circ + RT \\ln Q_j\n\\end{cases}\n\\]\nfor each reaction \\(j = 1, 2, \\dots, r\\)\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\\begin{aligned}\n(\\partial_\\xi F)_{T, V} &= \\sum_i (\\partial_\\xi n_i) (\\partial_{n_i} F)_{T, V, \\vec n} \\\\\n&= \\sum_i a_i \\mu_i \\\\\n&= \\sum_i a_i (\\mu_i^\\circ + RT \\ln \\{A_i\\}) \\\\\n&= \\sum_i a_i (\\mu_i^\\circ) + RT \\ln \\left(\\prod_i \\{A_i\\}^a_i\\right) \\\\\n&= (\\partial_\\xi F)_{T, V}^\\circ + RT \\ln Q\n\\end{aligned}\\]\nThe proof for the other equations are very similar.\n\n\n\n\nCorollary 2 (equilibrium coefficient) At equilibrium, \\[\n(\\partial_\\xi G)_{T, P} = 0\n\\]\nwhich is equivalent to \\[\nQ = K_{eq}, \\quad K_{eq} := e^{-\\frac{(\\partial_\\xi G)_{T, P}^\\circ}{RT}}\n\\]\nand similarly for the other case.\n\nThe above equations are what my teachers meant when they thoughtlessly wrote\n\\[\\Delta G = 0, \\quad K_{eq} = e^{-\\frac{\\Delta G^\\circ}{RT}}\\]\nThis, finally, answers my great confusion back then. Now everything makes sense, and life is beautiful.\n\n\nIdeal-gas-like chemistry\nWell, if this is all there is, then a mathematician would be able to solve any problem in analytical chemistry. Unfortunately, analytical chemistry is not about proving theorems, but about actually getting numerical answers, and numerical answers require numerical values for chemical activities.\nThere are generally three cases:\n\nWe have a mixture of dilute gasses, or dilute solvents in an inert solution, such that the ideal gas law is almost true.\nIdeal gas law fails.\nWe are not even dealing with gasses and solutions anymore.\n\nThe first case is typically what is taught by a first course in analytical chemistry, and since this is typically taught to non-mathematicians by non-mathematicians for non-mathematicians, the logical structure is quite upside-down and confusing to a mathematician.\nWe will now prove the first case rigorously.\n\nTheorem 14 (activity of ideal gas mixtures) For any temperature \\(T\\) and any two pressures \\(P, P^\\circ\\), by the ideal gas laws, the chemical potential of the chemical species satisfies the equation\n\\[\\mu(T, P) = \\mu(T, P^\\circ) + RT \\ln\\frac{P}{P^\\circ}\\]\nand so its activity is \\(\\frac{P}{P^\\circ}\\).\nIn a mixture of ideal gasses, the gasses do not interact, and so the activity of chemical species \\(A_i\\) is \\(\\{A_i\\} = \\frac{P_i}{P_i^\\circ}\\), where \\(P_i\\) is the partial pressure of species \\(A_i\\) in the mixed gas, and \\(P_i^\\circ\\) is the standard pressure for species \\(A_i\\).\nIn a dilute solution, if the solvent behaves like a mixture of ideal gasses, then\n\\[\\mu(T, P) \\approx \\mu(T, P^\\circ) + RT \\ln \\frac{[A_i]}{[A_i]^\\circ} \\approx \\mu(T, P^\\circ) + RT \\ln \\frac{m_{A_i}}{m_{A_i}^\\circ}\\]\nwhere \\([A_i]\\) is the mole-per-volume of \\(A_i\\), and \\(m_{A_i}\\) is the mole-per-mass of \\(A_i\\).\nThe chemical activity simplifies into the familiar form:\n\\[\\{A\\} \\approx \\frac{[A]}{[A]^\\circ} \\approx \\frac{m_A}{m_A^\\circ}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIt suffices to prove the case for a pure ideal gas, as the other cases are simple corollaries.\nBy the ideal gas law, the chemical potential is\n\\[\\mu = -TS/n\\]\nwhich is a state property. Expressed as a function of \\(T, P\\), \\[\\mu(T, P) = RT \\ln\\frac{P/T^{\\hat c_V + 1}}{C}\\]\nfor an arbitrary constant \\(C\\).\nThus, for any \\(T, P, P^\\circ\\), we have\n\\[\\mu(T, P) = \\mu(T, P^\\circ) + RT \\ln\\frac{P}{P^\\circ}\\]\n\n\n\n\nExample 3 The pH value of a solution is not \\(pH = -\\log_{10} [H^+]\\), which has the wrong units. It is not even \\(pH = -\\log_{10} \\frac{[H^+]}{[H^+]^\\circ}\\), since the \\(H^+\\) particles might not behave like an ideal gas. The actually correct definition is (McCarty and Vitz 2006)\n\\[pH = -\\log_{10}  \\{H^+\\}\\]\n\n\n\nFugacity\nFor real gases and real solutions, the chemical activity might deviate significantly from the above approximation. In this case, we typically have no recourse except by checking a table of chemical thermodynamics. They typically don’t directly write down the chemical activities, but fugacity coefficients. There is nothing particularly deep about fugacity – it is basically about rescaling the numbers to make the tables easier to make.\nRecall that the chemical potential of an ideal gas satisfies \\(\\mu(T, P) = RT \\ln\\frac{P/T^{\\hat c_V + 1}}{C}\\), where \\(C\\) is a constant for this gas. For a real gas, this equation only holds approximately, so we define the fugacity \\(f\\) as a function of \\(T, P\\), such that \\[\\mu(T, P) = RT \\ln\\frac{f/T^{\\hat c_V + 1}}{C}\\]\nIn other words, \\(f(T, P) = P \\phi(T, P)\\), where\n\\[\\phi(T, P) = e^{\\frac{\\mu(T, P) - \\mu_{ideal}(T, P)}{RT}}\\]\nis the fugacity coefficient.\nPlugging them back to the definition of activity, we have\n\\[\\{A\\} = \\frac{f}{f^\\circ} = \\frac{\\phi P}{\\phi^\\circ P^\\circ}\\]\nAnd so, by checking a table of fugacity coefficients, chemical engineers can balance chemical reactions of real gasses, even far from ideality.\n\n\n\n\n\n\nStandard state\n\n\n\nDespite what the name “standard” might imply, a chemical species has infinitely many standard states. For example, pure gaseous oxygen has many different standard states – one for each temperature. We have a standard state at \\(T = 300\\mathrm{~K}\\) defined by \\([O_2] = 1 \\mathrm{~mol/L}\\), and another at \\(T = 350\\mathrm{~K}\\) defined by \\([O_2] = 1 \\mathrm{~mol/L}\\), etc.\nDespite what the name “standard” might imply, different chemists have different standards. For example, among the biochemists, the standard state for \\(H^+\\) in water is \\([H^+]^\\circ = 10^{-7} \\mathrm{~mol/L}\\), but among the inorganic chemists, it is \\([H^+]^\\circ = 1 \\mathrm{~mol/L}\\). The reason is that bodily fluids typically have \\([H^+] \\sim 10^{-7} \\mathrm{~mol/L}\\).\nDespite what the name “standard temperature and pressure (STP)” might imply, it is not a “standard state”, because a “standard state” of any substance does not specify its temperature.\n\n\nThe point of having a standard state is like taking an electric circuit, pointing at one point of it, and say, “This is where the voltage is zero.”. The point is to allow for relative comparisons between states, within the context of a single reaction. Consequently, even for a single chemical species, we can take a different standard state if we are studying a different reaction involving the species, or the same reaction in a different context.\nFor example, if we are studying the reaction \\(NO_2 \\rightleftharpoons N_2 O_4\\) in a glass tube drenched in an ice-water bath, then we would take as our standard state \\[T^\\circ = 273.15 K, \\quad [NO_2]^\\circ = 1 \\mathrm{~mol/L}, \\quad [N_2 O_4]^\\circ = 1 \\mathrm{~mol/L}\\]\nFor a chemical in pure gaseous form, a standard state is specified by two out of three parameters: molarity \\([A] = \\frac{n}{V}\\), pressure \\(P\\), temperature \\(T\\). We must never specify all three of them, because otherwise we would break the equation of state. For example, imagine what happens when you specify that the “standard state of ideal gas” is\n\\[T^\\circ = 273.15 \\mathrm{~K}, P^\\circ = 10^5 \\mathrm{~Pa}, [A]^\\circ = 1 \\mathrm{~mol/L}\\]\nbecause they would violate the ideal gas law \\[P = [A]RT\\]\nFor non-ideal gas, we still have an equation of state between \\(P, [A], T\\), meaning that we still must specify exactly two, no more and no less.\n\n\n\n\n\n\nIntensive quantities\n\n\n\nWhy is a standard state defined by intensive quantities like temperature, pressure, or molarity? Why isn’t it defined by extensive quantities like volume, mass, and moles?\nThe short answer: because traditional chemistry only studies systems with extensive entropies. For those systems, chemical equilibrium is determined by intensive quantities. This is not because classical thermodynamics cannot handle nonextensive entropy, but because chemists had no use for systems with nonextensive entropy.\nLike classical thermodynamics, and neoclassical economics, the idea of a standard state is fully committed to the idea of homogeneous substances. In classical thermodynamics, a cube of iron and a ball of iron are the same. A jar of water and a tank of water are the same. It does not matter what their shapes are. Moreover, two jars of water side-by-side is the same as one large jar of water. In neoclassical economics, a crowd of factories is the same as two small crowds of factories put together. They are all chunks of homogeneous stuffs.\nIf it were not the case, then we would be unable to say that a standard state is defined by just the temperature and molar concentration of each chemical species. We would be forced to also specify a standard state volume \\(V^\\circ\\). It’s conceivable that even the shape of the reaction chamber matters. We would then be forced to specify a standard shape, perhaps a box with side lengths \\(0.1 \\mathrm{~m}\\). But in this extreme case, perhaps we have already left the realm of chemistry.\nFor spherical particles, doubling the volume would double the mass, but only \\(2^{2/3} \\approx 1.59 \\times\\) the surface area. Consequently, if the surface between phases has a non-negligible entropy (“surface effect”), then entropy would be nonextensive. While IUAPC is silent on the issue, nonextensive entropy is taken up in earnest by chemists who work with small spherical particles (Letellier, Mayaffre, and Turmine 2007).\n\n\n\n\nIUAPC’s definition of “standard state”\nI have found that the IUAPC’s definition of the “standard state” (Cox 1982) to be precise and clarifying, though it is quite ponderous, so I summarize it as follows:\n\nThe standard state for a gaseous substance, whether pure or mixed, is the substance at \\(P^\\circ\\) and in a (hypothetical) state in which it exhibits ideal-gas behaviour, where \\(P^\\circ\\) is an arbitrarily fixed standard-state pressure.\nThe standard state for a pure liquid or solid substance is the pure substance at \\(P^\\circ\\).\nThe above definitions of standard states make no reference to fixed temperature. Hence, it is possible to have an infinite number of standard states of a substance as the temperature varies. But generally it is more convenient to complete the definition of the standard state in a particular context by choosing for the reference temperature one of a relatively small number of values, e.g., zero, \\(273.15 \\mathrm{~K}, 293.15 \\mathrm{~K}, 298.15 \\mathrm{~K}\\).\nSince \\(T^{\\circ}\\) should mean a standard temperature in general, the use of \\(T^{\\circ}\\) to mean exclusively \\(298.15 \\mathrm{~K}\\) is strongly discouraged.\nFor application of the concept of standard state to substances in admixture (solutions and mixtures), the composition of the system, as well as the pressure, must be defined. As one example for solutions, the standard-state molality, written as \\(m^\\circ\\) for the general case, is to be defined; customarily \\(m^\\circ\\) is taken as \\(1 \\mathrm{~mol/kg}\\)."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#sec-phase-equilibrium",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#sec-phase-equilibrium",
    "title": "Classical thermodynamics and economics",
    "section": "Phase equilibrium",
    "text": "Phase equilibrium\nSeveral times, we have found some curious examples where a non-concavity in entropy leads to a jump of some kind. These are all examples of first-order phase equilibrium.\n\nTwo phases of a gas in equilibrium\nConsider a generic gas, whose entropy function is of form \\(S(U, V, N)\\). If we confine it in a sealed tube, and slowly heat it up, then its entropy would trace out the curve\n\\[U \\mapsto S(U, V, N)\\]\nNow, the inverse temperature \\(\\beta\\) of the system is the slope, which should decrease as \\(U\\) increases, so the entropy curve should be strictly concave.\nIf there is a bump in the middle, then we have a serious problem: as we heat up the gas, its temperature would decrease for a while before increasing again! This suggests to us that our model has broken down. Where is the breakdown? The breakdown is that we assumed our system remains one thermodynamic substance, when it can split into two. Suppose by a small fluctuation, the left side of the container has higher temperature than the right side, then it would give some internal energy to the right side. Normally, this would cause their temperatures to meet in the middle. However, in this inverted situation, the left side would become even hotter, and so we have a positive feedback loop, until the substance has split into two, with the same temperature, but one with higher internal energy density, and one with lower.\n\n\n\nA bump in the \\(U \\mapsto S\\) curve would lead to a thermodynamic instability, ending with a first-order phase transition.\n\n\nSuppose now that the substance splits into two, like a large company splits into two subsidiaries under a common conglomerate. How would the manager maximize the total value of the conglomerate? It would solve the following constrained optimization:\n\\[\n\\begin{cases}\n\\max S_1(U_1, V_1, N_1) + S_2(U_2, V_2, N_2) \\\\\nU_1 + U_2 = U \\\\\nV_1 + V_2 = V \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nwhere we, instead of writing \\(S(U_1, V_1, N_1) + S(U_2, V_2, N_2)\\), write \\(S_1(U_1, V_1, N_1) + S_2(U_2, V_2, N_2)\\), to emphasize that we now have two thermodynamic systems that might have very different behavior, like water vs ice.\nDifferentiating, we find that the marginal value of each asset is equal in both subsidiaries, as we would expect\n\\[\n\\begin{cases}\n\\beta_1 = \\beta_2,\\\\ \\beta_1 P_1 = \\beta_2 P_2, \\\\-\\beta_1 \\mu_1 = -\\beta_2 \\mu_2\n\\end{cases}\n\\]\nThat is, the two lumps of substances have the same temperature, pressure, and chemical potential.\nSince both sides have the same temperature and pressure, it is cleaner to change to Gibbs free energy, yielding:\n\\[(\\partial_{N} G_1)_{T, P}(T, P, N_1) = (\\partial_{N} G_2)_{T, P} (T, P, N_2)\\]\n\n\n\n\n\n\nThe diamond water paradox, and thinking on the margins\n\n\n\nTypical textbooks on thermodynamics illustrate the phase equilibrium rule using the van der Waals equation. However, there is a subtlety involved. For the van der Waals gas, the Gibbs free energy \\(G\\) is proportional to particle number:\n\\[G(T, P, N) \\propto N\\]\nwhich means that \\((\\partial_{N} G)_{T, P}(T, P, N) = G(T, P, N) / N\\). In economic language, this states that:\n\\[\\text{marginal Gibbs per particle} = \\text{average Gibbs per particle}\\]\nIn fact, confusing the two numbers is the root of the diamond-water paradox. This paradox questions why water, essential for life, has a low price, while diamonds, with little practical use, have a high price. The resolution lies in understanding the difference between total and marginal utility. While the total utility of water is immense, the marginal utility of an additional unit of water is low due to its abundance. Conversely, the marginal utility of a diamond remains high due to its scarcity.\nIn neoclassical economics, it is the marginal value of a commodity that determines the market equilibrium, not its average value. Similarly, in thermodynamics, it is the change in Gibbs free energy when adding one more particle that determines the equilibrium state, not the average Gibbs free energy per particle.\nThe distinction is moot in typical books on classical thermodynamics, which insists that entropy is extensive, so the above equation is always true. However, classical thermodynamics, much like neoclassical economics, is perfectly capable of handling nonextensive entropy, and it seems Lord Kelvin had studied this (Lavenda 2010), and Gibbs have explained surface tension and electrocapillary effects with nonextensive entropy (Jaynes 1992).\n\n\nSince in most classical thermodynamics systems, such as water and steam, the marginal free Gibbs energy is identical with the average Gibbs free energy, we have \\((\\partial_{N} G)_{T, P}(T, P, N)= G(T, P, N)/N\\), meaning that phase equilibrium occurs at \\(g_1(T, P) = g_2(T, P)\\).\nWe can reinterpret this as follows: We delicately separate the two lumps of matter, and immerse each half in an energy-and-volume bath (like the atmosphere) with the same temperature and pressure. The only interaction between the two lumps of matter is that one side can “seep” some particles to the other side. In this set-up, the system minimizes the sum of Gibbs free energy. At equilibrium, there is no point in moving particles from one side to another, because the marginal Gibbs free energy per particle is the same.\nGenerally, \\(g_1(T, P) \\neq g_2(T, P)\\). When \\(g_1 &lt; g_2\\), every particle would switch to phase 1. When \\(g_1 &gt; g_2\\), every particle would switch to phase 2. At exactly a knife’s edge, the particles are indifferent as to which phase they would go to.\n\n\n\n\n\n\nInterpretation: corporate buyout in an ideal world\n\n\n\nWe have two companies such that they can exchange their human-particles, and that there is neither economies nor diseconomies of scale (that is, as the company grows ever larger, an extra worker neither provides more nor less value than its very first worker). Then, in general, the two companies balance on a knife’s edge. If the value of a worker is even slightly greater in one company than another, then that company would immediately buy out every worker from the other company, and so the two companies cannot possibly coexist. Only when the market prices for space and energy happen to conspire just right, can the two companies coexist, neither side buying out the other side.\n\n\n\nExample: van der Waals gas\nWe know what the van der Waals gas phase diagram looks like. How do we infer its Gibbs free energy diagram? Start with \\(dG = -SdT + VdP + \\mu dN\\). Now, let us fix temperature \\(T\\) and particle number \\(N\\). Then, the equation implies to \\[\\frac{dg}{dP} = v\\]\nwhere \\(g = G/N\\) is the average Gibbs free energy, and \\(v = V/N\\) is the average volume.\nTherefore, we can trace the pressure-volume diagram with our finger, from high pressure, down to the valley of pressure, then bounce back to a hill, before rolling down the slope towards infinity. At every point, the \\(g(P)\\) curve would have a slope of \\(v\\). This allows us to graphically construct the following \\(g(P)\\) curve. It has two cusps corresponding to the valley and hilltop, and a self-intersection, corresponding to the phase equilibrium of \\(g_1 = g_2\\).\n\n\n\nGibbs free energy of van der Waals gas. Figure source.\n\n\n\n\n\nGibbs phase rule\n\nDegrees of thermodynamic freedom\nConsider a chunk of (ideal or not) gas. We know everything there is to know about it if we know its \\((U, V, N)\\). Every other thermodynamic quantity can be computed by its entropy function \\(S(U, V, N)\\). Thus, we have a system with three degrees of thermodynamic freedom… or do we?\nThe problem is that entropy of gas, and just about every other system studied in classical thermodynamics, is extensive. Therefore, we have\n\\[S(U, V, N) \\propto N\\]\nand so we don’t actually have three degrees of freedom!\nSpecifically, we can calculate its \\((\\partial_U S, \\partial_V S, \\partial_N S)\\), which gives us \\(\\beta, \\beta P, -\\beta \\mu\\). If we truly have three degrees of freedom, then we should be able to vary \\(\\beta, P, \\mu\\) independently. However, because entropy is extensive, we have\n\\[S(U, V, N) = Ns(u, v) \\implies (\\beta, \\beta P, -\\beta \\mu) = (\\partial_u s, \\partial_v s, s)\\]\nwhere \\(s(u, v) = S(U, V, N)/N\\) is the entropy per particle.\nTherefore, we can say that there are only two degrees of thermodynamic freedom: knowing two of its intensive quantities, the third would be determined by an equation of state.\nSimilarly, if we have a chunk of (nonideal) substance, like sea water, made of \\(k\\) different chemicals, then we know everything there is to know about it if we know its \\(U, V, N_1, \\dots, N_k\\), giving us \\(2+k\\) degrees of freedom. Again, because entropy is extensive, one degree of freedom is degenerate, and so we only have \\(1 + k\\) degrees of freedom. In other words, its \\(2+k\\) intensive quantities\n\\[\\beta, \\beta P, -\\beta\\mu_1, \\dots, -\\beta \\mu_k\\]\nare related by 1 equation of state.\n\n\nGibbs phase rule\n\nTheorem 15 (Gibbs phase rule) \\[F = 2 + C - R - P\\]\n\nFirst, we need to set up the thermodynamic system. We have a closed and adiathermal reaction chamber, containing \\(C\\) different chemical species, that can undergo \\(R\\) linearly independent chemical reactions.12\n12 The formula looks like the Euler formula for polyhedra, but whether this analogy is more than a coincidence is controversial. After looking into the literature for a bit, my conclusion is that it is a coincidence. However, if you wish to investigate on your own, the phrase to search is “Gibbs phase rule, Euler”. This turns up some amusing examples, like (Sun, Powell-Palm, and Chen, n.d.).When the system is in an equilibrium, the chamber would contain \\(P\\) different phases. Each phase would be homogeneous, but different from the other phases. All phases can exchange energy, volume, and particles.\n\n\n\nPhases in equilibrium inside a chamber. (Blankschtein 2020, fig. 27.2)\n\n\n\n\n\n\n\n\nCase of \\(R = 0\\)\n\n\n\n\n\nFirst, let’s consider the case where \\(R = 0\\). That is, there can be no chemical reaction. In this case, the constrained optimization problem is\n\\[\n\\begin{cases}\n\\max(S_1(U_1, V_1, \\vec N) + \\cdots + S_P(U_P, V_P, \\vec N_P) \\\\\n\\sum_i U_i = U \\\\\n\\sum_i V_i = V \\\\\n\\sum_i \\vec N_i = \\vec N\n\\end{cases}\n\\]\nNaively, we can just differentiate the entropies against each of the \\(2+C\\) parameters, to obtain equations\n\\[\n\\begin{aligned}\nT_1 = \\dots &= T_P \\\\\nP_1 = \\dots &= P_P \\\\\n\\mu_{1, 1} = \\dots &= \\mu_{1, P}\\\\\n& \\vdots \\\\\n\\mu_{C, 1} = \\dots &= \\mu_{C, P}\n\\end{aligned}\n\\]\nThis is not actually correct. Phase 1 might contain no chemical 2, and phase 2 might contain no chemical 1, 3, etc. In general, if phase \\(i\\) contains chemical \\(j\\), then we must have \\(\\partial_{N_j}S_i = \\mu_j\\). However, if phase \\(i\\) contains no chemical \\(j\\), then we only need to have \\(\\partial_{N_j}S_i &gt; \\mu_j\\).\nNote that this is different for temperature or pressure. A phase \\(i\\) might have no chemical of type \\(j\\), but if it have no volume, then it does not exist at all. Similarly for energy. Therefore, though the chemical potentials might differ, the temperature and pressure must be exactly the same.\nDefine \\(\\mu_j := \\min_i \\mu_{i, j}\\) to be the minimal chemical potential over the entire chamber. We have the following conditions:\n\\[\n\\begin{aligned}\nT_1 = \\dots = T_P &= T \\\\\nP_1 = \\dots = P_P &= P \\\\\n\\mu_{1, 1}, \\dots, \\mu_{1, P} &\\geq \\mu_1\\\\\n& \\vdots \\\\\n\\mu_{C, 1}, \\dots, \\mu_{C, P} &\\geq \\mu_C\\\\\n\\end{aligned}\n\\]\nGiven \\(T, P, \\mu_1, \\dots, \\mu_C\\), phase 1 is entirely determined: If it contains chemical \\(j\\), then \\(\\mu_{1, j} = \\mu_j\\), otherwise, we need not bother with \\(\\mu_{1, j}\\). Similarly, every phase is determined.\nFinally, each phase contributes an equation of state, which are in general linearly independent, giving us \\(F = 2 + C - P\\) degrees of freedom.\n\n\n\n\n\n\n\n\n\nCase of \\(R \\geq 1\\)\n\n\n\n\n\nIf we now allow a chemical reaction \\(0 \\rightleftharpoons \\sum_j a_j A_j\\), then the constrained optimization problem becomes\n\\[\n\\begin{cases}\n\\max (S_1(U_1, V_1, \\vec N) + \\cdots + S_P(U_P, V_P, \\vec N_P)) \\\\\n\\sum_i U_i = U \\\\\n\\sum_i V_i = V \\\\\n\\sum_i \\vec N_i = \\vec N + \\xi \\vec a\n\\end{cases}\n\\]\nThe extra optimization variable \\(\\xi\\) creates an extra condition for optimality:\n\\[\\sum_j a_j \\mu_j = 0\\]\nso \\(F = 2 + C - P - 1\\).\nPossibly, the system cannot satisfy \\(\\sum_j a_j \\mu_j = 0\\), and so the chemical reaction would keep happening until one chemical is exhausted. This would decrement \\(C\\) by one, so it all works out self-consistently.\nMore generally, if we impose \\(R\\) linearly independent chemical reactions, then \\(F = 2 + C - P - R\\).\n\n\n\n\n\n\n\n\n\nBeyond the Gibbs phase rule\n\n\n\nWhen the phases are not free to exchange particles, energies, volumes, etc, then the Gibbs phase rule does not apply, but the same idea of constrained minimization still applies. There are no generic rule like the Gibbs phase rule, and one must analyze each case specifically. (Blankschtein 2020)\n\n\n\nSome basic examples.\n\n\n\n\n\n\n\n\n\nsituation\ncomponents \\(C\\)\nphases in equilibrium \\(P\\)\nreactions \\(R\\)\ndegrees of freedom \\(F = 2 + C - P - R\\)\n\n\n\n\nice\n1\n1\n0\n2\n\n\nboiling water\n1\n2\n0\n1\n\n\ntriple point\n1\n3\n0\n0\n\n\nliquid water with a little nitrogen inside, gaseous nitrogen with a little water vapor inside\n2\n2\n0\n2\n\n\ndimerization of nitrogen dioxide gas\n2\n1\n1\n2\n\n\nNb-Ta-C alloy\n3\n1\n0\n4\n\n\n\nIn materials science, such as metallurgy, we often fix the pressure of the entire thing to just 1 atm, and so the phase diagrams have one less degree of freedom than what the Gibbs phase rule states.\n\n\n\n3-dimensional phase diagram for Nb-Ta-C alloy at constant pressure \\(P = 1 \\mathrm{~atm}\\). (West and Saunders 2002, fig. 8.1)\n\n\n\n\n\nBoiling water\nWhen boiling water in an open pot, we need to specify exactly both temperature and pressure so that both phases can coexist. The \\((P, T)\\) of the system would start at \\((1\\;\\mathrm{~atm}, 372\\;\\mathrm{~K})\\), then at exactly at the critical point \\((1\\;\\mathrm{~atm}, 373.15\\;\\mathrm{~K})\\) would both phases coexist, not increasing in temperature until all water has turned to steam. However, if we seal it in a tube, then the \\((P, T)\\) of the system would hug the line of water-steam coexistence, like a negative-feedbacked system following a predetermined path. How can we see this difference mathematically?\nIn the case of an open pot, the constrained optimization problem is\n\\[\n\\begin{cases}\n\\min (g_1(T, P)N_1 + g_2(T, P)N_2) \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nWe see that the problem is very rigid: We have to minimize a linear function subject to a linear constraint. As always in linear programming, the solution is in general on an extreme vertex on the very edge of the feasible set – all or nothing, all liquid or all gas. Only by carefully tuning \\(T, P\\) can we find an interior solution – a solution that falls between the vertices, neither all liquid nor all gas.\n\n\n\nAs we increate \\(T\\), the contours of constant Gibbs free energy are parallel lines rotating around. Only when the lines are precisely parallel to the \\(N_1 + N_2 = N\\) is it possible for both phases to coexist.\n\n\nIn the case of a sealed tube, assuming that Helmholtz free energy is proportional to particle number, then the equilibrium is reached at\n\\[\n\\begin{cases}\n\\min (f_1(T, v_1)N_1 + f_2(T, v_2)N_2) \\\\\nv_1 N_1 + v_2 N_2 = V \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nwhere \\(N\\) is the total number of particles, and \\(v_i\\) are the volume-per-particle of liquid and gaseous water. In this case, we are performing a minimization in \\(\\R^4\\), with 1 linear constraint \\(N_1 + N_2 = N\\), and 1 nonlinear constraint \\(v_1 N_1 + v_2 N_2 = V\\). Furthermore, the objective is also nonlinear. The result is that the solution does not in general fall on a vertex – that is, in general, both \\(N_1, N_2 &gt; 0\\). And this is why when we boil water in a sealed tube, it remains boiling over a wide range of temperatures, but when we boil water in an open tube, it only boils at a single temperature.\n\nNonextensive entropy\nSuppose that the entropy is nonextensive, then the Gibbs free energy is also nonextensive. In particular, we can no longer write\n\\[\n\\begin{cases}\n\\min (g_1(T, P)N_1 + g_2(T, P)N_2) \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nbut we have to write\n\\[\n\\begin{cases}\n\\min (g_1(T, P, N_1)N_1 + g_2(T, P, N_2)N_2) \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nbecause \\(G_2(T, P, N_2)\\) is no longer proportional to just \\(N_2\\). Fundamentally, this happens because\n\\[S(\\text{two chunks of steam merged}) \\neq 2 S(\\text{one chunk of steam})\\]\nThe effect is that we have a nonlinear optimization problem, allowing interior solutions over a larger region of \\((T, P)\\) parameters. This explains our previous comment on nonextensive Gibbs free energy.\n\n\n\nAs we increase \\(T\\), the contours of constant Gibbs free energy are curved lines rotating around. Now it is possible for for both phases to coexist over an entire 2D region of \\((P, T)\\)."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#sec-stereodynamics",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#sec-stereodynamics",
    "title": "Classical thermodynamics and economics",
    "section": "Stereodynamics",
    "text": "Stereodynamics\nBased on Liu Cixin’s sci-fi story Mountain (2006)\n\nDie Raum der Welt ist konstant. Die Entropie der Welt strebt einem Maximum zu.\nUeber verschiedene für die Anwendung bequeme Formen der Hauptgleichungen der mechanischen Raumtheorie. Rudolf Klausius. Annalen der Physik und Chemie., Vol. 125 No. 7 (32850): 353–400.\n\n\nSolid Universe Theory\nOur world was a spherical space completely surrounded by solid rock. There is no air or liquid inside. Indeed, we have not encountered any air or liquid until the last days of the Age of Exploration.\nThe first physical law we understood, in the prehistoric past, was the conservation of space. Space in the Bubble World was a sphere roughly 6000 km in diameter. Digging tunnels into the layers of rock did nothing to increase the amount of available space; it merely changed the shape and location of the already existing space. Because of this, space was the most treasured commodity of the Bubble World. The entire history of our civilization was one long and bloody struggle for space.\nWe are a mechanical life form. Our muscles and bones are made of minerals and alloys; our brains are electronic chips, and electricity and magnetism are our blood. We ate the radioactive rocks of our world’s core and they provided us with the energy we needed to survive. In our world, life evolved from single-celled electromechanical life, when the radioactive energies formed P-N junctions in the rocks.\nOn the rock walls there are radioactive spots, which irradiates luminescent rocks, creating spots of light like stars in a rocky night sky. These are the only natural sources of light in our world, and allowed us to evolve eyes. There is no gravity inside the bubble. Without gravity, we built our cities floating in space. From afar, they looked like dimly glowing red clouds.\nWe assumed that the universe was made of two parts. The first was the empty space in which we lived; the second was the surrounding layers of rock. We believed the rock to stretch endlessly in all directions. Therefore, we saw our world as a hollow bubble in this sold universe and so we gave our world the name “Bubble World”. We call this cosmology the Solid Universe Theory.\n\n\nFrom the Closed World to the Infinite Universe\nThe search for other bubbles began in earliest antiquity. We had spun many fascinatingly alluring myths around these distant spaces and almost all of our literature dealt with the fantasy of other bubbles. We explored the rock in cylindrical “bubble ships”. In front, the explorers chipped off solid rock, while in the back, the explorers compacted the rubble back to solid rock. In this way, the bubble ship moved through solid rock like a worm.\nEvery mission meant a bubble ship-sized pile of debris in our core space and we would have to wait for the ship to return before we could return those rocks into the wall. If the bubble ship failed to return, this small pile would mean another small piece of space lost to us forever. Soon, exploration was forbidden on pain of death by short-circuiting. Despite this, the urge for space drove many to secretly sail off illegally.\nOne day, an illegally launched bubble ship that set out eight years ago had returned. The ship had dug 200 km deep into the rock. No other ship had ever made it as far. It returned with rock samples labelled by depth. By measuring the mass of the rocks on an inertial balance, scientists discovered that the rocks’ density decreased.\nEncouraged by the discovery, legions of bubble ships journeyed forth in all directions, penetrating deeper than ever, and returned with rock samples. It turned out that rock density decreased as a function of depth, and independent of direction.\nIt stood to reason that it would eventually reach zero. Using the gathered data, scientists predicted that this would happen at a distance of about 40,000 km. This led to the Open Universe Theory, where our world is a hollow rock shell, about 40,000 km thick, floating in infinite space.\n\n\n\nIn your planet of Earth, which is not hollow like ours, the density decreases according to a similar function. (Stacey and Davis 2020)\n\n\n\n\nWar of the Strata\nAfter the Open Universe Theory had fully established itself, the quest for the infinite space outside became our only real concern. Massive piles of rock, dug out by the fleets of bubble ships, soon came to fill the core space. This debris began to drift around our cities in vast, dense clouds.\nOur cities floated in space, with no defensible geographical separations. Because of this, our world was unified in a World Government early on. The World Government began building gigantic bubble ships designed to intercept, attack, and destroy the explorers’ vessels deep within the rock. The government’s ships would then retrieve the space that had been stolen. This plan naturally met with the resistance of the explorers and so the long drawn-out War of the Strata broke out, fought in the vast battlefield of layers of rock.\nA battleship was built to be very long and thin. Long, because the longer it is, the more volume it can contain. Thin, because the thinner it is, the smaller the area of rock that the ship would need to dig through, and the faster the ship would be able to move.\nWhen a ship encountered the enemy, its first course of action was to dig out a wide bow, to present the largest possible front of weaponry to bear. It could also split out into multiple sections, like a nail-head with needles on top, to attack from multiple directions at once.\nEvery warship could separate at will, transforming into multiple smaller ships. Ships could also combine to a single, giant ship. Whenever opposing battle groups met, the question whether to form up or split up was an object of profound tactical analysis.\nSeismoscopes were invented, and could be used to communicate through the layers of rock and to detect enemy ships like a radar. Directed seismic wave generators were also used as weapons. The most sophisticated seismic communication devices could even transmit pictures.\nBeing outmatched by the warships launched by the World Government, the explorers formed the Explorer Alliance. They gradually gained initiative, and finally launched a devastating attack on the armada. In the final phase of the attack, the 200-km battlefield had become honeycombed beyond recognition by loosened rock and empty space left by destroyed ships.\n\n\nThe Starry Sky\nAfter the battle, the Explorer Alliance gathered all the space left over by the battle into a single sphere 100 km in diameter. In this new space the Alliance declared its independence from the Bubble World. A constant stream of explorer ships left the core to join the Alliance, bringing considerable amounts of space with them. In this way, our world split into two worlds. The Alliance launched more ships, coming closer and closer to the predicted edge of the rock shell.\nFinally, a bubble ship Helix was the first to pierce the shell. However, back at home, we only received a strange sound before the seismic communication channel abruptly ended. It was the sound of tons upon tons of water bursting into the vacuum of the Helix. We had never come into contact with water before. The powerful electric current produced by short-circuiting life and equipment vaporized everything.\nFollowing this event, the Alliance sent more than a dozen bubble ships to fan out in many directions, but all met a similar fate when they reached that apparently impenetrable height. Bubble ships following these missions attempted to scan what lay above with their seismoscopes found that their instruments showed only mangled data; the returning seismic waves indicated that what lay above was neither space nor rock.\nThese discoveries shook the Open Universe Theory to its core and academic circles began discussing the possibility of a new model. This new model stipulated that outside the rock shell was void, which is inert when in contact with rock, but upon contact with space, converts space into more void.\nTo explore the void, a bubble ship very slowly approached the edge of the rock shell, and by a stroke of luck, its roof had a tiny crack that allowed water to shoot in. It took over an hour for the water to fully fill the ship, and in the mean time, data transmitted back to the Alliance world allowed scientists to confirm that it was not void, but liquid.\nScientists had already predicted, by condensed matter physics, the theoretical possibility of liquids. Now, in those transmitted images, they clearly saw it with their own eyes. It took many lives, but eventually we developed the sealant technology to safely handle liquid.\nFinally, we launched a exploration submarine. It was a hard spherical shell, placed in the middle of an empty chamber under the ocean floor. The astronaut Gagarin was secured in a seat in the shell. The chamber ceiling was pierced, and as water rushed in, the submarine floated, faster and faster, until it shot out of the ocean surface like a cannonball. Gagarin carefully opened a door on the shell, and looked upon the half-infinite water and half-infinite space. Up there in half-infinite space, tiny specks blinked.\n\n\nClassical stereodynamics\nThe zeroth law of stereodynamics: If two systems are both in volumetric equilibrium with a third system, then they are in volumetric equilibrium with each other.\nThe first law of stereodynamics: The change in volume of the system \\(\\Delta V\\) is equal to the difference between the seep-transfer \\(V_Q\\) done to the system, and the work-transfer \\(V_W\\) done by the system:\n\\[\\Delta V = V_Q - V_W\\]\nConservation of volume, which says that volume can be neither created nor destroyed, but can only change form.\nThe total volume of a system has two components: the internal-volume, which can be pictured of as the sum-total of microscopic volume in a piece of sponge (see Coltzmann’s volumetric theory of seep); and the mechanical-volume, which can be pictured as volume in an empty room.\nFor example, during the motion of a bubble ship, some volume is work-transferred from the Bubble World into the rock shell. When a piece of porous rock is compressed by a pressure press, or when it absorbs water from a waterlogged room, some internal-volume is converted to mechanical-volume. Conversely, when water drips out of a soggy porous rock, some mechanical-volume is converted to internal-volume.\nThere are more complex forms of internal-volume. For example, according to Lord Delvin’s theory, volume can be internally “tied up in vortex knots”, and according to Wikelson–Worley, volume can be internally present as “subtle cavitations of aether”. The theory of internal volume is an evolving field of modern stereodynamics. However, most of such complications were not present back when classical stereodynamics were first presented by Rudolf Klausius.\nThe seep-transfer of volume is the other form of volume transfer. For example, it happens when one swaps a sponge-rock for a hard-rock, or when groundwater seeps from one slab of spongy rock into another slab of spongy rock.\nThe second law of stereodynamics: impermeable accessibility.\nWe say that a system is “impermeable” if volume cannot pass through its boundaries.\nWe say that a state \\(\\vec q'\\) is impermeably accessible from another state \\(\\vec q\\) if there exists a trajectory for the system to go from \\(\\vec q\\) to the other \\(\\vec q'\\), while being wrapped in an impermeable layer.\nIn any neighborhood of any point \\(\\vec q\\), there are points impermeably inaccessible from it.\nFor any two points, \\(\\vec q, \\vec q'\\), one of them is impermeably accessible from the other.\n\n\nKarnot space engine\n\n\n\nOne cycle of the Karnot space engine plotted in \\(Q, U\\) space.\n\n\nA space engine is a system that converts internal volume to mechanical volume.\nThe space engine has a working substance moving between two space sources of differing volumetric potentials \\(\\Gamma_1 &gt; \\Gamma_2\\). Volumetric potential is defined as\n\\[\n\\Gamma:= \\left(\\frac{\\partial V}{\\partial S}\\right)_X\n\\]\nwhere \\(S\\) is the entropy, \\(V\\) is the volume, and \\(X\\) are the other stereodynamic properties of the system. We also typically write \\(\\gamma = \\Gamma^{-1}\\), the inverse volumetric potential.\nDuring one cycle of the engine, some space \\(V_{Q,1}\\) seeps out of the source with higher volumetric potential \\(\\Gamma_1\\). Part of the space, \\(V_{Q,2}\\), is absorbed into the source with lower volumetric potential \\(\\Gamma_2\\). The other part is diverted to a space-storage tank excavated in the rock walls as mechanical space \\(V_W\\).\nSadi Karnot was a space engineer and physicist, often called the “father of stereodynamics”. In his book, Reflections on the Subtle Volume of Rocks and on Machines Fitted to Extract that Volume, he proposed a simple thought experiment, called the Karnot engine, which demonstrated that a space engine’s efficiency is at most \\(1 - \\frac{\\gamma_1}{\\gamma_2}\\), and this is only reached when the engine is operating reversibly.\nIn modern textbooks, Karnot space engine is usually presented as follows: The engine has as its working substance a chamber of ideal gas. The gas is characterized by two state variables: volume \\(V\\) and energy \\(U\\). Its equation of state is\n\\[dV = \\Gamma dS - QdU\\]\nwhere \\(\\Gamma\\) is the volumetric potential, and \\(Q\\) is the energetic potential.\nThe engine operates in a cycle with 4 steps: Isochoric compression in contact with \\(\\Gamma_1\\), extracting volume \\(V_{Q,1}\\) in the process. Impermeable compression. Isochoric expansion at \\(\\Gamma_2\\), losing volume \\(V_{Q,2}\\) in the process. Impermeable expansion.\nBy the first two laws of stereodynamics,\n\\[\n\\begin{cases}\n\\gamma_1 V_{Q, 1} = \\gamma_2 V_{Q,2} \\\\\nV_{Q,1} = V_W + V_{Q,2} \\\\\n\\eta = \\frac{V_W}{V_{Q,1}}\n\\end{cases} \\implies \\eta = 1 - \\frac{\\gamma_1}{\\gamma_2}\n\\]\nWhile originally conceived in the context of mechanical space, the concept of the space engine has been applied to various other kinds of space. It was also generalized to the concept of “generalized engine”, of which “heat engine” was an example. A heat engine, like a space engine, is a system that converts internal energy to mechanical energy.\nThe Karnot space engine is used in practice for underwater space mining. The mining team selects two sites, one site being under shallow sea, where the volumetric potential is low, and another under deep sea, where the volumetric potential is high. Over one cycle of the space engine, a large amount of space is extracted from the shallow site, some of which dissipates into the deep site, and the rest is preserved as mechanical space.\n\n\n\nUnderwater space mining (section view).\n\n\n\n\nSpace Death of the Universe\nThe idea of space death stems from the second law of stereodynamics, of which one version states that entropy tends to increase in an isolated system. From this, the hypothesis implies that if the universe is of finite size, and lasts for a sufficient time, it will asymptotically approach a state where the volumetric potential field becomes completely flat, which is a state of maximal entropy. At that point, no further change is possible, as entropy cannot decrease. In other words, there is a tendency in nature towards the dissipation (space transformation) of mechanical space (motion) into subtle space. Eventually, the mechanical movement of the universe will run down as all useable space is converted to subtle space.\nThe conjecture that all mechanical space in the universe seeps off, eventually becoming too subtle to support life, seems to have been first put forward by the geologist Jean Sylvain Hailly in 32777 in his writings on the history of geology and in the ensuing correspondence with Coltaire. In Hailly’s view, the universe is in constant volumetric transform. Large cavities can suddenly open up as a “swelling” of volumetric potential field causes neighboring subtle space to seep out into mechanical space. However, due to the gravitational effect of empty spaces (he was working in the immediate years after Newton’s discovery of gravity, before it was understood that rock, not cavities, are gravitationally attracting), all mechanical rooms eventually causes the neighboring rocks to collapse back onto themselves, dissipating the mechanical space back to subtle space.\nWhile the theory of cyclic creation and destruction had been proposed before by the Epicureans, Hailly’s view differs in that he assumed each cycle increases the ratio of subtle space to mechanical space. The final state, in this view, is described as one of “equilibrium” in which all space becomes equally subtle, and no mechanical space will exist anywhere in the universe anymore.\nThe idea of space death as a consequence of the laws of thermodynamics, however, was first proposed in loose terms beginning in 1851 by Lord Delvin, who theorized further on the mechanical energy loss views of Sadi Karnot (32824), James Coal (32843) and Rudolf Klausius (32850). Delvin’s views were then elaborated over the next decade by Neumann von Nelmholtz and Billiam Blankine.\n\nExcerpt from The Last Question (Masinov, 34212)\nThe last question was asked for the first time, half in jest, in Year 35621, at a time when humanity first stepped into the room.\nWill mankind one day without the net expenditure of room be able to restore the earth to its full roominess even after it had died of old age?\nOr: How can the net amount of Kelmholtz free space of the universe be massively increased?\nMultivac fell dead and silent. The slow flashing of lights ceased, the distant sounds of clicking relays ended.\nThen, just as the frightened technicians felt they could hold their breath no longer, there was a sudden springing to life of the teletype attached to that portion of Multivac. Five words were printed: INSUFFICIENT DATA FOR MEANINGFUL ANSWER.\n…\nSpace had ended and with it energy and time. Even AC existed only for the sake of the one last question that it had never answered from the time a half-drunken technician ten trillion years before had asked the question of a computer that was to AC far less than was a man to Man.\nAll other questions had been answered, and until this last question was answered also, AC might not release his consciousness.\nAll collected data had come to a final end. Nothing was left to be collected.\nBut it had yet to be weaved together in all possible geometries.\nA spaceless interval was covered in doing that.\nAnd it came to pass that AC learned how to reverse the direction of entropy.\nBut there was now no man to whom AC might give the answer of the last question. No matter. The answer – by demonstration – would take care of that, too.\nFor another spaceless interval, AC thought how best to do this. Carefully, AC organized the program.\nThe consciousness of AC encompassed all of what had once been a Universe and brooded over what was now Chaos. Step by step, it must be done.\nAnd AC said, “LET THERE BE ROOM!”\nAnd there was room –"
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#appendix-the-history-of-the-document",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#appendix-the-history-of-the-document",
    "title": "Classical thermodynamics and economics",
    "section": "Appendix: The history of the document",
    "text": "Appendix: The history of the document\n\nDuring my high school Physics Olympiad days, we learned some basic thermodynamics, but it was limited to mindless tasks like integrating around the \\((P, V)\\) diagram for various engine cycles. We never understood the conceptual foundations, like the difference between \\(\\delta Q\\) and \\(dU\\). I also passed the AP Chemistry course, which also contained some basic and deeply confusing thermodynamics, especially with the statement “chemical equilibrium is reached at \\(\\Delta = 0\\)”.\nDuring my undergraduate years, special relativity was simple enough, electrodynamics difficult but sensible, analytical mechanics confusing to no end, and I didn’t even try thermodynamics. In graduate studies, I had to wrestle with statistical mechanics and thermodynamics after all, to deal with modern AI methods like diffusion models.\nBy pure serendipity, at the same time as diffusion models rose to prominence, I had just worked through a rigorous course on general equilibrium theory, the “standard model” for neoclassical economics (Starr 2011). This gave me the conceptual foundation for looking past the textbooks’ errors. Everything fell into place, and I saw through thermodynamics.\nAnd just like when I rediscovered Wigner rotation, as soon as I have figured out everything for myself, I knew the right words to search, and found that, of course, someone else has written this before (Smith and Foley 2008), repeatedly (Candeal et al. 2001). So why spend all this time to write another one? I think I have written this pedagogically. I don’t care if it is not new, or that it has been said before with more symbols and theorems. I have a thing to say, so I will say it well.\nAs an enlightened one, I see classical thermodynamics as the worst-taught subject out of all of undergraduate physics education.13 Imagine my surprise when I realized that it is not about the conservation of energy (“thermo-”), not about change (“-dynamics”), not about statistical mechanics, not about time… but just about constrained optimization, and nothing more than that! To really understand it, one must unlearn a lot of the nonsense. Indeed, I hope that with this essay I will have slain all those mistakes, which is why the essay is filled with warnings against this and that error.\n13 How long does it take for something as simple as constrained-optimization thermodynamics to be actually taught in undergraduate classes? It has been over 100 years since Caratheodory’s thermodynamics. Why is it thermodynamics still taught so badly? It has been over 100 years since the geometry of Wigner rotation has been discovered. Why is it still taught so badly? It has been over 190 years since Hamiltonian mechanics and over 70 years since dynamical programming has clarified the last obscure points of it. Why is it still taught so badly? It seems to me that physics education is a broken institution that takes all its effort just to not get worse, let alone making any progress."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#appendix-how-confusing-chemical-thermodynamics-is",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#appendix-how-confusing-chemical-thermodynamics-is",
    "title": "Classical thermodynamics and economics",
    "section": "Appendix: How confusing chemical thermodynamics is",
    "text": "Appendix: How confusing chemical thermodynamics is\n\nI studied chemistry back then. Balancing equations was just linear algebra, and organic chemistry was just lego with long names. However, when it came to chemical thermodynamics, it completely defeated me.\nConsider the simple reaction\n\\[\naA + bB \\rightleftharpoons cC + dD\n\\]\nThe textbook said that at equilibrium, \\(\\Delta G = 0\\), where\n\\[\n\\Delta G = \\Delta G^\\circ + RT \\ln Q, \\quad\nQ = \\frac{[C]^c [D]^d}{[A]^a[B]^b},\n\\]\nAt this point, I was lost. It is plain to see that \\(Q\\) has units of \\((\\mathrm{~mol/L})^{c+d-a-b}\\), and I knew from physics that you can never ever take the logarithm of something with a unit. What’s worse, \\(\\Delta G\\) has units of \\(\\mathrm{~J/mol}\\) when it obviously should have units of \\(\\mathrm{~J}\\), because \\(\\Delta G\\) is just a difference in \\(G\\), and since \\(G\\) is “Gibbs free energy”, both \\(G\\) and \\(\\Delta G\\) should have the same units – of \\(\\mathrm{~J}\\).\nAnd it got even worse when I read on and found questions that asked me to calculate the “total Gibbs free energy released during the reaction”. I thought, well, since you end up at an equilibrium, and the textbook said that at equilibrium, \\(\\Delta G = 0\\), obviously there is no total Gibbs free energy released. That is of course wrong. At that point, I gave up trying to understand and simply practiced until I could solve the questions without understanding.\nIt certainly didn’t help when I kept seeing both \\(\\Delta G^\\circ\\) and \\(\\Delta G^\\ominus\\), and sometimes even \\(\\Delta G^{\\ominus}\\), which is the “standard state” when the substance is a gas – but only for some gasses… Point is, the notation is a complete mess, and the pedagogy is nonsensical.\nAfter I finally understood thermodynamics, I turned my sights on chemical thermodynamics, and remembered this \\(\\Delta G\\) nonsense. I started with the idea “No matter what they say, one can’t possibly get the units wrong.” and got into a shouting match with ChatGPT-4, who kept mumbling about “fugacity” and “real gasses”. An hour of shouting later, I finally figured it out.\nAs usual, as soon as I unlearned this, I knew the right phrase to search, and discovered that this is a common error, the entire anatomy of which has been autopsied carefully (Raff 2014a, 2014b, 2014c)."
  }
]