---
title: "How to Renormalization"
author: "Yuxi Liu"
date: "2024-04-07"
date-modified: "2024-04-07"
categories: [math, physics, scaling]
format:
  html:
    toc: true
description: "How to do renormalization theory."

# image: "figure/banner.png"
status: "draft"
confidence: "certain"
importance: 4
---

{{< include ../../../utils/blog_utils/_macros.tex >}}

## What is renormalization?

Renormalization is not group theory. The name "renormalization group theory" is truly terrible. To an applied physicist, the name "group theory" is abstract and inspires fear and uncertainty. To a mathematician, the name "group theory" is plain wrong, because you can always undo a group-action, but you can never undo a coarse-graining.



## The logistic map: RN on $\R$

## The Ising model: RN on a lattice

![](figure/firefox-renormalization-group-theory.jpeg)

![When an Ising model is at the critical temperature $T_c$, coarse-graining would not end up with all-black or all-white, but would have the same amount of details no matter how much we coarse-grain. Figure 13 from [@sethnaCourseCracklingNoise2007].](figure/ising%20model.png)

This is an example of **real space RN**. Real space RN is a garden of tricks,

[@yangSelectedPapers194519802005]

### The Ising model in $\Z$

So far, we have been doing it for the Ising model on $\Z$. But it's clear that we can also do it for two $\Z$ put side-by-side like a ladder. Each "rung" of the ladder can be thought of as one big atom, with 4 possible states: up-up, up-down, down-up, down-down. We can then do the same calculation as the previous case, except that instead of a $2\times 2$ matrix, we have a $4 \times 4$ matrix.

For that matter, we can do it for any finite number of those $\Z$ put together. We can then imagine doing that for such ladders with widths $2, 3, 4, 5, 6, \dots$, then discover a pattern, and take the limit. If this works, we would solve the Ising model on $\Z^2$.

Arduous as it sounds, this is exactly how Lars Onsager arrived at his solution of the Ising model in $\Z^2$. He calculated up to ladders with width 6, diagonalizing matrices of size $64\times 64$ in the process, then he guessed the general pattern and proceeded from there, emboldened by the guess. As Chen-Ning Yang reported:

> In 1944, L. Onsager produced, quite unexpectedly, an exact evaluation of the partition function of the model in two dimensions. It was a real tour de force. I had studied his paper in Chicago in the spring of 1947, but did not understand the method, which was very, very complicated, with many algebraic somersaults. ...
> 
> In March, 1965 ... I asked him how it came about that he took all those complicated algebraic steps in his paper of 1944. He said he had a lot of time during the war, so he began to diagonalize the transfer matrix, which had already been discussed by E. Montroll and by H. A. Kramers and G. H. Wannier. He started with a $2 \times \infty$, then a $3 \times \infty$, then a $4 \times \infty$ lattice. He then went on to a $5 \times \infty$ lattice, for which the transfer matrix is $32 \times 32$ in size. Such a matrix is quite large, but the experience he had gained with the smaller matrices came in handy, and he was able, after some time, to find all 32 of the eigenvalues. He proceeded then to the $6 \times \infty$ case, and eventually diagonalized the $64 \times 64$ matrix, finding that all the eigenvalues were of the form $e^{\pm \gamma_1 \pm \gamma_2 \pm \gamma_3 \pm \gamma_4 \pm \gamma_5 \pm \gamma_6}$. That led to the concept that the algebra of the problem was a product algebra, and hence the manipulations in his paper. [@yangSelectedPapers194519802005, pages 11--13]

I also want to quote this from the same pages, because it describes accurately what it feels like to do real-space RN:

> a long calculation, the longest in my career. Full of local, tactical tricks, the calculation proceeded by twists and turns. There were many obstructions. But always, after a few days, a new trick was somehow found that pointed to a new path. The trouble was that I soon felt I was in a maze and was not sure whether in fact, after so many turns, I was anywhere nearer the goal than when I began. This kind of strategic overview was very depressing, and several times I almost gave up. But each time something drew me back, usually a new tactical trick that brightened the scene, even though only locally. Finally, after about six months of work off and on, all the pieces suddenly fitted together, producing miraculous cancellations, and I was staring at the amazingly simple final result \[spontaneous magnetization of the Ising model\] [@yangSelectedPapers194519802005, page 12]

### Kadanoff decimation, take 0

The problem: given a hexagonal grid, you make $p$ of them black and the rest white. What is the critical p where you get a percolation (infinitely big black island)?

![](figure/hexagonal_decimation.png)

![](figure/hexagonal_decimation_fixed_point.png)

[@stinchcombeIntroductionScalingConcepts1991]

Occupation probability $p$

Renormalization by the triangles, as shown. After one iteration, the lattice length increases from $l$ to $\sqrt 3 l$ , and the renormalized occupation probability changes from $p$ to $p^3 + 3p^2(1-p) = p^2(3-2p)$ .

The equilibrium point is $p=1/2$ . This is the percolation probability.

Let reduced probability be $\bar p = p-1/2$ . We find that renormalization makes $\bar p_{n+1} = \frac 32 \bar p_n$

Suppose we start with $\bar p_0$ , and we perform $N$ renormalizations to reach some constant $\Delta \bar p$ (for example, 0.001), then 
$$N = \frac{\ln \Delta \bar p - \ln \bar p_0}{\ln \frac 32}$$  and during the time, the lattice length increased by  
$$3^{\frac 12 N} \propto \bar p_0^{-\frac{\ln 3}{2\ln \frac 32}} = \bar p_0^{-1.36}$$- Since at constant $\Delta \bar p$ , the lattice has a certain fixed look with a certain characteristic size for its clusters, we find that when the occupation probability is $p$ , the characteristic size of clusters scales as about $(p-1/2)^{-1.36}$ .

### Kadanoff decimation, take 1

![[Figure source](https://phas.ubc.ca/~berciu/TEACHING/PHYS502/PROJECTS/21-Jonah.pdf)](figure/Kadanoff%20decimation.png)

### Kadanoff decimation, take 2

[@marisTeachingRenormalizationGroup1978]

> This paper attracted much favorable notice since, beyond obtaining all the scaling properties, it seemed to lay out a direct route to the actual calculation of critical properties. On closer examination, however, the implied program seemed -- as I will explain briefly -- to **run rapidly into insuperable difficulties** and interest faded. In retrospect, however, Kadanoff's scaling picture embodied important features eventually seen to be basic to Wilson’s conception of the full renormalization group.
>
> [@fisherRenormalizationGroupTheory1998]

### Migdal bond-moving

![[Figure source](https://phas.ubc.ca/~berciu/TEACHING/PHYS502/PROJECTS/21-Jonah.pdf)](figure/Migdal%20bond-moving.png)

With that simple idea I somehow got within 0.23% of the exact answer. 

## A bag of intuitions

### Power laws are born of two exponential parents

In psychophysics, this is known as Stevens' power law [@stevensNeuralEventsPsychophysical1970].

Why is there a phase transition with polynomial decay? Two exponentials cancel exactly, leaving only a polynomial.

Consider the Ising model on the plane. Fix an origin $0$ , and we ask, how strong is the correlation between the origin $0$ and a point that is at distance $(n, n)$ away from the origin, where $n$ is large?

Well, the two points are correlated because they are connected by chains of spins. The more chains there are, the stronger the correlation, but the longer each chain is, the weaker the correlation.

How many chains? The shortest chains are of length $2n$ , and there are
$${2n \choose n} \sim \frac{4^{n}}{\sqrt{n\pi }}$$ 	  of them (use Stirling approximation).  

Each chain has an exponential decay. We can use the 1D Ising model transfer matrix
$$M^n = \begin{bmatrix}
	  \frac{1 + \tanh^n(\beta J)}{2} & \frac{1 - \tanh^n(\beta J)}{2} \\
	  \frac{1 - \tanh^n(\beta J)}{2} & \frac{1 + \tanh^n(\beta J)}{2}
	  \end{bmatrix}$$
The correlation is $\tanh^n(\beta J)$ .

Since the chain has length $2n$ , we need to use $\tanh^{2n}(\beta J)$ .

We can think of spin at origin as $x_{(0,0)} + z_1 + z_2 + \cdots$ and the spin at $(n, n)$ as $x_{(n,n)} + z_1 + z_2 + \cdots$ , where $z_1, z_2, ...$ are random variables that are responsible for creating the correlations between the two spins along each chain. Then, since $\ket{z_i}=0$ , we have correlation
$$\sim \frac{4^{n}}{\sqrt{n\pi }} \tanh^{2n}(\beta J)$$
The two terms are exactly balanced when $\beta J = \tanh^{-1}(1/2) = 0.549\cdots$ .

Now, the exact result is $\beta J = 0.44\cdots$ , so our crude estimate is only $25\%$ too high.

Now, right at the critical point, the correlation is $\sim (n\pi)^{-1/2}$ , so we see that when the exponential decay in correlation is exactly matched with the exponential growth in possible paths, the remaining polynomial decay comes to the fore.

Notice that we have also calculated one of the Ising critical exponents: $\nu = 1/2$ . The actual answer is $1$ , but it is actually $1/2$ for all dimensions $\geq 4$ (the mean field theory).

Similarly, with 
$$\binom{kn}{n, \cdots n}\sim \frac{k^{kn}}{n^{\frac{k-1}2}}\frac{k^{1/2}}{(2\pi)^{\frac{k-1}2}}$$ 	  we can estimate the critical $\beta J \approx \tanh^{-1}(1/k)$ in $k$ -dimensions.

## Universality

[@battermanUniversalityRGExplanations2019]

- The theory of critical points in phase transitions is the paradigm example of universality.
- macroscale - mesoscale - microscale
  - macroscale: thermodynamics, continuum, no fluctuation.
  - mesoscale: fluctuations in aggregates of atomic scale properties may be important, order parameters code for some feature of the microstructure.
  - microscale: particles and quantum mechanics.
- Criticality is just the space between two characteristic scales. The real question is why do we have characteristic scales that are so wide apart?
  - BIB. Statistical physics: statics, dynamics and renormalization (Kadanoff 2000), p. 251
  - shows an amazing variety of length scales: There is the Hubble radius of the universe, $10^{10}$ light years or so and the radius of our own solar system, $10^{11}$ meters roughly, and us-two meters perhaps, and an atom $-10^{-10}$ meters in radius, and a proton $10^{-16}$ meters, and the characteristic length of quantum gravity-which involves another factor of about $10^{20}$ .
    How these vastly different lengths arise is a very interesting and fundamental question. ... However, we can think about how one describes the region between these lengths. If one is looking at scales between two fundamental lengths, there are no nearby characteristic lengths. Similarly in critical phenomena, whenever one looks [at] any event which takes place between the scale of the lattice constant [the spacing between molecules or spins] and the much larger scale of the coherence length, one is in a situation in which there are no nearby characteristic lengths.  
- Near the critical point, fluctuations are dominant and average values for the order parameters lose their meaning. Equilibrium statistical mechanics is unable to describe the critical behavior because there are fluctuations at all length scales.
- The universality hypothesis
  - BIB. From Order To Chaos II, Essays: Critical, Chaotic And Otherwise, p 273.
  - All phase transition problems can be divided into a small number of different classes depending upon the dimensionality of the system and the symmetries of the order state. Within each class, all phase transitions have identical behaviour in the critical region, only the names of the variables are changed.
- Universality hypothesis implies that if we have two Hamiltonians $H_1, H_2$ where one can be smoothly perturbed to the other by $H_\lambda := (1-\lambda)H_1 + \lambda H_2$ , and renormalization works on $\lambda$ , then we can run the same scaling law method with $\lambda$ too, and so the scaling laws for $H_1, H_2$ are the same.
  - BIB. From Order To Chaos II, Essays: Critical, Chaotic And Otherwise, p 275-276
- Correlation length $\xi$ is the largest fluctuation droplet that can form before it contains so much excess free energy that equilibrium thermodynamics asserts itself.
  - BIB. From Order To Chaos II, Essays: Critical, Chaotic And Otherwise, p 274.
  - Just below the critical temperature, the system has a free choice to make between being a liquid at high density or a vapor at low density. Both choices are equally good. But since liquid wants to be in contact with liquid and vapor with vapor, the system must decide.
  - A natural fluctuation produces a droplet of the wrong phase. This droplet secretes other material of the same density, and it grows larger and larger. It stops growing when the cost in available—i.e. free-energy for making the droplet of wrong phase becomes comparable with kT. Since near the critical point it cost very little free energy per unit volume to make the wrong phase, the droplet can grow very large.
  - The coherence length $\xi$ is a size of a typical droplet. As the critical point is approached, the free energy cost of making a droplet goes to zero, and the size of a typical droplet $\xi$ , goes to infinity.
- Boiling water is just a magnet. Vapor is just up-spin and liquid is just down-spin.
  collapsed:: true
  - BIB. From Order To Chaos II, Essays: Critical, Chaotic And Otherwise, pp 297--299
  - Droplet picture of correlation behaviour
  - However, as criticality is approached, the difference in magnetization between the two different phases gets smaller and smaller. Hence the energetic cost per unit area of producing a region of the wrong phase approaches zero. For this reason, the area of a droplet and its radius both can get very, very large. Critical phenomena are connected with large-scale but weak fluctuations in the magnetization.
  - So far, our picture of critical fluctuations is like that in Fig. 1.3. Droplets with spin down of all sizes up to a maximum size $\xi$ appear near the critical point. However, this picture is incomplete. Each fluctuating region is also a nearly-critical system. Fluctuations appear within the droplets.
  - ![image.png](../assets/image_1689752136072_0.png){:height 469, :width 477}
  - ![image.png](../assets/image_1689752122707_0.png){:height 285, :width 458}
  -
- The two questions to be explained. Both can be explained by renormalization.
  - Why are the phase transitions stable under perturbation of the microscopic Hamiltonians of the systems?
    - The universality class is the basin of attraction of the fixed point. Scaling exponents and other universal properties are determined by the flow in a neighborhood of the fixed point.
  - Why are the universality classes dependent upon the symmetry of the order parameter and the dimension?
    - The renormalization flow depends on the symmetry and the dimension.
- Renormalization is not trivial.
  - We may well try to simplify the nature of a model to the point where it represents a ‘mere caricature’ of reality. But notice that when one looks at a good political cartoon one can recognize the various characters even though the artist has portrayed them with but a few strokes. … [A] good theoretical model of a complex system should be like a good caricature: it should emphasize those features which are most important and should downplay the inessential details.” (Fisher 1983, p. 47) Lange claims that here Fisher “seems to be supporting a ‘common features account’: the minimal model, despite being a caricature of some actual system, shares with it ‘those features which are most important’ ” (Lange 2015, p. 299, fn. 3).
  - But renormalization theory explains more: it explains why those features matter and others don't. It explains why we have universal classes according to symmetry and dimension and not, say, the precise shape of the atomic force laws.
- Renormalization theory explains the use of effective Hamiltonians and toy models instead of microscopically accurate models. Specifically, one starts with some microscopic accurate model, then apply renormalization and show that the details fall away and we end up with the toy model.

### The view from symmetries

| physical system | site types |
|---|---|---|
| uniaxial magnet | up / down | 
| fluid | has atom / no atom |
| brass crystal | zinc / copper |
| simple lattice field theory | has particle / no particle |

| d | n | Theoretical Model (Ising Model) | Physical System | Order Parameter |
|---|---|---|---|---| 
| 2 | 1 | Two dimensions | Adsorbed films | Surface density |
|  | 2 | XY model in two dimensions | Helium-4 films | Amplitude of superfluid phase |
|  | 3 | Heisenberg model in two dimensions |  | Magnetization |
| >2 | ∞ | "Spherical" model | None |  |
| 3 | 0 | Self-avoiding random walk | Conformation of long-chain polymers | Density of chain ends |
|  | 1 | Ising model in three dimensions | Uniaxial ferromagnet | Magnetization |
|  |  |  | Fluid near a critical point | Density difference between phases |
|  |  |  | Mixture of liquids near consolute point | Concentration difference |
|  |  |  | Alloy near order-disorder transition | Concentration difference |
|  | 2 | XY model in three dimensions | Planar ferromagnet | Magnetization |
|  |  |  | Helium 4 near superfluid transition | Amplitude of superfluid phase |
|  | 3 | Heisenberg model in three dimensions | Isotropic ferromagnet | Magnetization |
| ≤4 | -2 |  | None |  |
|  | 32 | Quantum chromodynamics | Quarks bound in protons, neutrons, etc. |  |

Table reproduced from [@wilsonProblemsPhysicsMany1979].

### $d=1$: Droplets inside droplets

Critical opalescence, boiling, 

![High-resolution reprint of [@guggenheimPrincipleCorrespondingStates1945, figure 2] in [@herbutModernApproachCritical2007, page 13].](figure/herbut_2007_corresponding_states.png)

![[@kadanoffOrderChaosII1999, page 298]](figure/kadanoff_1999_fig_1_3.png)

![[@kadanoffOrderChaosII1999, page 299]](figure/kadanoff_1999_fig_1_4.png)


Just below the critical temperature, the system has a free choice to make between being a liquid at high density or a vapor at low density. Both choices are equally good. But since liquid wants to be in contact with liquid and vapor with vapor, the system must decide.

A natural fluctuation produces a droplet of the wrong phase. This droplet secretes other material of the same density, and it grows larger and larger. It stops growing when the cost in available—i.e. free-energy for making the droplet of wrong phase becomes comparable with kT. Since near the critical point it cost very little free energy per unit volume to make the wrong phase, the droplet can grow very large.

The coherence length $\xi$ is a size of a typical droplet. As the critical point is approached, the free energy cost of making a droplet goes to zero, and the size of a typical droplet $\xi$, goes to infinity.

However, as criticality is approached, the difference in magnetization between the two different phases gets smaller and smaller. Hence the energetic cost per unit area of producing a region of the wrong phase approaches zero. For this reason, the area of a droplet and its radius both can get very, very large. Critical phenomena are connected with large-scale but weak fluctuations in the magnetization.

So far, our picture of critical fluctuations is like that in Fig. 1.3. Droplets with spin down of all sizes up to a maximum size $\xi$ appear near the critical point. However, this picture is incomplete. Each fluctuating region is also a nearly-critical system. Fluctuations appear within the droplets.

As you approach $T_c = 2.269\dots$ from above, you notice that little droplets seem to condense out of a hot grey gas.
Define reduced temperature as $t = T/T_c - 1$. So that critical point is $t=0$. When t is 0.1, there are small droplets. When t = 0.05, the droplets grow larger, but! If you zoom out by a factor of x (you can measure it experimentally by running two simulations and try to match them by eye, or by taking screenshots and match them with an image frequency analyzer), they look the same. 

So, this means that spatially zooming out by x is equivalent to increasing the reduced temperature by 2.

It is a similar thing for $t < 0$. At $t = -1$, the entire field freezes into one color. As t increases, small droplets appear... There is another scaling law. By renormalization theory, the two scaling laws have the same exponent. 

Renormalization is doing a zooming of the system. We start with the full system, then zoom it to describe it in a similar way that loses some details (and gains some details). This gives us another system. Repeated renormalization then is moving from one system to another in the space of possible systems.

This is called "renormalization flow in the space of Hamiltonians".

The fixed points of the flow are then critical systems. You apply the RN and get the same thing. This is a fractal, because zooming in you get the same thing. So it also has a power law, $1/f^a$ noise, and other things that fractals have.

> Big whirls have little whirls that feed on their velocity,
> 
> and little whirls have lesser whirls and so on to viscosity.

### Videos and interactives

[Complexity Explorables | I sing well-tempered](https://www.complexity-explorables.org/explorables/i-sing-well-tempered/)

[Ising model](https://www.ibiblio.org/e-notes/Perc/ising1k.htm)

[The Renormalisation Group - YouTube](https://www.youtube.com/watch?v=MxRddFrEnPc)

## Field theory: RN on $\R^d$ where $d\to \infty$

## Wilson's Nobel Prize: RN flow in theory-space

Kenneth Wilson was awarded the 1982 Nobel Prize in Physics for his work on phase transitions

The modern perspective is the perspective of  [@fisherRenormalizationGroupTheory1998]

![[@fisherRenormalizationGroupTheory1998, figure 4]](figure/fisher_1998_fig_4.png)

![[@fisherRenormalizationGroupTheory1998, figure 5]](figure/fisher_1998_fig_5.png)



## Reprise: What is renormalization?

### Universality

In the early 20th century, material scientists noticed the remarkable phenomenon of "corresponding states".

### Sociophysics

A koan

> "The details don't matter." said them triumphantly as they declared their independence from biophysics.
> 
> "'the details don't matter.'" said them mockingly as they declared their insurrection against sociophysics.

Explanation:

The traditional approach of historians, going back to the days of "kings and battles", is to run to personality theory and the individual acts when confronted by a problem in sociology or economics! One establishes the individual actors, makes some (hopefully) sensible approximations of their personality makeups and then proceeds to attempt to explain for events, actions and so on. However, for truly complicated systems in what, these days, is much better called "sociophysics", this is a hopeless task; furthermore, in many ways it is not even a very sensible one! The modern attitude is, rather, that the task of the theorist is to understand what is going on and to elucidate which are the crucial features of the problem. If one had a perfect archive of exactly what every person thought and said during the start of WWI, one would still have no understanding of why it started!

More is different [@andersonMoreDifferent1972]

## Appendix

I want to write about this someday: [@amirElementaryRenormalizationgroupApproach2020]