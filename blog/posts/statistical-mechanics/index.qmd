---
title: "Statistical Mechanics"
author: "Yuxi Liu"
date: "2024-07-01"
date-modified: "2024-07-01"
categories: [math, physics, philosophy, probability]
format:
  html:
    toc: true
    resources:
        - "figure/**"
description: ""

# image: "figure/banner.png"
# image-alt: ""
status: "draft"
confidence: "certain"
importance: 7
---

{{< include ../../../utils/blog_utils/_macros.tex >}}

## Introduction

### Quick reference

* Liouville's theorem: Hamiltonian dynamics preserves density in phase space.

## Overview

### Philosophical comments

It is fair to say that, although it originated in the 19th century like all other classical fields of physics, statistical mechanics is unsettled. 

Trajectory-centric statistical mechanics. In this view, we start with the equations of motion for a physical system, then study statistical properties of individual trajectories, or collections of them. For example, if we have a pendulum hanging in air, being hit by air molecules all the time, we would study the total trajectory $(\theta, x_1, y_1, z_1, x_2, y_2, z_2, \dots)$, where $\theta$ is the angle of the pendulum swing, and $(x_i, y_i, z_i)$ is the location of the $i$-th air molecule. Then we may ask that, over a long enough period, how frequent would the pendulum visit a certain angle range of $[\theta_0, \theta_0 + \delta\theta]$:

$$
Pr(\theta \in [\theta_0, \theta_0 + \delta\theta]) = \lim_{T \to \infty} \frac{1}{2T} \int_{-T}^{+T} 1[\theta \in [\theta_0, \theta_0 + \delta\theta]] dt
$$

In the trajectory-centric view, there are the following issues:

* Problem of ergodicity: When does time-average equal ensemble-average? A system is called "ergodic" iff for almost all starting conditions, the time-average of the trajectory is the ensemble-average over all trajectories.
* Problem of entropy: How is entropy defined *on a single trajectory*?
* H-theorem: In what sense, and under what conditions, does entropy increase?
* Problem of equilibrium: What does it mean to say that a trajectory is *in equilibrium*?
* Approach to equilibrium: In what sense, and under what conditions, does the trajectory converge to an equilibrium?
* Reversibility problem (*Umkehreinwand*): If individual trajectories are reversible, why does entropy increase instead of decrease?

While these philosophical problems are quite diverting, we will avoid them as much as possible, because we will be working with the ensemble-centric equilibrium statistical mechanics. This is the statmech that every working physicist uses, and this is what we will present. If you are interested in the philosophical issues, read the [Stanford Encyclopedia entry on the *Philosophy of Statistical Mechanics*](https://plato.stanford.edu/entries/statphys-statmech/).

### Principles of statistical mechanics

* A **physical system** is a classical system with a state space, evolving according to some equation of motion.
* An **ensemble** of that system is a probability distribution over its state space.
* The idea of (ensemble-centric) statistical mechanics is to study the evolution of an entire probability distribution over all possible states.
* The **entropy** of a probability distribution $\rho$ is

$$S[\rho] := -\int dx\; \rho(x) \ln \rho(x)$$

* Under any constraint, there exists a unique ensemble, named the **equilibrium ensemble**, which maximizes entropy under constraint.

Most of the times, the state space is a phase space, and the equation of motion is described by a Hamiltonian function. However, the machinery of statistical mechanics, as given above, is purely mathematical. It can be used to study any problem in probability whatsoever, even those with no physical meaning.

Believe it or not, the above constitutes the entirety of equilibrium statistical mechanics. So far, it is a purely mathematical theory, with no falsifiability (Popperians shouting in the background). To make it falsifiable, we need to add one more assumption, necessarily fuzzy:[^einstein-quote-reality]

* The equilibrium ensemble is physically meaningful and describes the observable behavior of physical systems.

In other words, when a physical system is *at equilibrium*, then everything observable can be found by studying it *as if* it has the maximum entropy distribution under constraint.

[^einstein-quote-reality]: 
    > As far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality. 
    > 
    > --- Albert Einstein, *Address to Prussian Academy of Sciences* (1921)

Of course, just what that "is physically meaningful" means, is another source of endless philosophical arguments. I would trust that you will know what is physically meaningful, and leave it at that, while those who have a taste for philosophy can grapple with the [Duhem--Quine thesis](https://plato.stanford.edu/entries/scientific-underdetermination/).

### Differential entropy depends on coordinates choice

There is a well-known secret among information theorists: differential entropy is not well-defined.

Consider the uniform distribution on $[0, 1]$. It is the maxent distribution on $[0, 1]$ -- relative to the Lebesgue measure. However, why should we pick the Lebesgue measure, and what happens if we don't?

Suppose we now stretch the $[0, 1]$ interval nonlinearly, by $f(x) = x^2$, then the maxent distribution relative to *that* would no longer be the uniform distribution on $[0, 1]$. Instead, it would be the uniform distribution after stretching.

The problem is this: Differential entropy is not coordinate-free. If we change the coordinates, we change the base measure, and the differential entropy changes as well.

To fix this, we need to use the KL-divergence, which is invariant under a change of base measure, as in
$$-D_{KL}(\rho \| \mu) := - \int dx\; \rho(x) \ln\frac{\rho(x)}{\mu(x)}$$

In typical situations, we don't need to worry ourselves with KL-divergence, as we just pick the uniform distribution $\mu$. When the state space is infinite in volume, the uniform distribution is not a probability measure, but it will work. Bayesians say that it is an *improper prior*.

In this interpretation, the principle of "maximum entropy distribution under constraint" becomes the principle of "minimal KL-divergence under constraint", which *is* Bayesian inference, with exactly the same formulas.

In almost all cases, we use the uniform prior over phase space. This is how Gibbs did it, and he didn't really justify it other than saying that *it just works*, and suggesting it has something to do with Liouville's theorem. Now with a century of hindsight, we know that it works because of quantum mechanics: We *should use* the uniform prior over phase space, because phase space volume has a natural unit of measurement: $h^N$, where $h$ is Planck's constant, and $2N$ is the dimension of phase space. As Planck's constant is a universal constant, independent of where we are in phase space, we should weight all of the phase space equally, resulting in a uniform prior.

## Mathematical developments

### Fundamental theorems

::: {#thm-liouville}

## Liouville's theorem

For any phase space and any Hamiltonian over it (which can change with time), phase-space volume is conserved under motion.

For any probability distribution $\rho_0$, if after time $t$, it evolves to $\rho_t$, and a point $x(0)$ evolves to $x(t)$, then $\rho_0(x(0)) = \rho_t(x(t))$.

:::

The proof is found in any textbook, and also [Wikipedia](https://en.wikipedia.org/wiki/Liouville's_theorem_(Hamiltonian)). Since it is already simple enough, and I can't really improve upon it, I won't.

::: {#cor-conservation-entropy}

## conservation of entropy

For a Hamiltonian system, with any Hamiltonian (which can change with time), for any probability distribution $\rho$ over its phase space, its entropy is conserved over time.

::: 

In particular, we have the following corollary:

::: {#cor-ensemble-conservation}

Given any set of constraints, if the Hamiltonian preserves these constraints over time, then any constrained-maximal entropy distribution remains constrained-maximal under time-evolution.

:::

In most cases, the constraint is of a particular form: the expectation is known. In that case, we have the following theorem:

::: {#thm-constrained-optimization}

## maximal entropy under linear constraints

For the following constrained optimization problem

$$
\begin{cases}
\max_\rho S[\rho] \\
\int A_1(x) \rho(x) &= \bar A_1 \\
\cdots &= \cdots \\
\int A_n(x) \rho(x) &= \bar A_n \\
\end{cases}
$$

Consider the following ansatz

$$
\rho(x) = \frac{1}{Z(a_1, \dots, a_n)} e^{-\sum_i a_i A_i(x)}
$$

where $Z(a_1, \dots, a_n) = \int e^{-\sum_i a_i A_i(x)} dx$, and $a_1, \dots, a_n$ are chosen such that the constraints $\int A_i(x) \rho(x) = \bar A_i$ are satisfied.

*If* the ansatz exists, then it is the unique solution.

:::

The ansatz solution is what you get by Lagrangian multipliers. For a refresher, see the [*Analytical Mechanics*#Lagrange's devil at Disneyland](https://yuxi-liu-wired.github.io/essays/posts/analytical-mechanics/index.html#lagranges-devil-at-disneyland). The theorem shows that the solution is unique -- provided that it exists. Does it exist? Yes, in physics. If it doesn't exist, then we are clearly not modelling a physically real phenomenon.

::: {.callout-tip}

In physics, these are "Boltzmann distributions" or "Gibbs distributions". In statistics, these are [exponential families](https://en.wikipedia.org/wiki/Exponential_family). Because they are everywhere, they have many names.

:::

::: {.callout-note title="Proof" collapse="true"}

Define a distribution $\rho$ as given in the statement of the theorem. That is,

$$
\rho(x) = \frac{1}{Z(a_1, \dots, a_n)} e^{-\sum_i a_i A_i(x)}
$$

etc. 

Now, it remains to prove that for any other $\rho'$ that satisfies the constraints, we have $S[\rho] \geq S[\rho']$.

By routine calculation, for any probability distribution $\rho'$,

$$
D_{KL}(\rho' \| \rho) = -S[\rho'] + \sum_i a_i \braket{A_i}_{\rho'} + \ln Z(a_1, \dots, a_n)
$$

If $\rho'$ satisfies the given constraints, then $D_{KL}(\rho' \| \rho) = -S[\rho'] + Const$ where the constant does not depend on $\rho'$, as long as it satisfies the constraints. Therefore, $S[\rho']$ is maximized when $D_{KL}(\rho' \| \rho)$ is minimized, which is exactly $\rho$.

::: 

The following proposition is often used when we want to maximize entropy in a two-step process:

::: {#thm-compound-entropy}

## compound entropy

If $\rho_{X,Y}$ is a probability distribution over two variables $(X, Y)$, then

$$S[\rho_{X,Y}] = S[\rho_Y] + \braket{S[\rho_{X|y}]}_y$$

or more succinctly,

$$S_{X,Y} = S_Y + \braket{S_{X|y}}_y$$

:::

::: {.callout-note title="Notations"}

$\rho_Y$ is the probability distribution over $Y$, after we integrate/marginalize $X$ away:

$$
\rho_Y(y) := \int \rho_{X,Y}(x,y)dx
$$

$\rho_{X|y}$ is the conditional probability distribution over $X$, conditional on $Y=y$:

$$
\rho_{X|y}(x) := \frac{\rho_{X,Y}(x,y)}{\int \rho_{X,Y}(x,y) dx}
$$

$\braket{\cdot}_y$ is the expectation over $\rho_Y$:

$$
\braket{S_{X|y}}_y := \int S_{X|y} \rho_Y(y)dy
$$

:::

::: {.callout-note title="Proof" collapse="true"}

Consider a compound system in ensemble $\rho(x, y)$. Its entropy is

$$S[\rho] = -\int dxdy \; \rho(x, y) \ln \rho(x, y)$$

We can take the calculation in two steps:

$$S[\rho] = -\int dxdy \; \rho(x|y)\rho(y) (\ln \rho(x|y) + \ln  \rho(y)) = S[\rho_Y] + \E_y[S[\rho_{X|y}]]$$

:::

Intuitively, what does $S_{X,Y} = S_Y + \braket{S_{X|y}}_y$ mean? It means that the entropy in $(X, Y)$ can be decomposed into two parts: the part due to $Y$, and the part remaining after we know $Y$, but not yet knowing $X$. In the language of information theory, the total information in $(X, Y)$ is equal to the information in $Y$, plus the information of $X$ conditional over $Y$:

$$
I(X, Y) = I(Y) + I(X|Y)
$$

### Microcanonical ensembles

If the only constraint is the constant-energy constraint $H(x) = E$, then the maximal entropy distribution is the uniform distribution on the shell of constant energy $H = E$. It is uniform, because once we enforce $H(x) = E$, there are no other constraints, and so by @thm-constrained-optimization, the distribution is uniform. 

Thus, we obtain the **microcanonical ensemble**:

$$\rho_E(x) \propto 1_{H(x) = E}$$

It is sometimes necessary to deal with the "thickness" of the energy shell. In that case, $\rho_E(x) \propto \delta(H(x) - E)$, where $\delta$ is the Dirac delta function.

By @thm-constrained-optimization, the microcanonical ensemble is the unique maximizer of entropy under the constraint of constant energy. In particular, if the Hamiltonian does not change over time, then any microcanonical ensemble is preserved over time. In words, if we uniformly "dust" the energy shell of $H(x) = E$ with a cloud of system states, and let all of them evolve over time, then though the dust particles move about, the cloud remains exactly the same.

More generally, we can impose more (in)equality constraints, and still obtain a microcanonical ensemble. For example, consider a ball flying around in an empty room with no gravity. The Hamiltonian is $H(q, p) = \frac{p^2}{2m}$, and its microcanonical ensemble is $\rho(q, p) \propto \delta(p = \sqrt{2mE})1[p \in \text{the room}]$. That is, its velocity is on the energy shell, while its position is uniform over the entire room.

If we want to specify the number of particles for each chemical species, then that can be incorporated into the microcanonical ensemble as well. For example, if we want the number of species $i$ be exactly $N_{i0}$, then we multiply $\rho$ by $1[N_i = N_{i0}]$.

### Canonical ensembles

If we have a small system connected to a large system, then we typically don't care about the large system, and only want to study the ensemble of the small system. In this case, we would first find the microcanonical ensemble for the total system, then integrate out of the large system, resulting in an ensemble over just the small system, as in

$$\rho_{\text{small}}(x) = \int \rho_{\text{total}}(x, y) dy$$

where $x$ ranges over the states of the small system, and $y$ of the large system.  

Assuming that the energy of the compound system is extensive, we obtain the canonical ensemble. Assuming that the energy and volume are both extensive, we obtain the grand canonical ensemble, etc. The following table would be very useful

| extensive constraint | ensemble | free entropy |
| --- | --- | --- |
| none | microcanonical | entropy |
| energy | canonical | Helmholtz free entropy |
| energy, volume | ? | Gibbs free entropy |
| energy, particle count | grand canonical | Landau free entropy |
| energy, volume, particle count | ? | ? |

There are some question marks in the above table, because there are no consensus names for those question marks. What is more surprising is that there is no name for the ensemble of constrained energy and volume. I would have expected something like the "Gibbs ensemble", but history isn't nice to us like that. Well, then I will name it first, as the *big canonical ensemble*. And while we're at it, let's fill the last row as well:

| extensive constraint | ensemble | free entropy |
| --- | --- | --- |
| none | microcanonical | entropy |
| energy | canonical | Helmholtz free entropy |
| energy, volume | big canonical | Gibbs free entropy |
| energy, particle count | grand canonical | Landau free entropy |
| energy, volume, particle count | gross canonical | EVN free energy |

::: {.callout-tip title="Extensivity"}

In classical thermodynamics, extensivity means that entropy of the compound system can be calculated in a two-step process: calculate the entropy of each subsystem, then add them up. The important fact is that a subsystem still has enough independence to have its own entropy.

This is not always obvious. If we have two galaxies of stars, we can think of each as a "cosmic gas" where each particle is a star. Now, if we put them near each other, then the gravity between the two galaxies would mean it is no longer meaningful to speak of "the entropy of galaxy 1", but only "the entropy of galaxy-compound 1-2".

In statistical mechanics, extensivity means a certain property of each subsystem is unaffected by the state of the other subsystems, and the total is the sum of them. So for example, if $A$ is an extensive property, then it means

$$
A(x_1, \dots, x_n) = A_1(x_1) + \dots + A_n(x_n)
$$

Like most textbooks, we assume extensivity by default, although as we noted in [*Classical Thermodynamics and Economics*](https://yuxi-liu-wired.github.io/essays/posts/equilibrium-thermoeconomics/), both classical thermodynamics and statistical mechanics do not require extensivity. We assume extensivity because it is mathematically convenient, and good enough for most applications.

:::

In the following theorem, we assume that the total system is extensive, and is already in the maximal entropy distribution (microcanonical ensemble)

::: {#thm-canonical-ensembles}

If the two systems are in energy-contact, and energy is conserved, and energy is extensive, and the compound system is in a microcanonical ensemble, the small system is in a **canonical ensemble**

$$
\rho(x) \propto e^{-\beta H(x)}
$$

where $\beta$ is the marginal entropy of energy of the large system:   

$$\beta := \partial_E S[\rho_{bath, E}]$$

Similarly, if the two systems are in energy-and-particle-contact, then the small system has the **grand canonical ensemble**

$$
\rho(x) \propto e^{-(\beta H(x) + (-\beta \mu) N(x))}
$$

where $-\beta\mu$ is the marginal entropy of particle of the large system:   

$$-\beta\mu := (\partial_N S[\rho_{bath, E, N}])_{E}$$

Most generally, if the two systems are in $q_1, \dots, q_m$ contact, and $q_1, \dots, q_m$ are conserved and extensive quantity, then

$$\rho(x) \propto e^{-\sum_i p_i q_i(x)}$$

where $p_i = (\partial_{q_i} S[\rho_{bath, q}])_{q}$ is the marginal entropy of $q_i$ of the large system.

:::

::: {.callout-note title="Proof" collapse="true"}

We prove the case for the canonical ensemble. The other cases are similar.

Since the total distribution of the whole system is the maximal entropy distribution, we are faced with a constrained maximization problem:

$$\max_\rho S[\rho]$$

By @thm-compound-entropy, 

$$S = S_{system}(E_{system}) + \braket{S_{bath|system}(E_{total} - E_{system})}_{system}$$

Since the bath is so much larger than the system, we can take just the first term in its Taylor expansion:

$$S_{bath|system}(E_{total} - E_{system}) = S_{bath}(E_{total}) - \beta E_{system}$$

where $E_{total}$ is the total energy for the compound system, $\beta = \partial_E S_{bath}|_{E = E_{total}}$ is the marginal entropy per energy, and $E_{system}$ is the energy of the system.  

This gives us the linearly constrained maximization problem of

$$\max_{\rho_{system}} (S_{system} - \beta \braket{E_{system}}_{\rho_{system}})$$

and we apply Lagrange multipliers to finish the proof.

:::

::: {.callout-note title="extensivity"}

Extensivitiy in statistical mechanics yields extensivity in thermodynamics. Specifically, writing $S_{bath}(E)$, instead of $S_{bath}(E, E_{system})$, requires the assumption of extensivity. Precisely because the bath and the system do not affect each other, we are allowed to calculate the entropy of the bath without knowing anything about the energy of the system.

$S_{bath}$ is the logarithm of the surface area of the energy shell $H_{bath} = E_{bath}$. By extensivity, $H(x_{bath}, x_{system}) = H_{bath}(x_{bath}) + H_{system}(x_{system})$, so the energy shells of the bath depends on only $E_{bath}$, not $E_{system}$.

::: 

::: {.callout-tip}

The proof showed something extra: If the small system is in distribution $\rho$ that does not equal to the equilibrium distribution $\rho_B$, then the total system's entropy is

$$S = S_{max} - D_{KL}(\rho \| \rho_B)$$

which reminds me of Sanov theorem and large deviation theory...  

:::

::: {.callout-note title="Enthalpic ensemble"}

What if we have a system in volume-contact, but not thermal-contact? This might happen when the system is a flexible bag of gas held in an atmosphere, but the bag is thermally insulating. Notice that in this case, the small system still exchanges energy with the large system via $d\braket{E} = -Pd\braket{V}$. We don't have $E = -PdV$, because the small system might get unlucky. During a moment of weakness, all its particles has abandoned their frontier posts, and the bath has taken advantage of this by encroaching on its land. The system loses volume by $\delta V$, without earning a compensating $\delta E = P \delta V$. In short, the thermodynamic equality $E = -PdV$ is inexact in statistical mechanics, and only holds true on the ensemble average.

In this case, because pressure is a constant, we have $d(E + PV) = 0$, and so we have the enthalpic ensemble $\rho \propto e^{-\beta H}$, where $H := E + PV$ is the [enthalpy](https://en.wikipedia.org/wiki/Enthalpy),[^enthalpy-confusion].

Specifically, if you work through the same argument, you would end up with the following constrained maximization problem:

$$
\begin{cases}
\max_{\rho_{system}} (S_{system} - \beta \braket{E_{system}}_{\rho_{system}} - \beta P \braket{V}) \\
\braket{E_{system}} + P\braket{V_{system}} = Const
\end{cases}
$$

yielding the enthalpic ensemble (or the [isoenthalpic-isobaric ensemble](https://en.wikipedia.org/wiki/Isoenthalpic%E2%80%93isobaric_ensemble)).

[^enthalpy-confusion]: Sorry, I know this is not Hamiltonian, but we are running out of letters.

:::

### Free entropies

Just like in thermodynamics, it is useful to consider free entropies, which are the convex duals of the entropy:

* Helmholtz free entropy: $f[\rho] := S[\rho] - \beta \braket{E} = \int dx \; \rho(x) (-\ln \rho(x) - \beta E(x))$;
* Gibbs free entropy: $g[\rho] = S[\rho] - \beta \braket{E} - \beta P \braket{V}$;
* Landau free entropy: $\omega[\rho] = S[\rho] - \beta \braket{E} + \beta \mu \braket{N}$;

etc. Of those, we would mostly use the Helmholtz free energy, so I will write it down again:

$$
f[\rho] := S[\rho] - \beta \braket{E} = \int dx \; \rho(x) (-\ln \rho(x) - \beta E(x))
$$

::: {#thm-chain-rule}

## chain rule for free entropies

$f_X = S_Y + \braket{f_{X|y}}_y$, and similarly $g_X = S_Y + \braket{g_{X|y}}_y$, and similarly for all other free entropies.

:::

::: {.callout-note title="Proof" collapse="true"}

$$
\begin{aligned}
  f_X &= S_X - \beta \braket{E}_x  \\
  &= S_Y + \braket{S_{X|y}}_y - \beta \braket{\braket{E}_{x \sim X|y}}_y \\
  &= S_Y + \braket{f_{X|y}}_y
\end{aligned}
$$

:::

A common trick in statistical mechanics is to characterize the same equilibrium in many different perspectives. For example, the canonical ensemble has at least 4 characterizations at least. "Muscle memory" in statistical mechanics would allow you to nimbly applying the most suitable one for any occasion.

::: {#thm-canonical-characterization}

## 4 characterizations of the canonical ensemble

1. (total entropy under fixed energy constraint) The canonical ensemble maximizes total entropy when the system is in contact with an energy bath that satisfies $\partial_E S_{bath} = \beta$, and the total energy is fixed.
2. (entropy under mean energy constraint) A system maximizes its entropy under constraint $\braket{E} = E_0$ when it assumes the canonical ensemble with $\beta$ that is the unique solution to $\int dx \; e^{-\beta E(x)} = E_0$.
3. (thermodynamic limit): Take $N$ copies of a system, and connect them by energy-contacts. Inject the system with total energy $NE_0$, and let the system reach its microcanonical ensemble. Then at the thermodynamic limit of $N\to \infty$, the distribution of a single system is the canonical distribution with $\beta$ that is the unique solution to $\int dx \; e^{-\beta E(x)} = E_0$.
4. (free entropy) A system maximizes its Helmholtz free entropy when it assumes the canonical ensemble. At the optimal distribution $\rho^*$, the maximal Helmholtz free entropy is $f[\rho^*] = \ln Z$, where $Z = \int dx \; e^{-\beta E(x)}$ is the partition function.  

::::

::: {.callout-note title="Proof" collapse="true"}

1. We already proved this.
2. Use the Lagrange multiplier.
3. Isolate one system, and treat the rest as an energy-bath.
4. $f[\rho] = \ln Z - D_{KL}(\rho \| \rho_B)$.

:::

### The partition function

When the system is in a canonical ensemble, we can define a convenient variable $Z = \int dx\; e^{-\beta E(x)}$ called the **partition function**. As proven in @thm-canonical-characterization, the partition function is equal to $e^f$, where $f$ is the Helmholtz free entropy of the canonical ensemble.

::: {#thm-partition-cumulant}

## the partition function is the cumulant generating function of energy

Let a system be in canonical ensemble with inverse temperature $\beta$, and let $K(t) := \ln \braket{e^{tE}}$ be the [cumulant generating function](https://en.wikipedia.org/wiki/Cumulant) of its energy, then
$$K(t) = \ln Z(\beta-t) - \ln Z(\beta)$$

In particular, the $n$-th cumulant of energy is  
$$\kappa_n(E) = K^{(n)}(t) |_{t=0} = (-\partial_\beta)^n (\ln Z)$$

A similar proposition applies for the other ensembles and their free entropies.

:::

The proof is by direct computation.

For example, the first two cumulants are the mean and variance:

$$\braket{E} = (-\partial_\beta) (\ln Z), \quad \mathrm{Var}(E) = \partial_\beta^2 (\ln Z)$$

Typical systems are made of $N$ particles, where $N$ is large, and that these particles are only weakly interacting. In this case, the total Helmholtz free entropy per particle converges at the thermodynamic limit of $N \to \infty$:

$$
\lim_N \frac 1N \ln Z \to \bar f_\beta
$$

Thus, for large but finite $N$, we have

$$\braket{E} \approx -N \partial_\beta \bar f_\beta, \quad \mathrm{Var}(E) = N\partial_\beta^2 \bar f_\beta$$

In particular, the relative fluctuation scales like $\frac{\sqrt{\mathrm{Var}(E)}}{\braket{E}} \sim N^{-1/2}$.

### Conditional entropies

Given any two random variable $X, Y$, and an "observable" variable $Y$ that is determined by $X$ by some function $h$, such that $Y = h(X)$. If we know $X$, we would know $Y$, but it is not so conversely, as multiple $X$ may correspond to the same $Y$. Typically, we use $Y$ as a "summary statistic" for the more detailed, but more complicated $X$. For example, we might have multiple particles in a box, such that $X$ is their individual locations, while $Y$ is their center of mass.

::: {#thm-cond-ent}

## conditional entropy

Given any random variable $X$, and an "observable" variable $Y$ that is determined by $X$, and some constraints $c$ on $X$, if $X$ is the distribution that maximizes entropy under constraints $c$, with entropy $S_X^*$, then the observable $Y$ is distributed as

$$\rho_Y^*(y) = e^{S_{X|y}^* - S_X^*}, \quad e^{S_X^*} = \int dy\; e^{S_{X|y}^*}$$

where $S_{X|y}^*$ is the maximal entropy for $X$ conditional on the same constraints, plus the extra constraint that $Y = y$.

::: 

::: {.callout-note title="Proof" collapse="true"}

By assumption, $X$ is the unique solution to the constrained optimization problem

$$
\begin{cases}
    \max S_X \\
    \text{constraints on $x$}
\end{cases}
$$

By @thm-compound-entropy, the problem is equivalent to:

$$
\begin{cases}
    \max S_Y + \braket{S_{X|y}}_{y\sim Y} \\
    \text{constraints on $x$}
\end{cases}
$$

Now, we can solve the original problem in a two-step process: For each possible observable $y\sim Y$, we solve an extra-constrained problem:

$$
\begin{cases}
    \max S_{X|y} \\
    \text{original constraints on $x$} \\
    \text{$x$ must be chosen such that the observable $Y = y$}
\end{cases}
$$

Then, each such problem gives us a maximal conditional[^measure-disintegration] entropy $S_{X|y}^*$, and we can follow it up by solving for $Y$ with

$$\max\lrb{S_Y + \braket{S_{X|y}^*}_{y \sim Y}}$$

[^measure-disintegration]: If you're a pure mathematician, you can formalize this using [measure disintegration](https://en.wikipedia.org/wiki/Disintegration_theorem).

Again, the solution is immediate once we see it is just the KL-divergence:

$$S_Y + \braket{S_{X|y}^*}_{y \sim Y} = - \int dy \; \rho_Y(y) \ln\frac{\rho_Y(y)}{e^{S_{X|y}^*}} = \ln Z - D_{KL}(\rho_Y \| \rho_Y^*)$$

where

$$Z = \int dy\; e^{S_{X|y}^*}, \quad \rho_Y^*(y) = \frac{e^{S_{X|y}^*}}{Z}$$

At the optimal point, the entropy for $X$ is maximized at $S_X^* = \ln Z - 0$, so $Z = e^{S_X^*}$.

:::

::: {.callout-note title="deriving the canonical ensemble yet again"}

Consider a small system with energy states $E_1, E_2, \dots$ and a large bath system, in energy contact. We can set $X$ to be the combined state of the whole system, and $Y$ to be the state of the small system. Once we observe $y$, we have fully determined the small system, so the small system has zero entropy, and so all the entropy comes from the bath system:
$$S_{X|y}^* = S_{bath} = S_{bath}(E_{total}) - \beta E_y$$

Consequently, the distribution of the small system is $\rho_Y(y) \propto e^{-\beta E_y}$, as we expect.

A similar calculation gives us the grand canonical ensemble, etc.

:::

::: {#thm-cond-free-ent}

## conditional free entropy

Given any random variable $X$, and an "observable" variable $Y$ that is determined by $X$, and some constraints $c$ on $X$, if $X$ is the distribution that maximizes Helmholtz free entropy under constraints $c$, with Helmholtz free entropy $f_X^*$, then the observable $Y$ is distributed as
$$\rho_Y^*(y) = e^{f_{X|y}^* - f_X^*}, \quad e^{f_X^*} = \int dy\; e^{f_{X|y}^*}$$

where $f_{X|y}^*$ is the maximal Helmholtz free entropy for $X$ conditional on the same constraints, plus the constraint that $Y = y$.  

Similarly for Gibbs free entropy, and all other free entropies.

:::

::: {.callout-note title="Proof" collapse="true"}

First note that $f_X = S_Y + \braket{f_{X|y}}_y$, then argue in the same way.

:::

## Thermodynamic limit

### Fluctuation of observables

Suppose we have a tank of oxygen gas, and it is in the equilibrium distribution (Maxwell-Boltzmann). Now, if we sample its pressure $P$, then every time we sample it, we sample a particular microstate $x$ from its equilibrium distribution, and each corresponds to a different pressure $P(x)$. We know that these particular pressures should be tightly bunched around its average value -- the thermodynamic pressure $\braket{P}$... but how bunched-up is it?

More generally, suppose we have a system in the equilibrium state (maximal entropy under constraint), how much fluctuation does it have?

EXP. systems in energy-contact, the zeroth law

Take several systems, and let them exchange energy, but nothing else. For concreteness, we can imagine taking several copper tanks of gas, and let them touch each other. The tanks hold their shape, not expanding or contracting.

The system has total entropy 
$$S = \sum_i S_i(E_i, A_i)$$

where $A_i$ stand for the other state variables we don't care about, because they are held constant  

There is a single constraint of constant total energy:
$$E = \sum_i E_i$$

In the thermodynamical limit, the compound system reaches the maximal entropy state $E_1^*, \dots, E_n^*$, which solves the following constrained maximization
$$
\begin{cases}
    \max \sum_i S_i(E_i, A_i)\\
    E = \sum_i E_i
\end{cases}
$$

By calculus, at the optimal point, all systems satisfy  

$$
(\partial_{E_i} S_i)_{A_i} = \beta
$$

for some number $\beta$. This is the zeroth law of thermodynamics.

However, we are in statistical mechanics, so the compound system actually does not stay exactly at the optimal point. Instead, the energy levels fluctuate.

Let us write the fluctuation vector as 

$$Y = (\Delta E_1, \dots, \Delta E_n)$$

which satisfies the constraint $\sum_i \Delta E_i = 0$.  

Let the fluctuation vector be the observable. As proved previously, the fluctuation satisfies

$$\rho_Y(y) \propto e^{S^*_{X|y}}$$

where $S^*_{X|y}$ is the entropy of the compound system, given $Y = y$. For small fluctuations, this is just:

$$S^*_{X|y} = \sum_i S_i(E_i^*) + (\partial_{E_i} S_i)_{A_i} \Delta E_i + \frac 12 (\partial_{E_i}^2 S_i)_{A_i} (\Delta E_i)^2 + \cdots$$

Since $\sum_i \Delta E_i = 0$, this just gives

$$\rho_Y(y) \propto e^{\sum_i \frac 12 (\partial_{E_i}^2 S_i)_{A_i} (\Delta E_i)^2}$$

Now, $\partial_E S = \beta$, and $\partial_E^2 S = -\frac{1}{T^2 C}$ in typical thermodynamic notation, where $C$ is $\partial_T E$, the heat capacity (holding all other variables $A$ constant).

In particular, for gases, it is 

$$\rho_Y(y) \propto e^{-\sum_i \frac{1}{2T^2 C_{V, i}} (\Delta E_i)^2}$$

where $C_{V, i}$ is the constant-volume heat capacity of the $i$ -th gas.  

In particular, if we have monoatomic ideal gas, with $C_V = \frac 32 N$, then the size of a typical fluctuation is on the order of 

$$\sim\sqrt N T \sim N^{-1/2} E^*$$

That is, a typical fluctuation energy is only $N^{-1/2}$ that of the mean energy.  

### Density fluctuation in the canonical ensemble

TODO sethna section 6.7


## Maximum caliber

In typical developments, statistical mechanics is a "static" theory: It deals with the ensemble of states, but not with how the states change over time. Maximal caliber statistical mechanics handles trajectories in the most obvious way: Collect all paths into a "path space", define a measure over path space, then study constrained entropy maximization over path space. Jaynes, who proposed the idea, called entropy in path space "caliber", so the name "maximum caliber" stuck, even though it is really just another instance of maximum entropy.

### Markov chains

$N$ timesteps in total.

$s_t$ is the state of timestep $t$.

There are $m$ states in total.

::: {#thm-todo}

If we fix the singleton probability $p_i$ of each state, then the maximum entropy distribution is the Markov chain with uniform initial probability and $p_{i\to j} = p_i p_j$.

:::

::: {.callout-note title="Proof" collapse="true"}

By Lagrange multipliers.

If we fix the singleton probability of each state, then the problem is a constrained maximization problem

$$
\begin{cases}
    \max S \\
    \frac 1N \sum_t 1[s_t = k] = p_k, \quad \forall k = 1, \dots, m
\end{cases}
$$

This is the same problem as $N$ balls in a certain constrained microcanonical ensemble. When $N$ is large enough, the discrete "macrostate" $\frac 1N \sum_t 1[s_t = k]$ becomes continuous, and we can use the Lagrange multiplier to obtain
$$\rho(s_1, \dots, s_N) = \frac 1Z e^{-\sum_{k=1}^m \lambda_k (\frac 1N \sum_{t=1}^N 1[s_t =k] - p_k)}$$

This factors over time $t$, giving us
$$\rho(s_1, \dots, s_N) = \prod_{t=1}^N \rho(s_t)$$

with   
$$\rho(s_t=k) \propto e^{-\sum_{k=1}^m\frac{\lambda_k}{N}(1[s_t =k] - p_k)} \propto e^{-\frac{\lambda_k}{N}}$$

The multiplier $\lambda_k$ can be found by the typical method of solving $p_k = -\partial_{\lambda_k}\ln Z$, or we can take the shortcut and notice that $\rho(s_t=k) = p_k$, and be done with it.

:::

::: {#thm-todo}

If we fix the transition probability $p_{i \to j}$ of each state-pair, then the maximum entropy distribution is the Markov chain with uniform initial probability. If we fix the initial probability, then it's still the Markov chain with the same transition probabilities.

And more generally, if we fix $n$-th order transition probability $p_{i_1, \dots, i_n \to j}$, then we obtain an $n$-th order Markov chain model.

:::

::: {.callout-note title="Proof" collapse="true"}

Similarly as above, the path-space distribution is
$$\rho(s_1, \dots, s_N) \propto \prod_{t=1}^{N-1} e^{-\sum_{k, k' \in 1:m} \frac{\lambda_{k, k'}}{N} 1[s_t = k, s_{t+1} = k']} \propto \prod_{t=1}^{N-1} p_{s_t \to s_{t+1}}$$

Because the distribution does not specify $s_1$, it is uniformly distributed on $s_1$. Otherwise, we can constrain $s_1$ with yet another set of Lagrange multipliers and obtain $\rho(s_1, \dots, s_N) \propto \rho(s_1) \times_{t=1}^{N-1} \rho(s_t, s_{t+1})$. Similarly for higher orders.

:::


### diffusion

The path-space entropy 

$$
S = - \int \rho(x) \ln \rho(x) D[x]
$$

where $D[x]$ means we integrate over path space, and $x: [0, T] \to \R^n$ is a path.  

We discretize the path into $x: \{0, 1, 2, \dots, N\} \to \R^n$, then because we can decompose

$$
\rho(x) = \rho(x_0) \rho(x_1 | x_0) \cdots \rho(x_N | x_{0:N-1})
$$

the path-space entropy decomposes sequentially:  

$$
S = S[x_0] + E[S[x_1 | x_0]] + E[S[x_2 | x_{0:1}]] + \dots + E[S[x_N | x_{0:N-1}]]
$$

To prevent the entropy from diverging, we need to impose some constraints. If we constrain the second moment of each step, as 

$$(E[x_t|x_{0:t-1}], E[x_t^2|x_{0:t-1}]) = (0, \sigma^2), \quad \forall t \in 0:N$$

by reasoning backwards from $t = N$ to $t=0$, we find that the maximal entropy distribution is a white noise:  

$$
\rho(x) = \prod_{t\in 0:N} \rho(x_t), \quad \rho(x_t) \propto e^{-\frac{\|x_t\|^2}{2\sigma^2}}
$$

If you have studied dynamic programming and cybernetics, this should look very similar to the argument by which you derived the [LQR](https://en.wikipedia.org/wiki/Linear-quadratic_regulator).

To keep the path from exploding into white noise, we instead impose the constraints on the step sizes

$$(E[x_t - x_{t-1}|x_{0:t-1}], E[\|x_t - x_{t-1}\|^2|x_{0:t-1}]) = (0, \sigma^2), \quad \forall t \in 1:N$$

and $x_0 = 0$.  

Now, as in dynamical programming, we can reason backwards from $t = N$ to $t=0$, and we find that the maximal entropy distribution is the Brownian motion

$$\rho(x) \propto e^{-\frac{\sum_{t\in 1:N} \| x_t-x_{t-1}\|^2}{2\sigma^2}}$$

If we constrain the first *and* second moments of each step, and allow them to be affected by the previous step, as in

$$
\begin{cases}
    E[x_t - x_{t-1}|x_{0:t-1}] &= \mu(t, x_{t-1}) \\
    E[(x_t - x_{t-1}) (x_t - x_{t-1})^T|x_{0:t-1}] &= \Sigma(t, x_{t-1})
\end{cases}, \quad \forall t \in 1:N
$$

then, reasoning backwards as before, we would obtain the [Fokker--Planck equation](https://en.wikipedia.org/wiki/Fokker-Planck_equation).  

Other results, such as the Green-Kubo relation, the Onsager reciprocal relations, etc, can be similarly derived by imposing the right constraints in path space. [@hazoglouCommunicationMaximumCaliber2015]


### Fluctuation-dissipation relations

Imagine if you have a weird liquid. You put a piece of pollen into it, and watch it undergo Brownian motion. Now I flip a switch and suddenly all friction disappears from the liquid. What would happen? The pollen is still being hit by random pieces of molecules, so its velocity is still getting pushed this and that way. However, without friction, the velocity at time $t$ is now obtained by integrating all past impulses, with no dissipation whatsoever. Consequently, its velocity undergoes a random walk, and its position undergoes $\int_0^t W_s ds$. This is very different from what we actually observe, which is that its position undergoes a random walk, not a time-integral of it.

In order to reproduce the actual observed behavior, each fluctuation of its velocity must be quickly dissipated by friction, in a perfect balance such that $\frac 12 m \braket{v^2} = k_BT$ exactly. This perfect conspiracy is the fluctuation-dissipation relation (FDR), or rather, the *family* of FDRs, because there have been so many of those.

Each FDR is a mathematical equation of form

$$
\text{something about fluctuation} = \text{something about dissipation}
$$

The prototypical FDR is the [Einstein relation](https://en.wikipedia.org/wiki/Einstein_relation_(kinetic_theory)), to be derived below:

$$
\ub{(\beta D)^{-1}}{fluctuation} = \ub{\gamma}{dissipation}
$$

where the left side can be interpreted as the strength of molecular noise, and the right side as the strength of molecular damping.

### Equality before the law

Why should there be a mathematical relation between fluctuation and dissipation? I think the deep reason is "equality before the second law".

If we pause and think about it, then isn't it strange that, despite the molecular storm happening within a placid cup of water, it does not actually fall out of equilibrium? That, despite every molecule of water rushing this way and that, the cup of water as a whole continues to stay without a ripple? What is this second law, the invisible hand pushing it towards statistical equilibrium and keeping it stable? And that is not a question I'm going to answer here. Perhaps calling it an "invisible hand" is already a bad metaphor. Nevertheless, the second law requires the stability of equilibrium distributions (if equilibria exist).

Now, a system in a statistical equilibrium would, once in a while, deviate significantly from the maximally likely state, just like how we might occasionally flip 10 heads in a row. Nevertheless, if the second law is to be upheld, then significant deviations must be pushed back. In other words, fluctuation is dissipated.

Assuming that in equilibrium, the system is undergoing a random walk, then by the theory of random walks, internally generated fluctuation must be dissipated according to a mathematical law. The law does not need any further input from physics. Indeed, it has no dependence on any physical observations, and belongs to the pure mathematical theory of random walks.

For example, a little pollen in a dish of water might occasionally have a large momentum, but it is very rare. Because it is rare, if we do observe it, we can predict that the next time we look at a pollen, it would have a much lower momentum.

Now, when physicists use the word "dissipation", they mean the restoring effect of a system under *external* dissipation, such as pulling on a pollen by an electrostatic field. However, physical laws are equal, so internal dissipation is external dissipation.

Thus, we see that each FDR manifests as an "equality before the law":

$$
\begin{aligned}
&\text{fluctuation} \\
\ub{=}{random walk theory} &\text{dissipation (of internal fluctuations)} \\
\ub{=}{equality before the law} &\text{dissipation (of external fluctuations)}
\end{aligned}
$$

### One-dimensional FDR

Consider a simple model of Brownian motion. We have a particle in a sticky fluid, moving on a line. The particle starts at $x = 0, t = 0$, and at each time-step of $\Delta t$, it moves by $\Delta x$ to the left or the right.

The fluid is at temperature $\beta^{-1}$, and we pull on the particle at constant force $F$. We expect that $F = \gamma \braket{v}$, where $v$ is the ensemble-average velocity of the particle, and $\gamma$ is the viscosity constant.

Now, we let the particle move for a time $t = N\Delta t$, where $N$ is a large number. The particle would have arrived at some point $x$, which is a random variable. The particle's time-averaged velocity is $v = x/t$.

The number of possible paths that connect $(0, 0)$ with $(t, x)$ is $\binom{N}{\frac N2 - \frac{x}{2\Delta x}}$, therefore, the path-space entropy is

$$S_{path} = \ln \binom{N}{\frac N2 - \frac{x}{2\Delta x}} \approx N \lrb{\ln 2 - \lrb{\frac{x}{N\Delta x}}^2}$$

where the approximation is either by Stirling's approximation, or the binary entropy function.

Because the external force performs work $Fx$, which is dissipated into the sticky liquid at temperature $\beta$, we also have

$$S_{work} = \beta F x$$

Because $N$ is large, $\braket{x}$ should be highly concentrated around the point of maximal entropy. That is, we should have

$$
\braket{x} \approx \argmax_x (S_{path} + S_{work})
$$

Notice how this is the exact same problem as the case where we have a rubber band.

The equation on the right is quadratic in $\braket{x}$, and achieves maximum at $2\braket{x} = \beta FN(\Delta x)^2$, which simplifies to the Einstein relation 

$$\beta D \gamma = 1$$

where $D = \frac{\Delta x^2}{2\Delta t}$ is the diffusion coefficient. 

::: {.callout-tip}

We can calculate not just the mean $\braket{x}$, but also its variance $\braket{x^2}$ if we expand $S_{path} + S_{work}$ to second order around its maximum, then apply @thm-cond-ent.

:::

## Arrhenius equation

Often in chemistry and biology, we have a particle that is stuck in a shallow potential well. If it ever gets over an energetic barrier, then it can drop to a much deeper minimum. If the particle is in thermal equilibrium with a bath, then it will eventually gain enough energy by random chance to get over the barrier. Of course, conversely, a particle in the deeper minimum would occasionally gain enough energy to go back to the shallow well.

The point is that "getting over a potential barrier" is inherently a non-equilibrium phenomenon, and therefore does not logically belong to an essay on *equilibrium* statistical mechanics. Still, if we are willing to approximate, then we can derive it easily.

We model a particle in a shallow potential well as a simple harmonic oscillator. If it ever gains more than $\Delta E$ of energy, then it would escape the well. Thus, we can take a look at the particle. If it has yet to escape, then we wait for time $\tau$ (relaxation time) until the particle has reached thermal equilibrium again with the bath, and take another look. The time-until-escape is $N \tau$, where $N$ is the number of times we look at the system.

![Getting over a potential well.](figure/metastable_potential_well.png)

::: {.callout-tip title="Relaxation time"}

When we look at the oscillator, its state is totally fixed at some $(q, p)$. If we immediately look again, we would not find it in a Boltzmann distribution over phase space, but still at $(q, p)$. How long must we wait? That is beyond the scope. Suffice to say that we should wait a while, not too short, called relaxation time $\tau$, after which the system is close enough to equilibrium. But, here's the key -- we cannot wait for too long, because in the long run, the particle would have escaped the potential well. Formalizing all these things requires a proper stochastic calculus, which I might write about later.

:::

Let the oscillator have $n$ dimensions, then its energy function is $H = \sum_{i=1}^n \frac{p_i^2}{2m_i} + \frac{k_i q_i^2}{2}$, where $q_i, p_i$ are the generalized position and momentum, and $m_i, k_i$ are the effective masses and elastic constants. The Boltzmann distribution is $\rho(q, p) = Z^{-1} e^{-\beta H}$, and the probability that it has enough energy to overcome the barrier is

$$
P = \frac{\int_{H \geq \Delta E} \rho(q, p)dqdp}{\int_{H \geq 0} \rho(q, p)dqdp}
$$

Notice that the proportionality constant $Z$ is removed. After a change of variables by $x_i = \frac{p_i}{\sqrt{2m_i}}, y_i = \sqrt{\frac{k_i}{2}} q_i$, we get

$$
P = \frac{\int_{\sum_i x_i^2 + y_i^2 \geq \Delta E} e^{-\beta \sum_i (x_i^2 + y_i^2)} dxdy}{\int_{\sum_i x_i^2 + y_i^2 \geq 0} e^{-\beta \sum_i (x_i^2 + y_i^2)} dxdy}
$$

Integrating in spherical coordinates, and simplifying, we get $P = e^{-\beta \Delta E}$. Thus, the expected time until escape is

$$
T = \braket{N}\tau = \frac{1}{P}\tau = \tau e^{\beta \Delta E}
$$

the Arrhenius equation. The same calculation, for a system held in contact with an energy-and-volume bath, gives us $T = \tau e^{\beta \Delta G}$, where $\Delta G$ is the Gibbs free energy barrier.

In practice, the Arrhenius equation is used in the form of $\ln f = - a T^{-1} + Const$, where $f = 1/T$ is the "reaction rate" or "characteristic frequency", and $a = \frac{\Delta E}{k_B}$ is the slope of the $T^{-1} - \ln f$ plot ("the Arrhenius plot").

### Applications

The argument given above for the Arrhenius equation is quite generic. It only assumes there is a system that is stuck in some kind of potential well, and is held at a constant temperature somehow. There is no requirement for the system to be an actual particle in an actual well. The "particle" can very well be the 100-dimensional configuration of a protein during folding, or even the simultaneous position of $10^{23}$ helium atoms in a helium gas. Indeed, the Arrhenius equation pops up everywhere as the time until a system escapes an energy barrier.

In a glass of water held at constant temperature $300 \ut{K}$, each water molecule might occasionally reach enough energy to escape into open air. This is evaporation. By this argument, the rate of evaporation follows the Arrhenius law, and indeed it does. I suspect that Arrhenius law holds for the spontaneous crystallization of [supercooled liquid](https://en.wikipedia.org/wiki/Supercooling) and spontaneous conversion of [disappearing polymorphs](https://en.wikipedia.org/wiki/Disappearing_polymorphs), but I cannot find good data on this.

In the simplest model for biochemical process, we just have one chemical reaction following another, until it is complete. If there is a single biochemical step that is much slower than the other steps, then the waiting time for that step dominates, and the total reaction should depend on the temperature by an Arrhenius law. This might explain the observed Arrhenius-law-like dependence on temperature in biological phenomena like tree cricket chirping, alpha brain wave frequency, etc.

![(1) Tree cricket chirping frequency. (2) Firefly flashing frequency. (3) Terrapin heartbeat frequency. (4) Human silent counting rate [@hoaglandPhysiologicalControlJudgments1933]. These figures are reproduced in [@laidlerUnconventionalApplicationsArrhenius1972]. (5) evaporation rate of octane [@brennanEvaporationLiquidsKinetic1974]. (6) Alpha frequency of brains of normal, [syphilitic paretic](https://en.wikipedia.org/wiki/General_paresis_of_the_insane), and *very* paretic humans [@hoaglandPacemakersHumanBrain1936].](figure/arrhenius_plots.png)

While writing this, I suddenly recognized [@hoaglandPhysiologicalControlJudgments1933] from when I read Feynman's book all those years ago!

> When I was in graduate school at Princeton \[1939--1942\] a kind of dumb psychology paper came out that stirred up a lot of discussion. The author had decided that the thing controlling the "time sense" in the brain is a chemical reaction involving iron... his wife had a chronic fever which went up and down a lot. Somehow he got the idea to test her sense of time. He had her count seconds to herself (without looking at a clock), and checked how long it took her to count up to 60. He had her counting -- the poor woman -- all during the day: when her fever went up, he found she counted quicker; when her fever went down, she counted slower... he tried to find a chemical reaction whose rates varied with temperature in the same amounts as his wife's counting did. He found that iron reactions fit the pattern best... it all seemed like a lot of baloney to me -- there were so many things that could go wrong in his long chain of reasoning.
>
> [@feynmanWhatYouCare1989, page 55]

And yes, that is the one!

> My wife, having fallen ill with influenza, was used in the first of several experiments. Without, in any way, hinting to her the nature of the experiment, she was asked to count 60 seconds to herself at what she believed to be a rate of 1 per second. Simultaneously the actual duration of the count was observed with a stop-watch. 
> 
> [@hoaglandPhysiologicalControlJudgments1933]

## Crooks fluctuation theorem

### In a closed system (microcanonical)

SETUP. The system is a classical-mechanical system, with time-reversible dynamics, and follows Liouville's theorem.

We have a thermodynamic system held under variable constraints $x$.

The system starts in microcanonical ensemble of energy $E_1$. Then we change the constraints $x(t)$, quickly or slowly, over a time interval $t\in [0, \tau]$. Let the microstate trajectory of the system be $y(t)$, arriving at the energy shell of $E_2$.

During the forward process, if the system undergoes microstate trajectory $y(t)$, then we have to expend work $W[x(t), y(t)] = E_2 - E_1$.

Let $S_1^*$ be the maximal entropy of the system when held under the constraints of $x(0)$, and when the system has energy $E_1$. Similarly for $S_2^*$.

::: {.callout-warning}

$S_1 = S_1^*$, since the system starts in thermal equilibrium. However, by Liouville's theorem, entropy is *conserved*! So we actually have $S_2 = S_1 \neq S_2^*$, because the system does not end in thermal equilibrium.

:::

$x', y'$ are $x, y$ time-reversed.

For example, if we have a piston of gas made of only a few gas molecules, then the constraint is the volume $V$, and we want to study the probability of expending work $W$ if we give the piston head a push. The push can be slow or fast -- arbitrarily far from equilibrium. Crooks theorem applies no matter how we push the piston head.

![[@sethnaStatisticalMechanicsEntropy2021, figure 4.10]](figure/Crooks_sethna_2021_4_10.png)

![[@sethnaStatisticalMechanicsEntropy2021, figure 4.11]](figure/Crooks_sethna_2021_4_11.png)

SETUP. probability density over path-space.

Let $\delta E_1, \delta E_2$ be infinitesimals, and let $E_1, E_2$ be real numbers.

Given a small bundle of microtrajectories $y$, we can measure its path-space volume as $D[y]$. Suppose they start on the energy shell $[E_1, E_1 + \delta E_1]$, then they would end up *somewhere*. If we're lucky, they would end up on the energy shell $[E_2 + \delta E_2]$.

Suppose the system starts in the microcanonical ensemble on the energy shell $[E_1, E_1 + \delta E_1]$, and we perform the constraint-variation $x$, then there is a certain probability $\delta P$ that we would sample a trajectory from the small bundle. That small probability is
$$\rho(y | x) D[y]$$

where $\rho(y | x)$ is a probability density over path-space. In particular, $\rho(y | x) = 0$ identically, unless $y(0)$ is on the energy shell $[E_1, E_1 + \delta E_1]$.  

Running the argument backwards, we can define $\rho'(y' | x')$, another probability density over paths. This one satisfies $\rho'(y'| x') = 0$ unless $y'(0)$ is on the energy shell $[E_2, E_2 + \delta E_2]$.

::: {#thm-todo}

## Crooks fluctuation theorem (microcanonical)

For any trajectory $y$ such that it starts on the $[E_1, E_1 + \delta E_1]$ energy shell, and ends on the $[E_2, E_2 + \delta E_2]$ energy shell,

$$\frac{\rho(y | x)}{\rho'(y' | x')} = e^{\Delta S}$$

where $\Delta S= \ln\Omega_2 - \ln\Omega_1$, $\Omega_1$ is the phase space volume of the first energy shell, and $\Omega_2$ the second.

:::

::: {.callout-note}

If $y$ does not start on the first energy shell, or does not end on the second energy shell, then either the nominator or the denominator is zero, and so the equation fails to hold.

:::

::: {.callout-note title="Proof" collapse="true"}

In the forward process, the probability of going along that trajectory is 
$$\rho(x|y) D[x] = \frac{\delta V}{\Omega_1}$$

where $\delta V$ is the phase-space volume of the shaded set.  

In the backward process, the probability of reversing that trajectory is 
$$\rho'(x'|y') D[x']= \frac{\delta V'}{\Omega_2}$$

$\delta V' = \delta V$ by Liouville's theorem, and $D[x] = D[x']$ because $x'$ is just $x$ time-reversed.

:::

### In an energy bath (canonical)

Now, suppose we take the same piston of gas, and put it in energy-contact with an energy bath, then at thermal equilibrium, the piston of gas would have the Boltzmann distribution $\propto e^{-\beta E}$. We can then give the piston head a push, which would cause it to undergo

::: {#thm-todo}

## Crooks fluctuation theorem

$$\frac{\rho(y | x)}{\rho'(y' | x')} =  e^{S[x, y]} = e^{\beta (W[x, y] - \Delta F^*)}$$

where $S[x, y]$ is the entropy produced during the forward process, after the system has equilibrated, and $\Delta F^* = F^*_2 - F^*_2$ is the increase in *equilibrium* Helmholtz free energy of the system.

:::

::: {.callout-warning}

In both forward and backward cases, we start with a thermal equilibrium, and end with a thermal *dis*equilibrium.

For example, suppose we have a small tank of a few gas molecules in thermal equilibrium with a large energy bath.

Now, we *quicky* push the piston head in according to the function $x(t)$. The trajectory of the system would go through is $y(t)$, which is *determined* by both $x(t)$ and the initial state of both the system and the energy bath.

Now, we *wait a long time*, until the tank is in thermal equilibrium again. Then we pull the piston head out with time-reversed trajectory. Because the forward trajectory was quick, the backward trajectory was also quick.

::: 

::: {.callout-warning title="equilibrium Helmholtz"}

We wrote $F^*$ instead of $F$, to emphasize that we are dealing with *equilibrium* Helmholtz free energy, defined by $F^* = \min_\rho (\braket{E} - TS[\rho])$, and *not* the generic version $\braket{E} - TS[\rho]$.

This is vitally important, because at time $\tau$, when the constraints have just reached their new values, the system is *not* in equilibrium. We would have to hold the constraints constant for a while for the system to return to equilibrium with the energy bath. Despite this, Crooks fluctuation theorem uses $\Delta F^*$, which is computed at equilibrium.

:::

::: {.callout-note title="Proof" collapse="true"}

Apply the microcanonical version of Crooks theorem to the entire compound system that includes both the bath and the system, then integrate over all possible microstate trajectories of the bath $y_{bath}$.

$$\begin{aligned}
  S[x, y] &= \Delta S_{bath} + \Delta S_{system} \\
  &= \beta \Delta E_{bath} + \Delta S_{system} \\
  &= \beta(W[x, y] - \braket{\Delta E_{system}}_2) + \Delta S_{system} \\
  &= \beta (W[x, y] - \Delta F^*)
\end{aligned}$$

where $\braket{\cdot}_2$ means the canonical ensemble average under constraint $x(\tau)$.  

Notice that the work expended/entropy produced depends only on the system's microtrajectory $y(t)$, and *not* on the bath's microtrajectory $y_{bath}(t)$. That is, 

$$S[x, y, y_{bath}] = S[x, y]$$

This will be used again in the next step when we integrate over $D[y_{bath}]$.  

$$\begin{aligned}
  \rho(y|x) &= \int_{y, y_{bath}, x \text{ is valid}}D[y_{bath}]\; \rho(y, y_{bath} | x)  \\
  &=  \underbrace{\int_{y', y'_{bath}, x' \text{ is valid}}D[y'_{bath}]}_{\text{reversible dynamics}}\; \underbrace{e^{S[x, y, y_{bath}]}\rho'(y', y'_{bath} | x')}_{\text{microcanonical Crooks}}  \\
  &=  \int D[y'_{bath}] \; e^{\red{S[x, y]}}\rho'(y', y'_{bath} | x') \\
  &=  e^{S[x,y]} \int D[y'_{bath}] \; \rho'(y', y'_{bath} | x') \\
  &=  e^{S[x,y]} \rho'(y'|x')
\end{aligned}$$

:::

::: {#cor-todo}

$$\frac{\rho(W | x)}{\rho'(-W | x')} =  e^{\beta (W - \Delta F^*)}$$

where $\rho(W|x)$ is the probability density of expending work $W$ in the forward process.  

:::

::: {.callout-note title="Proof" collapse="true"}

Integrate over all forward microtrajectories $y$ satisfying $W[x, y] \in [W, W+\delta W]$. By reversibility, $W[x', y'] = [-W - \delta W, -W]$ for such microtrajectories.

:::

### Other ensembles

Looking at the proof for Crooks theorem for the canonical ensemble, we immediately obtain many other possible Crooks theorems, one per free entropy.

EXP. Crooks theorem for Gibbs free energy $G$

Suppose we have a piston of magnetic gas in energy-and-volume contact with a bath. Now suppose the gas is in equilibrium with the bath, and we vary the external magnetic field over a trajectory $x$. Over the microstate trajectory $x$, the external world would expend both some energy $W[x, y]$ and some volume $V[x, y]$.

$$\frac{\rho(y | x)}{\rho'(y' | x')} =  e^{S[x, y]} = e^{\beta (W[x, y] + PV[x, y] - \Delta G^*)}$$

EXP. Crooks theorem for Landau free energy $\Omega$

Suppose we have a chemical reaction chamber of fixed volume, and in energy-and-particle contact with a bath with chemical potentials $\mu_i$.

$$\frac{\rho(y | x)}{\rho'(y' | x')} =  e^{S[x, y]} = e^{\beta (W[x, y] -  \sum_i \mu_i N_i[x, y] - \Delta \Omega^*)}$$

## Jarzynski equality

Let $W$ be the total work we expended by changing the constraints during the interval $[0, \tau]$. Since the work expended depends on the details of the heat bath and the starting state of the system at $t=0$, this is a random variable.

::: {#thm-todo}

## Jarzynski equality

$$\braket{e^{-\beta W}} = e^{-\beta \Delta F^*}$$

where the expectation is taken over many repeats of the same experiment (ensemble average).  

:::

::: {.callout-note title="Proof" collapse="true"}

Integrate Crooks over all forward trajectories $D[x]$.

$$\rho(y | x) e^{-\beta W[x, y] } = \rho'(y' | x') e^{-\beta\Delta F}$$

now integrate over $\int D[y]$, using the fact that $D[y] = D[y']$.  

::: 

::: {#cor-todo}

## violation of second law is exponentially unlikely

$$Pr((W - \Delta F^*) \leq - \delta W) \leq e^{-\beta \delta W}$$

:::

::: {.callout-note title="Proof" collapse="true"}

Apply Markov's inequality.

:::

EXP. high probability of work extraction

Classically, if we have a single system in thermal equilibrium with a single energy-bath, and we perform a cyclic operation on it, then we can't extract work, lest we violate the second law.

Statistically, $\braket{e^{-\beta W}} = 1$, and so it is entirely possible for us to extract work with high probability, as long as there is a small probability to lose a large enough amount of work.

[@mailletOptimalProbabilisticWork2019] constructed a quantum mechanical device with a single-electron transistor. The electron can expend work. They managed to extract work from the device with over 75% probability.

![Figure from [@mailletOptimalProbabilisticWork2019, figure 3.c]](figure/maillet_2019_3_c.png)

### Worked example: bouncing ball

This example is from [@sethnaStatisticalMechanicsEntropy2021, exercise 4.8], which itself derives from [@luaPracticalApplicabilityJarzynski2005]. See also [@hijarJarzynskiEqualityIllustrated2010] for another solved example, of a chest expander with mass points stuck in the middle of the springs. You might need to read my tutorial on [field-theoretic calculations](https://yuxi-liu-wired.github.io/sketches/posts/field-theory-how-to/) before attempting that example.

We have a one-dimensional system, of a single ball bounding between two walls of a piston. The only control we have is that we can move one of the piston heads. At the start, the piston has length $L$, and the system is in thermal equilibrium at inverse temperature $\beta$. We plunge the piston head at velocity $v$ for time $\Delta L / v$, then immediately reverse it, taking another $\Delta L / v$. We explicitly calculate that $\braket{e^{-\beta W}} = 1$.

![[@sethnaStatisticalMechanicsEntropy2021, figure 4.12]](figure/Jarzynski_bouncing_ball_sethna_2021_4_12.png)

The phase space of the ball has 2 dimensions, $(p, x)$. The Boltzmann distribution is

$$\rho(p, x) = \rho(p) \rho(x) = \frac{1}{\sqrt{2\pi m/\beta}}e^{-\frac{\beta}{2m}p^2} \times \frac{1}{L}$$

We assume that $L$ is large enough, such that the ball hits the piston head at most once. There are three possibilities:

1. If the piston head hits the ball during the in-stroke, then the ball's velocity increases by $2v$, and its kinetic energy increases by
$$W = \Delta KE = 2v(mv - p)$$
1. If the piston head hits the ball during the out-stroke, then the ball's velocity decreases by $2v$, and its kinetic energy increases by  
$$W = 2v(mv+p)$$
1. Otherwise, the piston head avoids the ball, and we have $W = 0$.  

If at $t=0$, the ball is in the phase space region labelled "in region", then it will be hit in the in-stroke. If at $t=\Delta L/v$, the ball is in the phase space region labelled "out region", then it will be hit in the out-stroke. Otherwise, it will not be hit.

![Jarzynski_bouncing_ball_1.jpg](figure/Jarzynski_bouncing_ball_1.jpg)

![Jarzynski_bouncing_ball_2.jpg](figure/Jarzynski_bouncing_ball_2.jpg)

Therefore,

$$
\begin{aligned}
    \braket{e^{-\beta W}} &= \int_{in} \rho dpdx \; e^{-\beta 2v(mv-p)} + \int_{out} \rho dpdx \; e^{-\beta 2v(mv+p)} + \int_{other} \rho dpdx \; 1 \\
    &= e^{-2\beta mv^2} \left(\int_{in} \rho dpdx \; e^{2\beta vp} + \int_{out} \rho dpdx \; e^{-2\beta vp}\right) +  \int_{other} \rho dpdx \; 1
\end{aligned}
$$

Since $\rho(p, x) = \rho(-p, x - \Delta L)$, the first two integrals can be combined by flipping the "out region", then moving it by $\Delta L$, to "out' region".

::: {.callout-note}

Because $L$ is large, this is *mostly* correct, as the regions where this is incorrect has $\rho$ so small that it is negligible, as seen in the figure.

:::

Now we continue:

$$
\begin{aligned}
\braket{e^{-\beta W}} &\approx e^{-2\beta mv^2} \int_{in, out'} \rho dpdx \; e^{2\beta vp} +  \int_{other} \rho dpdx \; 1 \\
&= \frac{1}{L\sqrt{2\pi m/\beta}} \left(\int_{in, out'} e^{-\frac{\beta}{2m}(p - 2mv)^2} dpdx + \int_{other} e^{-\frac{\beta}{2m}p^2} dpdx\right)
\end{aligned}
$$

Because the "in-out' region" is symmetric across the $p = mv$ line, we can reflect the first integral across the $p=mv$ line and obtain
$$
= \frac{1}{L\sqrt{2\pi m/\beta}} \left(\int_{in, out'} e^{-\frac{\beta}{2m}p^2} dpdx + \int_{other} e^{-\frac{\beta}{2m}p^2} dpdx\right) = 1
$$

### Fluctuation-dissipation relations

::: {#cor-todo}

## Fluctuation-dissipation relations

Since $e^t$ is convex, we have $\Delta F^* \leq \braket{W}$, meaning that the average work expended is more than the increase in Helmholtz free energy. This has better be true, else we would be violating the second law of thermodynamics on average.

Since $\Delta F^* = -\frac{1}{\beta} \ln\braket{e^{-\beta W}}$, we find that to second order,
$$\underbrace{\braket{W} - \Delta F^*}_{\text{work dissipation}} = \frac 12 \beta \underbrace{\sigma_W^2}_{\text{work fluctuation}}$$

It is more familiarly written as
$$\mu = D\beta$$

where $D = \frac 12 \sigma_W^2$ is the fluctuation coefficient, and $\mu = (\braket{W} - \Delta F^*)$ is the dissipation coefficient.  

:::

For periodic forcing, the CFT has a simpler form.

Consider a time-reversible dynamical system immersed in an energy bath with inverse temperature $\beta$ , driven by periodically varying constraints. For example, a pendulum in a sticky fluid subjected to a periodic driving torque, or a water-cooled electric circuit driven by a periodic voltage.

Such a system will settle into a "dynamical equilibrium" ensemble, much like a canonical ensemble. If it is driven by the same process time-reversed, then, it will settle into another dynamical equilibrium ensemble.

::: {#thm-todo}

## Gallavotti--Cohen fluctuation theorem

$$\frac{\rho(Q)}{\rho'(-Q)} = e^{\beta Q}$$

where $\rho(Q)$ is the probability density that a forward cycle, randomly sampled from the dynamical equilibrium ensemble, emits energy $Q$ into the energy bath. Similarly, $\rho'$ is for the backward cycle.

:::

::: {.callout-note title="Proof" collapse="true"}

Construct a process that starts at equilibrium, then mount up the periodic driving, runs it for $NT$ time where $N$ is a large integer, then remove the driving. At the $N\to\infty$ limit, the CFT reduces to the result.

:::

### Arrow of time

[@jarzynskiEqualitiesInequalitiesIrreversibility2011]

Suppose a scientist has recorded a microscopic movie of pulling on an RNA, flipped a coin to decide whether to reverse the movie, then given you the movie. Your task is to guess whether the movie is reversed.

By Bayes theorem,
$$Pr(\text{forward}|x, y) = \frac{1}{1 + e^{-S[x, y]}}$$

where $S[x, y] = \beta(W[x, y] - \Delta F^*)$.  

In words, the higher the entropy production, the more likely it is that the movie is playing forward. This is one way to say that entropy production provides an arrow of time.

[@parrondoThermodynamicsInformation2015]

$$\braket{e^{-\beta W - I}} = e^{-\beta \Delta F^*}$$
