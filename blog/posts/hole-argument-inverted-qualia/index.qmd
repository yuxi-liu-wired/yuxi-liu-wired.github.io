---
title: "Hole Argument and Inverted Qualia"
author: "Yuxi Liu"
date: "2023-11"
date-modified: "2024-01-20"
categories: [fun, philosophy, math, physics]
format:
  html:
    toc: true
description: "TODO description."

# image: "figure/banner.png"
status: "draft"
confidence: "low"
importance: 2
---

## Abstract

## The hole argument

In general relativity, the hole argument is a thought experiment that apparently shows that general covariance is impossible. Einstein in late 1913, and David Hilbert in 1915, both fell into the hole argument. Misled by the hole argument, Einstein attempted to study theories of gravity that are not generally covariant, before finally giving up and . [@nortonHoleArgument1999]

### General relativity

Briefly, general relativity models spacetime as a manifold $\mathcal M$, with a metric tensor field $g_{\mu\nu}$ and an energy-momentum tensor field $T_{\mu\nu}$. The metric tensor describes the spacetime separation between points on the manifold, and thereby the geometry of spacetime. The energy-momentum tensor describes the flow of energy and momentum in spacetime. In particular, a body with mass $m$, such as a black hole, is a flow of energy $mc^2$ in time, and therefore can be described within the energy-momentum tensor.

General relativity is a geometric theory. That is, the metric tensor field and the energy-momentum tensor field are "braided together" by Einstein's field equations. The old song goes like "Matter tells space-time how to curve, and space-time tells matter how to move." but this is often misunderstood as saying "Matter exists, *then* space-time reacts to matter, and *then* matter reacts to space-time by *changing* its motion." This fundamentally misunderstands what general relativity is. There is no time nor causality as commonly understood in general relativity. 

Let us imagine a universe that is swirling with stars and galaxies. Locally, the spacetime manifold is curved, but globally, it is topologically the same as $\R^4$ -- no loops, no singularities, and no wormholes. Now, construct a coordinate system $(t, x, y, z)$. We can then select a "snapshot" of the universe by selecting the 3D submanifold at $t = 0$. If we know the exact value of $g, T$ on that snapshot, then we can crank the Einstein field equations to solve for $g, T$ for all $t > 0$. Does this mean that "the past determines the future"?

Not really. We could very well select $t = 10000$ and crank the field equations to solve for $g, T$ for all $t < 10000$. Does this mean that "the future determines the past?" Perhaps we can compromise by saying "one point in time determines the past and the future", but even that is not necessarily true. We can design much wilder boundary conditions. We can make two lightcones determine the rest of the universe (double-null, or Sachs), make one lightcone plus a "left side" of the universe determine the rest (null-timelike, or Winicour--Tamburino), make half of the universe's left-side and half of the universe's $t=0$ determine the rest, etc. 

![Different initial value conditions. The first is the commonly used Cauchy condition, but the others, more exotic, are also valid. Figure modified from [@dinvernoFormalismGeneralRelativity1984]](figure/ivp_conditions_GR.png)


The spacetime manifold $\mathcal M$ can be transformed, in that we can write down a function $f: \mathcal M \to \mathcal M$, such that it maps one point in the manifold to another point.

A theory is generally covariant if

The hole argument, in Norton's formulation

* Given two distributions of metric and material fields, related by a hole transformation, they are indeterminant by both observation and theory, since
  * The two distributions are observationally identical.
The laws of the theory cannot pick between the two developments of the fields into the hole.
  * But by manifold substantivalism, they represent distinct physical systems.
* Therefore, manifold substantivalism has a problematic metaphysics.

### Gauge theory

[@taoWhatGauge2008]

### A brief history of spacetime

The history of spacetime is a history of expanding symmetries.

In the most ancient cosmology of China, the earth is a square, while the sky is a half-bowl covering the earth. Each direction of earth -- east, west, south, north -- has a mystical significance. Up is not down, and east is not west. Not only that, there is a center of earth, somewhere in The Middle Kingdom. Thus, there is no spatial symmetry. The world was born an unspecified number of years in the past out of a chaotic egg. Thus, there is no temporal symmetry. Therefore, ancient Chinese spacetime is $\R^1 \times \R^3$, with no (nontrivial) symmetry.

In Aristotle's physics, there is a center of the universe, where everything heavy (water and earth) is moving towards, and everything light (air and fire) is fleeing from. Other than that, space is spherically symmetric -- he knew that earth is round. However, though space has a center, time is translation-invariant. Therefore, Aristotle's spacetime is $\R^1 \times \R^3$ with symmetry group $\R^1 \times SO(3)$, where $\R^1$ is the time-translation symmetry group, and $SO(3)$ is the spherical symmetry group.

The Christian spacetime, with a beginning and an end for time, has a smaller symmetry group of $\{0\} \times SO(3)$. 

Copernicus and Kepler replaced the sun for the earth as the center of the universe, but they still insisted on a center. The first true breakthrough was Giordano Bruno's infinite spacetime, where both space and time are infinite and without center. Therefore, Bruno's spacetime has symmetry group $\R^1 \times E(3)$, where $E(3)$ is the 3D Euclidean symmetry group. That is, we allow all spatial translations.

Galileo braided together space and time, resulting in an even bigger symmetry group. Specifically, he argued that the universe does not have a special "at rest" velocity. To see why this is a breakthrough, consider what happens in Bruno's universe. In Bruno's universe, it matters whether you are staying still, or moving at $1 \;\mathrm{m/s}$ relative to the universe. If you are staying still, then your trajectory is like $\dots, (0, p), (1, p), (2, p), \dots$. You can, metaphorically speaking, "peek at your spacetime point" and see that you have been sitting at the same space-point $p$. However, if you are moving relative to the universe, you can see that your space-point is changing.

Galileo says, you cannot, because there is no "space-point". You can take a slice of the universe at $t=0$, and another slice of the universe at $t=1$, but you cannot point at a point in each slice and ask, "Are these two points the same space-point?".



### Responses to the hole argument

**Relationalism**. This is Einstein's response, and also the typical response nowadays.

This line of thought can be traced back to Leibniz's theory of relative space against Newton's theory of absolute space. In Leibniz's third paper during the [Leibniz--Clarke correspondence](https://en.wikipedia.org/wiki/Leibniz%E2%80%93Clarke_correspondence) [@clarkeCollectionPapersWhich1717], Leibniz proposed the "inverted space" thought experiment:[^leibniz-inverted-space] Suppose at the moment of creation, God were to switch between East and West, nothing would act different. Since God must have created the world according to the principle of sufficient reason, God must have had no such degree of freedom in the first place. Ergo, space is relational, not absolute.

[^leibniz-inverted-space]: 
    > ... supposing Space to be Something in it self, besides the Order of Bodies among themselves, that 'tis impossible there should be a Reason, why God, preserving the same Situations of Bodies among themselves, should have placed them in Space after one certain particular manner, and not otherwise; why every thing was not placed the quite contrary way, for instance, by changing East into West.
    >
    > [@clarkeCollectionPapersWhich1717]

While God or the principle of sufficient reason is no longer so assured, Leibniz's thought experiment remains potent.

**Gauge freedom**. If two criteria are true, then it is a gauge freedom, and indeterminancy in it is no longer an issue.

(i) verifiability—changes in the candidate surplus structure make no difference to what can be verified in observation;

(ii) determinism—the laws of the theory are unable to fix the candidate surplus structure.

**Metric essentialism**. Maudlin (1990) urges that each spacetime event carries its metrical properties essentially; that is, it would not be that very event if (after redistribution of the fields) we tried to assign different metrical properties to it.

**Non-duality**. It is quite possible to regard smooth manifold as derivative, and Riemannian manifold as original. Indeed, we may construct a smooth manifold by starting with $\R^N$, then quotienting out smooth deformations.

The point is this: mathematically speaking, there is no necessary division between substance and property, though it is a very useful intuition. Physically speaking, the division has led to errors, such as Einstein's failed 1914 attempt at non-relativistic theory of gravity.

This is especially true in the case of general relativity, because it developed from 19th century differential geometry, which tended to be understood as about geometry of deformable surfaces -

where there is literally an unchanging *substance* (a rubber sheet) with changeable *property* (the strain field).

## The geometry of color

### Smooth geometry

The human eye, abstractly speaking, is a light detector with 4 kinds of sensors: the rod cell, active in the dark, and 3 kinds of cone cells, active when it's bright. Each cell carries its own kind of light-sensitive proteins ("opsins"), which are molecular switches. If a photon hits an opsin in the "passive" shape, then the opsin *may* absorb the photon and flip into its "active" shape. An active opsin would then set off a molecular chain-reaction in the cell, that *may* result in an electric signal down the optic nerve.

Mathematically, suppose we shine a light on a patch of long-wavelength-type cone cells, we can represent the electric response as:

$$I_L = \int S_L(\lambda) R(\lambda) d\lambda$$

where

* $I_L$ is the **response intensity** of long-wavelength-type cone cells, in units of neural spike per second. Though each cell's operation is quantum-mechanically random, when averaged over many cone cells, the response is deterministic.
* $R(\lambda)$ is the spectral radiance at wavelength $\lambda$, or **spectrum** for short. It has units of watt per square-nanometer (of retinal area) per nanometer (of wavelength).
* $S_L$ is the **spectral sensitivity** function of the long-type cone cells.

We can similarly define $I_M, I_S$, for the other two cone cell types (medium and short). Each $S_L, S_M, S_S$ is approximately bell-shaped.

![Schematic diagram of human cone cell sensitivity. Each curve is "normalized", meaning that it is multiplied by a positive real number, so that its maximal value is exactly 1.](figure/cone_cell_sensitivity.svg)

If we ignore the rod cells, and assume no adaptation to darkness ("scotopic vision"), then human color vision is just a deterministic function that maps a spectrum to three real numbers:

$$C(P) := (I_S(P), I_M(P), I_L(P))$$

with type $(\R^+ \to \R^+) \to (\R^+)^3$, where $\R^+ = [0, \infty)$ is the space of non-negative real numbers. Define this as the $(I_S, I_M, I_L)$ as the **LMS color space**.[^sense-data] Furthermore, the biochemical limit on neural firing is 1000 Hz [@NeuronFiringRates2015], thus the LMS color space is bounded within a cube.

[^sense-data]: This seems as close to "sense data" [@hatfieldSenseData2021] as it gets in science.

Any smooth deformation of the LMS color space gives us another color space. In theory, it doesn't matter which one we use, because the underlying color space is still the same. In practice, some color spaces are easier to use than others.

![Different depictions of the same color space. By a smooth map, we can deform the LMS color space into any shape we want, such as a cone, a cube, a cylinder, a double cone, etc. Figure from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Color_solid_comparison_hsl_hsv_cube_cylinder_cone.png)](figure/Color_solid_comparison_hsl_hsv_cube_cylinder_cone.png)

In people with only two types of cone cells, the color vision function $C$ loses a dimension. For example, if that person has deuteranopia, without medium-wavelength cone cells, then they would see all colors in LMS space with the same $(S, L)$-coordinates as the same.

### Projective geometry

Because $C$ is a linear functional, and any two colors can be mixed to give a third color, LMS color space is a convex cone. On the tip of the cone is $(0, 0, 0)$, the color of pure darkness. It is an old experimental fact that the geometry of colors is invariant under scaling. So, if you have two lights with spectra $P, P'$, such that their colors look the same/different/very different, then we make them brighter or dimmer, to $cP, cP'$ where $c > 0$, then their colors will still look the same/different/very different.

Thus, we can factor the space of colors into two components: an [apparent lightness](https://en.wikipedia.org/wiki/Lightness), and an apparent [chromaticity](https://en.wikipedia.org/wiki/Chromaticity). So, if we take two dim red lights, and shine both of them on the same pane of [frosted glass](https://en.wikipedia.org/wiki/Frosted_glass), the frosted glass would look lighter, but have the same chromaticity. The space of chromaticities is the space of lines passing the origin, which allows us to use [projective geometry](https://en.wikipedia.org/wiki/Projective_geometry).

The space of all colors looks like a cone, and since each line in the cone can be represented as a point on the line, the space of all chromaticities looks like the intersection of the cone with a plane -- each line is represented by its intersection with the plane. What does the space of all chromaticities look like?

Because any spectrum $I$ is the convex sum of pure spectra

$$\{I_{\lambda}: \lambda \in (400 \;\mathrm{nm}, 700\;\mathrm{nm})\},$$

the space of all colors is the convex sum of all pure spectral colors 

$$\{C(I_{\lambda}): \lambda \in (400 \;\mathrm{nm}, 700\;\mathrm{nm})\}.$$

Consider a wall covered with a "pure spectral paint", in the sense that it reflects exactly light at wavelength $500 \;\mathrm{nm}$, and nothing else. Then, under any illumination, the color of the wall has the same chromaticity. Pure spectral colors are special colors, in the following diagram, on the edge of the cone are lines of pure spectral color, each produced by a spectrum that is concentrated at just one wavelength.

![The pure spectral colors in LMS color space. The rainbow curve represents the spectrum of visible light, from violet to red. Each point on this curve corresponds to a specific wavelength of light and its unique combination of stimulations to the three types of cone cells. For each point on the spectral curve, we can draw a straight line to the origin. Each point on the line has the same color, but appears increasingly bright.](figure/neural_color_space_3d.svg)

Because the cone shape is uninteresting, the color space is typically represented by chopping off the cone midway, producing a roughly horseshoe-shaped region on $\R^2$, named the **gamut**. Mathematically, it is the projective transform:
$$(s, m, l) := \left(\frac{I_S}{I_S + I_M + I_L}, \frac{I_M}{I_S + I_M + I_L}, \frac{I_L}{I_S + I_M + I_L} \right)$$

The curving edge of the chromaticity space are points of pure spectral colors, from pure $700 \;\mathrm{nm}$ line on the red end, to the pure $400 \;\mathrm{nm}$ line on the purple end. Every point inside the gamut can be mixed by two pure spectral colors. However, this is not the entirety of chromaticity space. The ray at the shortest-wavelength end (pure spectral purple) and the ray at the longest-wavelength end (pure spectral red) do not touch each other. Instead, they shoot out like two ends of a horseshoe. Chromaticity space, then, has a second, straight edge, obtained by mixing the shortest and the longest wavelength. This is the [purple boundary](https://en.wikipedia.org/wiki/Line_of_purples).

![Schrödinger's diagram of chromaticity space. *Spektralkurve*: spectral curve. *Schnitt mit einer Ebene*: intersection with an arbitrarily inclined plane. *R*: red. *G*: green. *I*: indigo. *V*: violet. *O*: origin.
[@schrodingerGrundlinienTheorieFarbenmetrik1920, figure 3]](figure/schrodinger_1920_spektralkegel.png)

We can construct the chromaticity space of someone with deuteranopia by starting with the purple line, then draw one line for each point on the purple line that is perpendicular to the $(S, L)$-plane in LMS color space. The deuteranopic observer sees each line of chromaticities as a single chromaticity.

Theoretically, we can imagine creating the world's best computer display by putting in a full-spectral display unit into each pixel. It will then be able to cover the entire gamut space. It will not only display true-life colors for humans, but also for dogs, bees, and mantis shrimps. Unfortunately, we don't have that luxury, and computer displays are built for human-use only, with just three spectra.

Now, if we have a pixel containing three little LED units, capable of emitting light of spectra $P_1, P_2, P_3$, then we can take any convex sum, and create a mixed color. The space of all colors created by their convex sum is the convex sum of $C(P_1), C(P_2), C(P_3)$, which looks like a triangular cone. Thus, the chromaticity that this pixel can display is a triangle. Every color inside the triangle can be created by mixing the three spectra, but any color outside cannot.

![The triangle of displayable chromaticities for cathode-ray televisions (left) and LCDs (right). Figure from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:CRT_and_LCD_Gamut.jpg)](figure/CRT_and_LCD_Gamut.jpg)

When you print on a page, the page does not emit color, and can only acquire color by selectively reflecting light. When under a standard white light, the more ink you lavish on a page, the more saturated the color can be, but the darker it would be, because more light is absorbed. Conversely, if all the light is reflected, then it would look white. Because of this trade-off, the gamut of printable colors is even smaller.

![The gamut of printable colors when placed under [standard white illumination](https://en.wikipedia.org/wiki/Standard_illuminant). [@macadamGeometryColorSpace1944, plate 1]](figure/cmyk_gamut_macadam_1944.png)

Evolution has created a multi-spectral display in some octopuses and chameleons. The best octopus camouflagers have 2 kinds of color organs in their skins: the chromatophores, the leucophores, and the iridocytes. Chromatophores contain pigment cells, which expand and contract by radial muscles like wheel spokes around an axis. The leucophores are roughly [ideal "matte" reflectors](https://en.wikipedia.org/wiki/Lambertian_reflectance), meaning they reflect incoming light uniformly, with little loss.

The iridocytes are the most exotic, and approximates our "world's best television screen". Specifically, they are [dielectric mirrors](https://en.wikipedia.org/wiki/Dielectric_mirror), which reflects light at a specific wavelength. They are alternating layers of guanine crystals and cytoplasms. To change color, it simply adjusts the water content of the cytoplasm, which makes them expand or contract, changing the distance between guanine layers. More pictures are found in [@cloneyChromatophoreOrgansReflector1983].

![How the iridocytes work in cephalopods. Figure from [@cossinsGroovyColor2013]](figure/iridocyte_the_scientist_2013.jpg)

![A, B. cephalopod before and after camouflage. C. structure of cephalopod skin. D. before and after chromatophore expansion. F. before and after iridocyte turning iridescent. G. the dielectric mirrors inside an iridocyte. [@chatterjeeIntersectionNaturalStructural2022, figure 4]](figure/cephalopod_biomimetics_chatterjee_2022.png)

Meanwhile, chameleons have iridocytes that operate by a different mechanism: [photonic crystals](https://en.wikipedia.org/wiki/Photonic_crystal) [@teyssierPhotonicCrystalsCause2015].

### Linear geometry

Grassmann, famous for originating linear algebra, studied color theory and applied linear algebra to it. Essentially, he discovered that the human color vision function $C$, defined previously, is a linear function. He discovered this by color-mixing experiments, in the style of 19th century psychophysics. Considering it was 50 years before the [neuron doctrine](https://en.wikipedia.org/wiki/Neuron_doctrine) became accepted, and 100 years before cone cells were observed, he did very well.

For any three spectra $P_1, P_2, P_3$, we can define their colors as $C_i := C(I_i)$. Since $C$ is a linear function, as long as $\{C_1, C_2, C_3\}$ are linearly independent, we can represent any color as $C(x_1 P_1 + x_2 P_2 + x_3 P_3)$ for some $(x_1, x_2, x_3) \in \R^3$.

For example, we can go to a scientific standard shop and buy a set of standard lamps, which when plugged into a standard plug, viewed in a standard room, at a standard distance and a standard angle, by a standard observer,[^standard-observer] will create a standard red, a standard green, and a standard blue. Then, using opaque to cover up parts of the lamp, and combining the lights, we can create any color $C(x_1 P_1 + x_2 P_2 + x_3 P_3)$, for any $(x_1, x_2, x_3) \in [0, 1]^3$. By buying more lamps, we can create all colors with $(x_1, x_2, x_3) \in (\R^+)^3$.

[^standard-observer]: Because humans are resistant to standardization, the standard observer is obtained by taking data from real observers in good health that are physiologically similar, and their average. The methodology resembles *l'homme moyen* ("the average man") of [Adolphe Quetelet](https://en.wikipedia.org/wiki/Adolphe_Quetelet), a fanatic for anthropometry. Also, the standard observer is not required to drink [standard cups of tea](https://en.wikipedia.org/wiki/ISO_3103).

Here, we notice a difficulty: we can't take a negative amount of lamp. Fortunately, we can bypass the difficulty by adding a fourth lamp, a "standard white" lamp emitting a spectrum $P_0$. Then, for any other spectrum $I$, there exists $(x_1, x_2, x_3, x_4) \in (\R^+)^4$, such that

$$
C(I) + C(x_0 P_0) = C(x_1 P_1 + x_2 P_2 + x_3 P_3)
$$

which allows us to place the color of $I$ at the unique point. Of course, the choice of $(x_1, x_2, x_3, x_4)$ is not unique. However, since color space is linear, the sum $C(x_1 P_1 + x_2 P_2 + x_3 P_3 - x_0 P_0)$ is unique. Once $C(P_0)$ is itself constructed as a linear sum of $C(\sum_{i=1}^3 x_{i, 0}P_i)$, we would have located $C(I)$ in color space, at

$$
C(I) = \sum_{i=1}^3 (x_i - x_0x_{i, 0}) C(P_i)
$$

This is essentially the state of the art of colorimetry in 1931, when CIE 1931 was constructed by color-mixing experiments. An observer is seated in a standard room, and sees two light sources. On the left, a to-be-measured light $I$ is mixed with a standard white light $P_0$, and on the right, are three standard blue, green, red lights $P_1, P_2, P_3$. The observer turns the 4 knobs until two sides look indistinguishable. This was repeated for many observers, over many days, for many light sources. The result is a table with three columns, and many rows. Each row is an industrially important light source, and the three columns are the standard red, standard green, standard blue. It schematically looks like this (I made up the data):

| color | standard red | standard green | standard blue |
|---|---|---|---|
| standard red | 1.000 | 0.000 | 0.000 |
| standard green | 0.000 | 1.000 | 0.000 |
| standard blue | 0.000 | 0.000 | 1.000 |
| standard white | 0.334 | 0.334 | 0.332 |
| ... | ... | ... | ... |

::: {.callout-note title="Technically"}

Technically, the CIE 1931 color of a spectrum $I$ is a point in $\R^3$ defined by

$$
C_{\text{CIE 1931}}(I) := \left(\int I(\lambda) \bar r(\lambda) d\lambda , \int I(\lambda) \bar g(\lambda) d\lambda , \int I(\lambda) \bar b(\lambda) d\lambda \right)
$$

where $\bar r, \bar g, \bar b$ are "standard observer color matching functions". They are not any real observer's sensitivities, because they have negative values. Instead, they are *roughly* a linear transform of the real sensitivities $S_S, S_M, S_L$, meaning CIE 1931 color space is roughly a linear transform of LMS color space.

Why did they go for a *roughly* linear transform? I know it's confusing (it confused me), but it's simply a temporary hack. Back then, they had no way to measure the neural spikes, so they had to infer the real sensitivities by indirect psychophysics data. And the negative values are for some kind of numerical stability considerations. Point being, it's really not fundamental to science, but rather a 1930s technical hack.

:::

### Opponent process

Have you ever wondered why things seem bluer just after sunset, or under a high full moon? This is where *opponent process theory* and Purkinje effect comes in. 

While the retina might be operating with the LMS color space, it is not what gets sent to the brain. Specifically, before leaving the retina, the spikes from the 3 cone cells and the rod cell (we are finally accounting for them now!) are linearly transformed by 3 paired-kinds of neurons within the retina, before sending down the optic nerve. Greatly simplified, the linear transform is:

$$
\begin{cases}
  I_{\text{Red-Green}} &= I_L - I_M \\
  I_{\text{Yellow-Blue}} &= I_L + I_M - 2 I_S \\
  I_{\text{Brightness}} &= 2I_L + I_M + 0.05 I_S + I_R
\end{cases}
$$

![[@huntMeasuringColour2011, figure 1.4]](figure/opponent_process_fig_1_4.png)

In words, the Red-Green-pair of neurons take the long-wavelength (reddish) cone cells, and subtract away the medium-wavelength (greenish) cone cells. If the result is positive, then the positive half of the pair sends down a signal at the rate of $I_L - I_M$, otherwise, the negative half of the pair sends down a signal at the rate of $-(I_L - I_M)$. This linear transform, while mathematically equivalent (as long as the rod cells don't appear) to LMS space, allows the optic nerves to carry more information in *Homo Sapiens*' natural habitat [@buchsbaumTrichromacyOpponentColours1997].

When the light level is around $0.5 \;\mathrm{lux}$, which corresponds to twilight, or a full high moon, both the rod cells and the cone cells are active [@dominyLiminalLightPrimate2020].

So, let us look at the linear transform

$$
\begin{cases}
  I_{\text{Red-Green}} &= I_L - I_M \\
  I_{\text{Yellow-Blue}} &= I_L + I_M - 2 I_S \\
  I_{\text{Brightness}} &= 2I_L + I_M + 0.05 I_S + I_R
\end{cases}
$$

Let's pretend we are the brain, interpreting the signals sent down the optic nerves. Suppose the retina secretly increases $I_R$ by a small amount of $\Delta I$, but we don't know that. How would we interpret it? We would interpret it as a color in LMS space with color 

$$(I_S + \Delta I', I_M + \Delta I', I_L + \Delta I')$$

where $2.05\Delta I' = \Delta I$. That is, it looks as if each type of cone cell has increased firing rate by the same amount. Looking at the sensitivity curve, this effect can be created by shifting the spectrum to the shorter wavelength, then increase its power slightly. Thus, things look bluer.

![Purkinje effect illustrated with a flower. As the lighting condition dims, the entire scene shifts more to the bluish shade. At low enough lighting, all cone cells deactivate, and the entire scene becomes monochromatic. Figure modified from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Purkinje_effect.png)](figure/purkinje_effect.jpg)

### Riemannian geometry

> Magnitude-notions are only possible where there is an antecedent general notion which admits of different specialisations... the only simple notions whose specialisations form a multiply extended manifoldness are the positions of perceived objects and colours. More frequent occasions for the creation and development of these notions occur first in the higher mathematic.
>
> Riemann's Habilitation dissertation, 1854 [@riemannHypothesesWhichLie2016]

Now that we have a space of colors, how do we measure distances in it? Some colors are close, while some colors are far apart. How do we quantify it? This question occupied the minds of some famous scientists, including Riemann, Grassmann, Helmholtz, and Schrödinger [@pavlidisBriefHistoryColour2021].

In 1920, Schrödinger (more famous for [his other equation](https://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation)) hypothesized that color space has a Riemannian geometry, and the subjective difference between two colors is the geodesic distance between the two points in color space [@schrodingerGrundlinienTheorieFarbenmetrik1920]. This is the foundation of modern colorimetry [@niallErwinSchrodingerColor2017]. Over the years, there had been a mess of increasingly detailed theoretical models for the Riemannian metric of color space, of interest only to specialists -- see [@wyszeckiColorScienceConcepts1982, chapter 8.4] for a review. Here, we bypass most of the theory by experimental data.

Given two spectra $I, I'$, if their colors $C(I), C(I')$ are close enough, an observer would judge them as equal. This is the concept of "just noticeable difference" (JND), a foundational concept of psychophysics. In general, the JND method goes like this:

* Fix one stimulus $S$, and vary the other stimulus $S'$. The prior probability that $S = S'$ is $1/2$.
* Present both $S, S'$ to the observer.
* The observer judges whether they are the same or different.
* Repeat many times.
* If, when truly $S' = S$, the observer judges that they are the same with probability $p_0$, then the JND point is the point where the observer judges that $S' = S$ with probability $p_0/2$.

In the original experiments, MacAdam fixed one spectrum $I$, and varied the other spectrum $I'$ on a curve that passes $I$. He repeated the JND measurement along many curves across many spectra, and found that around each spectrum, the JND points make up a rough ellipsoid.

![The JND of a single observer around a single color, when approached from 14 different directions. The JND points fall roughly on an ellipse. [@wyszeckiColorScienceConcepts1982, figure 1(5.4.1)]](figure/macadam_ellipsoid_fig_1_541.png)

If the JND measurement is binary classification in color space, then what is real-valued regression in color space? Answer: color matching experiment.

Specifically, suppose we fix $I$, and let the observer turn a knob that varies $I'$ along a curve passing $I$, then we would find that $I'$ is normally distributed centered upon $I$. Perform the experiment with 3 knobs, and we would obtain an ellipsoidal cluster. The ellipsoids of $1\sigma$ are the [MacAdam ellipsoids](https://en.wikipedia.org/wiki/MacAdam_ellipse). As ellipsoids are very hard to draw, we typically only see 2D slices of them -- the MacAdam ellipses.

![The color matching experiment data of a single observer around a single color. The points are projected from 3D space to [three orthogonal views](https://en.wikipedia.org/wiki/Orthographic_projection). The ellipsoid is the ellipsoid of $3\sigma$. [@wyszeckiColorScienceConcepts1982, figure 1(5.4.3)]](figure/macadam_ellipsoid_fig_1_543.png)

::: {.callout-note title="Conjecture: perceived lightness and hues are totally geodesic foliations"}

Given two colors $C_0, C_1$, we can construct the geodesic curve between them as the shortest sequence of colors $C_a, C_b, C_c, \dots$, such that $C_0, C_a$ are JND, and $C_a, C_b$ are also JND, etc. It sounds reasonable in my head that, if $C_0, C_1$ have the same [perceived lightness](https://en.wikipedia.org/wiki/Lightness), then the geodesic connecting them should all have the same perceived lightness, because it seems like we would be wasting some precious JND on "jumping up in lightness, only to jump down again". Similarly, if $C_0, C_1$ have the same perceived hue, then I guess the geodesic through them would stay along the same perceived hue.

If this is true, then we can construct two families of foliations in color space, one for equal-lightness surfaces, and one for equal-hue surfaces. Each surface is a totally geodesic foliation [@johnsonTotallyGeodesicFoliations1980], meaning that each geodesic within a foliation is also a geodesic in the total color space.

However, this definitely isn't true for perceived saturation, as the shortest path between slightly saturated red and slightly saturated green (opposite of red) goes through perfect gray, so who knows whether this conjecture is true or not?

:::

Though JND and color matching are two different methods, they are both using people as statistical detectors, and it stands to reason that they should measure the same thing. Indeed, the ellipsoids of JND are roughly the $3\sigma$ MacAdam ellipsoids [@wyszeckiColorScienceConcepts1982, section 5.4].

![MacAdam ellipses plotted on the CIE 1931 $xy$-diagram, 10× actual size. Figure from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:CIExy1931_MacAdam.png).](figure/CIExy1931_MacAdam.png)

Given the [Tissot's indicatrix](https://en.wikipedia.org/wiki/Tissot's_indicatrix), it is natural to try to draw a distortion-less map of earth, where all Tissot ellipses are equally-sized circles. This is impossible, and Gauss knew exactly why: earth has positive gaussian curvature, but a flat sheet of paper has zero gaussian curvature.

![Tissot's indicatrix on [Behrmann projection](https://en.wikipedia.org/wiki/Behrmann_projection). Figure from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Tissot_behrmann.png)](figure/Tissot_behrmann.png)

Given the MacAdam ellipses, it is natural to try to draw a distortion-less map of color space. This is impossible, for the same reason: color space has nonzero curvature. It was already known to MacAdam in the 1940s that his experimental data shows color space has significant curvature.

![A paper model of a 2D subspace of the color space -- the space of colors with unit subjective brightness. The metric on the paper model faithfully matches the metric implied by MacAdam ellipse. We can see the curvature. [@macadamGeometryColorSpace1944, figure 5]](figure/paper_model_macadam_1944.png)

::: {.callout-note title="Conjecture: color space is not conformally flat"}

Is it possible to at least stretch the MacAdam ellipses into spheres, even though they aren't of the same radius? That is, is color space [conformally flat](https://en.wikipedia.org/wiki/Conformally_flat_manifold)? For example, in [Mercator's projection](https://en.wikipedia.org/wiki/Mercator_projection), the Tissot ellipses are indeed circular, though they become larger near the poles, so earth is conformally flat.

However, by [Liouville's theorem](https://en.wikipedia.org/wiki/Liouville%27s_theorem_(conformal_mappings)), conformal flatness is very stringent at 3 dimensions and above, so my conjecture is that color space is *not* conformally flat. Proof sketch: download the metric tensor from CIE, and check its [Cotton tensor](https://en.wikipedia.org/wiki/Cotton_tensor) is (statistically) nonzero.

:::

![A nonlinear map of CIE 1931 $xy$-graph designed to make MacAdam ellipses look roughly circular. [@wyszeckiColorScienceConcepts1982, figure 4(5.4.1)]](figure/macadam_ellipsoid_fig_4_541.png)

CIELAB color space is a smooth mapping from CIE 1931 color space to $\R^3$, such that the MacAdam ellipses are stretched spherical enough for practical purposes.

![All visible colors, plotted in CIELAB color space. Figure from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Visible_gamut_within_CIELAB_color_space_D65_whitepoint_mesh.png)](figure/Visible_gamut_within_CIELAB_color_space_D65_whitepoint_mesh.png)

### Information geometry

Imagine a hiker navigating a mountain path equipped only with an altimeter and a detailed altitude map. The hiker's ability to pinpoint their location on the map relies on sensing altitude changes. In regions where the terrain is steep (representing high sensitivity), even a small step forward (change in stimulus intensity) will register a noticeable altitude change on the altimeter (change in perceived sensation). This allows for precise localization -- a small JND. However, along flatter sections of the trail (low sensitivity), the hiker might need to traverse a longer distance to observe a meaningful altitude difference, leading to a larger JND and greater uncertainty about their position on the map.

Similarly, as we move around in color space, we may distinguish colors by the photoreceptor responses, which can be inferred from the sensitivity curves $S_S, S_M, S_L$. That is, we can reduce Riemannian metric to [information geometry](https://en.wikipedia.org/wiki/Information_geometry). Working this out in detail, [@dafonsecaDerivationHumanChromatic2016] showed that the Riemannian metric in color space is roughly the same ("explains 87% variance") as the [Fisher information metric](https://en.wikipedia.org/wiki/Fisher_information_metric).

![The ellipses measured by MacAdam (green) vs ellipses predicted by information theory (red). [@dafonsecaDerivationHumanChromatic2016, Figure 8b]](figure/Fonseca_2016_fig_8b.png)

In the same vein, people have argued for centuries about why certain colors are perceived as "pure" or "primary" (white, black, red, blue, green, etc), while others are "mixed" or "derived" from the primary colors. [@macevoyGeometryColorPerception2015] argues that the primary colors are "landmarks" in the geometry of color space, much like how on a map, the peaks and troughs are local maxima of gaussian curvature, and the mountain passes are the local minima, or how on a spacetime, a black hole singularity is the point where the [Kretschmann scalar is infinite](https://en.wikipedia.org/wiki/Curvature_invariant_(general_relativity)). Intuitively, we can see this on the CIELAB color solid. The top-most point is white, the bottom-most color is violet, and you can just see yellow at another point behind the back, etc.

::: {.callout-note title="Beyond Riemannian geometry"}

While color space is locally Riemannian, this is not so over longer distances. That is, once we are measuring the subjective distances between pairs of far-different colors, the data no longer behave like distances on a curved 3D space. [@bujackNonRiemannianNaturePerceptual2022] reported that there is "diminishing returns" in color distances.

Indeed, this non-Riemannian geometry has been known for a while. CIE in 1994 proposed a color difference, $\Delta E$, that is [not symmetric](https://en.wikipedia.org/wiki/Color_difference#CIE94). That is, if we ask a subject "How far is color 1 from color 2?" and then ask the opposite direction, we usually get a different numerical answer. This reminds me of [information-geometric divergence](https://en.wikipedia.org/wiki/Divergence_(statistics)), which is also not symmetric. I cannot find anyone who has studied this in detail, but it ought to interest the information geometers.

:::

## Inverted qualia

### Inverted qualia

The inverted qualia thought experiment has been used, like the philosophical zombie, in a whole host of arguments involving consciousness and the qualia. We consider the case involving functionalism, which is currently the most fashionable among cognitive psychologists and computer scientists. Other variants are reviewed in [@byrneInvertedQualia2004].

According to functionalism, mental states are best understood as functional states, that is, mathematical functions that map perceptual inputs to behavioral outputs. It's the intricate web of causal relations that constitutes a mental state, rather than the specific physical makeup realizing those relations.

In the anti-functionalism case, we consider two individuals, "Invert" and "Nonvert", are functionally identical. They receive the same visual input (a tomato), undergo the same internal processing, and produce the same behavioral outputs (saying "that's a red tomato"). However, their subjective experiences -- their qualia -- differ drastically. Where Nonvert sees red, Invert experiences green (or another color qualia entirely). They outwardly behave in the same way, and all functional measurements, from verbal reporting, psychological experiments, to MRI scanning, all find them the same, and yet the qualia of any color the Invert sees is rotated 180 degrees compared to that of the Nonvert.

Formally:

* The following spectrum inversion scenario is possible: Invert and Nonvert are functionally alike, and are both looking at a tomato.
* Thus, the mental does not supervene on functional organization.
* Thus, functionalism is false.

### Precursors

[@eastwoodAlhazenLeonardoLatemedieval1986]

Alhazen had considered the theory that the eye works like a camera obscura, and he had pronounced it impossible, as it would create an inverted image. Similarly, da Vinci developed no less than 8 different hypothetical mechanisms inside the eye to invert the image again, so that the image would land on the retina right-side-up.

![Leonardo da Vinci's drawings comparing the eye to a camera obscura. From *Codex Atlanticus* (1490-1495). Figure from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:1490-95_da_vinci_-_codex_atlanticus.jpg).](figure/1490-95_da_vinci_-_codex_atlanticus.jpg)

> ... certain extravagant situations are to be avoided, as they would create 'monstrosities', or disfigurations. The concern about hypothetical monstrous results occurs at four points in the description. (1) If the refracting surface of the vitreous were not completely regular and spherical, a monstrous visual form would appear, (2) If the refracting surface of the vitreous were the surface of a small sphere, causing the intersection of rays before even reaching the centre of curvature of the cornea and the anterior glacial surface, once again there could occur a monstrous visual form. Presumably the disfiguration anticipated here by Alhazen is simply the inverted image after intersection, but he does not say.  

![Illustration from Descartes' *Treatise of Man*.](figure/Descartes_diagram.png)

### Alternative examples

One objection from color science states that color space is not symmetric, and does not actually allow inversion as in the thought experiment. For example, saturated yellow does not merely look different from saturated red, but also looks brighter. In this view, "simply yellow" is not *simply* yellow. A point in color space is not *simply* a point. It is already inherently structured. Yellow is the brightest of all saturated colors, while violet is the dimmest, etc. [@hilbertColorInvertedSpectrum2000] argued that every possible quality space must be asymmetrical, in the sense that the only [automorphism](https://en.wikipedia.org/wiki/Automorphism) is the identity map, of $x \mapsto x$.

This appears to me an objection that is too strong, as there really do exist quality spaces that are symmetric. In humans, left and right are symmetric. Indeed, there are some highly symmetric quality spaces in nature.

Light, being electromagnetic waves, can be polarized. The space of possible polarizations is isomorphic to a ball, the [Poincare ball](https://en.wikipedia.org/wiki/Poincar%C3%A9_sphere_(optics)). The mantis shrimp species *Gonodactylus smithii* can detect the polarization of light over the entire 3-dimensional Poincaré ball [@kleinlogelSecretWorldShrimps2008]. It performs this by building 3 kinds of ommatidia, each specialized for two kinds of polarization. One is specialized for the horizontal-vertical, one for the diagonal-antidiagonal, and one for the clockwise-anticlockwise.

![Polarization states on the Poincaré ball. Figure from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Poincaresp.png).](figure/Poincare_polarization_ball.png)

Now, a Gonodactylus philosopher might propose the following inverted qualia problem: What if my qualia on the Poincaré ball is inverted compared to yours? That is, what if when you see a horizontally polarized light, you feel the same way as I see a vertically polarized light, and similarly across all of the ball? We can even imagine more exotic reflections, such as one that reflects across the $(0.3, 0.3, 0.9)$ direction, etc.

## Consciousness

### Easy, hard, meta

David Chalmers proposed three problems of consciousness. The easy problem is essentially the problem of consciousness information processing as studied by scientists: how memories work; how the brain recognizes objects; how to create human-level intelligence; etc. The hard problem is the easy problem, but with something extra that seems impossible to even fit into a scientific system. What that *something extra* is, philosophers are unable to say, but they typically give it the name of "qualia", "experience", "phenomenal awareness", etc.

> What makes the hard problem hard and almost unique is that it goes beyond problems about the performance of functions. To see this, note that even when we have explained the performance of all the cognitive and behavioral functions in the vicinity of experience—perceptual discrimination, categorization, internal access, verbal report--there may still remain a further unanswered question: *Why is the performance of these functions accompanied by experience?*
> 
> [@chalmersFacingProblemConsciousness1995]

While decades of science have made good progress on the easy problem, centuries of philosophical disputations have not made progress on the hard problem. To bypass the impasse, Chalmers proposed the meta problem: Why is the hard problem a problem? [@chalmersMetaproblemConsciousness2018]

Let's consider an analogy. The easy problem of biology would be: How does biological machines work? The hard problem: Why is the performance of these functions accompanied by *life*? The meta problem: What kind of cognition do people have, such that they can see a machine performing all the motions of life, and yet still call it "lifeless"?

In John Searle's Chinese room story, a man who knows nothing of Chinese, by executing an algorithm with pen and paper, could converse in Chinese writing. Many, including Searle, thought that the Chinese room does not really understand Chinese. This gives us another analogy.

The easy problem of Chinese: What algorithms can converse in Chinese text? The hard problem of Chinese: Why is the performance of Chinese speaking in a Chinese-speaker accompanied by *understanding*? The meta problem: What kind of cognition do people have, such that they say the Chinese room "lacks understanding"?

### Dark phenomena

> \[Folk Psychology\] suffers explanatory failures on an epic scale, that it has been stagnant for at least 25 centuries, and that its categories appear (so far) to be incommensurable with or orthogonal to the categories of the background physical science whose long-term claim to explain human behavior seems undeniable. Any theory that meets this description must be allowed a serious candidate for outright elimination.
> 
> [@churchlandEliminativeMaterialismPropositional1981]

> Neurophenomenology is possible; phenomenology is impossible.
> 
> [@metzingerBeingNoOne2004, page 83]

Everything I see, I know that I see. Everything that I hear, I know that I hear. Everything that I think, I know that I think. What could be clearer? Descartes based his entire philosophy on these kinds of self-evident truths, and these are still the starting points of many modern philosophies of the mind and consciousness.

However, such self-evident truths can be questioned. In [blindsight](https://en.wikipedia.org/wiki/Blindsight), I see things that I don't know that I see. In [Anton's syndrome](https://en.wikipedia.org/wiki/Anton_syndrome), I don't see things, yet I think that I see.[^anton-syndrome-a-priori] In [Cotard's delusion](https://en.wikipedia.org/wiki/Cotard's_syndrome), I live yet I think that I am dead.

[^anton-syndrome-a-priori]: 
    There was a philosopher who had taken Anton's Syndrome very seriously, but in the opposite direction, in the spirit of [one man's _modus ponens_ is another man's _modus tollens_](https://gwern.net/modus):

    > I still vividly remember one heated debate at an interdisciplinary conference in Germany a number of years ago, at which a philosopher insisted, in the presence of eminent neuropsychologists, that Anton's syndrome does not exist because *a priori* it cannot exist.
    >
    > [@metzingerBeingNoOne2004, page 235]

As a mathematician, I often know things without knowing how I know. When doing mental arithmetics, usually I do it both ways. One algorithm, operating consciously, goes from the highest digit down; the other algorithm, operating unconsciously, goes from the lowest digit up. As I consciously grind out digits from one end, digits simply "emerge" out of the other end. Like two teams digging a tunnel, they finally meet in the middle; the digits ripple-carry; the mouth vocalizes the final answer.

During deep contemplations of high-dimensional geometric objects, my self-awareness is turned down to a whimper, dimly illuminated by the sparks and piezoluminescence of vast gears and pulleys turning in the [dark mill of the brain](https://en.wikipedia.org/wiki/Leibniz%27s_gap), where the light of consciousness can never penetrate. A few times, I came back to consciousness on the carpet, not knowing how I got there, but with a clear feeling that an answer is close. Then I find the answer -- or not. The non-conscious parts of the brain make plenty of mistakes too.

Consider a pair of pure lights, at $550 \;\mathrm{nm}$ and $554 \;\mathrm{nm}$. For an observer with good vision, they are separated by a JND, so if the observer sees two patches of light shining on two plates of frosted glass placed close to each other, then the observer can just barely see that they are not the same color. However, as soon as the two lights are turned off, the difference disappears. The observer cannot recall one as "green-550" and the other as "green-554". Both would be recalled as "kind of green". The observer cannot tell if a single patch of light is closer to green-550 or green-554. The observer cannot tune a laser by sight so that its color matches green-550 rather than green-554.

There are several ways to interpret this result.

Daniel Dennett's approach would be to eliminate inaccessible phenomena -- there is neither green-550 phenomenon nor green-554 phenomenon, but only the "one patch looks greener than another" phenomenon, which is available for conscious information processing.

Thomas Metzinger and David Roden's approach is "dark phenomenology" [@rodenPosthumanLifePhilosophy2015, chapter 4]. A dark phenomenon rises from dust, does its job, then falls back to dust. It cannot be interrogated, redirected, paused, vocalized, remembered, threatened, or inspected. In this way, green-550 and green-554 are dark phenomena. They are real phenomena and have real mental functions, but they cannot be captured or interrogated. A dark phenomenon, such as green-550, is an information object that only flows along hardwired circuits. The conscious part of the brain might echo a command "Store this phenomenon in long-term memory!" or "Reroute this phenomenon for verbal report!" but such commands are futile. The green-550 and green-554 phenomena are sent to some visual comparison module then discarded. The visual comparison module might output a *bright phenomenon* "They are different.", but this bright phenomenon is merely an impoverished derivative of the dark phenomena that came before.

The just noticeable difference (JND) in color perception possibly shows that we see metric, not colors themselves.

If the phenomenology of color is but the metric geometry of color space, then we have not only dispelled the inverted qualia problem, but also have dispelled the problem of other minds. We should be able to construct the phenomenal geometry of color from retinal measurements. By comparing my retina and your retina, we can objectively determine whether my green is the same as yours.

If the inverted qualia problem is dissolved by the hole argument, then the hard problem of color perception is gone as well. What remains is merely determining the metric of color space. In this way, the biophysics of the eye-brain system would mostly solve the easy problem of color perception.

### Dual-process theory of the meta-problem

In cognitive psychology, there are many dual-process theories for explaining many cognitive processes. A theory is a dual-process theory if it follows the dual-process template. That is, if it models a cognitive process with an algorithm that has two parts, termed System 1 and System 2. System 1 is characterized by automatic, fast, and intuitive processing, while System 2 is deliberative, slower, and more analytical.

[@fialaPsychologicalOriginsDualism2012] proposed a dual-process theory for the meta-problem of consciousness. According to them, people recognize something as an agent or not by a dual process. This is evolutionarily important for ancestral humans, because detecting whether that shaking in the grass is caused by an animal or not could be a life-and-death decision.

System 1 for detecting agency uses the following heuristics: eye-like shapes on a head-like bump, unpredictable environmental reactions, and self-initiated movement beyond mere inertia. System 2 for detecting agency involves rational deliberation, theory application, and conscious reasoning. These processes are engaged when evaluating complex concepts, such as brain-based theories of consciousness.

Now, the meta-problem of consciousness occurs when one attributes agency to the brain. The brain, lacking visible features like eyes, appearing inert within the skull, and not exhibiting self-propelled motion, is not an agent according to System 1. The persistent conflict between System 1 and System 2 is verbalized into the hard problem of consciousness: System 2 admits that the brain is enough for agency, while System 1 insists that it is still lacking something, be it "consciousness", "qualia", or "experience".

Similarly, this explains how both Einstein and Poincare stumbled over the hard problem of General Relativity. General Relativity is acceptable to System 2 processes, but not to System 1 processes, which insists that spacetime *is* $\R^4$, and that General Relativity may describe a metric field *over* it, but not what spacetime *is*. Consequently, there is a persistent "explanatory gap", as a nagging feeling of the hard problem. "Even when we have explained the observable results from the astronomical to the microscopic, there may still remain a further unanswered question: *Why did the galaxy pass over point A, not point A'?*"

They start with $\R^N$, then quotient out smooth deformations.

Mathematically speaking, there is no necessary division between substance and property, though it is a very useful intuition. Physically speaking, the division has led to errors, such as Einstein's failed 1914 attempt at non-relativistic theory of gravity. This is especially true in the case of general relativity, because it developed from 19th century differential geometry, which tended to be understood as about geometry of deformable surfaces -- where there is literally a substance with changeable property (the strain field). Indeed, the famous image of "earth sitting on a rubber sheet" is a stubbornly persistent illusion created by System 1.

![The stubbornly persistent illusion. Figure from [xkcd: Teaching Physics](https://xkcd.com/895/)](figure/xkcd_teaching_physics.png)