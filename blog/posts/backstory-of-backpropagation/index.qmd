---
title: "The Backstory of Backpropagation"
author: "Yuxi Liu"
date: "2023-12-26"
date-modified: "2023-12-27"

categories: [AI, economics, history]
format:
    html:
        toc: true

description: "What took them so long to just use the chain rule?"
image: "figure/banner_cropped.png"

status: "finished"
confidence: "likely"
importance: 4
---

## Abstract

The backpropagation algorithm could have been invented by a slight generalization of the adjoint method, or the costate method, known widely among control theorists in the 1950s. Hybrid control theory leads almost inexorably to the backpropagation algorithm.

The backpropagation algorithm was reinvented multiple times during the 1970 -- 1986. It was finally no longer being reinvented after the popularity of connectionism finally taught everyone how backpropagation works.

Seppo Linnainmaa's claim of priority is his 1970 master thesis -- in Finnish, and never published. An English paper in 1976 was published that may have been relevant to automatic differentiation research, but not to neural network research. He has priority, but no paternity.

Paul Werbos' claim of priority is that he developed the backpropagation algorithm in 1971, but was frustrated at every turn whenever he tried publishing it, not managing until around 1982. After that, it was taken up by many neural network researchers. He has priority and paternity.

David Rumelhart freely admits that he was not the first to develop it, but he did develop it independently in 1982 and taught others in his research circle. The 1986 paper he coauthored [@rumelhartLearningRepresentationsBackpropagating1986] became popular enough that nobody would need to reinvent it again. He has no priority, but paternity.

I am unable to explain why none of the first wave of neural network researchers developed backpropagation. The best hypothesis is that everyone was misled by the then current understanding of real neurons. Back in the days, everyone "knew" that real neurons operated by discrete spikes. Possibly they were also misled by the McCulloch--Pitts neuron model, as well as a mistaken attempt to achieve parity with digital computers, which were discrete.

## The backpropagation algorithm

To set the stage, we should at least quickly derive the backpropagation algorithm. In one sentence, backpropagation is an algorithm that calculates the gradient of every parameter in an acyclic directed graph with respect to a single final parameter.

The graph can be finite or infinite, but in all cases, the acyclic directedness allows us to assign a "logical time" to each node of the graph.[^existence-of-logical-time] Some nodes are independent variables: they point at other nodes, but no nodes point at *them*. Other nodes are dependent. If we know the independent variables, we can propagate their values *forward* in logical time and obtain the values of every node. This is the "forward pass". Backpropagation goes backwards in logical time.

[^existence-of-logical-time]: Because anything that happens, happens in the real world, and the real world has a single direction in time, any computation that can happen must have a logical time that is identical with physical time, even though logical time does not have to coincide with physical time. This is an existence proof only. 

We use the convention of putting the derivative on the rows. So for example, for $f: \R^2\to\R^2$, we have

$$
\nabla_x f = \frac{df}{dx} = \begin{bmatrix} 
 \frac{df_1}{dx_1} & \frac{df_1}{dx_2} \\
 \frac{df_2}{dx_1} & \frac{df_2}{dx_2} 
 \end{bmatrix}
$$

This convention simplifies a lot of equations, and completely avoids transposing any matrix.

### Discrete logical time

Consider an acyclic directed graph with a finite number of nodes. Each graph node is a finite-dimensional real vector, though they may have different dimensions We order the acyclic graph on the number line, so that each node depends only on the earlier nodes. Now denote the graph nodes as $x_0, x_1, ..., x_T$. By our ordering, $x_1$ depends on only $x_0$, and $x_2$ depends on only $x_1, x_2$, and so on:

$$
\begin{aligned}
x_0 &= x_0 \\
x_1 &= f_1(x_0) \\
&\cdots \\
x_T = f_T(x_0, x_1, ..., x_{T-1})
\end{aligned}
$$

Now we perform an infinitesimal perturbation on every of its independent variables. A forward propagation then causes an infinitesimal perturbation on every variable, independent or dependent. Denote these as $dx_0, dx_1, ..., dx_T$. We can now compute the derivative of $dx_T$ with respect to every other variable by backpropagating the perturbation. Suppose we can see only $dx_T$, then the change in $x_T$ due to $dx_T$ is the identity. That is,

$$
\frac{dx_T}{dx_T} = I
$$

Now suppose we can see only $dx_{T-1}$, then the change in $x_T$ due to $dx_{T-1}$ can only come from the final step in the forward propagation. Therefore

$$
\frac{dx_T}{dx_{T-1}} = \nabla_{x_{T-1}} f_T(x_0, x_1, ..., x_{T-1})
$$

Similarly, the change in $x_T$ due to $dx_{T-2}$ can come from either directly changing $x_T$, or changing $x_{T-1}$ and thereby changing $x_T$. Therefore

$$
\frac{dx_T}{dx_{T-2}} = \nabla_{x_{T-2}} f_{T-1}(x_0, x_1, ..., x_{T-2}) + \nabla_{x_{T-2}} f_{T-1}(x_0, x_1, ..., x_{T-2}) + 
$$

This generalizes to all steps.

The above computation is fully general, but this makes it inefficient. In practical applications of backpropagation, the computation graph is usually very sparse, meaning that each $x_t$ only directly influence a few more nodes down the line. In standard neural networks, typically $x_{t}$ only directly influences $x_{t+1}, x_{t+2}$. Thus sparsity is vital for backpropagation to be relevant.

As a side note, we could in fact compute *all* derivatives, not just the first, in one single backward pass. Other than the second derivatives $\nabla^2_{x_t}x_T$, there are rarely any use for the other derivatives, such as $\nabla_{x_t}\nabla_{x_s}x_T, \nabla^3_{x_t}x_T$, etc. The second derivative was occasionally used for neural networks circa 1990, with the further simplification that they only  keep the positive diagonal entries of $\nabla^2_{x_t}x_T$ and set all other entries to zero.[@lecunGeneralizationNetworkDesign1989]

### Continuous logical time

Consider the problem of controlling a car along a highway. At a sufficient level of simplification, the car is just a single dot on a line, and the state of the car is determined by its speed and velocity. To avoid waiting forever, we require time $t\in [0, T]$. Write the state variable as follows:

$$
x_t = (\text{location at time }t, \text{velocity at time }t)
$$

It might be confusing to use $x_t$ for the state at time $t$, instead of location, but it makes the notation consistent.

Suppose the only thing we can influence is how much we press the pedal. Write $u_t$ to be the resulting acceleration we inflict on the car by pressing on the pedal. Further, assume the car is slowed down by friction that is proportional to velocity. We then have:

$$
\dot x_t = f(x_t, u_t)
$$

where $\mu$ is the friction coefficient, and $f(x_t, u_t) = (x_{t, 1}, -\mu x_{t, 1} + u_t)$ is the dynamics equation of the system.

Now, we can draw the computation graph as before. The computation graph has infinitely many nodes, which is inconvenient, but it has a very simple structure. The graph has two kinds of nodes: the $x_t$ nodes and the $u_t$ nodes. Its independent variables are $x_0$ and all the $u_t$ nodes. Each $x_{t+dt}$ depends on only $x_{t}$ and $u_t$. This makes the two propagations particularly simple.

The forward propagation is obtained by integration, which we can unwrap into a form resembling the discrete time case:

$$
\begin{aligned}
x_0 &= x_0 \\
x_{dt} &= x_0 + f(x_0, u_0) dt \\
&\cdots
\end{aligned}
$$

The backpropagation is similarly obtained. By inspecting the computation graph, we can see that each $u_t$ only directly influences $x_{t+dt}$, thus

$$
\frac{dx_T}{du_t} = \frac{dx_T}{dx_{t+dt}} \nabla_{u_t}f(x_t, u_t) dt 
$$

It remains to compute $\frac{dx_T}{dx_t}$. This can be found by backpropagation too, since each $x_t$ only directly influences $x_{t+dt}$, thus

$$
\frac{dx_T}{dx_t} = \frac{dx_T}{dx_{t+dt}} \lrs{I + \nabla_{x_t} f(x_t, u_t) dt}
$$

If we denote the gradient as $g_t := \frac{dx_T}{dx_t}$, then we find an equation for $g_t$:

$$
g_t = \lrs{I + (g_t + \dot g_t dt) \nabla_{x_t} f(x_t, u_t) dt}\implies \dotg_t = -g_t\nabla_{x_t} f(x_t, u_t)
$$

This equation bottoms out at the last time $t=T$, for which $g_T = \frac{dx_T}{dx_T} = I$. Thus we have the [costate equations](https://en.wikipedia.org/wiki/Costate_equation):

$$
\begin{cases}
g_T &= I \\
\dot g_t &= - g_t \nabla_{x_t} f(x_t, u_t) 
\end{cases}
$$

which must, as you can see, be integrated *backwards in time* -- backpropagation again! Indeed, control theory practically compels us to find backpropagation.

### Hybrid logical time

When the computation graph has both nodes with discrete logical times, and nodes with continuous logical times, we call such a system as having hybrid logical time.[^hybrid-control-theory]

[^hybrid-control-theory]: The name "hybrid" comes from "hybrid control theory", which are systems with both discrete changes, or jumps, and continuous changes, or flow. They appear whenever a digital device is used to control a continuous system, such as a computer piloting an airplane.

The idea can be illustrated by the fundamental problem in control theory: how to optimize control? You cannot optimize without optimizing a real number, so we write down the number as $J$, representing the "cost" of the trajectory. For example, in driving a car, we might want to optimize comfort, time-until-arrival, and fuel cost. We can then write $J$ down as something like

$$
J = \ub{(x_{T, 0} - x_{goal})^2}{\text{location should be at the goal location at the end time}} 
    + \ub{(x_{T, 1} - 0)^2}{\text{speed should be zero at the end time}} + \int_0^T u_t^2 dt
$$

More generally, the objective to be optimized is in the form

$$
J = A(x_T) + \int_0^T L(x_t, u_t)dt
$$

for some real-valued functions $A, L$.

We can of course care about more than the state at the last time-step. We can care about multiple time-steps $t_0, t_1, ..., t_n$ by writing down a cost function $J = \sum_{i=0}^n A_i(x_{t_i}) + \int_0^T L(x_T, u_T)dt$, but this merely creates complications without introducing new ideas. Even more ornate computation graphs, weaving together multiple strands of continuous and discrete logical times, can be backpropagated in the same way without involving new ideas.

Define the costate $\lambda_t := \nabla_{x_t} J$, then the costate backpropagates as: 

$$
\lambda_t = L(x_t, u_t) dt + \lambda_{t+dt}(I + \nabla_{x_t}f(x_t, u_t)dt)
$$

and simplifying, we have the costate equation:

$$
\begin{cases}
\lambda_T &= \nabla_{x_T} A(x_T) \\
\dot \lambda_t &= - \nabla_{x_t} L(x_t, u_t) - \lambda_t \nabla_{x_t} f(x_t, u_t) 
\end{cases}
$$

which can be solved by integrating backwards in time.

Once we have obtained all the costates, we can compute $\nabla_{u_t} J$. Since $u_t$ can only influence $J$ either directly via $L(x_t, u_t)$ or indirectly via $x_t$, we have

$$
\nabla_{u_t} J = \lrs{\nabla_{u_t}f(x_t, u_t) \lambda_t + \nabla_{u_t}L(x_t, u_t)}dt
$$

### Optimal control theory

An optimal trajectory must have $\nabla_{u_t} J = 0$, since otherwise we can shave off a little piece of cost by giving $u_t$ a little boost in the other direction (infinitesimal gradient descent!). This gives us two equations that any optimal trajectory must satisfy

$$
\begin{cases}
- \dot \lambda_t &= \nabla_{x_t} L(x_t, u_t) + \lambda_t \nabla_{x_t} f(x_t, u_t) \\
0                &= \nabla_{u_t} L(x_t, u_t) + \lambda_t \nabla_{u_t} f(x_t, u_t) \\
\end{cases}
$$

Now, what is the effect of perturbing $u_t$ by $du_t$? It would perturb $x_{t+dt}$ by $\nabla_{u_t} f(x_t, u_t) du_t dt$, a second-order infinitesimal. Consequently, it would perturb $x_T$ by only a second-order infinitesimal, and thus $\lambda$ too. Therefore, we have

$$
\nabla_{u_t}\lambda_t = 0
$$

giving us simplified equations for optimality:

$$
\begin{cases}
- \dot \lambda_t &= \nabla_{x_t} L(x_t, u_t) + \lambda_t \nabla_{x_t} f(x_t, u_t) \\
0                &= \nabla_{u_t} (L(x_t, u_t) + \lambda_t f(x_t, u_t)) \\
\end{cases}
$$

Unfortunately, the first equation cannot be simplified similarly, because $\nabla_{x_t}\lambda_t \neq 0$. Still, it seems $L(x_t, u_t) + \lambda_t f(x_t, u_t)$ should be an important quantity. So let us define it to be $H(x_t, u_t, \lambda_t) := L(x_t, u_t) + \lambda_t f(x_t, u_t)$. At this point, we should come out and say what the letters come from. $L$ comes from the "Lagrangian", and $H$ the "Hamiltonian". Indeed, classical Hamiltonian mechanics is a special case of (first order) optimal control theory!

If we interpret economically the quantities, then $J$ is the cost of the entire trajectory, $\lambda_t$ is the marginal cost of the point $x_t$ in the trajectory, and $L(x_t, u_t)$ is the cost-rate at time $t$. The second equation of optimality $\nabla_{u_t} H(x_t, u_t, \lambda_t) = 0$ states that when the trajectory is optimal, the marginal cost of control is zero: there is no saving in perturbing the control in any direction.

Therefore, define the "optimized" Hamiltonian and optimized control relative to it:

$$
\begin{cases}
H^*(x_t, \lambda_t) &:= \min_{u_t}H(x_t, u_t, \lambda_t) = \min_{u_t} \lrb{L(x_t, u_t) + \lambda_t f(x_t, u_t)} \\
u^*(x_t, \lambda_t) &:= \argmin_{u_t}H(x_t, u_t, \lambda_t)
\end{cases}
$$

Then, by [Hotelling's lemma](https://en.wikipedia.org/wiki/Hotelling's_lemma), we obtain the [Hamiltonian equations of motion](https://en.wikipedia.org/wiki/Hamiltonian_mechanics):

$$
\begin{cases}
- \dot \lambda_t &= \nabla_{x_t} L(x_t, u_t^*) + \lambda_t \nabla_{x_t} f(x_t, u_t^*) &= \nabla_{x_t} H^*(x_t, \lambda_t) \\
  \dot x_t       &= f(x_t, u_t^*)                                                     &= \nabla_{\lambda_t} H^*(x_t, \lambda_t) \\
\end{cases}
$$

This is often called the [Pontryagin's maximum principle](https://en.wikipedia.org/wiki/Pontryagin%27s_maximum_principle), as Pontryagin's school of optimal control theory is based on this principle and its extensions. Control theory was a national security issue during the Cold War, as it is used from the space race to the missile race.

In classical control theory, the equation is sometimes solved in closed form, as in the case of [linear quadratic control](https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator). When it cannot be solved in closed form, it can still be numerically solved using a continuous form of the Bellman equation. Bellman developed the equation as a center piece of his dynamic programming theory. As you will see, the name "dynamic programming" appears later in Paul Werbos' invention of backpropagation, which he named "dynamic feedback".

In economics, cne can modify this framework slightly to allow discounting cost over time, yielding theories about "optimal investment" such as the [Ramsey optimal growth theory](https://en.wikipedia.org/wiki/Ramsey%E2%80%93Cass%E2%80%93Koopmans_model).

## Leibniz

The chain rule dates back to *Calculus Tangentium differentialis* \[Differential calculus of tangents\], a manuscript by Leibniz dated 1676 November [@childManuscriptsLeibnizHis1917]. It says

> it does not matter, whether or no the letters $x, y, z$ have any known relation, for this can be substituted afterward.

In mathematical notation, he found that $dy = dx \frac{dy}{dx}$, or in his notation, $\overline{dy} = \overline{dx} \frac{dy}{dx}$, where the overbar denotes the thing to be differentiated. You can read it as a bracket: $d(y) = d(x) \frac{dy}{dx}$.

He then gave the following examples (Yes, there is a sign error. No, I'm not going to fix it. He just have to live with his mistakes.):

$$
\begin{aligned}
 & \overline{d \sqrt[2]{a+b z+c}} z^2 \text {. Let } a+b z+c z^2=x \text {; } \\
\text{Then} \quad  & \overline{d \sqrt[2]{x}}=-\frac{1}{2 \sqrt{x}} \text {, and } \frac{d x}{d z}=b+2 c z \text {; } \\
\text{Therefore} \quad  & \overline{d \sqrt{a+b z+c z^2}}=-\frac{b+2 c z}{2 \overline{d z} \sqrt{a+b z+c z^2}} \\
&
\end{aligned}
$$

He then proceeded to apply the method to "inverse method of tangents, with the assistance of quadratures", meaning solving differential equations of form $F(x, y, dy/dx) = 0$ by help of using a table of integrals. Here $dy/dx$ is what he meant by "tangents". For example, $y + yy' = xy$ has solution $y = x^2/2 - x$.

## McCulloch and Pitts

In the famous paper [@mccullochLogicalCalculusIdeas1943], McCulloch and Pitts proposed that

> Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes.

The McCulloch and Pitts paper is difficult to read, and is rooted in early 20th century mathematical logic. If you have seen [*Principia Mathematica*](https://en.wikipedia.org/wiki/Principia_Mathematica), you would notice that the McCulloch and Pitts paper resembles it in its abstruse notation. This is not coincidental, as the paper directly cites *Principia Mathematica*.

::: { layout-ncol=2}
![The infamous proof of 1+1=2 in Principia Mathematica](figure/principia_mathematica.png)

![The same notation is used by McCulloch and Pitts](figure/principia_mathematica_McCulloch_and_Pitts.png)
:::

The McCulloch and Pitts paper, like the *Perceptron* book, is something often cited but rarely read. The best introduction to the McCulloch and Pitts neural network is still [@minskyComputationFiniteInfinite1967, Chapter 3], which explicitly designs, neuron by neuron, threshold by threshold, how to construct neural networks that perform various tasks, such as simulating arbitrary Turing machines, process signals, perform additions, etc.

::: { layout-ncol=2}
![](figure/minsky_1976_finite_state_machine.png)

![](figure/minsky_1976_serial_binary_addition_network.png)
:::

## Frank Rosenblatt

Frank Rosenblatt is the originator of many concepts in neural networks, and he was also popularly famous even during his time. During the period of 1957 -- 1962, he did a lot of the theory and experiments with perceptron networks. Some experiments ran on digital computer simulations, and others on the Mark I Perceptron, a bespoke machine that takes up a whole room. A perceptron is a function of form $\theta(w^T x + b)$, where $\theta$ is the 0-1 step function, and $w \in \R^n, b \in \R$ are its learnable parameters. A perceptron network was a network of perceptrons. He experimented with a large number of perceptron network architectures, documented in his book [@rosenblattPrinciplesNeurodynamicsPerceptrons1962]. Its breadth is quite astonishing. It contains:

* perceptrons with continuous activation functions (section 10.2);
* perceptron convergence theorem for perceptrons with continuous activation functions that are also monotonic and sign-preserving (page 249);
* perceptron layers with random delay in transmission time (chapter 11);
* layers with connections between units within the same layer, with possibly closed loops (chapter 17--19);
* layers with connections from a later layer to a previous layer (chapter 20);
* perceptron networks that learns to associate image and audio inputs ()
* program-learning perceptrons (chapter 22)
* perceptron networks that analyze videos and audios (chapter 23)

From our vantage point, we can fairly say that he has invented randomization, weight initiation schemes, neural Turing machines, recurrent neural networks, vision models, time delay neural networks, multimodal neural networks...

![Figure 58. The first multimodal neural network?](figure/rosenblatt_figure_58.png)

What is even more astonishing is that as far as I can see, he did not make use of gradient descent learning at all, not even on a single layer like Widrow and Hoff.

The first use of the word "backpropagation" is also in [@rosenblattPrinciplesNeurodynamicsPerceptrons1962], at chapter 13. He was trying to train both layers in a two-layered perceptron network by extending the perceptron learning algorithm. The algorithm works on each individual perceptron unit, but only if the desired output is known. The output units ("Response units" or "R units") have known desired output -- it is supplied by the experimenter Rosenblatt. However, what are the desired outputs for the hidden units ("Association units" or "A units")? There is no way to tell, so Rosenblatt tried some hacks. Essentially, he took the error signals at the output units, and used heuristic rules to "backpropagate the error" to synthesize error signals at the hidden units. The precise details are hacky, and no longer of any relevance.

One last thing about his backpropagation rule: he also discovered the per-layer learning rate trick. Because his backpropagation algorithm was so hacky, he found that he had to stabilize it by making the first layer learn at a much lower rate than the second.

> It is found that if the probabilities of changing the S-A connections are large, and the threshold is sufficiently small, the system becomes unstable, and the rate of learning is hindered rather than helped by the variable S-A network. Under such conditions, the S-A connections are apt to change into some new configuration while the system is still trying to adjust its values to a solution which might be perfectly possible with the old configuration. Better performance is obtained if the rate of change in the S-A network is sufficiently small to permit an attempt at solving the problem before drastic changes occur.

## Bernard Widrow and Marcian Hoff

Widrow and Hoff worked on multilayered perceptrons in early 1960s. They trained a single-layered perceptron with gradient descent on the squared loss, then proceeded to not try gradient descent after a year of trying to train a network with *two layers*.

The Widrow--Hoff machine, which they called the ADALINE ("adaptive linear"), is a function of type $\R^n \to \{0,1\}$ defined by

$$
f(x) = \theta(w^T x + b)
$$

and here it is again, that dreaded Heaviside step-function that was the bane of every neural network before backpropagation. What was odd about this one is that ADALINE was *trained* by gradient descent with the squared loss function $\frac 12 (w^T x + b - y)^2$, which is *continuous*, not discrete:

$$
w \leftarrow w - \alpha (w^T x + b - y) w, \quad b \leftarrow b - \alpha (w^T x + b - y) b
$$

The first ADALINE machine was a box that learned to classify binary patterns on a $3\times 4$ grid. It was pretty amusing, as everything was manual. The patterns were inputted by flipping 12 switches by hand. The error $w^T x + b - y$ was read from a voltmeter, and the parameters $w, b$ were individually adjusted by turning knobs that controlled rheostats inside the box. They then automated the knob-turning process using electrochemical devices called memristors, though the pattern input and the error read-out was still manual. Widrow recounts an amusing encounter with Rosenblatt:

> I just put the pattern in and the Adaline went "phut," and the needle was reading to the right or to the left. So I just held the adapt button down so some of the cells are plating while others are deplating, depending on the direction of the error signal. Rosenblatt's students put the pattern into the percept ron. You could see it in the lights on the percept ron. You could hear the potentiometers grinding away. We put another pattern into the Adaline, and it went "blip ," and there it was, adapted. They put it in the perceptron, and it's still grinding away. We put in a couple more patterns. Then we test the Adaline and test the percept ron to see whether the patterns are still in there.
>
> They're in the Adaline. In the perceptron, they're all gone. I don't know whether the machine was temperamental or what, but it was difficult to train. I argued with Rosenblatt about that first random layer. I said, "You'd be so much better off if you just took the signal from the pixels and ran them straight into the weights of the second layer." He insisted that a perceptron had to be built this way because the human retina is built that way. That is, there's a first layer that's randomly connected to the retina. He said the reason why you can get something to interpret and make sense of random connections is because it's adaptive. You can unravel all this random scrambling. What I was trying to do was to not model nature. I was trying to do some engineering.

After the ADALINE showed good behavior, they went on and on trying to train a two-layered ADALINE ("MADALINE", or "many ADALINE"), which of course cannot be trained by gradient descent, because the hidden layer has a Heaviside step function! It is almost inconceivable nowadays, but they simply refused to use a continuous activation function, but tried every other trick *except* that. They ended up with the [MADALINE I rule](https://en.wikipedia.org/wiki/ADALINE#MADALINE), which is both complicated, confusing, and did not work well. So they gave up. Hoff went to Intel to coinvent the microprocessor, and Widrow set about applying the ADALINE to small problems that it can solve well[^marvin-minsky-approves], such as adaptive antennas, adaptive noise filtering in telephone lines, adaptive filtering in medical scanning, etc. Even with just a single ADALINE, he brought breakthroughs to those fields.

[^marvin-minsky-approves]: 
    Marvin Minsky would approve

    > \[The perceptron\] is such a simple machine that it would be astonishing if nature did not make use of it somewhere. It may be that one of the best things a neuron can have is a tiny Perceptron, since you get so much from it for so little. You can’t get one big Perceptron to do very much, but for some things it remains one of the most elegant and simple learning devices I know of.” [@bernsteinMarvinMinskyVision1981]

Perhaps Widrow and Hoff were limited to considering only learning rules they could implement electrochemically? But it does not seem so, in the words of Widrow himself:

> The Madaline had an adaptive first layer and a fixed-logic second layer. What Rosenblatt had was a fixed-logic mst layer and an adaptive-logic second layer or output layer. Now, it is easy to adapt on output layer. But it's very difficult to adapt a hidden layer. ... We could adapt an adaptive first layer with a fixed second layer as long as we knew what the second layer was. But we never succeeded in developing an algorithm for adapting both layers, so that the second layer is not fixed and both layers are adaptive. It wasn't that we didn't try. I mean we would have given our eye teeth to come up with something like backprop.
> 
> Backprop would not work with the kind of neurons that we were using because the neurons we were using all had quantizers that were sharp. In order to make backprop work, you have to have sigmoids; you have to have a smooth nonlinearity, a differentiable nonlinearity. Otherwise, no go. And no one knew anything about it at that time. This was long before Paul Werbos. **Backprop to me is almost miraculous**. [@rosenfeldTalkingNetsOral2000]

When he heard about the "miraculous" backpropagation in the 1980s, he immediately started writing papers in neural networks again.

If this were an individual case, then I might have dismissed it as Widrow being misled by his background. Before entering a PhD program Widrow took an internship at the military [@widrowOralHistoryBernard1997]:

> ... the sky was divided up into a grid, with one mile by one mile regions, and all the computer sensed was that there was a target within that one mile by one mile square. ... The radar pulses were transmitted periodically, and then periodically you got samples of the flight path. Say you take a piece of graph paper and draw and nice, smooth flight path for an airplane, and then at uniform points in time put a mark on the actual path based on the radar sample. If you don't show anybody that nice, smooth line but only the sequence of grid squares that the airplane was in, one could still reconstruct the smooth track.

The problem of reconstructing a smooth flight path from discretized radar signals got him interested in quantization, and he did his PhD thesis (1954) in the statistical theory of quantization noise. In the interview 43 years later, he called it "the best piece of work I ever did in my whole life". He is still the world expert on quantization noise, and wrote an entire textbook on quantization noise [@widrowQuantizationNoiseRoundoff2008].

So regarding this background, one might be tempted to say that Widrow was misled by his infatuation with quantization, and attempted to use quantized neural networks. However, this does not explain why all the *other* pioneers went down the same wrong path.

## Seppo Linnainmaa

It's said that Seppo Linnainmaa's masters thesis in 1970 contains a backpropagation algorithm, but it is in Finnish, and not available online either. A bibliography search shows that his thesis were basically never cited before the 2010s. It is one of those papers that people cite for ritual completion only. I think we can safely say that his thesis has priority but no paternity. [@griewankWhoInventedReverse2012] describes in detail what is in the thesis, according to which he developed it to analyze the cumulative effect of round-off error in long chains of computer computations.

I checked all his English papers during the 1970s, and it seems only [@linnainmaaTaylorExpansionAccumulated1976] has been cited non-negligibly before 2000, and may have had some impact on automatic differentiation, but I cannot tell by how much. It contains two algorithms, one for computing the first derivative, one for computing the first two. Both were special cases of the general backpropagation algorithm for computing arbitrary orders of derivatives.

## Rumelhart and Sejnowski

Rumelhart rediscovered backpropagation around 1982 and immediately set about publishing and telling others about it. Some still resisted, but others immediately grasped it and set about using it. Among them were Widrow, as we saw previously, and Sejnowski. In the interview, he was rather unbothered by the priority dispute [@rosenfeldTalkingNetsOral2000, Chapter 12]:

> as far as I know it's been discovered at least three times, maybe more. ... I had no idea that Paul Werbos had done work on it. ... There are other examples of work in the control literature in the '60s \[the [adjoint method](https://en.wikipedia.org/wiki/Adjoint_state_method)\]. ... it's just no one had really done anything with them. My view is that we not only discovered them, but we realized what we could do with them and did a lot of different things. I consider them independent discoveries, at least three and maybe more. \[Shun'ichi\] Amari, I think, suggests that he had another discovery, and you know, I believe that. You can look at his paper. He had, but he didn't do anything with it. I think that was in the late '60s. I don't feel any problem. You know, maybe we should have done better scholarship and searched out all of the precursors to it, but we didn't know there were any.

In 1983, Rumelhart showed Sejnowski backpropagation, who immediately tried it, and found that it was much faster than Boltzmann machine. What a refreshing change from all those others who stubbornly refused to try it. [@rosenfeldTalkingNetsOral2000, Chapter 14]

> ... I was still working on the Boltzmann machine and was beginning to do simulations using backprop. I discovered very rapidly that backprop was about an order of magnitude faster than anything you could do with the Boltzmann machine. And if you let it run longer, it was more accurate, so you could get better solutions.

This was vitally important later, when Sejnowski used backpropagation to train [NETtalk](https://en.wikipedia.org/wiki/NETtalk_(artificial_neural_network)), a *huge* network with 20k parameters.[^nettalk-scaling] The model was a popular hit and appeared on prime-time television.[@sejnowskiDeepLearningRevolution2018, pages 112--115]

[^nettalk-scaling]:
    People told him that it had too many parameters and would hopelessly overfit, but the network just needed enough data and it generalized uncannily well It really seems like scaling skepticism is never going to leave.

    > There were 18,629 weights in the network, a large number by the standards of 1986, and impossibly large by the standards of mathematical statistics of the time. With that many parameters, we were told that we would overfit the training set, and the network would not be able to generalize. [@sejnowskiDeepLearningRevolution2018, page 113]

## Geoffrey Hinton

The interview with Geoffrey Hinton is hilarious, mostly about how he *spent an entire year refusing to use backpropagation* despite Rumelhart's incessant prodding. Like the last section, this is mostly made of quotations from [@rosenfeldTalkingNetsOral2000, Chapter 16].

After learning about Hopfield networks and simulated annealing both in 1982, he tried combining them, and discovered the Boltzmann learning algorithm in 1983, which he published in 1985 [@ackleyLearningAlgorithmBoltzmann1985].

> I remember I had to give a talk, a sort of research seminar, at Carnegie Mellon in either February or March of '83 ... I wanted to talk about this simulated annealing in Hopfield nets, but I figured I had to have a learning algorithm. I was terrified that I didn't have a learning algorithm.
> 
> Then we got very excited because now there was this very simple local-learning rule. On paper it looked just great. I mean, you could take this great big network, and you could train up all the weights to do just the right thing, just with a simple local learning rule. It felt like we'd solved the problem. That must be how the brain works.
> 
> I guess if it hadn't been for computer simulations, I'd still believe that, but the problem was the noise. It was just a very, very slow learning rule. It got swamped by the noise because in the learning rule, you take the difference between two noisy variables, two sampled correlations, both of which have sampling noise. The noise in the difference is terrible.
> 
> I still think that's the nicest piece of theory I'll ever do. It worked out like a question in an exam where you put it all together and a beautiful answer pops out.

And now, the hilarity we have been waiting for. When Rumelhart told him in 1982 about backpropagation,

> I first of all explained to him why it wouldn't work, based on an argument in Rosenblatt's book, which showed that essentially it was an algorithm that couldn't break symmetry. ... the standard way of doing learning is to have random initial weights, or you could just put a little bit of noise in the learning rule ...
> 
> The next argument I gave him was that it would get stuck in local minima. There was no guarantee you'd find a global optimum. Since you're bound to get stuck in local minima, it wasn't really worth investigating.
> 
> Then I tried to use it to get a very obscure effect. I couldn't get this very obscure effect with it, so I lost interest in backpropagation. I managed to get this very obscure effect later with a Boltzmann machine. I'd realized that if you've got a net to learn something, and then you add noise to the weights by making it learn something else, it should be much faster at relearning what it had previously learned. You should get very fast relearning of stuff that it previously knew, as compared to the initial learning. We programmed a backpropagation net, and we tried to get this fast relearning. **It didn't give fast relearning, so I made one of these crazy inferences that people make -- which was, that backpropagation is not very interesting**.

After one year of failing to get Boltzmann machines to train, he discovered that Boltzmann machines *also* got stuck in local minima. Desperate times call for desperate measures, and he finally resorted to gradient descent.

> After initially getting them to work, I dedicated over a year to refining their performance. I experimented with various techniques, including weight decay, which helped in preventing large energy barriers. We attempted several other approaches, but none of them yielded satisfactory results. I distinctly recall a point where we observed that training a Boltzmann machine could model certain functions effectively, but prolonged training caused its performance to deteriorate, a phenomenon we referred to as "going sour." We couldn't initially comprehend why this was happening. I generated extensive reports, filling stacks of paper about three inches thick, as I couldn't believe that these networks would degrade as you acquired more knowledge.
> 
> It took me several weeks to realize the root of the problem. The learning algorithm operated under the assumption that it could reach thermal equilibrium. However, as the weights grew larger, the annealing process failed to achieve thermal equilibrium, trapping the system in local minima. Consequently, the learning algorithm started behaving incorrectly. This realization led us to introduce weight decay as a solution to prevent this issue.
> 
> **After investing over a year** in attempting to optimize these methods, I ultimately concluded that they were unlikely to deliver the desired results. In a moment of desperation, I considered revisiting \[backpropagation\]. 

Then he asked the research group for volunteers. None of the ten students took up the offer (and thus giving up a place as the coauthor of the backpropagation paper??):

> They'd all been thoroughly indoctrinated by then into Boltzmann machines. ... They all said, "You know, why would you want to program that?" We had all the arguments: It's assuming that neurons can send real numbers to each other; of course, they can only send bits to each other; you have to have stochastic binary neurons; these real-valued neurons are totally unrealistic. It's ridiculous." So they just refused to work on it, not even to write a program, so I had to do it myself.
> 
> I went off and I spent a weekend. I wrote a LISP program to do it. 

Hinton almost had one last chance at giving up on backpropagation. 

> I almost blew it because the first thing I tried it on was an 8-3-8 encoder; that is, you have eight input units, three hidden units, and eight output units. You want to turn on one of the input units and get the same output unit to be on. 
>
> In a Boltzmann machine, since the units are binary, the states of the three hidden units have to be the eight different binary codes, so you know what the hidden units will look like when they've solved the problem. So I ran backpropagation on this problem and looked at the weights of the hidden units. They didn't have the right structure of weights. ... I thought, "Oh well, it turns out backpropagation's not that good after all." Then I looked at the error, and the error was zero. I was amazed.

And so one year of excuses later, Hinton finally surrendered to backpropagation. They published the algorithm along with several examples, which became widely cited.

> That was at the stage when we were just completing the PDP books, so we'd already agreed on what was going to be in the books. The final chapters were being edited. We decided we'd just slip in an extra chapter on backpropagation, so it was a late addition to the book. But I was always a bit disappointed. I mean, intellectually, backpropagation wasn't nearly as satisfying as Boltzmann machines. ... it didn't have the nice probabilistic interpretation.

## Werbos

After reviewing the story of multiple people, my personal opinion is that Paul Werbos most deserves the title of "originator of the backpropagation algorithm", as he both has the priority and paternity of the algorithm. In 1993, Paul Werbos gave an interview, where he described the backstory of his invention of backpropagation [@rosenfeldTalkingNetsOral2000, Chapter 15]. I will let him speak, only interjecting with brief comments.

Before entering Harvard, he tried implementing Hebbian learning, but noticed it wouldn't work. 

> It was obvious to me from a statistical point of view that Hebbian learning was going to be measuring correlation coefficients, and for multivariate problems it would not work. I never gave the talk. I said, "Now I'm going to figure out something with the same flavor that does work."
>
> \[Understanding human learning\] will help us understand the human mind; it will help human beings understand themselves. Therefore, they will make better political decisions, and better political decisions are vital for the future of humanity.
>
> Minsky was one of my major influences. Well, Minsky and Hebb and Asimov and Freud.

Sometime before 1968, he was inspired to do backpropagation from reading Freud.

> I talk in there about the concept of translating Freud into mathematics. This is what took me to backpropagation, so the basic ideas that took me to backpropagation were in this journal article in '68 [@werbosElementsIntelligence1968]. I talked a lot about what was wrong with the existing \[two state\] McCulloch-Pitts neuron model, and how it was only "1" and "0." I wanted to make that neuron probabilistic. I was going to apply Freudian notions to an upper-level, associative drive reinforcement system. When 

For his masters (1969), he majored in mathematical physics, and minored in decision and control. During his PhD at Harvard, he had to take some compulsory courses, which he didn't want to. To salvage the situation, he took computer courses which would provide him with some computer hours, a scarce resource at the time. I would guess the following event happened early 1971.

> Initially, I was going to do something on quantum physics. I learned something about partial differential equations, but not enough. I couldn't produce a really useful product at the end of $x$ number of months.
> 
> So I went back to the committee, and I said, "Gee, I can't do that, **but I have this little method for adapting multilayer perceptrons. It's really pretty trivial.** It's just a by-product of this model of intelligence I developed. And I'd like to do it for my paper for this computer course."
> 
> \[Larry\] Ho's position was, "I understand you had this idea, and we were kind of open-minded. But look, at this point, you've worked in this course for three months, admittedly on something else. I'm sorry, you're just going to have to take an incomplete in the course."
> 
> And I said, "You mean I can't do it?"
> 
> "No, no, you'll have to take an incomplete because, basically, the first thing didn't work. We're very skeptical this new thing is going to work."
> 
> "But look, the mathematics is straightforward."
> 
> "Yeah, yeah, but you know, **we're not convinced it's so straightforward**. You've got to prove some theorems first."
> 
> So they wouldn't let me do it. One of the reasons that is amusing to me is that there are now some people who are saying backprop was invented by Bryson and Ho. They don't realize it was the same Larry Ho, who was on my committee and who said this wasn't going to work.

I am not sure if this is sarcastic or not. It reminds me of the "summer vision project" [@papertSummerVisionProject1966] that expected some undergraduate students to construct "a significant part of a visual system" in a single summer.

> By the time my orals came around, it was clear to me that the nature of reality is a hard problem, that I'd better work on that one later and finish my Ph.D. thesis on something small -- something I can finish by the end of a few years, like a complete mathematical model of human intelligence.

The oral was amusing, and touched on the still-hot issue of [recent human evolution](https://en.wikipedia.org/wiki/Recent_human_evolution).

> ... I made a statement that there might be parameters affecting utility functions in the brain, parameters that vary from person to person. You could actually get a significant amount of adaptation in ten generations' time. I was speculating that maybe the rise and fall of human civilizations, as per Toynbee and Spengler, might correlate with these kinds of things. The political scientist on the committee, [Karl Deutsch](https://en.wikipedia.org/wiki/Karl_Deutsch), raised his hand. ... His book, *The Nerves of Government*, which compares governments to neural networks, is one of the classic, accepted, authoritative books in political science.
> 
> He raised his hand and he said, "Wait a minute, you can't get significant genetic change in ten generations. That cannot be a factor in the rise and fall of civilizations. That's crazy."
> 
> Next to him was a mathematical biologist by the name of [Bossert](https://en.wikipedia.org/wiki/William_H._Bossert), who was one of the world's authorities on population biology. He raised his hand and said, "What do you mean? In our experiments, we get it in seven generations. This guy is understating it. Let me show you the experiments."
> 
> And Deutsch said, "What do you mean, it's common knowledge? All of our political theories are based on the assumption this cannot happen." And Bossert said, "Well, it happens. Here's the data."
> 
> ... I passed the orals having said about two sentences and not having discussed models of intelligence.

It turns out Werbos interpreted backpropagation as the mathematical interpretation of Freudian psychic energy flow. The thesis committee was not amused.

> But the backpropagation was not used to adapt a supervised learning system; it was to translate Freud's ideas into mathematics, to implement a flow of what Freud called "psychic energy" through the system. I translated that into derivative equations, and I had an adaptive critic backpropagated to a critic, the whole thing, in '71 or '72. ... The thesis committee said, "We were skeptical before, but this is just unacceptable ... You have to find a patron. You must find a patron anyway to get a Ph.D. That's the way Ph.D.s work.

The committee gave him three acceptable patrons. He first went to Steve Grossberg.

> he said, 'Well, you're going to have to sit down. Academia is a tough business, and you have to develop a tough stomach to survive in it. I'm sure you can pull through in the end, but you're going to have to do some adaptation. So I want you to sit down and hold your stomach, maybe have an antacid. The bottom line is, this stuff you've done, it's already been done before. Or else it's wrong. I'm not sure which of the two, but I know it's one of the two."

Thanks, Grossberg, for using the law of excluded middle to crush Werbos' dream.

He then went to Marvin Minsky, who gave us some new clues about why backpropagation took so long to discover: "everybody knows a neuron is a 1-0 spike generator"!

> "I've got a way now to adapt multilayer perceptrons, and the key is that they're not Heaviside functions; they are differentiable. And I know that action potentials, nerve spikes, are 1 or 0, as in McCulloch-Pitts neurons, but here in this book that I had for my first course in neurophysiology are some actual tracings. If you look at these tracings in Rosenblith, they show volleys of spikes, and volleys are the unit of analysis. This is an argument for treating this activity as differentiable, at least as piecewise linear. If you look at that, I can show you how to differentiate through it."
> 
> Minsky basically said, "Look, **everybody knows a neuron is a 1-0 spike generator. That is the official model from the biologists**. Now, you and I are not biologists. If you and I come out and say the biologists are wrong, and this thing is not producing 1s and 0s, nobody is going to believe us. It's totally crazy. I can't get involved in anything like this."
> 
> He was probably right, I guess, but he was clearly very worried about his reputation and his credibility in his community.

Out of curiosity, I looked up the "Rosenblith" book [@rosenblithSensoryCommunicationContributions2012], and indeed there were a few tracings that show continuously varying neural activation.

::: {#fig-rosenblith layout-ncol=2}

![Page 146 of the book. *Mechanisms of gustatory and olfactory receptor stimulation*, Figure 2.](figure/no_bg_beidler_2.png)

![Page 366 of the book. *The physiological basis of wavelength discrimination in the eye of the honeybee*, Figure 4.](figure/no_bg_goldsmith_4.png)
:::

Then Minsky dunked on reinforcement learning as well, because he had an unpublished "jitters machine" that failed to optimize its reward. Presumably the name "jitters machine" refers to how it would jitter in place, not able to move towards the goal.

> Minsky also said, "You know, I used to believe in all this kind of stuff with reinforcement learning because I knew reinforcement learning would work. I knew how to implement it. I had a nice guy named Oliver Selfridge who came in and acted as my patron and gave me permission to do it. We coauthored a paper, but it was really my idea, and he was just acting as patron on the Jitters machine. I'll hand you the tech report, which we have deliberately never published."
> 
> It was his bad experience with the Jitters machine that turned him off on reinforcement learning and all the neural net ideas. It just didn't work. I later looked at that paper ... He had a system that was highly multivariate with a single reinforcement signal. The system can't learn efficiently with that. At any rate, he was totally turned off.

The brief description of the unpublished jitters machine was not making sense to me, so I looked around the literature. It was *so* unpublished that I found only two more references in the entire literature [@werbosApplicationsAdvancesNonlinear1982; @werbosBuildingUnderstandingAdaptive1987], both by Werbos. [@werbosBuildingUnderstandingAdaptive1987] described it as:

> There are also some technical reasons to expect better results with the new approach. Almost all of the old perceptron work was based on the McCulloch-Pitts model of the neuron, which was both discontinuous and nondifferentiable. It was felt, in the 1950's, that the output of a brain cell had to be 1 or 0, because the output consisted of sharp discrete "spikes." More recent work in neurology has shown that higher brain cells output "bursts" or "volleys" of spikes, and that the strength of a volley may vary continuously in intensity. This suggests the CLU model to be discussed in the Appendix. In any event, to exploit basic numerical methods, one must use differentiable processing units. Minsky once experimented with a "jitters" machine, which estimated one derivative per time cycle by making brute-force changes in selected variables; however, the methods to be discussed in the Appendix yield derivatives of all variables and parameters in one major time cycle and thereby multiply efficiency in proportion to the number of parameters $(N)$, which may be huge.

This makes things perfectly clear. Minsky's jitters machine was an RL agent with multiple continuously adjustable parameters running a very primitive form of policy gradient. During every episode, it would would estimate $\partial_{\theta_i} R(\theta)$ using finite difference, but since it used finite difference, they could only estimate the partial derivative for *only one* of the parameters $\theta_i$! No wonder it never managed to learn. It is almost comical how much they failed to just use gradient descent. It sometimes feels as if did everything to *avoid* just taking the gradient. In the case of Minsky, he made it very clear, in the new additions to the 1988 version of *Perceptrons*, that he did not believe in gradient descent, period. But what explains the gradient-phobia of all the others...?

Anyway, back to the interview. Werbos went to [Jerome Lettvin](https://en.wikipedia.org/wiki/Jerome_Lettvin), the neuroscientist famous for *What the Frog's Eye Tells the Frog's Brain*. Turns out he was a proto-[eliminativist](https://en.wikipedia.org/wiki/Eliminativism). While I'm an eliminativist too, Werbos was a Freudian, which can only collide badly with eliminativism.

> "Oh yeah, well, you're saying that there's motive and purpose in the human brain." He said, "That 's not a good way to look at brains. I've been telling people, 'You cannot take an anthropomorphic view of the human brain.' In fact, people have screwed up the frog because they're taking bachtriomorphic views of the frog. If you really want to understand the frog, you must learn to be objective and scientific."

Without patrons, he faced the committee again.

> I tried to simplify it. I said, "Look, I'll pull out the backprop part and the multilayer perceptron part." I wrote a paper that was just that - that was, I felt, childishly obvious. I didn't even use a sigmoid \[non-linearity\]. I used piecewise linear. I could really rationalize that to the point where it looked obvious. I handed that to my thesis committee. I had really worked hard to write it up. They said, "Look, this will work, **but this is too trivial** and simple to be worthy of a Harvard Ph.D. thesis."

Oh, now it's too trivial?? I swear the backstory of backpropagation gets more and more ridiculous by the day.

> ... they had discontinued support because they were not interested, so I had no money. ... Not even money to buy food. A generous guy, who was sort of a Seventh Day Adventist and a Harvard Ph.D. candidate in ethnobotany, had a slum apartment that rented for about $40 a month in Roxbury in the ghetto. He let me share a room in his suite, in his suite with the plaster falling off, and didn't ask for rent in advance. I had no money at that time for food. There was a period of three months when I was living there in the slums. To conserve the little bit of money I had, I remember eating soybean soup and chicken neck soup. I remember getting the shakes from inadequate nutrition.
> 
> Finally, they said, "Look, you know, we're not going to allow this." There was this meeting where we sat around the table. The chairman of the applied math department at that time was a numerical analyst D. G. M. Anderson. He said, "We can't even allow you to stay as a student unless you do something. You've got to come up with a thesis, and it can't be in this area."

Karl Deutsch, who believed in Werbos, sponsored his PhD thesis on a "respectable" problem: fitting an ARMA model to a time-series data of political regimes, and use it to forecast nationalism and political assimilation. [Box–Jenkins method](https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method) ran too slowly, so Werbos programmed in the backpropagation which worked, and he finally obtained his PhD in 1974. This is the original reference point for modern neural network backpropagation algorithm.

> Deutsch said, "You 're saying we need an application to believe this stuff? I have an application that we could believe. I have apolitical forecasting problem. I have this model, this theory of nationalism and social communications? What causes war and peace between nations? I have used up ten graduate students who've tried to implement this model on real-world data I've collected, and they've never been able to make it work. Now, do you think your model of intelligence could solve this problem and help us predict war and peace?"
>
> The first application of backpropagation in the world in a generalized sense was a command that was put into the [TSP](https://en.wikipedia.org/wiki/TSP_(econometrics_software)) at MIT , available to the whole MIT community as part of their standard software. It was published as part of MIT 's report to the DOD [the Department of Defense] and part of the DOD's report to the world . It was part of the computer manual, so the first publication of backpropagation was in a computer manual from MIT for a working command for people to use in statistical analysis. I was a second author of that manual. It was Brode, Werbos, and Dunn.
>
> ... Once I started doing software for them, they discovered I was pretty good at it. Once I had put backpropagation into a TSP command, they offered me a full-time job at the Harvard Cambridge Project, so I did the Ph.D. thesis while having a full-time job. While I was doing these things at MIT, I remember going to one party. ... one of the people there said, 'We have heard through the grapevine that somebody has developed this thing called continuous feedback that allows you to calculate all the derivatives in a single pass."

But the saga is not over. after the Ph.D., Werbos was hired by the DoD, and promptly got sucked into several more years of misadventure, which meant that he could not even talk about backpropagation in a public journal until 1978 -- and the algorithm got removed anyway due to page limits. This annoyed the DoD and he had to move to the Department of Energy.

> I found out that DARPA had spent a lot of effort building a worldwide conflict forecasting model for the Joint Chiefs of Staff that was used in the long-range strategy division of the Joint Chiefs. ... I wound up sending a couple of graduate students to create a really good database of Latin America. I said, "You want variance, high variance. Something hard to predict." I thought conflict in Latin America would be the most beautiful case. I figured there were enough cultural homogeneities that it would be a single stochastic process, but with lots and lots of variance in that process. So we got truly annual data going back, I don't know, twenty or thirty years for all the countries in Latin America and then reestimated the Joint Chief's model on it. It had an r2 of about .0016 at forecasting conflict. By jiggling and jiggling we could raise it to about .05. It could predict GNP and economic things decently. Conflict prediction we couldn't improve, though; it was hopeless.
> 
> DARPA wasn't happy when I published that result. They wanted me to do something real and practical and useful for the United States. This was useful, but it was an exposé. They didn't like that. Therefore, the report, which included backpropagation in detail and other advanced methods, was not even entered into DOCSUB. Every time I tried to enter it into DOCSUB, somebody jiggled influence to say, "No, no, no, we can't publish this. This is too hot."
> 
> It was published in [@werbosEmpiricalTestNew1978] anyway because they couldn't block the journals, but it didn't include the appendices. So that paper in 1978 said, "We've got this great thing called dynamic feedback, which lets you estimate these things. It calculates derivatives in a single swoop, and you can use it for lots of things, like AI." ... **the appendix on how to do it was not there because of page limits** ... At that point, DARPA was no longer happy.

So he went to the Department of Energy and used backpropagation to make another model, and managed to get silenced again, unable to publish that report until 1988 [@werbosGeneralizationBackpropagationApplication1988].

> They had a million-dollar contract at Oak Ridge National Laboratory to study that model. They wanted me for several things. One, they wanted me to be a translator between engineering and economics. Two, they wanted a critique. They wanted exposés. They wanted me to rip apart the models of the Department of Energy in a very scientific, objective way that didn't look like I was trying to rip them apart but was anyway. That's exactly what they wanted to hire me for, and I didn't really know that was the motive. These particular people didn't like modeling very much.
> 
> So at some point, they wanted sensitivity analysis. And I said, "You know, I know a little bit about calculating derivatives." ... I applied it to a natural gas model at the Department of Energy. In the course of applying it, I did indeed discover dirt. They didn't want me to publish it because it was too politically sensitive. It was a real-world application, but the problem was that it was too real. At DOE, you know, you don't have First Amendment rights. That's one of the terrible things somebody's got to fix in this country. The reality of the First Amendment has deteriorated. Nobody's breaking the law, but the spirit of the First Amendment has decayed too far for science. At any rate, they finally gave me permission to publish it around '86 and '87. I sent it to the journal Neural Nets - that is, how you do simultaneous recurrent nets and time-lag recurrent nets together. Then the editorial process messed around with it, made the paper perhaps worse, and it finally got published in '88, which makes me very sad because now I gotta worry about, 'Well, gee, didn't Pineda do this in '88?

As a side note, one might have felt that Werbos' "turning Freud into mathematics" seem rather strange. This feeling is completely justified. I found a recent paper by him [@werbosIntelligenceBrainTheory2009] with this crackpot illustration:

![The text at the end of the arrow says "quantum and collective intelligence (Jung, Dao, Atman...)?"](figure/werbos_2009.jpg)
