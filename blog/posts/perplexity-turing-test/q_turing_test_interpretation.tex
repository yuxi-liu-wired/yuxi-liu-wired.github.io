\qns{Interpreting Log-likelihood Loss via the Turing Test (Optional)}
\qcontributor{Yuxi Liu}

\textit{Alternative title: How much would it cost to train the first AI scientist?}

This question is based on \footnote{Barnett, Matthew, and Tamay Besiroglu. "Scaling transformative autoregressive models." (2023).}

\begin{quote}
    The Direct Approach framework bounds the compute requirements for transformative AI by extrapolating neural scaling laws. We combine those estimates with simple models of future progress in algorithms, investment, and compute costs to produce a user-adjustable forecast over the date at which TAI will be achieved.
\end{quote}

We first set up the Turing test as a statistical decision problem.

The two players are a human (or more accurately, a natural English corpus), and a machine (autoregressive language model). The judge is a Bayes classifier. 

The judge considers two hypotheses $H_0$ (human) and $H_1$ (machine). It starts by considering two hypotheses equally: $P(H_0) = P(H_1)$. It then samples an endless stream of tokens $X_{1:\infty}$ one by one from the unknown player, performing updates on its posterior, until it finally reaches a high enough posterior odds, at which point it stops sampling and makes a decision:

$$
\text{If  }\ln \frac{P(H_0|X_{1:n})}{P(H_1|X_{1:n})} > \ln 10 \text{ then decide }H_0
$$
$$
\text{If  }\ln \frac{P(H_0|X_{1:n})}{P(H_1|X_{1:n})} < - \ln 10 \text{ then decide }H_1
$$

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

By Bayes theorem, the posterior probability ratio is
$$\ln \frac{P(H_0|X_{1:n})}{P(H_1|X_{1:n})} = \ln \frac{P(H_0)}{P(H_1)}  + \ln\frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)}$$
which suggests that we should use a \href{https://en.wikipedia.org/wiki/Sequential_probability_ratio_test}{sequential probability ratio test}: make a decision as soon as $\ln\frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)}$ exceeds a lower or higher bound.

Since $\ln \frac{P(H_0)}{P(H_1)} = \ln 1 = 0$, we get the this decision rule based on sequential probability ratio:
$$
\text{If  }\ln \frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)} > \ln 10 \text{ then decide }H_0
$$
$$
\text{If  }\ln \frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)} < - \ln 10 \text{ then decide }H_1
$$

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Now we need to connect the ratio test with loglikelihood loss. We make an assumptions to keep the mathematics tractable: We assume that the human and the computer are \textbf{fairly indistinguishable}, such that it takes a long time (at least 50 tokens, say) for the judge to unmask them.

Suppose the judge is really facing a human, how long does it take for us to unmask it? It would take about time $T_0^*$. How long is time $T_0^*$? About as long as it takes for the probability ratio to hit $+\ln 10$.
$$E_{X \sim P(\cdot | H_0)}\left[ \ln\frac{P(X_{1:T_0^*}|H_0)}{P(X_{1:T_0^*}|H_1)}\right] \approx \ln 10$$
And similarly, suppose the judge is really facing a computer, it would take $T_1^*$ to unmask it, where
$$E_{X \sim P(\cdot | H_1)}\left[ \ln\frac{P(X_{1:T_1^*}|H_0)}{P(X_{1:T_1^*}|H_1)}\right] \approx -\ln 10$$

Now, the above two expectations are not exactly equal, because one is an expectation over tokens generated by a human, and the other is an expectation over tokens generated by a computer. However, if the computer is fairly good, then the two expectations are taken over roughly the same distribution: $P(\cdot | H_0) \approx P(\cdot |H_1)$, and so $T_1^* \approx T_0^*$. This means that it takes roughly the same time to unmask a computer and a human.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Suppose the judge is facing a human, then its log-raio evolves $E_{X \sim P(\cdot | H_0)}\left[ \ln\frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)}\right]$ which 

$$\frac 1n E_{X \sim P(\cdot | H_0)}\left[ \ln\frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)}\right] = \frac 1n 
 E_{X \sim P(\cdot | H_0)}\left[ \ln\frac{1}{P(X_{1:n}|H_1)}\right] - \frac 1n  E_{X \sim P(\cdot | H_0)}\left[ \frac{1}{\ln P(X_{1:n}|H_0)}\right]$$

So we have rewritten the computer's negative log-likelihood loss per token $L$ as:
$$L = L_\infty + \frac 1n D_{KL}(P(\cdot | H_0)\| P(\cdot | H_1)) $$

The lower $L$ gets, the more $\frac 1n D_{KL}(P(\cdot | H_0)\| P(\cdot | H_1))$ approaches zero, until at the limit, the computer is a perfect replica of the human, and $L$ reaches the theoretical minimum of $L_\infty$. Why not further? Well, the human has some intrinsic randomness. For the computer to perfectly replicate the human, the computer must also have the exact same entropy rate as the human.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

To recap, we have derived
$$E_{X \sim P(\cdot | H_0)}\left[ \ln\frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)}\right] \approx n(L-L_\infty )$$
$$
E_{X \sim P(\cdot | H_0)}\left[ \ln\frac{P(X_{1:T^*}|H_0)}{P(X_{1:T^*}|H_1)}\right] \approx \ln 10
$$

Combining these two results, we obtain
$$T^* \approx \frac{\ln 10}{L - L_\infty}$$

% Interpretation: Each token on average moves the log-probability-ratio away from 0 by another $(L-L_\infty)$. Decision is triggered when it finally moves out of the interval $[-\ln 10, \ln 10]$.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}


\begin{enumerate}
    \qitem We need the following equality:
    
    $$\frac 1n E_{X \sim P(\cdot | H_0)}\left[ \ln\frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)}\right] = \frac 1n 
    E_{X \sim P(\cdot | H_0)}\left[ \ln\frac{1}{P(X_{1:n}|H_1)}\right] - \frac 1n  E_{X \sim P(\cdot | H_0)}\left[ \frac{1}{\ln P(X_{1:n}|H_0)}\right]$$

    \textbf{Explain what each of the three terms of the equality mean. Suppose the machine is a perfect replica of the human, what would each term be?}

    \sol{
    $$\underbrace{\frac 1n E_{X \sim P(\cdot | H_0)}\left[ \ln\frac{P(X_{1:n}|H_0)}{P(X_{1:n}|H_1)}\right]}_{\frac 1n D_{KL}(P(\cdot | H_0)\| P(\cdot | H_1)) } = \underbrace{\frac 1n 
 E_{X \sim P(\cdot | H_0)}\left[ \ln\frac{1}{P(X_{1:n}|H_1)}\right]}_{\text{negative log-likelihood loss per token}} - \underbrace{\frac 1n  E_{X \sim P(\cdot | H_0)}\left[ \frac{1}{\ln P(X_{1:n}|H_0)}\right]}_{\text{entropy rate of the human itself}}$$

    The first term is the KL-divergence between the machine and the human, as two sources of symbolic strings, averaged over sequence length. Roughly speaking, it is how different they are, per symbol emitted. It is an information-theoretic quantity.
    
    The second term is negative log-likelihood loss per token. This is what the language model is trained to minimize.

    The third term is the entropy rate of the human. It is how random the human is, as a source of symbols. 

    If the machine is a perfect replica of the human, then the second term is zero, and the first term equals the third term.
    }

    \qitem 
    $$T^* \approx \frac{\ln 10}{L - L_\infty}$$
    
    \textbf{Interpret this formula.}    

    \sol{Interpretation: Each token \textit{on average} moves the log-probability-ratio away from 0 by another $(L-L_\infty)$. Decision is triggered when it finally moves out of the interval $[-\ln 10, \ln 10]$.
    
    We are not able to simply look at a few tokens, draw a straight line, and call it a day, because the trajectory of log-probability-ratio is much closer to a random walk with drift. Subjectively, if you were a judge and watching the log-probability-ratio moving, you'd see ups and downs, keeping you in suspense, until it finally crosses the decision boundaries.}

    \qitem (\textbf{Slowdown factor}) Humans are not perfect judges. However, as a crude approximation, we can model them as a slowed-down version of the perfect judge. Specifically, if it takes about $T$ tokens for the perfect judge to reach a likelihood ratio of $r$, it would take about $sT$ tokens for a human judge. Here, $s$ is a number larger than one.

    We do not have good data on what $s$ is, or whether it is even consistently measurable. However, for our current question, let's assume $s=10$.

    \textbf{What should the language model's $L-L_\infty$ be, before it can pass the Turing test against a human judge for 1000 tokens?}

    \sol{
        $$10 \times \ln 10 / 1000 = 0.023$$
    }


    \qitem (\textbf{Estimating entropy of English}) We found that $L_\infty$ should be interpreted as the intrinsic entropy of the source material. In this case, it is the entropy of natural English. Now, the intrinsic entropy of English is not very easy to estimate, but there had been several attempts.

    The earliest attempt is by Shannon himself, in 1951.\footnote{Shannon, Claude E. "Prediction and entropy of printed English." Bell system technical journal 30.1 (1951): 50-64.} He estimated that the entropy of English is about 0.6 -- 1.3 bits per character. Now, we cannot use this number directly, because it is not in the right units -- logliklihood loss is in units of nat/token.

    The conversion between nat and bit is known exactly: $1 \;\mathrm{nat} = \ln(2)\;\mathrm{nat}= 0.693\;\mathrm{nat}$. The conversion between character and token can be estimated by running a tokenizer over a large natural English corpus. I have estimated this by running the GPT-2 tokenizer on WikiText2 corpus, and found that on average, 0.22 tokens/character, 1.17 tokens/word.

    \textbf{What is Shannon's estimated entropy of English, in units of nat/token?}

    \sol{
    $$\ln 2 \times [0.6, 1.3] / 0.22 = [1.89, 4.09]\;\mathrm{nat/token}$$
    }

    Another way to estimate is by attempting to compress a large natural English corpus. It is known in basic information theory that the entropy rate is the lower bound on compression rate. In more detail, if you have a source of information emitting symbols, and its symbol stream has an entropy rate of $x \;\mathrm{bit/symbol}$, then it takes $xl$ bits to encode a long segment $l$ symbols long.
    
    The \href{http://prize.hutter1.net/}{Hutter prize} is a competition for compressing a $10^9$-byte segment of the English Wikipedia as much as possible. The competition has been ongoing since 2005. 

    The zipped file is only about 300 Mb in size, meaning that the total entropy in the corpus is no more than $3\times 10^8$ bytes.
    
    Over the years, the progress has been slow but somewhat steady. If we extrapolate the prize-winning entries over the years, we see that the best possible compression ratio seems to be about 10x.
    
    I ran the GPT-2 tokenizer through 1/100 of the dataset, and got 2.96 million tokens. So the entire dataset should have about 300 million tokens. Assuming this, \textbf{what is the entropy of English, in units of nat/token?}

    \sol{
    If we assume the maximal compression ratio of 10x, then the corpus contains entropy 
    $$10^8\;\mathrm{byte} = 8\times 10^8 \;\mathrm{bit} = 5.55\times 10^8 \;\mathrm{nat}$$

    $$\frac{5.55\times 10^8}{3\times 10^8}= 1.85 \;\mathrm{nat/token}$$
    }

    And the third way is to look at what the scaling laws for the largest language models imply what $L_\infty$ is. According to page 25 of "Training Compute-Optimal Large Language Models" (2022) (hereafter "Chinchilla scaling law"), $L_\infty = 1.69\;\mathrm{nat/token}$.

    \textbf{Comment on the agreement between these three numbers.}

    \sol{This is fairly open-ended. If you say something reasonable, that would be fine.
    
    The estimate by compression and language modelling are remarkably close. 
    
    Shannon's estimate of entropy is above the other two estimates by about 2x. Considering that he measured this by asking volunteers to just look at a small passage of text and keep guessing the next character, we can consider his estimate a noisy estimate biased to be too high (since the volunteers would not be able to extract the very last predictabilities of English by only looking at a small preceding passage).}

    \qitem (\textbf{Compute scaling law})

    The "Chinchilla scaling law" paper reported a series of training runs on language models, trained by Google DeepMind researchers. According to them, if we have a fixed amount of computing budget $C$ (in units of FLOP), by choosing the model and dataset size correctly, the minimal reducible loss achievable is

    $$L - L_\infty = \frac{1070}{C^{0.154}}$$

    Assuming slowdown factor $s=10$, and Chinchilla scaling law, then, we have a direct method to predict how long a language model can pass a Turing test, according to how many FLOPs it cost to compute:
    
    $$T^* \sim \frac{10\ln 10}{1070}C^{0.154}$$

    This gives, as a rule of thumb, 100x compute means 2x length of Turing test.

    If GPT-4 costs 2e25 FLOP in compute,\footnote{Best estimate, as OpenAI is famously secretive.} \textbf{for how many words can it pass the Turing test?} Assume 1 word is 1.2 tokens, as described previously.

    \sol{
    $$T^* \approx 170 \text{ tokens} \approx 150 \text{ words}$$
    meaning it has a good chance of passing the Turing test if limited to only 150 words. For context, the \textit{Attention is All You Need} paper has an abstract that's 200 tokens long. 
    }

    \qitem  A typical scientific paper is about 4000 words long. \textbf{How much training compute would it cost, in units of of GPT-4? Assuming GPT-4 cost 10 million USD to train, how much money would this model cost to train?}
    
    \sol{4000 words is 27x more than 150 words, so it would need $27^{1/0.153} = 2e9$ amount of compute. Assuming 1 GPT-4 cost 10 million USD, that would cost 2e16 USD, or 200 years of global GDP (2023).
    
    So it seems like it would take a very long time to train the first AI scientist by just scaling up the GPT-like models. We need more than pure language modelling.
    }

\end{enumerate}

I encourage you to play with the interactive website \href{https://epochai.org/blog/direct-approach-interactive-model}{Direct Approach Interactive Model - Epoch AI} to explore a mathematical model behind this assignment question.

I leave you with the inspirational quote from Edward Teller\footnote{Teller, Edward, and Allen Brown. "The legacy of Hiroshima." (1962), page 211.}:

\begin{quote}
    The possibilities of developing an atomic weapon and the desirability of doing it secretly were discussed at a Princeton University conference in which I participated in March 1939... Bohr said this rare variety could not be separated from common uranium except by turning the country into a gigantic factory. Bohr was worried that this could be done and that an atomic bomb could be developed—but he hoped that neither could be accomplished. Years later, when Bohr came to Los Alamos, I was prepared to say, "You see..." But before I could open my mouth, he said: “You see, I told you it couldn't be done without turning the whole country into a factory. You have done just that.”
\end{quote}
