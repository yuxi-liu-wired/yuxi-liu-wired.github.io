---
title: "The Cyc Project"
author: "Yuxi Liu"
date: "2024-12-23"
date-modified: "2024-12-23"
categories: [AI, scaling, history]
format:
  html:
    toc: true
description: "Monument to the greatest dream of logical AGI."

# image: "figure/banner.png"
status: "wip"
confidence: "likely"
importance: 5
---

{{< include ../../../utils/blog_utils/_macros.tex >}}

## Abstract



## In lieu of an introduction

Many years later, surrounded by the humming servers of the knowledge base, Douglas Lenat was to remember that distant afternoon when he taught Cyc that everyone can only see their own dream.

## Automated Mathematician

Lenat's first claim to fame was his PhD thesis topic, the Automated Mathematician (AM) [@lenatAMArtificialIntelligence1976], which became a minor legend. It occurred in the context of many "automated discovery" systems in the 1970s. Neural networks and other "self-organized" machine learning methods have mostly [died out in the 1960s](https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/), but machine learning did not die, and indeed, was going through its first spring, under the name "automated discovery".

Nowadays, when we think of "machine learning", we think of random forests, neural networks, and such. But back then, machine learning was understood as "logical AI gone meta". Simon and Newell, the two giants of logical AI, showed the way: logical AI is heuristic search. 

* To play chess, one simply write a program that does alpha-beta search over the game tree, guided by heuristics of piece-worth, mobility, and such.
* To solve planar geometry problems, one simply write a program that construct a path in the space of reasons, until one connects the axioms with the conclusions.
* To solve problems of type X, one simply write a program that can move through the space of possible solutions to X, and code in the heuristic rules, its north star, so that it will not be lost in the space of solutions.

Well, chess is a game, and so is Euclidean geometry, symbolic integration, and... perhaps scientific discovery itself? The famed scientific method, to deserve the name "method", ought to be one more problem for logical AI to solve.

During 1977--1983, Pat Langley wrote a series of programs named BACON, after [Francis Bacon](https://en.wikipedia.org/wiki/Francis_Bacon), a father of scientific empiricism. It purported to discover scientific laws from mere data. For example, when given a table of the moles $N$, pressures $P$, volumes $V$, and temperatures $T$ of gas samples, BACON would first try out simple equations involving two of the terms. It would discover that the data for $PV$ appears more "clusterly" than either $P$ or $V$, so it makes a new quantity $PV$, and add that to the table. It then repeat this process, discovering that $PV/T$ is an interesting quantity, and finally discovers that $PV/NT$ is a *constant* quantity -- the ideal gas law discovered! [@langleyDataDrivenDiscoveryPhysical1981]

In the logical AI framework, the problem space of BACON is the space of all elementary functions involving the table columns, and a *solution* is a (nontrivial) function that results in a nearly constant column. At each step, the program tries out simple combinations of the current columns, and heuristically pick the one most nearly constant.

As another example, the famed Meta-Dendral was a further development of Dendral. With Dendral, the problem space is the space of possible molecular cleavages and atomic transfers, and the heuristics are the chemical rules for plausible and implausible cleavages and transfers. For example, it is plausible for a protein to be cleaved at the `-CO-*-NH-` peptide bond, but implausible to be cleaved at the double bond between `C` and `O`. The goal is, given molecular spectroscopic data for a single molecule, construct a molecular structure and a sequence of cleavages and transfers, such that it would produce the data. 

With Meta-Dendral, the problem space becomes the space of possible chemical rules, and the goal is to find plausible rules (plausible according to heuristic meta-rules) that can explain a large collection of molecular structures and their spectroscopic data. Meta-Dendral proved useful for working chemists by discovering some cleavage rules for a family of [steroids](https://en.wikipedia.org/wiki/Steroid). For details, see my essay on [expert systems](TODO).

In general, such a system begins with some simple rules that allow the system to a low score according to some criteria. and as it runs, it builds, prunes, and modifies the rules, so that in the end, its rule set allows it to achieve a high score. The following tabulates a few [@walkerHowFeasibleAutomated1987]:

| System Name | Date | Task | Data | Rules | Discovery Method |
|---|---|---|---|---|---|
| Meta-Dendral | 1976 | Discover molecular cleavage and transfer rules for mass spectrometry | Molecular structures and their spectra | Molecular cleavage and transfer rules | Use meta-heuristic rules to generate possible rules, and test on data |
| Bacon | 1977--1983 | Discover physical laws | Numeric data from experiments | Elementary functions | Symbolic regression |
| RX | 1982 | Discover drug side effects and interactions | Patient information database | Drug effect and interaction rules | Symbolic regression with time-lag |

Within this context, AM is different. It is still a logical AI with a problem space and a heuristic search. However, there is no data: It is mostly "self-play". Lenat would begin by entering 115 concepts in set theory and ~300 heuristic rules, and AM would just keep running, discovering more and more constructions are uninteresting, but a few would be interesting, and these interesting constructions would be stored, allowing further constructions upon them. It was a minor legend, since it reportedly rediscovered many concepts, such as natural numbers, prime numbers, and the Goldbach conjecture.

A **concept** is essentially a [frame](https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)), which are essentially objects in [object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming). A concept has 25 possible **facets**, which are "slots", or "data fields". The following is an example concept with many facets populated:

- NAME: Prime Numbers
- DEFINITIONS:
    - ORIGIN: Number-of-divisors-of(x) = 2
    - PREDICATE-CALCULUS: Prime(x):= $(\forall z)(z | x \to z = 1 \wedge z = x)$
    - ITERATIVE: (for x>1): For i from 2 to $\sqrt x,\neg(i | x)$
- EXAMPLES: 2,3,5,7,11,13,17
    - BOUNDARY: 2, 3
    - BOUNDARY-FAILURES: 0,1
    - FAILURES: 12
- GENERALIZATIONS: Nos., Nos. with an even number of divisors, Nos. with a prime number of divisors.
- SPECIALIZATIONS: Odd Primes, Prime Pairs, Prime Uniquely-addables
- CONJECTURES: Unique factorization, Goldbach's conjecture, Extremes of Number-of-divisors-of
- INTUITIONS: A metaphor to the effect that Primes are the building blocks of all numbers
- ANALOGIES:
    - Maximally-divisible numbers are converse extremes of Number-of-divisors-of
    - Factor a non-simple group into simple groups.
- INTEREST: Conjectures tying Primes to TIMES, to Divisors-of, to closely related operations
- WORTH: 800

![The network of concepts at the beginning of AM. `|` means "is a". `|||` means "is an example of". [@lenatAMArtificialIntelligence1976, page 106]](figure/AM_ontology.png)

AM has an **agenda**, a list of **tasks** along with their **reasons**. Each task is of form "Do operation A to facet F of concept C". A task's **worth** is the sum of its reasons' worths. AM always performs the task with the highest worth.

To perform a task, AM looks for **heuristic rules** whose conditions are (mostly) satisfied, and whose worth is (pretty) high. Each heuristic rule is of form "if `<condition>`, then run `<actions>`". Each action has 3 kinds of possible effects:

- Add a new task to agenda.
- Create a new concept.
- Add or delete an entry to a facet of a concept.

Some example heuristic rules:

- If the task is to fill in examples of X, and X is a special case of Y, then for each example of Y, check if it a definition of X. If so, then add it to the list of examples of X.
- If some but not most examples of X are also examples of Y, then create a new concept "X and Y".
- If very few examples of X are found, then add the following task to the agenda: "Generalize the concept X", for the following reason: "X are quite rare; a slightly less restrictive concept might be more interesting".

At this point, the careful reader would notice several problems:

How does AM know that the concept should be called "Prime Numbers"? Ah, that's because Lenat would regularly interrupt and inspect AM, and if Lenat notices that AM has rediscovered, say, prime numbers, he would rename that from something like `concept-421` to `prime-numbers`.

To discover prime numbers, AM must have a way to check if a Lisp object is a prime number or not. That is, the definition must also be a program. Ah, the famous [homoiconicity](https://en.wikipedia.org/wiki/Homoiconicity) of Lisp came to the rescue! A definition, as stored within a facet of a concept, is a data, but for Lisp, data is program, and program data. Consequently, AM can run a subroutine that enumerates possible programs *as data*, and for each, interpret it *as program*, until AM hits upon a program that works (or times out). 

How does it check that two definitions actually define the same thing? In general, this is impossible by [Rice's theorem](https://en.wikipedia.org/wiki/Rice's_theorem), so Lenat must have used some heuristic rules. I looked, but can't find Lenat explaining this anywhere. It seemed like a trick of the hand.

If we look carefully at the example heuristic rules, then something odd happens. The heuristic rule "If some but not most examples of X are also examples of Y, then..." just talks about X and Y, but what is the context? Compare this with "If the task is to fill in examples of X, and X is a special case of Y, then...", where X is specified by the context "the task is to fill in examples of X". Now, we might be charitable and assume Lenat made a slip. However, there are even vaguer rules: "Formulate a parameterized conjecture, a 'template', which gets slowly specialized or instantiated into a definite conjecture".

These problems were particularly acute, as the achievements of AM was impressive enough to attract surprise and controversy [@haaseDiscoverySystemsAM1987], and anecdotes suggested that the mathematician AI was here:

> On one memorable occasion, one of my advisors, George Polya, was looking at its results, and remarked “That reminds me of something a student of a friend of mine once did.” He rummaged through an old trunk, found the relevant correspondence, and it turned out that his friend was G. H. Hardy, and the student was Srinivasa Ramanujan! Even though that regularity (involving highly composite numbers) has no practical significance, Polya and I were happy to see AM behaving much like the young self-taught Indian genius had, in his explorations in search for interesting regularities.
>
> [@lenatCreating30MillionRuleSystem2022]

The issue came to a head with the publication of [@ritchieAMCaseStudy1984], which argued that AM was badly documented. The control structure probably was not "simply pick the task with the highest worth", but more complicated. Some crucial *quantitative* heuristic rules were probably hidden behind vague *qualitative* words like "A nonconstructive existence conjecture is interesting". The source code was unpublished and not available for other researchers for reproduction.

In short, because of the various issues, AM was a thing that happened, but not a thing that could be built upon. One cannot build upon it directly in source code, since it's unavailable. One cannot build upon it by reimplementing the pseudocode, since the rules were vaguely specified, and the control structure was probably wrong. One cannot build upon it by reimplementing the high-level ideas, since it's unclear which part, out of the several dozen tightly integrated parts of AM, was responsible for AM's good outputs, and which were just implementation details. (In our language, there was no ablation study.)

simplifying large pieceLenat quickly replied with [@lenatWhyAmEurisko1984]. He dismissed the criticism as mostly miscommunication, and went on to describe the *real* lesson of AM. If I were to be dab, Lenat was saying that it is fine for AM's source,[^am-source-code] or even pseudocode, to be unavailable, because Lenat had learned the lessons, and you, dear reader, need only listen to the lessons from him.

> The AM thesis never explained, precisely, how concepts such as 'not very often' and 'related to' were implemented. By and large, these omissions were due to the fact that the code Lenat wrote for these predicates was quite trivial... inevitable, yet regrettable process of s of code, translating them to brief English phrases. This process left out many exceptional cases, and made the English condensations less accurate... Some problems that Ritchie and Hanna cite... are simply errors of mis-reading what was stated in the thesis or articles... A few of the problems raised in Ritchie and Hanna's article are, annoyingly, genuine inconsistencies in the thesis document, such as whether or not facets had subfacets. These reflect the fact that AM was a running and evolving program, changing daily in small ways even as the thesis document was being written... the changes in representation were driven simply by AM'S running out of list space in 1975 [INTERLISP](https://en.wikipedia.org/wiki/Interlisp) code; we were forced to shift representations time and time again just to gain a few hundred precious list cells.
>
> [@lenatWhyAmEurisko1984]

[^am-source-code]: Though Lenat admitted that "the code ought to have been provided", he would never publish the code, not with AM, nor with Eurisko. He had often claimed it had been long lost, yet the source code for AM and Eurisko had [recently been found](https://white-flame.com/am-eurisko.html), right where it should be -- the [DBL folder in the Stanford AI Laboratory backup data](https://www.saildart.org/DBL). It could only be found because Lenat had died, thus releasing the password protection on the folder. I wonder if Lenat had lied, and simply wished to protect his source code.

And what are the lessons?

First lesson: AM exhausts itself. Roughly speaking, each new interesting discovery depends on ~24 heuristics, and each heuristic has a hand in ~24 discoveries. Therefore, with ~N heuristic rules, there would be ~N interesting discoveries. Because AM cannot discover heuristic rules, with ~300 starting heuristic rules, it would run out of interesting discoveries and "die of boredom". Lenat could help AM by adding new heuristics, and AM would make some new discoveries, but this never lasted.

> Eventually, AM acquired an uncommon ailment for a computing system: intellectual exhaustion. Having explored the esoteric reaches of mathematics, AM suddenly downshifted into a preoccupation with rudimentary arithmetic. Finally, with the remark, “Warning! No task on the agenda has priority over 200,” the system virtually expired, as though from boredom.
>
> [@hiltzikBirthThinkingMachine2001]

Second lesson: Representation matters a lot. AM works so well, because AM uses Lisp code as data. Lisp is the perfect tool if you want to search over the space of interesting mathematical functions. You can modify a Lisp expression, and get a different mathematical function that is possibly interesting. In contrast, if you modify an assembly code, you'd most likely end up with nonsense. Indeed, Lenat found that he could not extend AM to "go meta" and discover new heuristics, because Lisp is good for math, not "heuretics" (the study of heuristics). Modifying a Lisp expression for a heuristic most likely ends up with nonsense, much like assembly code.

> It was only because of the intimate relationship between LISP and Mathematics that the mutation operators (loop unwinding, recursion elimination, composition, argument elimination, function substitution, etc.) turned out to yield a WHY AM AND EURISKO APPEAR TO WORK 273 high 'hit rate' of viable, useful new math concepts when applied to previously- known, useful math concepts--concepts represented as LiSP functions. But no such deep relationship existed between LISP and Heuretics, and when the basic automatic programming (mutations) operators were applied to viable, useful heuristics, they almost always produced useless (often worse than useless) new heuristic rules.
>
> ...
> 
> We did not perceive until writing this paper that the way in which Similar-To, Not-Often, Notice-Regularity, and scores of other 'primitives' were coded do themselves embody a large amount of heuristic knowledge. We exploited the structure of (or, if you prefer, partially encoded) the domain of elementary mathematics, in the process of making trivial yet adequate usP versions of those extremely complex and subtle notions (such as similarity of concepts).
>
> [@lenatWhyAmEurisko1984]

Third lesson: There is no free lunch. Intelligence is a lot of work. You need to put in the right representational language for reasoning and discovery -- not just Lisp, but a lot more. You need to put in a lot of loosely organized, kind of correct heuristic rules -- not just a few eternal truths of discovery. You need to put in a lot of facts. You need to interact with the program, to help it along and to be helped along, not just to sit and watch.

Fourth lesson: Intelligence is messy. Some might wish for a neat unitary framework for intelligence, and point at the messiness of AM's ~300 heuristic rules as a failing, but we should take it as strength and embrace the messiness.

> The apparent adhoc-ness in both the heuristics' content themselves, and in the control knowledge guiding the application of those heuristics, is clearly the source of many methodological objections to the work. But we believe that this adhocracy -- indeed, adhocracy controlling adhocracy -- may be the source of EURISKO'S underlying potential especially as a model for cognition.
>
> [@lenatWhyAmEurisko1984]

How do we know that Lenat learned the lessons?

## Eurisko

Though 1981 is still near, Eurisko is already shrouded in a reverential mystery like legends do, among the Deep Blue, the Samuel Checkers Player, and the apprentice's broom. A symbol, a moral archetype. A program that discovered loopholes in a sci-fi ship-building tournament, allowing its creator to win twice in a row using fleets so unaesthetic that the people running the tournament threatened to stop the tournaments if Eurisko would enter and win a 3th one, so it retired as honorary Admiral, undefeated.

Brilliant, dramatic, but what do we glean from it, other than a moral play about the power of unconventional thinking and [seeing through](https://gwern.net/unseeing)? Quite a lot.

Lenat wrote Eurisko to answer problems AM raised. Specifically, he wanted to see if Eurisko can avoid getting exhausted if it *could* discover heuristics. If AM exhausted itself because it has used up the worth of its heuristics (first lesson), then why not let the computer discover more heuristics? Since AM could not efficiently search over heuristic rules using Lisp (second lesson), Lenat designed a new language called RLL, in which heuristic rules are easy to search.

In short, Lenat solved the problems from the first two lessons, and learned the next two lessons. It turns out that Eurisko *did* exhaust itself eventually after all. Self-discovery of heuristic rules eventually ran out, because self-discovery of heuristic rules relies on heuristic rules about heuristic rules (meta-heuristic rules), and *those* rules run out of steam after a dozen or so uses. Lenat concluded that there really is no way to get a working automated discovery program without doing the hard work of hand-coding in a lot of common sense. 

Why would common sense help? Lenat observed how humans don't seem to get stuck like Eurisko. He concluded that humans don't run out of steam because they have a vast store of common sense knowledge about the world, from which they can draw upon for analogies, those far-flung flights of fancies that give us genuinely new ideas endlessly. For example, we can analogize the military with the medical, so that a doctor can "fighting an infection by an encircling movement with antibiotics". This is the third lesson.

Why would analogies give genuinely new ideas? Well, intelligence is messy! If everything is so uniform, then there is no way to make a far-flung analogy -- everything is pretty much the same already. Besides, just look at all the broken dreams of logical AI -- their corpses tell us that no elegant theory of intelligence exists. This is the fourth lesson.

Those are the lessons Lenat drew, but what did Eurisko really do?

As one expect from a program reasoning about and making its own rules, it found meta-bugs. The simplest example was one that kept triggering itself, creating an infinite loop. [@johnsonMachineryMindNew1986] Others were more amusing.

> One of the first heuristics that EURISKO synthesized (H59) quickly attained nearly the highest Worth possible (999). Quite excitedly, we examined it and could not understand at first what it was doing that was so terrific. We monitored it carefully, and finally realized how it worked: whenever a new conjecture was made with high worth, this rule put its own name down as one of the discoverers! It turned out to be particularly difficult to prevent this generic type of finessing of EURISKO's evaluation mechanism. Since the rules had full access to EURISKO's code, they would have access to any safeguards we might try to implement. We finally opted for having a small 'meta-level' of protected code that the rest of the system could not modify.
> 
> The second 'bug' is even stranger. A heuristic arose which (as part of a daring but ill-advised experiment EURISKO was conducting) said that all machine-synthesized heuristics were terrible and should be eliminated. Luckily, EURISKO chose this very heuristic as one of the first to eliminate, and the problem solved itself.
>
> [@lenatEURISKOProgramThat1983]

> Often I'd find it in a mode best described as "dead." Sometime during the night, EURISKO would decide that the best thing to do was to commit suicide and shut itself off. More precisely, it modified its own judgmental rules in a way that valued "making no errors at all" as highly as "making productive new discoveries." As soon as EURISKO did this, it found it could successfully meet its new goal by doing nothing at all for the rest of the night... I eventually had to add a new heuristic to EURISKO-one it couldn't modify in any way-to explicitly forbid this sort of suicide.
> 
> [@HALsLegacy2001s1998, page 194]

![The only known image of Eurisko, reasoning about the Traveller game, probably on the Xerox 1100 Lisp machine. The GUI probably was in [Genera](https://en.wikipedia.org/wiki/Genera_(operating_system)). Lenat claimed Eurisko ran for 1300 CPU-hours in total. [@lenatComputerSoftwareIntelligent1984]](figure/Eurisko_GUI.png)

In his review article, Lenat made a brief philosophical comment that Eurisko is the new perceptron:

> ... the paradigm underlying AM and EURISKO may be thought of as the new generation of perceptrons, perceptrons based on collections or societies of evolving, self-organizing, symbolic knowledge structures. In classical perceptrons, all knowledge had to be encoded as topological networks of linked neurons, with weights on the links. The representation scheme being used by EURISKO provides much more powerful linkages, taking the form of heuristics about concepts, including heuristics for how to use and evolve heuristics. Both types of perceptrons rely on the law of large numbers, on a kind of local-global property of achieving adequate performance through the interactions of many small, relatively simple parts.
> 
> The classical perceptrons did hill-climbing, in spaces whose topology was defined explicitly by weights on arcs between nodes (nodes which did straightforward Boolean combinations plus thresholding). The EURISKO style of system does hill-climbing at both the object- (performance-program) and meta- (control decision) levels, in spaces whose terrain is defined implicitly, symbolically, by the contents of the nodes (nodes which are full-fledged concepts, at both object- and meta-levels). The new scheme fully exploits the same source of power (synergy through abundance) yet it is free from many of the limitations of the classical perceptron scheme.
> 
> [@lenatWhyAmEurisko1984]

If this sounds familiar, it is because this Lenat had the same idea as Marvin Minsky, and he was writing in 1984, at the second coming of neural networks. Minsky would soon write his *Society of Mind* (1986), and [re-reject neural networks by writing a long epilogue](https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/#minsky-and-papert-struck-back) to his infamous *Perceptrons* (1969), often blamed for the first neural network winter. Indeed, Lenat's objection to neural networks was essentially the same as Minsky's, if you compare that with Minsky's epilogue. Lenat's approach to Cyc was the same as the Society of Mind of Minsky. Reciprocating, Minsky had often called the field of AI "brain-dead", holding Lenat's Cyc as the only one worth mentioning. [@baardAIFounderBlasts2003]

Continuing the trend of AM, the source code for Eurisko was never published, and indeed, the only known attempt at reimplementation was [@haaseInventionExplorationDiscovery1990], with no offspring.

## 1 million common senses

> He divided the universe into forty categories or classes, which were then subdivided into differences, and subdivided in turn into species. To each class he assigned a monosyllable of two letters; to each difference, a consonant; to each species, a vowel. For example, `de` means element; `deb`, the first of the elements, fire; `deba`, a portion of the element of fire, a flame. In a similar language invented by Letellier (1850), a means animal; ab, mam malian; abo, carnivorous; `aboj`, feline; `aboje`, cat; `abi`, herbivorous; `abiv`, equine; etc... children could learn this language without knowing that it was artificial; later, in school, they would discover that it was also a universal key and a secret encyclopedia.
> 
> Having defined Wilkins' procedure, we must examine a problem that is impossible or difficult to postpone: the merit of the forty-part table on which the language is based. Let us consider the eighth category: stones. Wilkins divides them into common (flint, gravel, slate); moderate (marble, amber, coral); precious (pearl, opal); transparent (amethyst, sapphire); and insoluble (coal, fuller's earth, and arsenic). The ninth category is almost as alarming as the eighth. It reveals that metals can be imperfect (vermilion, quicksilver); artificial (bronze, brass); recremental (filings, rust); and natural (gold, tin, copper). The whale appears in the sixteenth category: it is a viviparous, oblong fish.
> 
> --- Borges, *The analytical language of John Wilkins*

In 1984, after taking the lessons of AM and Eurisko, Lenat began the Cyc project, an ambitious attempt to scale symbolic AI up to the real world. Like most expert systems, the Cyc project consists of a giant knowledge base that encodes all of common sense, upon which inference engines run. Unlike most expert systems, the ambition of Cyc was universal: Its knowledge base would not be restricted to expert knowledge in a particular domain, but *all* common sense knowledge in *all* domains that humans have common sense about.

The game plan was simple:

1. "Prime the knowledge pump" by manually encoding a large enough knowledge base of common senses in a logical language. Also, construct a translator between the logical language of the Cyc and the natural language of humans.
2. Obtain an AI with common sense and natural language, allowing it to learn by reading what people have written down and conversing with people.
3. When it reaches the human frontier of knowledge, it will start performing experiments to go beyond it.

This would solve three problems in one go: 

1. No more brittleness of expert systems, because Cyc would have all the common senses.
2. Once it has enough knowledge, it would be able to machine-learn, and thus get past the knowledge bottleneck.
3. Expert systems would be able to talk with each other if they are all based on Cyc's common knowledge base.

A metaphor that Lenat had used often is that of a circle of ignorance. The more knowledge Cyc has, the easier it is for Cyc to learn more. At first, it must be spoon-fed knowledge with every entry entered by hand. As it builds a basic understanding of the world, it would be able to parse sentences half-way from natural language to logic, and the ontologists would help finish the job, and the more it knew, the better it could parse, saving more time, until it would start parsing without human help. On that day, the pump would finally, triumphantly, be primed, and Cyc would become history's first AI with common sense.

> AI has for many years understood enough about representation and inference to tackle this project, but no one has sat down and done it... only by pulling together the latest human interface tools, Lisp machines, ideas of enforced semantics, and funding for a decade-long effort could we attempt a project of this scale.
> 
> [@lenatCycUsingCommon1985]

It remains to write the 1 million assertions.

1 million?

## ~~1~~ 10 million common senses

> Lenat began building Cyc by setting himself a seemingly modest challenge. He picked a pair of test sentences that Cyc would eventually have to understand: "Napoleon died in 1821. Wellington was greatly saddened." To comprehend them, Cyc would need to grasp such basic concepts as death, time, warfare, and France, as well as the sometimes counterintuitive aspects of human emotion, such as why Wellington would be saddened by his enemy's demise. Lenat and a few collaborators began writing these concepts down and constructing a huge branching-tree chart to connect them. They produced a gigantic list of axiomatic statements—fundamental assumptions—that described each concept in Cyc's database: its properties, how it interacted with other things. "We took enormous pieces of white paper," Lenat remembers, "and filled walls, maybe 150 feet long by about 8 feet high, with little notes and circles and arrows and whatnot."
>
> [@thompsonKnowItAllMachine2001]

By 1990, Lenat made the "midterm" report that the project was still on schedule, that the knowledge pump would be primed by 1995. Though they had met about 150 technical obstacles along the way, they had all be cleared away, and it remained to just add more knowledge. [@lenatCreating30MillionRuleSystem2022; @lenatBuildingMachineSmart2009] The most important technical obstacles were as follows.

Don't try to make the perfect "upper ontology". It just has to be good enough. You just lose a constant factor $O(1)$ in computational space/time if you use the wrong one.

> We even wasted quite a bit of time trying to get the very most general tip of Cyc's concept network “right”... at the 1986 Fifth-Generation Project conference in Tokyo, when we saw the ontology built by Japan’s answer to Cyc, named Electronic Dictionary Research (EDR). Their topmost distinction was between things with souls and things without souls. And large trees were in the former category, whereas small trees were in the latter category... They and their EDR system knew that both types of trees needed water and sunlight and had roots, etc., they just had to represent each of those assertions as two separate rules instead of one, as we did in Cyc. No big deal.
> 
> The important lesson was: Making suboptimal ontology choices just means that your ontology and knowledge base might have to be bigger, more verbose, to make up for those missed generalization opportunities.
>
> [@lenatCreating30MillionRuleSystem2022]

Instead of an object-oriented frame-and-slots language like Eurisko and AM, use a fully general higher-order logic language. This is necessary because people can do common sense higher-order reasoning: modals, reflection, pros and cons, counterfactual hypotheticals, contexts as first-class objects in our ontology, several different useful “species” of negation, etc. [@lenatCreating30MillionRuleSystem2022]

Instead of searching for the right representation, use as many representations for concepts and rules as you need. Each is represented in at least two ways. This is necessary for efficient inference. Similarly, use as many inference engines as needed, since the general logic engine is too slow. They had already 20 at that point, and they would eventually end up with >1100. [@lenatCreating30MillionRuleSystem2022]

They have also estimated that it takes about 80 minutes to encode a single rule, including the overhead for knowledge elicitation up front, and the overhead for debugging and testing [@lenatCreating30MillionRuleSystem2022].

With these lessons learned, they confidently marched towards 10M assertions, still expecting to finish by 1995. As 1995 came, 

As one can expect from Lenat's previous non-releases, there was also no release of Cyc itself, especially because Cyc is a commercial endeavor, as is necessary to sustain the 2000 person-year project. However, around 2001, at the start of the [Semantic Web](https://en.wikipedia.org/wiki/Semantic_Web) hype, Cycorp began seriously engaging with the many Semantic Web initiatives.

Squinting a bit, the visions of the semantic web and the vision of the Cyc were the same: Both wish to draw connections between data, such that computer programs can chain together multiple operations on data, synthesize them, and give the user what they meant, instead of what they literally typed out.

Cycorp participated in the [Standard Upper Ontology Working Group (SUO WG)](https://web.archive.org/web/20120609070903/http://suo.ieee.org/), which, like most Working Groups, died in 2003 among motions, procedures, and consensus-buildings. What it did provide is [the earliest copy](https://web.archive.org/web/20030515043435/http://www.cyc.com/SUO/opencyc-ontology.txt) of OpenCyc I have found, in 2003, which I have [backed up](code/opencyc-ontology.txt) for safekeeping.



While there were some partial open sources, Cyc is fundamentally a proprietary system since its inception. Academic publications were few; most publications became advertisements; OpenCyc shut down in 2017; 

Eventually, . The general inference engine had been long turned off, and the Cyc system was a mass of over 1000 little inference engines. [@lenatGettingGenerativeAI2023]


## Cyc in 10 minutes

The Cyc system consists of three main kinds of parts: the CycL language, based on SubLisp, itself based on Lisp; the knowledge bases, consisting of millions of assertions written in CycL; the inference engines, written in SubLisp. To increase speed and compatibility, the programs are compiled to Java, which is then compiled to bytecode.

The CycL language is intended to be the high-level language that humans can read and write, at the "Epistemological Level" (EL). Most inference engines do not process CycL directly, but process low-level translations of CycL at the "Heuristic Level" (HL). Each sentence at the EL can be translated into a multitude of HL sentences, since different translations allow different inference engines to process it, so that hopefully at least one of those would process it efficiently.

### CycL language

Like Lisp, the CycL language has almost no syntax:

* An **expression** is of one of 4 types: `(#$relation <arg1> … <argn>)`, `?variable`, `#$Term`, text string, and rational number.  
* A `#$relation` is either a function, or a predicate. Although I prefer to say that a predicate is a function that returns a boolean. It begins with a lowercase.
* A `#$Term` is basically a "word" in the vocabulary of CycL. It begins with an uppercase. Some terms can be thought of as classes, while others can be thought of as objects. For example, `#$Socrates` is an object, while `#$Human` is a class. The difference is that we can say `(#$isa #$Socrates #$Human)` but not `(#$isa #$Socrates #$Plato)`.
* A "concept" is a term. For example, here are some concepts: `#$Human`, `#$Socrates`, `#$MaleHuman`, ... Anything that begins with `#$` is a concept.

This allows us to get started writing some assertions that would imply "Socrates is the subject of some DeathEvent".

```lisp
(#$isa #$Socrates #$MaleHuman) ; #$isa means "is a"
(#$isa #$MaleHuman #$Predicate)
(#$genls #$MaleHuman #$Human) ; #$genls means "generalizes"
(#$genls #$MaleHuman #$MaleAnimal)

(#$genls #$Person #$Individual)
(#$isa #$Individual #$FirstOrderCollection) ; Things can go pretty meta
(#$isa #$FirstOrderCollection #$SecondOrderCollection)
(#$isa #$FirstOrderCollection #$MetaClass)

(#$forAll ?X
  (#$sameAs (#$MotherFn (#$MotherFn ?X)) 
    (#$MaternalGrandMotherFn ?X)))

; For X to be mortal means there exists a death event X is subject to
(#$implies
  (#$Mortal ?X)
  (#$exists ?DE
    (#$and
      (#$isa ?DE #$DeathEvent)
      (#$subject ?DE ?X))))

; Humans are mortal
(#$isa #$Mortal #$Predicate)
(#$implies (#$isa ?X #$Human) (#$Mortal ?X))
```

Like how programs can be packaged into a module, assertions can be packaged into a **context**, aka a **microtheory** (MT). Each assertion can belong to exactly one microtheory, though you can have two assertions that are literally the same occur in two microtheories. For example, "Socrates is alive." is true in the context of 500 BC, but not in the context of 1995. For a single problem-and-answer session with a user, Cyc typically sets up a temporary context that is deleted after the session.

Microtheories are necessary because human beliefs are incompatible, and there are a lot of humans. For example (very relevant, considering how much Cyc was involved with the War on Terror), `#$ChristianMT` and `#$IslamMT` could have:

```lisp
(#$isa #$ChristianMT #$Microtheory)
(#$isa #$IslamMT #$Microtheory)

; #$ist means "is true in the context of"
(#$ist (#$ChristianMT)
  (#$not (#$sameAs #$God #$Allah)))
(#$ist (#$ChristianMT)
  (#$sonOf #$Jesus #$God))
; Implementing the Trinity is left as an exercise for the reader

(#$ist (#$IslamMT)
  (#$sameAs #$God #$Allah))
; In Islam, God has no son
(#$ist (#$IslamMT)
  (#$not (#$sonOf ?X #$God)))
(#$ist (#$IslamMT)
  (#$prophetOf #$Jesus #$God))
```

Microtheories can contain each other. For example, both are `#$AbrahamicMT`, which allows us to make assertions that apply to both.

```lisp
(#$isa #$AbrahamicMT #$Microtheory)
(#$genls #$ChristianMT #$AbrahamicMT)
(#$genls #$IslamMT #$AbrahamicMT)

; Abraham is a prophet
(#$ist (#$AbrahamicMT)
    (#$prophetOf #$Abraham #$God))

; God exists uniquely
(#$ist (#$AbrahamicMT)
  (#$exists ?X
    (#$and
      (#$isa ?X #$God)
      (#$not
        (#$exists ?Y
          (#$and
            (#$isa ?Y #$God)
            (#$not
              (#$sameAs ?X ?Y))))))))
```

### Ontology

The Ontology of Cyc is the graph of all concepts in Cyc, with one directed edge per `(#$genls #$Thing1 #$Thing2)` assertion. At least, that's the simplest possible way to say it. In fact, the graph is more complicated, since CycL is a higher-order logic, which allows it to talk about the relation between relations between predicates, etc. For example, both `#$SetOrCollection` and `#$MathematicalObject` are subconcepts of `#$MathematicalThing`, but we also need to specify that any `#$MathematicalThing` is either `#$SetOrCollection` xor `#$MathematicalObject`. As of 2010, the topmost part of Cyc looks like this:

![The upper ontology of Cyc as of 2010. [@foxvogCyc2010, figure 1]](figure/Cyc_Upper_Ontology_2010.png)

As an example, here is how `#$Philosopher` is described in [OpenCyc](code/opencyc-ontology.txt):

> A specialization of `#$Person`; in the context of `#$HumanActivitiesMt` this collection is an instance of `#$PersonTypeByActivity`, in the context of `#$JobMt` it is an instance of `#$PersonTypeByOccupation`. Each instance of `#$Philosopher` is a person who habitually thinks about philosophical matters such as what is or might be, what we can know, how we can know anything, etc. In the contemporary era most philosophers are academics or professionals, but a significant number (now and historically) don't fit this profile.

And here's the `#$Thing`: 

> `#$Thing` is the "universal collection": the collection which, by definition, contains everything there is. Every thing in the Cyc ontology -- every `#$Individual` (of any kind), every `#$Set-Mathematical`, and every `#$Collection` -- is an instance of (see `#$isa`) `#$Thing`. Similarly, every collection is a subcollection of (see `#$genls`) `#$Thing`. Trivially, `#$Thing` is both an instance of and a subcollection of itself, and is not a subcollection of any other collection. (Note that the above reference to "every thing in the Cyc ontology" is *not* meant to be limited to things actually *reified* in the Cyc system, but includes (e.g.) every instance -- reified or not, known or not -- of every collection recognized by Cyc.)

And if the parenthetical note sounds a bit theological, note that among those that Cycorp had hired included philosophers, botanists, chemists, and of course, theologians. Interestingly, they didn't ask botanists to encode what they know about botany, but about what they know about non-botany. Lenat's theory is that botanists' understanding of botany is not common sense. Instead, what botany he wants Cyc to encode is how non-botanists think about plants -- even, and especially, those botanical beliefs that a botanist would consider wrong, such as `(#$not (#$isa #$Banana #$BotanicalBerry))`.

Unfortunately for ontology, while common sense is common, the sense of common sense is uncommon. Let's take the example of how Cyc encodes events, which Cyc calls "Davidsonian semantics", since it's how [Donald Davidson](https://en.wikipedia.org/wiki/Donald_Davidson_(philosopher)) represented [events](https://en.wikipedia.org/wiki/Event_(philosophy)) (Cyc hired a lot of philosophy PhDs).

For example, to represent "John gave Mary a book yesterday." you can write something like `(#$Give #$John #$Mary #$Book1 #$Yesterday)`. The problem is that it's hard to add more details. What if you also want to say it happened in the library, or that it was a gift, or that John was happy about it? You'd have to change the definition of the predicate `#$Give`. To solve this problem, Cyc treats the event itself as an `#$Event`. That is, it reifies the process as an object. Now, instead of cramming everything into one assertion, you can construct a sequence of assertions:

```lisp
(#$isa #$Event123 #$Event)
(#$Sender #$Event123 #$John)
(#$Receiver #$Event123 #$Mary)
(#$GivenObject #$Event123 #$Book134)
...
```

Such problems are fairly subtle, and the world is a very big place. Fortunately, Lenat was a master of ontology, so it all worked out in the end:

> they were trying to encode all of botany and had a small staff of professional botanists doing knowledge entry. Naturally it was quite difficult for the botanists to try to translate their knowledge into the formalisms required by Cyc, and they would regularly puzzle over various questions... and if they could not come to a consensus, would have to take it before the Master, Doug Lenat, who would think for a bit, maybe draw some diagrams on a whiteboard, and come up with the Right Representation.
>
> --- [AMMDI: alpha ontologist](https://hyperphor.com/ammdi/alpha-ontologist) (2023-10-07)

### Inference engines

To a mathematician, 1 and 1 trillion are the same -- both are finite. To computer scientists, even $x^2$ and $x^3$ are different. Much work on Cyc was not on building the 100M-assertion knowledge base, but on building inference engines that allow fast inferences when there are 100M assertions to pick from.

Every logical system must face an impossible trilemma: an expressive language that can represent what people want to say, an efficient inference engine on the language, and a complete inference engine that can perform all correct inferences. Why is this a trilemma?

Suppose we want to allow Cyc express all common sense assertions, then since humans in their daily life say high-order statements like "So you meant to make me upset by spreading rumors about her, which you knew that he would hear about it?", Cyc needs to use a higher-order language. Now this immediately makes it impossible to have a *computable* complete inference engine by Gödel-style incompleteness [@shapiroFoundationsFoundationalismCase1991, theorem 4.14].

Lenat chose to use a fully higher-order language, giving up completeness, and then try his best improving efficiency.

Like most expert systems, the Cyc has a general resolution-based inference engine. Unlike most expert systems, its knowledge base is large and higher-ordered, so its general engine runs too slowly for most queries. Thus, the developers kept adding more specialized modules ("pattern-specific heuristic modules"), each capable of efficiently inferring on a few microtheories. If a specialized engine fails to make progress, a more general engine can be the fall-back, all the way up to the most general one.

As the simplest example of how inference engine can work, consider the following example of backward-chaining inference, in a semantic web. The system is asked basically "Why does Clyde want to possess a crescent wrench?", and it eventually replies "Because Clyde has not eaten lately.". Lenat expected that the ontology of Cyc would eventually power generally intelligent agents, which would use forward-chaining to construct goals from current states, and use backward-chaining to explain why others have their goals.

![A sketch of how to reason by backward-chaining together triples in a semantic web. Figure from [@wallichSiliconBabies1991]. I can't help but wonder if Lenat meant for a subtle joke against the *Elephants don't play chess* by [@brooksElephantsDonPlay1990] by adding in a "dead end" branch that ends up concluding Clyde is an elephant.](figure/CYC_triple_reasoning.png)

Most inference engines are highly specialized. They can only reason within a few microtheories, but very well. Here is a toy example for reasoning with mutually exclusive categories. We can define `#$MuTex` as 

```lisp
(#$implies
  (#$and (#$isa ?X ?A) (#$MuTex ?A ?B))
         (#$not (#$isa ?X ?B)))
```

Then this allows the general reasoning engine to reason about mutual exclusivity by always reducing to this definition. However, we can accelerate this by adding in some "lemmas":

```lisp
(#$implies (#$MuTex ?A ?B) (#$MuTex ?B ?A))
(#$implies
  (#$and (#$MuTex ?A ?B) (#$genls ?C ?A))
  (#$MuTex ?C ?B))
(#$implies
  (#$and (#$MuTex ?A ?B) (#$isa ?X ?A))
  (#$not (#$isa ?X ?B)))
```

After producing enough lemmas, we can then write a specialized inference engine that would be called upon whenever `#$MuTex` appears in an expression, and try to do some inference and simplifications by calling upon these lemmas. Indeed, most inference engines were constructed in this way: Cyc tried to solve a problem, and times out. An expert was called in and asked, "How did you do this?" and the expert would explain how they solved it with quick rules of thumb, which the knowledge engineers would write into Cyc, resulting in more assertions, and possibly more inference engines. This is essentially the same as "knowledge elicitation" used for making expert systems.

Other than these inference engines, there are also many "tacticians", modules that pick engines that are probably good for solving a problem, and there are a few "strategists" that pick tacticians.

The control structure of Cyc is a commercial secret. The *only* explanation I could find is a few paragraphs in an essay published near the end of his life [@lenatCreating30MillionRuleSystem2022]. According to it, the inference engines use a blackboard architecture similar to the one used in Hearsay-II speech recognition system [@ermanHearsayIISpeechUnderstandingSystem1980]. Lenat described it as:

> there is a whole battery of specialized inference engines and representations... and, when making progress, broadcasting the results... Whenever progress is made, all of them stop and work on the now-simpler subproblem. Some of the inference engines are very general, and work on general representations--e.g., a theorem prover that works on first-order logic. The more specialized inference engines are much faster whenever they do apply. For example, if some subproblem requires solving $N$ linear equations in $N$ unknowns, there is a very efficient way of representing and solving that.
> 
> In 1986, Cyc had two such representations; by 1990, there were 20, each with its own inference engine; today Cyc has over 1100. They work together as a community of agents, communicating by posting their intermediate results on a sort of blackboard that all the other agents can watch and react to if/when/as they see an opportunity that fits their specialty.
>
> [@lenatCreating30MillionRuleSystem2022]

![How Cyc processes a user's input as of 1990. The user enters at the EL-KB (epistemological level knowledge base), which gets translated down to the HL, where multiple modules  for generating and comparing arguments for and against a given proposition are run. [@lenatCycMidtermReport1990, figure 1]](figure/Cyc_EL_HL.png)


## Cycorp in 40 years

They themselves underestimated the difficulty. In 1990, they confidently titled a paper "Cyc: A midterm report" [@lenatCycMidtermReport1990], suggesting that they expected to be done around 1995.

The progress report in 1995 stated that, while the system is far from done, they have at least manually entered $10^5$ "general concepts" and $10^6$ "common sense axioms" into Cyc, at the price of 100 person-years. [@lenatCycLargescaleInvestment1995]

> Moreover, statistics, colocation, and frequency do not resolve such questions. But the task goes from impossible to trivial if one already knows a few things about boxes and pens, police and demonstrators, and water and teakettles. The same sort of chicken-and-egg relationship characterizes CYC and ML because learning occurs at the fringe of what one already knows. Therefore, in the early 1980s, when the rest of the world was so enthusiastic about Natural Language Understanding, Machine Learning, and AI in general, we were pessimistic. We concluded the only way out of this codependency would be to prime the pump by manually crafting a million axioms covering an appreciable fraction of the required knowledge. That knowledge would serve as a critical mass, enabling further knowledge collection through NLU and ML, beginning in the mid-1990s. Mary Shepherd and I embarked on that task in 1984, knowing we had little chance of success, but seeing no alternative but to try... we are now moving toward the transition point where NLU and ML are supported. The rest of the world is disillusioned and pessimistic about symbolic AI, but ironically, as CYC reaches closure, our hopes for NLU and ML in the next 10 years are very high.
>
> [@lenatCycLargescaleInvestment1995]

Cycorp regarded the semantic web effort as doing essentially the same thing, except with a less expressive frame language (like Java objects) on a much larger scale. During the 2000s, papers from Cycorp often talked of integrating Cyc with the semantic web by encoding knowledge in RDF, OWL, and XML. Well, in a sense, the Semantic Web did arrive, but instead of the dream of Cyc-like thinkers performing long queries over databases, we have forgetful agent swarms talking with each other with API calls, dying, [REST](https://en.wikipedia.org/wiki/REST)ing, and reincarnating, a game of [*Memento*](https://en.wikipedia.org/wiki/Memento_(film)) on the global scale.

> ... give CYC enough knowledge by the late 1990s to enable it to learn more by means of natural language conversations and reading. Soon thereafter, say by 2001, we planned to have it learning on its own, by automated-discovery methods guided by models or minitheories of the real world. To a large extent, that's just what we did. At the end of 1994, the CYC program was mature enough to spin off from MCC as a new company -- Cycorp -- to commercialize the technology and begin its widespread deployment.
>
> [@lenat20012001Common2001]

OpenCyc quietly shut down with no fanfare, probably in 2017-03, with a [curt message](https://web.archive.org/web/20170422212642/http://opencyc.org/):

> Part of the Cyc technology was released, starting in 2001, as OpenCyc, which provided an API, RDF endpoint, and data dump, under appropriate Apache and Creative Commons open source licenses. Its distribution was discontinued in early 2017 because such "fragmenting" led to divergence, and led to confusion amongst its users and the technical community generally that that OpenCyc fragment was Cyc.


In 2016, Lenat finally declared the Cyc project "done" and set about commercializing it.

> most of what is left to be added is relevant to a specific area of expertise, such as finance or oncology... Among other projects, the company is developing a personal assistant equipped with Cyc's general knowledge. This could perhaps lead to something similar to Siri... the CEO of Lucid says the new company is in talks with various others interested in using the Cyc knowledge base. Lucid has been working with the Cleveland Clinic, for example, to help automate the process of finding patients for clinical studies.
> 
> [@knightAI30Years2016]


## 1 billion dollars

As you may know at this point, the military is behind everything in AI, and that includes the Cyc project. It began in 1984-07 under the [Microelectronics and Computer Consortium](https://en.wikipedia.org/wiki/Microelectronics_and_Computer_Technology_Corporation), which, like [Strategic Computing Initiative](TODO), was formed in reaction to the threatening Japanese Fifth Generation Computer Systems project. Though it was not *directly* funded by the government, its head was [Bobby Inman](https://en.wikipedia.org/wiki/Bobby_Ray_Inman), who had held high positions in the Navy, the NSA, and the CIA, so...

In 1995-01, they left the MCC to continue development in Cycorp Inc., a for-profit company. Playing its part, academic publication almost immediately ceased after that point.[^cyberstalking-the-cyc]

[^cyberstalking-the-cyc]: Most papers published after that point were slim on details, and mostly about yet new exciting ways for them to ingest more data from the Internet, or about yet more ways to use their knowledge base. I could find no information about how the inference engines worked, or details of the CycL. Most of the "applications" were vaporware, with dead links everywhere. I basically resorted to cyberstalking, tracking down every Lenat talk and news report over the years, and digging up gossips by ex-Cyclists, to piece together what happened afterwards.

Who bought the services of Cyc and for what? The details are slim. Trade secrets, no doubt. Confirmed results:[^policy-wonk]

[^policy-wonk]: I swear I'm not a policy wonk, but 3 days of cyberstalking does take its toll.

* [Lycos search engine](https://en.wikipedia.org/wiki/Lycos), to disambiguate search terms. It ended in 2001. ([Source](https://web.archive.org/web/20150905165226/http://www.cyc.com/about/media-coverage/computer-save-world/))
* Cleveland Clinic, starting in 2007, to answer doctors' queries as an expert system. [@lenatHarnessingCycAnswer2010]
* Department of Defense, in 2001, to clean dataset. [@thompsonKnowItAllMachine2001]
* GlaxoSmithKline, in 2001, to clean dataset. [@thompsonKnowItAllMachine2001]
* Goldman Sachs, sometime around 2016, to "monitor the inner workings of its technological infrastructure" and detect insider trading. [@metzOneGeniusLonely2016; @shilohHeTaughtAI2023]
* CycSecure, a network vulnerability assessment tool, first beta in 2002. Trialed at the US Strategic Command Computer Emergency Response Team at some unknown point before 2005. [@anthesCycUse2002; @shepardKnowledgebasedApproachNetwork2005]
* CIA and the Department of Defense, probably identify terrorist threats. [@shilohHeTaughtAI2023]
* The NSA, to "identify terrorist threats in international communications data". [@metzOneGeniusLonely2016]
* Paul Allen had funded Cycorp sometime before 2001 for unknown purposes and an unknown sum. In 2003, he funded it by \$0.7M as part of his project of "Digital Aristotle", to create a tutoring AI. [@hiltzikBirthThinkingMachine2001; @richmanAllenClaimsSuccess2003; @friedlandProjectHaloDigital2004]
* The [Terrorism Knowledge Base](https://en.wikipedia.org/wiki/MIPT_Terrorism_Knowledge_Base) (2004--2008).
* [Total Information Awareness](https://en.wikipedia.org/wiki/Total_Information_Awareness) project, in 2003, funded Cycorp for \$9.8 million for a "prototype database" and "identify phone-calling patterns as they might exist among potential terrorists overseas". [@crensonBigBrotherCould2003]
* Electronic Surveillance System for the Early Notification of Community-Based Epidemics-II (ESSENCE-II), around 2006. Self-explanatory title. [@abbottIntegratedBiologicalWarfare2007]
* Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) program, sometime between 2001 and 2004. [@lenatBuildingMachineSmart2009]
* Seven unnamed big companies, for expert system things, but probably "common-sense platform for their applications, and as an interlingua to fully, semantically integrate all the data they generate and all the data they license from third parties". [@cycorpCycTechnologyOverview2021; @lenatCreating30MillionRuleSystem2022]

Looking at the list, it is obvious that much of Cycorp funding came from the American intelligence community, especially between 2001 and 2010, during the heights of [War on Terror](https://en.wikipedia.org/wiki/War_on_terror), as the American state struggled to expand its sovereign eye over the expanding cyberspace. Indeed, one of the early success was when it "predicted anthrax might be sent through the mail six months before trove of knowledge about past terrorist activities, tactics, and weapons". Though the success did not help anyone, it was great advertisement.[@hawkinsPredictingTerroristsNext2003][^explosive-dolphins] Lenat in a [2006 Google Talk](https://youtu.be/gAtn-4fhuWA?si=gAQ-TISZxxgeD1VN&t=1856) showed screenshots of Cyc answering "Which American city would be most vulnerable to an anthrax attack during summer?". (The answer was "Phoenix".)

![Screenshot of Cyc answering "Which American city would be most vulnerable to an anthrax attack during summer?". The system replied "Phoenix", with reasoning. [@lenatComputersCommonSense2006]](figure/Cyc_anthrax_phoenix.png)

[^explosive-dolphins]:
    But there's also this:

    > Once, developing a scenario for a terrorist attack on Hoover Dam, it hypothesized a school of 1,000 al Qaeda-trained dolphins bearing explosives.
    > 
    > [@hawkinsPredictingTerroristsNext2003]

The total funding of the project is hard to know, although we know that in 2002, its [total cost had been \$60M](https://stanfordmag.org/contents/wise-up-dumb-machine), of which [\$25M came from the military](https://web.archive.org/web/20120502151103/http://www.opencyc.org/cyc/company/news/APArticle060902), so I think it's fair to say 50\% came from the military. This is corroborated in 2005:

> In 1996, we got our first substantial government contract,” Lenat recalls. Since then, Cycorp has collected about half of its revenue from U.S. government agencies and the rest from companies, mostly for building “semantic maps” that help users pull information from various databases with a single query. By taking on paying projects, Cycorp has been able to stay profitable and debt-free. All of the firm’s stock is owned by its employees, making Cycorp answerable only to Cycorp. “But,” Lenat admits, “we have had to tack with the funding winds. Maybe 50 percent of the funding we get pushes us forward in the direction that we need to go.”
> 
> Cycorp doesn’t even want to be distracted by the rigors of the retail software business; instead, it licenses Cyc for use in third-party software packages... The time may come, Lenat says, when a greatly expanded Cyc will underlie countless software applications. But reaching that goal could easily take another two decades.
> 
> [@woodCycorpCostCommon2005]

## A hostile assessment of Cyc

> **bogosity**: At CMU, bogosity is measured with a bogometer; in a seminar, when a speaker says something bogus, a listener might raise his hand and say "My bogometer just triggered"... The agreed-upon unit of bogosity is the microLenat.
> 
> **microLenat**: The unit of bogosity. Abbreviated µL or mL in ASCII. Consensus is that this is the largest unit practical for everyday use. The microLenat, originally invented by David Jefferson, was promulgated as an attack against noted computer scientist Doug Lenat by a tenured graduate student at CMU. Doug had failed the student on an important exam because the student gave only "AI is bogus" as his answer to the questions. The slur is generally considered unmerited, but it has become a running gag nevertheless.
>
> --- [The Jargon File](http://www.catb.org/jargon/html/M/microLenat.html)

It is hard to interpret the state of Cyc today, if we take Lenat's word for it:

* There were 150 technical challenges to knowledge engineering and representation at the start of Cyc in 1984, but they were all solved by 1990. [@lenatCreating30MillionRuleSystem2022; @lenatBuildingMachineSmart2009]
* Cyc could already be tutored in (constrained) natural language in 2001. [@anthesComputerizingCommonSense2002]
* The upper ontology has remained stable for years as of 2015. [@lenat50ShadesSymbolic2015]
* The knowledge pump is 95\% primed in 2015, when there were just 15M assertions. [@lenat50ShadesSymbolic2015], and as of 2021, there were over 25M assertions, [@cycorpCycTechnologyOverview2021], so the knowledge pump had been more than primed.
* SubLisp is easy to learn, and knowledge engineering in SubLisp is 1000x more efficient than in a modern language like Python. [@lenatCycQuestSolve2021]
* The Cycorp has been profitable since its inception, had never taken on debt, had been almost entirely employee-owned, and could remain profitable entirely on doing business with non-government corporations in 2022. [@lenatCreating30MillionRuleSystem2022; @lenatCycQuestSolve2021]
* Cyc has natural language understanding of pragmatics, while statistical machine learning systems have none. [@lenatSometimesVeneerIntelligence2017; @lenatNotGoodGold2019]

What is stopping Cyc from learning? The finances were healthy. Cycorp is not subject to perverse interests of the market or middle managers. The employees are aligned. SubList is a great language. All technical challenges to knowledge engineering and representation had been solved by 1990. The knowledge pump has been more than primed.

According to the final work of Lenat [@lenatGettingGenerativeAI2023], the only holdup is natural language understanding. The knowledge pump is thoroughly primed, but Cyc still couldn't learn by reading human texts because natural language understanding is still too hard. In other words, we have the Vauquois triangle again. Cyc can read CycL perfectly well -- the interlingua -- but it is stubbornly difficult to parse English into the interlingua. But what *is* lacking in the parser? Why is it hard to parse?

![The Vauquois triangle of translation.](figure/Vauquois%20triangle.png)

Recall that Lenat had always argued that "just letting a system learn on its own by natural language understanding" is a free lunch, and that NLU requires a significant portion (~10--50%) of common sense already encoded. Indeed, there is a critical tension here:

1. The CycL language is enough to represent common sense language about the world. That is, the Vauquois triangle structure is exactly correct: You have natural languages, but they are all joined at the top by a common interlingua -- the CycL.
2. By the Winograd schema challenge, translation requires common sense.
3. CycL → English requires no common sense. The result would sound kind of wooden and robotic, but it doesn't require any understanding: Just follow the syntax substitution rules. Indeed, this already works in Cyc since ~2000.
4. Therefore, Japanese → CycL, or any X language → CycL, requires common sense. Indeed, Lenat had from always argued that Natural Language Understanding (NLU) requires common sense.
5. By the no free lunch hypothesis, neural networks trained from scratch can't have common sense.
6. But in Lenat's last paper, Lenat has argued that the only problem stopping Cyc from learning from natural language is that NLU doesn't yet work well enough, and hoped that neural networks can do the English → CycL translation.

On the topic of interlingua, it is interesting that [ABBYY](TODO_ABBYY's_last_stand) was almost a twin of Cycorp. Whereas Cycorp began building an ontology for the common sense world since 1984, spent \$120M, [@paulheimHowMuchTriple2018] and got stuck on NLU since around 2010, ABBYY began building an interlingua machine translation system since the 1990s, and spent over \$80M. By the early 2010s, they realized that they could not compete with Google statistical machine translation, and pivoted to doing NLU with semantic graph technology based on the knowledge base they produced for the sake of interlingua. [@skorinkinABBYYsBitterLesson2024]

Indeed, interlingua-based machine translation projects used to be common, but essentially went extinct (except ABBYY) after the rise of statistical machine translation in the 1990s [@hutchinsMachineTranslationHistory2023].

I suspect that it is not simply the problem of getting a better English → CycL translator, and then Cyc would finally begin learning, but that much knowledge in sentences doesn't translate to interlingua. If the failures of all interlingua machine translation systems is not enough evidence, then consider some more facts about Cyc's NLU:

* I only found a few examples that Cycorp gave for English → CycL: "A girl is on a white lounge chair" [@prattCYCReportPratt1994], "Bill Clinton sleeps.", "An AI researcher is a kind of computer scientist." [@pantonCommonSenseReasoning2006], and "Did you touch a blue object located in the capital of France on September 25th, 2022?" [@lenatGettingGenerativeAI2023]. None involves the "many AI-complete elements, such as correct disambiguation, understanding of idioms, metaphor, sarcasm, foreshadowing, irony, subtext, and so on." They were quite easy and unambiguous examples, almost as if they began with a CycL sentence, and then converted it to English.
* [@guhaEnablingAgentsWork1994] states that in 1994-03 "The Syntax module can properly handle about 75% of the sentences found in the news stories of a typical issue of the newspaper USA Today. And in cases in which Cyc knows all the proper nouns in the sentence, the Semantics module can properly handle most of the sentences parsable by the syntax module... as good as what our knowledge enterers independently come up with, when asked to manually translate the material into CycL.".
* [@pantonCommonSenseReasoning2006] states that in 2006, a search-and-verify system for English → CycL, that combines syntactic parser, statistical parser, and Cyc verification, resulted in "sentences that were correct, according to human review, approximately 50% of the time".
* [@sarjantAllYouCan2009] increased the common-sense knowledge in ResearchCyc by 30% in 2009, by guess-and-verify, where the Cyc does verification, and the guess was done by simplistic methods like regex parsing, infobox pairing, etc.
* Some governmental experimental uses of Cyc, such as ESSENCE-II and Total Information Awareness, might have involved some NLU, but I cannot find details concerning how much NLU is involved.
* Only one of the commercial applications of Cyc plausibly required NLU.
  * In [Mathcraft](https://cyc.com/mathcraft/), the "learning by teaching" game powered by Cyc, students could not type anything, but only make multiple choices generated by the Cyc itself.
  * In the Cleveland Clinic application [@lenatHarnessingCycAnswer2010], the user enters queries already in a constrained language (like "aortic valve replacement patients with a pericardial aortic valve"), and then compose a CycL translation by clicking from Cyc's parser's suggestions. Similarly for the Terrorism Knowledge Base application.
  * "Maintaining persistent user models in order to support extended, months-long online chats with their famous characters" seems to involve NLU. However, considering the complete lack of details as to what it involved (I can't find out which one it is), and the complete lack of Cyc in the recent chatbot boom, either Lenat was right to say that NN don't understand, but that this application didn't require NLU after all, or that chatbots do require NLU, but that Lenat was wrong.

Let's take it another way: If there is No Free Lunch to NLU, then what is Cyc's score on Winograd schema benchmark? Where is the Cyc-translator? Forget about Google Neural Translate -- is it even better than ABBYY's? Where's ChatCYC? Why does none of its commercial applications use much NLU, while most commercial applications of NLU use statistical or neural machine learning?

All evidences point to the conclusion that a sentence that can be parsed to CycL is already bureaucratic and formalistic, with the mark of interlingua written on its brow. For those, Cyc appears to have already working NLU in early 2000s, and yet, Cyc is still here, not machine-learning, lacking ... what? `<sarcasm>`A sarcasm parser?`</sarcasm>`

::: {.callout-note title="An information-theoretic estimate." collapse="true" }

The [information content of English](https://yuxi-liu-wired.github.io/essays/posts/perplexity-turing-test/#entropy-of-natural-languages) is about 0.8 bits per character, and each English sentence contains about 100 characters, giving 80 bits of information.

Now, one might argue that the last bit is the deepest, but it is hard to square the two claims:

* self-organized NN could capture 98.75% of the information content in natural language.
* self-organized NN don't understand.

[TODO make the argument more correct]

Stated in another way, this means 98.75% of information that people produce as they write is mindless, remembering and espousing, but not understanding or inferring. This is even harder to square with Lenat's assertion that 99\% of language understanding is pragmatics, which self-organized NN don't have. [@lenatSometimesVeneerIntelligence2017]

::: 

Take another look at the list of known applications of Cyc. Does it look like a path towards AGI, or does this look no different from building custom-made expert systems for specialized purposes, something that those generic professionals of Oracle, IBM, or Accenture have been doing for decades? Perhaps they did have a product differentiation in being able to consistently find particularly good knowledge engineers, and in programming in a particularly efficient language of SubLisp, which allowed them to stay in business despite having just a 50-person crew. It is not something to be dismissed, but is this a path towards AGI, or a veneer of AGI to enterprise solutions?

## Everyone can only see their own dream

In a paper coauthored with Feigenbaum, Lenat gave the most comprehensive statement for where he stands philosophically, which he held onto for the rest of his life:

- Knowledge Principle (KP). A system exhibits intelligent understanding and action at a high level of competence primarily because of the knowledge that it can bring to bear: the concepts, facts, representations, methods, models, metaphors, and heuristics about its domain of endeavor.
- Explicit Knowledge Principle: While knowledge may be compiled to opaque lumps of code for efficiency, there should always be a declarative version of that, so that they can be subject to meta-reasoning.
- Breadth Hypothesis (BH). Intelligent performance often requires the problem solver to fall back on increasingly general knowledge, and/or to analogize to specific knowledge from far-flung domains.
- Empirical Inquiry Hypothesis (EH). The most profitable way to investigate AI is to embody our hypotheses in programs, and gather data by running the programs. The surprises usually suggest revisions that start the cycle over again. Progress depends on these experiments being able to falsify our hypotheses. Falsification is the most common and yet most crucial of surprises. In particular, these programs must be capable of behavior not expected by the experimenter.
- Difficult Problems Hypothesis. There are too many ways to solve simple problems. Raising the level and breadth of competence we demand of a system makes it easier to test -- and raise -- its intelligence.
- Knowledge Is All There Is Hypothesis. No sophisticated, as-yet-unknown *control structure* is required for intelligent behavior.
- The Local Consistency Hypothesis. There is no need--and probably not even any possibility--of achieving a global consistent unification of several expert systems' KBs (or, equivalently, for one very large KB). Large systems need local consistency.
- The Coherence Hypothesis. Moreover, whenever two large internally consistent chunks C1, C2 are similar, their heuristics and analogies should cohere; e.g., if the "going up" metaphor usually means "getting better" for C1, then it should again mean "getting better" for C2, or else it should not apply at all there.

Lenat is the very example of a [hedgehog](https://en.wikipedia.org/wiki/The_Hedgehog_and_the_Fox): a single philosophy, a vision for AGI, pursued for 40 years. One does not pursue a single vision without rejecting alternative visions, and Lenat has been explicit in rejecting every alternative route to AGI, and he had a sharp tongue [@thompsonKnowItAllMachine2001; @lenatVoiceTurtleWhatever2008; @lenatGettingGenerativeAI2023; @lenatThresholdsKnowledge1991, section A.2]:

* Logical AI in the style of Simon and Newell's General Problem Solver. Such an elegant framework would not work beyond toy problem domains, by the Knowledge Principle.
* Highly accurate models of human behavior, in the style of Simon and Newell's *Human Problem Solving* or the [SOAR architecture](https://en.wikipedia.org/wiki/Soar_(cognitive_architecture)). Duplicating human cognitive architecture constitutes cargo cult science. AGI need not have the [magic number 7 ± 2](https://en.wikipedia.org/wiki/The_Magical_Number_Seven%2C_Plus_or_Minus_Two).
* Physical embodiment. It might be great fun to make robots, but physical embodiment is neither necessary nor sufficient for "grounding" the knowledge base, because of the Physical Symbol System Hypothesis. Such a "mystical worship of physical embodiment" would only delay AGI. In particular, the [subsumption architecture](https://en.wikipedia.org/wiki/Subsumption_architecture) does not lead to AGI.
* Genetic algorithms and other evolutionary algorithms. It gets stuck in local minima too often, and runs too slowly.
* Creating tiny morsels of little expert systems, and hope that bit by bit, AGI would emerge out of that. Remember that plateau-hopping requires breadth. Without an overarching plan, they will not fit together, like how the 1980s expert systems could never talk to each other.
* Logical machine learning without a large knowledge base already in place. It makes for good demos, but quickly exhausts itself. These are examples of the illusory hope for "free lunch" or elegant "Maxwell's equations of thinking", a severe case of laziness and "physics envy". Researchers should stop sitting on their asses mad with "physics evny", and start the dirty work of coding.
* Statistical machine learning, pattern matching, and neural networks. Just wait for enough compute and data, then magically a large model would learn on its own? Yet more wishful thinking for "free lunch", caused by laziness and "physics envy".
* Any form of machine learning without a large knowledge base to begin with. This is impossible because learning is possible only at the fringe of knowing. Any attempt to learn without a large starting knowledge base is, again, trying to get a "free lunch".
* Wait until philosophers have figured out the one true ontology for the world, then build the Cyc accordingly. Philosophers suffered from "Hamlet syndrome", unwilling to take decisive action, satisfied with publishing tiny morsels of ontologies that don't cover the whole world, or grand ontologies that cover a caricature of the whole world.

He had a particular grand history for AI, which I call the 3 optima theory.

With a little hard work (about 6 person-months), one can get a knowledge-free system working, such as self-organized neural networks, Simon and Newell's General Problem Solver, etc. This allows the researcher to publish a quick paper, a student to earn their PhD degree, and so on. Putting in more hard work does not result in a better system, but usually makes things worse as the code becomes bloated and unmanageable. Academic myopia stops people from trying to get out of this local maximum, since people just want to get published papers.

With a lot more hard work (about 10 person-years), one can get a system with a lot of specialized knowledge working. This is where the commercialized expert systems live. However, the general consensus is that as an expert system grows beyond 10K rules, it starts to suffer from its weight of all the rules. Standard expert systems were built for special fields, so people would use a simple language that works, but eventually collapses under the weight of 100K rules. Commercial myopia stops people from trying to get out of this local maximum, since people just want to sell products, and 50K rules is good enough for the customer. 

The problem is that all these efforts are wasted. Specialized expert systems cannot be glued together efficiently, because each of them lives in a differently simplified world. That is, "plateau-hopping requires breadth". The AI field as a whole would stagnate. The only way out of this is to go for the full common sense, to invest in the 2000 person-years of effort, and make a Cyc. After that, all the expert systems can interface with Cyc, and with each other using CycL, and all the computers can be preinstalled with their digital common sense.

![Lenat's 3 optima theory for the grand history of AI.](figure/Lenat_3_optima_theory.jpg)

According to multiple reports, Lenat was charismatic, able to sell his vision of AGI to many people. Having read most documents produced by Lenat or Cycorp over the 40 years, I have discovered that there is a consistent list of themes that Lenat just kept repeating over the last 40 years of his life. Each theme has a double structure: a technical statement that has an emotionally neutral valence, and a moral coloring that provides the call to action, the charisma, the coherence for the employees to align to his vision.

| technical statement | moral coloring |
|-----------|-------|
| the No Free Lunch Hypothesis | we are hardworking, you are lazy |
| there is no "Maxwell's Equations of Thought" | we are self-assured, you suffer from physics-envy |
| the Empirical Inquiry Hypothesis; Cyc is unaesthetic; we are building the Cyc profitably, not publishing academic papers | we are strong engineers that get things done, you are weak aesthetes playing the academic game |
| the Physical Symbol System Hypothesis | we are building real intelligence, you are just playing with robots |
| the Breadth Hypothesis | our systems are intelligent, yours are idiot savants |
| the Explicit Knowledge Principle | our systems understand deeply, yours pattern-match shallowly |
| we think you are putting pattern-matchers in places that require deep understanding | we are trustworthy, you are reckless |
| we believe the Cyc is the only current effort towards AGI | we are ambitious, you are academic careerists |
| writing the Cyc costs a lot and is unpopular with the academia | we rebel and think freely, you follow the crowd |
| Cycorp hires anyone -- including high school dropouts -- good at encoding common sense | we are egalitarian, you are elitists |
| Cycorp has always had just ~50 people, and has been mostly forgotten now | we are the elect, you will see |
: The doubled structure of Lenat's rhetoric

And more than technical, moral, and personal conviction is on the line: If Cyc really would take 1000 person-years (20 years with 50 philosopher PhDs), then it would cost about \$100 million just in human labor. The Cycorp, if it were to survive, has a strong commercial interest in rejecting all alternatives. It can be very hard to understand something, when misunderstanding it is essential to your [product differentiation](https://en.wikipedia.org/wiki/Product_differentiation).

Lenat's rejections progressed with time as each new challenger arose. In the 1980s, like other expert systems people, he focussed his rejection towards the previous logical AI methods exemplified by Simon and Newell. Logical AI was a dream that a graduate student might build an AGI during a thesis period, if only they knew the "Maxwell's equations of thinking". He took a little effort towards rejecting the *other* logical AI approach, that of *Human Problem Solving*, constructing models that reproduced every little detail of how humans really perform in psychometric experiments, such as their reaction times. Admitting its interest to psychologists, he considered it a distraction for machine intelligence.

In the 1990s, as the expert system hype died down, he turned his criticism towards expert systems. He recalled that, back when he was young, before academia had rejected him, he thought automated discovery with AI, such as AM and Eurisko, would lead the way to self-improving learning machines. But then he was disabused of this. Bacon discovered Kepler's three laws "only" from data, but that's because Pat Langley was careful in presenting nothing but the data necessary for this. The cost to discover Kepler's laws on the filtered dataset? A few CPU-hours. The cost to filter the dataset? 10 Kepler-years. Similarly, AM started out with the set-theory axioms and discovered prime numbers and some famous conjectures, but quickly ended up enumerating boring complications. Lenat had to keep adding in more heuristics to get something out of it. Similarly, Eurisko would run overnight and Lenat would check its outputs in the morning, remove some bad ideas, add some good ones, and so on. Lenat estimated that the Travellers 1981 win was "60/40% Lenat/Eurisko" [@lenatEuriskoProgramThat1983].

Generalizing, Lenat argued that there is a common thread across all these machine learning systems. They would all start out discovering many interesting basic things, but quickly "run out of steam" enumerating boring complications. Lenat called it having a "veneer of intelligence", but were really just "discharging potential energy that was stored in them". That is, the creators secretly put into the program with their own expert knowledge somehow, either through the right rules, heuristics, dataset, features, or some other thing. Once the expert knowledge is "exhausted", no more discoveries could be made. However, it makes for impressive demos, leading to hype-and-disappointment cycles. The only escape is to prime the knowledge pump. If the knowledge base is large enough, then it wouldn't run out of steam. [@lenatVoiceTurtleWhatever2008]

Lenat's approach was unwelcome from the academics, and the feeling was mutual. AI researchers thought the Cyc project was hyped, and was unhappy with the secretive nature of Cycorp. Philosophers considered the Cyc project premature -- how could Lenat build an ontology for the world when philosophers haven't even figured out what the ontology is? Lenat shot back, calling academics lazy, abstract, and unable to persist through decades of hard engineering work. [@thompsonKnowItAllMachine2001] Among the academics, the only one that still supported him was Marvin Minsky, who had no problem calling the rest of AI research "brain-dead since the 1970s", especially robotics: "Graduate students are wasting 3 years of their lives soldering and repairing robots, instead of making them smart.". [@baardAIFounderBlasts2003]

During this period, there were two main challengers to his idea of a monolithic logical system. On the one side, there was the challenge of bottom-up non-symbolic reasoning promoted by Rodney Brooks' subsumption architecture [@brooksElephantsDonPlay1990], and the statistical machine learning methods like support vector machines. He did not have much to say about the statistical methods -- not yet -- but he did reject the subsumption architecture as a mistaken attempt to reach AGI through robotics, much as Minsky did. Motors, sensors, etc, are simply not needed -- common sense, specified in logical language, is all you need. 

Though most expert systems people have shrunk their ambition to commercial products, some still believed that we could build little systems, brick by brick, until a general system is found. This is basically a "Society of Mind" approach of Marvin Minsky, and Lenat and Minsky liked each other's research, Lenat rejected this approach as well. One cannot settle for building common sense bit by bit, expecting a finished system to emerge, but must build the whole thing in one framework. Otherwise, it will fragment into a Tower of Babel, with little expert systems of incompatible ontologies, just like how Feigenbaum's dream of a "Library of Congress" of knowledge bases failed to materialize.

In the 2000s, big data arrived with the Internet, and statistical learning became dominant. No doubt trying to preempt customers' "Why don't I just Google it?", he turned his firepower towards statistical learning systems. He never tired of pointing out that, if you make an even slightly complex query like "Is the Space Needle taller than the Eiffel Tower?", Google will happily serve up results saying "The Space Needle is 605 feet high." and "The Eiffel Tower is 1,063 feet high.", but unable to actually answer your question. Despite having 15,000 servers, Google only ran dumb statistical algorithms, while Cyc running on a single server could answer it. Google-style statistical machine learning, like its trillion-token statistical machine translations systems [@brantsLargeLanguageModels2007], was just pattern matching, yet another example of hoping for a free lunch. Such systems could not truly understand. As an alternative, he held out Cyc as the foundation to the [Semantic Web](https://en.wikipedia.org/wiki/Semantic_Web), which would build a system that *would* truly understand.

Curiously, right from the start, Lenat considered self-organized neural networks as *the same as* logical AI programs of Simon and Newell, even though from our perspective, they couldn't be more different. To Lenat, neural nets, General Problem Solvers, n-gram models, whatever, are all just "explicit-knowledge-free systems", [too neat, not scruffy](https://en.wikipedia.org/wiki/Neats_and_scruffies), and would fail for the exact same reason.

> they are unaesthetic! And they entail person-centuries of hard knowledge-entry work. Until we are forced to them, Occam's Razor encourages us to try more elegant solutions, such as training a neural net "from scratch"; or getting an infant-simulator and then "talking to it". Only as these fail do we turn, unhappily, to the "hand-craft a huge KB" tactic.
> 
> ...
> 
> Our position regarding the aesthetes There is a methodological difference between our "scruffy" way of doing AI and the aesthetes' "neat" way... If only there were a secret ingredient for intelligence--Maxwell's equations of thought. If only we could axiomatize the world in a small set of axioms, and deduce everything. If only our learning program could start from scratch. If only our neural nets were big or cerebellar or hyperlinear enough. If only the world were like that. But it isn't. The evidence indicates that almost all the power is in the bulk knowledge. As Whitehead remarked, "God is in the details."
>
> [@lenatThresholdsKnowledge1991]

With the second neural network winter, he did not pay more attention to them, but as they arose yet again in the 2010s, with some exasperation, he would remind the world that, no, nothing has changed. Neural nets had already failed and they would fail. Thinking that "one large net for everything" would just work is yet another example of the logical AI fallacy that "If only we have the Maxwell's equations of learning, it will just work!". They are always "remembering and espousing", but never "understanding and inferring", and can only ever be the "right brain" to Cyc's "left brain" [@lenatGettingGenerativeAI2023]. As Deep Learning kept blowing past expectations, he rehashed the same 1980s argument with more apocalyptic tones:

> No matter how good your elegant theory of _syntax_ and _semantics_ is, there’s always this annoying residue of _pragmatics_, which ends up being the lower 99% of the iceberg.  You can wish it weren’t so, and ignore it, which is easy to do because it’s out of sight (it’s not explicitly there in the letters, words, and sentences on the page, it’s lurking in the empty spaces around the letters, words, and sentences.)  But lacking it, to any noticeable degree, gets a person labeled _autistic_. They may be otherwise quite smart and charming (such as Raymond in _Rain Man_ and Chauncey Gardiner in _Being There_), but it would be frankly dangerous to let them drive your car, mind your baby, cook your meals, act as your physician, manage your money, etc. And yet those are the very applications the world is blithely handing over to severely autistic AI programs!
> 
> [@lenatSometimesVeneerIntelligence2017]

> We would not be comfortable giving a severely neurologically-impaired person -- say someone with no functioning left brain hemisphere -- real-time decision-making authority over our family members' health, our life savings, our cars, or our missile defense systems. Yet we are hurtling in that direction with today’s AI's which are impaired in almost exactly that same fashion! They -- those people and those AI programs -- have trouble doing multi-step abstract reasoning, and that limitation makes their decision-making and behavior brittle, especially when confronted by unfamiliar, unexpected and unusual situations... Machine learning algorithms have scarcely changed at all, in the last 40 years... Current AI’s can form and recognize patterns, but they don’t really *understand* anything. That’s what we humans use our left brain hemispheres for.
> 
> ... Researchers and application builders tolerate their AI systems having just the thinnest veneer of intelligence, and that may be adequate for fast internet searching or party conversation or New York Times op-ed pieces, but that simple representation leads to inferences and answers which fall far short of the levels of competence and insight and adaptability that expert humans routinely achieve at complicated tasks, and leads to shallow explanations and justifications of those answers. There is a way out of that trap, though it’s not pleasant or elegant or easy. The solution is not a machine-learning-like “free lunch” or one clap-of-thunder insight about a clever algorithm: it requires a lot of hard work...
>
> [@lenatNotGoodGold2019]

Concurrently, on the Cycorp website, two white papers published in 2021-04 reiterated their product differentiation against the false promises of [neural networks](https://cyc.com/wp-content/uploads/2021/04/Cyc-Technology-Overview.pdf) and [Bayesian networks](https://cyc.com/wp-content/uploads/2021/04/bayesnetspaper.pdf).[^cyc-white-paper] Neural networks was of course a great commercial threat to their business, and the Bayesian networks, by promising to half-open the neural network black box, threatens their business as well. In any case, since both were not rule-based logical systems, they are. After such fear-uncertainty-doubt, they reassured the reader that true AI needs both the left brain and the right brain, and they sell the finest left brains on the planet.

[^cyc-white-paper]:
    Read [it](https://cyc.com/wp-content/uploads/2021/04/Cyc-Technology-Overview.pdf) yourself ($\sim 3 \times 10^6 \mathrm{\mu Lenat}$) to see how hard they had to work that product differentiation. Calling it "**A**ctually **I**ntelligent", claiming "ML can *never* give an explicit step-by-step explanation of its line of reasoning behind a conclusion, but Cyc *always* can." And insinuating that *nobody* could do Natural Language Understanding yet because none of those newfangled neural networks had any pragmatics... And this was uploaded in 2021-04, a year after GPT-3!

    The same [FUD](https://en.wikipedia.org/wiki/Fear%2C_uncertainty%2C_and_doubt)dy tone can be seen in websites that sell machine translation after Google Neural Translate, transcription services after OpenAI Whisper, or copywriting services after ChatGPT.

The same accusation of brain-damage that he leveled at neural networks was in fact a rehash of the exact same argument he had made against statistical machine learning systems like Cleverbot, Google, and Amazon recommender systems[@loveMostAmbitiousArtificial2014], since he made no distinction between statistical methods, be it keyword matching, n-gram models, or neural networks. They are all the same veneer of intelligence, same free lunch, same shallowness.[^hedgehogs-are-all-the-same]

::: {.callout-note title="Hedgehogs are all the same." collapse="true" }

Hedgehogs. They have one big idea and continue going on with it for decades. Chomsky did it, Minsky did it, and Lenat did it too. Benefit: If they got it right, they really got it right. Cost: If they got it wrong, then they would sound like a broken record.

For example, Lenat called expert systems "brittle" and "idiot savants" in the 1980s, and statistical machine learning systems "brittle" (probably also "idiot savant") in the 2000s, and neural networks "brittle" and "autistic" since 2015 until his death.

Similarly, he kept talking about the [Winograd schema challenge](https://en.wikipedia.org/wiki/Winograd_schema_challenge#Winograd_schemas), and how logically encoded common sense is the only way to solve it. He started talking about it in the 1990s [@lenatCycLargescaleInvestment1995], and he was still telling Stephen Wolfram in 2019 that surely if Cyc teams up with Wolfram Alpha they could finally solve the Winograd challenge [@wolframRememberingDougLenat2023], and still [tweeting in 2020-03-12](https://x.com/CycorpAI/status/1238183980580642816) about an essay saying that modern LLM was underperforming the WinoGrande benchmark. Etc, etc.

Not just his arguments were repetitive, but his "war stories" too. In 1994, Cyc could retrieve images by semantic search, so that it would retrieve an image of a rock climber if queried "an adventurous man" [@lenatArtificialIntelligence1995]. Great demo, and he would harp on this throughout the 2000s in his presentations, presumably to product-differentiate against Google-like Image Search engines. Similarly, he told of a story of an expert system diagnosing his rusty car with measles first in 1987 [@lenatThresholdsKnowledge1991], and would keep telling that story throughout the 2000s presentations.

Lenat could easily give the same criticism with the same counterexamples, without needing to inspect the details of these machine learning architectures, because he had the following general proof:

1. Unless common sense is fully represented and integrated, an AI system is an idiot savant at most. [@caiAmbientIntelligenceEveryday2006]
2. Machine-learning common sense from scratch is impossible, because learning occurs at the fringe of what one already knows. [@lenatCycLargescaleInvestment1995]
3. Therefore...

Some words reappear so often in Lenat's writings, I termed them "Lenatisms" ($\sim 10^6 \mathrm{\mu Lenat/word}$): free lunch, hard work, physics envy, Maxwell's equations, clever algorithm, measles, idiot-savant/autistic, veneer of intelligence, exponential, shallow, pattern matching, brittle, understand, trustworthy, left brain, hemisphere.

:::

In his last paper, coauthored with Gary Marcus, he updated his critique of statistical machine learning to the LLM age. Again the brittleness, free lunch, etc. 

> Given the arduous nature of the reasoning required... it is understandable almost all AI researchers and developers have gone in the opposite direction, abandoning or trivializing symbolic representation and reasoning, and instead seeking one or another sort of “free lunch” in the form of perceptrons, multi-layer neural networks and, most recently, LLMs... limiting an AI to such a narrow “baby talk” language would be a huge barrier to it ever becoming a trustworthy general AI.
> 
> [@lenatGettingGenerativeAI2023]

I am struck by the irony that a veteran of logical AI would call neural networks "brittle", or make an appeal to sunk cost. Lenat had devoted 2000 person-years to the project, therefore a "free lunch" shouldn't work, nevermind the fact that these "free" lunches took 20 years of gritty battles to [build the datasets](TODO_link_to_ImageNet), struggles with the [cussedness of CUDA](TODO_link_to_cuda), waking up to yet another divergent overnight training run, staring at tensors filled with NaNs, and eventually [cost \$100 million](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/) per serving, roughly the total budget of Cycorp through its life. How dare you to go "free lunch" on us... But *de mortuis nil nisi bonum*.

Lenat died in 2023. His passing was unmourned on [Lucid AI](https://lucid.ai/) and [Cycorp](https://cyc.com/), who, like ABBYY, still proudly advertise their product differentiation.

## In lieu of a conclusion

Napoleon died in 1821. Wellington was greatly saddened.

> Come as you are, as you were
> As I want you to be
> As a friend, as a friend
> As an old enemy
> Take your time, hurry up
> Choice is yours, don't be late
> Take a rest, as a friend
> As an old memoria...
