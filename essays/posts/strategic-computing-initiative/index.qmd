---
title: "Intelligence in the age of war machines"
author: "Yuxi Liu"
date: "2024-12-23"
date-modified: "2024-12-23"
categories: [AI, scaling, history]
format:
  html:
    toc: true
description: "Intelligence in the age of war machines."

# image: "figure/banner.png"
status: "wip"
confidence: "likely"
importance: 5
---

{{< include ../../../utils/blog_utils/_macros.tex >}}

## Abstract

> All mathematics is divided into three parts: cryptography (paid for by CIA, KGB and the like), hydrodynamics (supported by manufacturers of atomic submarines) and celestial mechanics (financed by military and by other institutions dealing with missiles, such as NASA.).
>
> Cryptography has generated number theory, algebraic geometry over finite fields, algebra, combinatorics and computers.  Hydrodynamics procreated complex analysis, partial differential equations, Lie groups and algebra theory, cohomology theory and scientific computing.  Celestial mechanics is the origin of dynamical systems, linear algebra, topology, variational calculus and symplectic geometry.
>
> --- Vladimir Arnold

## Planning

WWII was said to be a war of the physicists, or the code-breakers. It could equally said to be a war of the economists. The side that could sustain a higher industrial output slowly but surely ground down the other side, as strategic bombing destroyed industrial production.

That the soviet union had a bad economic system is either a myth or an triviality, yet two Soviets had been Nobel laureates in economics. The first was [Wassily Leontief](https://en.wikipedia.org/wiki/Wassily_Leontief), who saw economics not as a simple feedforward network of wheat-to-bread-to-stomach, but as a feedback network of hundreds of industries making commodities for and taking commodities from each other, as made clear in his PhD thesis *The Economy as Circular Flow* (1928).

Leaving the Soviet Union as a dissident in 1925, he settled in America and performed his [input-output analysis](https://en.wikipedia.org/wiki/Input%E2%80%93output_model) of the American economy. Let us divide the economy into $N$ sectors, from "agriculture" to "electricity" to "rubber". For simplicity, suppose each sector $i$ only makes one kind of commodity $x_i$ -- "agriculture" only outputs a generic kind of "food", etc. Each sector $i$, in order to create one unit of commodity $x_i$, requires some commodities $A_{1i}, A_{2i}, \dots, A_{Ni}$ as inputs. Then, if the entire economy produces $x_1, x_2, \dots, x_N$, then it requires total input $Ax$, where $A$ is the demand matrix. With this we have the fundamental equation:

$$
x = Ax + y
$$

where $y$ is the surplus that comes out of the economy, which Leontieff calls "demand", since that is what is demanded by consumers outside of the economic production. It seems simplistic -- just a linear equation? Yet this was the origin of linear programming.

In 1949, Leontief used an early computer at Harvard and data from the U.S. Bureau of Labor Statistics to divide the U.S. economy into 500 sectors, and balanced the linear equation on the grand scale.

![Part of an input-output matrix drawn by Leontief. [@leontiefStructureDevelopment1963, page 150]](figure/Leontieff_matrix.png)

Near the end of the war, Stigler wrote a paper calculating what is the cheapest possible nutritionally complete diet. He concluded that at 1939 prices, a minimal viable diet cost \$40 per year for an adult American man, which was 1/3 the price of previous results. He concluded that the previous dieticians were not ruthless enough, and confused biological needs with cultural needs:

> The dieticians take account of the palatability of foods, variety of diet, prestige of various foods, and other cultural facets of consumption. Primarily on such grounds can one explain their emphasis on meats and the inclusion of sugar... If the dieticians persist in presenting minimum diets, they should at least report separately the physical and cultural components of these diets.
>
> [@stiglerCostSubsistence1945]

In 1947, George Dantzig was tasked with the US Air Force to mechanize its planning. Inspired by the paper as well as Leontief's input-output analysis, he thought that constraint-optimization with linear algebra was just the tool to use. As a start, he attacked the same problem of diet, and from there, he developed the [simplex method](https://en.wikipedia.org/wiki/Simplex_algorithm). [@dantzigDietProblem1990]

> In the fall of 1947 ... undertook as a test of the newly proposed simplex method the determination of a least cost adequate diet based on Stigler's data. It was the first "large scale" computation in the field. The system consisted of 9 equations in 77 unknowns. Jack parcelled out a different 8 or 9 columns (of the 77 columns) to each of the 9 clerks who were assigned to process them. Using hand-operated desk calculators... the 9 clerks took approximately 120 man-days to obtain an optimal solution of \$39.69. Stigler's heuristic solution was only off from the true annual optimal cost by 24 cents: not bad!
>
> [@dantzigDietProblem1990]

While linear programming became the great success story of early AI -- when it was still considered as the "mechanization of thought process", and often subsumed under operations research -- and was quickly taken up by military logisticians and business managers, its original application in diet planning was less successful. Dantzig recounts how he unsuccessfully tried to go on a diet by running a 500-ingredient linear programming problem on an IBM 701. Hilarity ensues.

> One day I said to Anne, my wife, "Today is Der Tag, whatever the 701 says that's what I want you to feed me each day starting with supper tonight." ... I then read off the amounts of foods in the optimal diet. Her reaction: "The diet is a bit weird but conceivable. Is that it?" "Not exactly," I replied, "AND 500 gallons of vinegar." ... It turned out that our data source listed vinegar as a very weak acid with water content = zero. Therefore, according to the way the model was formulated the more vinegar you drank the greater would be your feeling of feeling full. I decided that vinegar wasn't a food.
>
> The next day the above scene was repeated except this time ... calling for the consumption of 200 bouillon cubes per day... I called my doctor and asked him how come the nutritional requirements didn't show a limit on the amount of salt? "Isn't too much salt dangerous?" He replied that it wasn't necessary; most people had enough sense not to consume too much. I placed an upper bound of three on the number of bouillon cubes consumed per day. That was how upper bounds on variables in linear programming first began.
> 
> The next day the above scene was repeated, except this time the diet called, among other things, for two pounds of bran per day... The model was revised with an upper bound put on the amount of bran. The next day the proposed menu was almost exactly the same except this time it was two pounds of blackstrap molasses which substituted for the bran... she said, "I have been studying the various menus the computer has been generating. There are some good ideas there that I can use. I'll put you on MY diet. She did and I lost 22 pounds.
> 
> [@dantzigDietProblem1990]

In our hindsight, we say that the AI performed an adversarial attack on the input data and the reward model misspecification, something that gaming and planning AI are wont to do.

The second Soviet Nobel laureate, Leonid Kantorovich, managed to win it while remaining *within* the USSR. This was no small feat, for in the USSR, the academic study of economics meant only *political* economy, and mathematical economics was merely a minor branch of political economy, with mathematical economists having to frame their research as a "critique of bourgeois economic thought". [@boldyrevCulturesMathematicalEconomics2017]

Like Dantzig, Kantorovich discovered linear programming in the context of a practical problem. Feeling burnt out by too much pure math, he decided to do something practical for a change of scenery, and to do something about the imminent threat of Nazi Germany. So he went to a plywoord factory in 1937, and worked on how to cut plywood sheets in such a way as to meet a specified assortment of pieces with minimum waste. And just like Dantzig, he reduced the problem to a maximization problem with linear objective and linear inequality constraints. 

Dizzy with success, he tried the same trick again and again, and accidentally improved efficiency so much that he almost ended up in jail.

> ... one of the major materials handling operations at the Leningrad E. I. Egorov Railroad Car Building Plant was the cutting of sheet metal for railroad cars. Ordinarily, this cutting produced tremendous quantities of scrap. After introducing Kantorovich's solution technique to the problem of minimizing waste, officials were able to reduce the amount of scrap by 50%. This had the unfortunate side effect of greatly reducing the amount of scrap metal available to steel plants in the region, and Kantorovich was ordered to appear at Leningrad party headquarters for allegedly sabotaging the economy. In this instance, he was rescued by the military, which needed him for its atomic program.

In fact, his method was so efficient that it made the Egorov plant achieve 94% efficiency, which led to...

> The Soviet Ministry used a ratchet principle: the plant must add 7 per cent to its efficiency level each year. Kantorovich’s Institute had to write to the ministry pointing out that a 101 per cent level of efficiency would never be possible.
>
> [@bollardEconomistsWarHow2020, page 153]

> According to Stalin, the planned economy of the USSR was already "[dizzy with success](https://ru.wikipedia.org/wiki/Головокружение_от_успехов)"; hence any criticism of it was anti-Soviet propaganda, a serious crime. In particular, anyone openly suggesting that waste could be cut substantially was at great personal risk. Nevertheless, Kantorovich ... wrote a letter to Gosplan suggesting a reform of the price system used in planning. 
> 
> [@gardnerLVKantorovichPrice1990]

Being a socially clueless nerd was not the stuff of romantic comedy in Soviet Russia, but gallows comedy. Fortunately for mathematical economics, his luck held:

> Gosplan wrote back saying that no such reform was necessary. This outcome was rather fortunate for its author, as similar letters critical of the authorities -- for example, one by Solzhenitsin -- landed their authors promptly in jail.
> 
> [@gardnerLVKantorovichPrice1990]

Reading Kantorovich's repeated attempts to reform Soviet economy, I imagined those old silent movies where a protagonist stumbles around, blindfolded, crossing a highway where the cars always *just missed*. Accidental economic sabotage was not the end of his misadventures, however. His luck was really tested when it came to the "shadow price" affair.

Consider the problem of managing a factory. It has some input constraints (fuel, woods, etc), and the objective of maximizing the total utility of output goods. This is simply solved by the Lagrangian multiplier method, and it turns out that the Lagrangian multipliers themselves have a natural interpretation: the multiplier $\lambda_i$ is the amount of extra utility that the factory can achieve, if the factory were to obtain one more unit of constraint $i$.

So Kantorovich had the great idea of calling it the "shadow price",[^shadow-price-kantorovich] and considered it a way to find the proper price of something, better than the free market mechanism. The free market mechanism leads to maximizing some kind of utility function, but that utility function is not designed by anyone. In contrast, the shadow price naturally falls out of attempts to maximize a social utility function designed by the [Gosplan (State Planning Committee)](https://en.wikipedia.org/wiki/Gosplan). It is a utility designed by humans, for humans, not a utility function emergent from capitalism, for capitalism.

[^shadow-price-kantorovich]: In his original terms, "objectively determined valuation" \[объективно обусловленные оценки\], where the "objectively" means "according to an objective function (the social utility function)".

This is, in short, how market socialism should work, according to Kantorovich:

1. Design a social utility function (the "social" part of "socialism").
2. Collect data on how the factories *should ideally* work (big data).
3. Compute the shadow prices by linear programming (big compute).
4. Set the prices on every commodity.
5. Let the factories compete on this market for goods and money, secure in the knowledge that this socialist market would maximize a properly designed social utility, and the market mechanism would allow only the factories that achieve the highest efficiency: The slackers are competed out, and only those who can perform as efficiently as the "ideal factories" (see step 2) can make a profit and thus survive.

The whole thing is not hard to prove in a simple model of economy, where all factories are linear functions, and the social utility function is also a linear function. Then the proof is just writing down the "[linear duality theorem](https://en.wikipedia.org/wiki/Dual_linear_program)", and interpreting it as economics.

> \[Boyarskii:\] You write, "any increase in the requirements of some article entails a corresponding increase in costs and consequently in its o.d. valuation. A decrease in requirements entails a reduction in its o.d. valuation." What is this, what can this possibly be, but a suggestion that value is determined by supply and demand? Supply and demand, for heaven’s sake: bourgeois ideology’s most transparent disguise for exploitation!
>
> ...
> 
> \[Kantorovich:\] It’s true that there is a formal resemblance, but they have a completely different origin, and therefore a completely different meaning. Whereas market prices are formed spontaneously, objective valuations -- shadow prices -- must be computed on the basis of an optimal plan. As the plan targets change, the valuations change. They are subordinate to the very different production relationships of a socialist society. Yet, yet, the scope for their use is actually bigger under socialism. The capitalists actually agree with you, Dr Boyarskii, that the mathematical methods we’re talking about should only be applied on the small scale, on the level of the individual firm. They have no choice: there is no larger structure, in the economy of West Germany or the United States, in which they can be set to work. They have had some success, I believe. I’m sorry to say that, since George Danzig and Tjalling Koopmans made their discoveries of “linear programming” in America during the war, the techniques have been adopted there far more eagerly, far more quickly, than in the Soviet Union. Linear programmers in the USA calculate routes for airlines, and devise the investment policies of Wall Street corporations. But we still have an opportunity before us which is closed to the capitalists. Capitalism cannot calculate an optimum for a whole economy at once. We can. There is a fundamental harmony between optimal planning and the nature of socialist society.
> 
> [@spuffordRedPlenty2010, chapter 5]

Though he kept trying to propose his idea of socialist optimal planning, and kept sending letters to leaders from Stalin to Andropov, his idea remained a linear programmer's fantasy.

## Shakey {#sec-shakey}

In 1964-04, the Stanford Research Institute (SRI) submitted a proposal to ARPA to build a robot that could move through a cluttered room using its own cameras, with the unassuming name "Intelligent Automata" so that it did not appear to be a sci-fi project. It was later renamed to "Intelligent Automata to Reconnaissance".[^shakey-reconnaissance] This resulted in Shakey the robot, developed during 1966--1972. Despite the deliberately unassuming name, it had the ambition of integrating all the subfields of AI as then understood into a single package.

> to develop an experimental test bed for integrating all the subfields of artificial intelligence as then understood. SRI wanted to integrate in one system representation and reasoning, planning, machine learning, computer vision, natural language understanding, even speech understanding, for the first time.
>
> [@kuipersShakeyConceptionHistory2017]

[^shakey-reconnaissance]: Annoyingly, I have checked all the official reports -- the interim reports [1](https://web.archive.org/web/20060316080944/http://www.ai.sri.com/pubs/files/rosen66-p5953-interim1.pdf), [2](https://web.archive.org/web/20060316081210/http://www.ai.sri.com/pubs/files/rosen67-p5953-interim2.pdf), [3](https://web.archive.org/web/20060316081226/http://www.ai.sri.com/pubs/files/rosen67-p5953-interim3.pdf), [4](https://web.archive.org/web/20060316081638/http://www.ai.sri.com/pubs/files/nilsson68-p5953-interim4.pdf), and the [final report](https://web.archive.org/web/20060316081339/http://www.ai.sri.com/pubs/files/nilsson68-p5953-final.pdf), and *none* of them told me what it is supposed to reconnoiter for!

Even before it was finished, it became a celebrity. The *Life* magazine called Shakey the "first electronic person" [in 1970](https://gwern.net/doc/reinforcement-learning/robot/1970-darrach.pdf).[^shakey-life-magazine] A [1972 documentary](https://www.youtube.com/watch?v=GmU7SimFkpU) shows its operations, in which one can see its shaking motion, and how it stops after each straight-line motion. [@stanfordresearchinstituteShakeyExperimentRobot1972] The shaking is due to the springs in two wheels, and the stopping is to wait for computer vision and planning to finish, and for the shaking to stop. Early during development, it was tethered. A bit later, the tether was removed, and the researchers found it to mysteriously rotate once in a while. It turned out to be a vestigial subroutine meant to untwist the tether. (Peter Hart, at 05:55)

[^shakey-life-magazine]:
    This article was considered sensational and misleading by most of the people who worked on Shakey.

    > an article that was more than science fiction: the AI community feels it was victimized in this instance by outright lies. Said Rosen, *He came here and all of us spent a great deal of time being very honest and candid with him. Then he didn’t present the whole story. He picked the sensational things and left out the others...* Bert Raphael, who had spent a lot of time with Darrach, goes further: *...He wrote it as if he’d seen many things he never saw, and wrote about seeing Shakey going down the hall and hurrying from office to office, when in fact all the time he’d been here we were in the process of changing one computer system to another and never demonstrated anything for him. There were many direct quotes that were imaginary or completely out of context.*
    > 
    > [@mccorduckMachinesWhoThink2004, page 273]

::: {#fig-shakey layout-ncol=2}

![The original design of Shakey. It had two arms that got cancelled. Dimensions: `56''L; 35'' W; 57'' H`. [Source](https://www.computerhistory.org/revolution/artificial-intelligence-robotics/13/289/1239).](figure/Shakey_original.png)

![Shakey the robot in 1972. Standing *square*ly away from the uncanny valley, it is almost cute in its boxy appearance. Just watching it makes me want to make "beep-bop" sound. [Source](https://en.wikipedia.org/wiki/File:SRI_Shakey_with_callouts.jpg)](figure/SRI_Shakey_with_callouts.jpg)

![Schematic cleaned diagram of Shakey. [@slocumRobotExcessMachine2024, figure 9]](figure/Shakey_overview_1.png)

![Why Shakey shakes: Shakey is moved by two diagonal drive wheels, and the other two diagonal wheels have springs on them to allow Shakey to go up slopes. Whenever it starts and stops, it shakes back and forth across the diagonal. [@slocumRobotExcessMachine2024, figure 10]](figure/Shakey_overview_2.png)

![[@kuipersShakeyConceptionHistory2017, figure 4]](figure/Shakey_predicate_calculus.png)

![[@kuipersShakeyConceptionHistory2017, figure 6]](figure/Shakey_hierarchical_planning.png)

![Ideally smoothed path (A), planned path (B), and actual shakey path (illustrative only) (C). [@slocumRobotExcessMachine2024, figure 11]](figure/Shakey_path_planning.png)

:::

It was a fine example of logical AI, and several classical algorithms were developed specifically for it, including the A* search for pathfinding on a graph, the Hough transform for its vision; and the visibility graph method specifically for pathfinding in a Euclidean plane with obstacles. Like SHRDLU, it represented the state of the world and the goal state using first-order predicate logic, and planned by logic programming (mostly backward chaining and [Robinson resolution](https://en.wikipedia.org/wiki/Resolution_(logic))). It also understood natural English commands. It had a primitive level of rote learning, in the sense that it memorized previous plans ("macros"), which could then be used as single components in later plans. These all worked and still continue to work.

Shakey also demonstrated the same lesson of expert systems against the generic logical AI approach:

> Shakey showed that you could not, for example, take a graph-searching algorithm from a chess program and hand-printed-character-recognizing algorithm from a vision program and, having attached them together, expect the robot to understand the world. As Raphael put it, there are serious questions about the interaction between knowledge in different domains... the overwhelming message--not always recognized by those doing the robotics work themselves--was that general principles of intelligence were insufficient... there was considerable resistance to that idea. Edward Feigenbaum and his \[DENDRAL group\] were coming to the same conclusion, but they felt very lonely in that discovery. Joel Moses, whose thesis had relied on expertise instead of general principles, remembers the frustration of trying to expound that point of view. "Papert almost cried once," Moses remembers. "He said, 'How can you get those guys to listen?' That was 1966, maybe 1968." But the robots seemed to prove the view beyond a shadow of a doubt.
>
> [@mccorduckMachinesWhoThink2004, page 269--276]

But since AI is whatever doesn't work yet, we will focus on what barely worked: its vision. Shakey's natural habitat was a toy model of an office space, designed specifically to be as easy to see as possible. If computer vision could not approach the real world, the real world could approach computer vision.

> The project team was well aware of Shakey's limited mechanical and sensory capabilities, and designed a correspondingly simple experimental environment consisting of half a dozen rooms populated with large, geometric blocks. The blocks were painted so that edges were visible to the low-resolution TV camera, while still being sufficiently reflective for our homemade laser rangefinder to work. We also used dark baseboards, again for visibility, and exploited them to update the position error that accumulated in the dead reckoning process that relied on Shakey's stepping motors.
>
> [@kuipersShakeyConceptionHistory2017]

Shakey's vision system was "scene analysis". In short, it analyzes a picture as a geometer would expect. It starts by discovering edges, then performs some computational geometry on them to recover outlines of basic shapes, such as cubes, then fills in the faces, then fills in the bodies between the faces. The algorithm had to do something clever to deal with occlusions. In the end, a complete 3D scene populated with 3D objects is recovered.

![Scene analysis. Figure from [@guzmanDecompositionVisualScene1968]](figure/guzman_1968.png)

After converting a grayscale image to a line drawing, the system uses a "formal grammar" of line drawings (look up "Huffman--Clowes scene labeling") to identify 4 kinds of objects: wedge, cube, wall, floor. For example, `Wedge = TRIANGLE OR TRIANGLE + QUAD`, while `Cube = QUAD + QUAD + QUAD`. The more grammatical rules, the more object classes the system could identify.

![The same 4 scenes in: digitized grayscale image, divided into sets of equal grayscale level, merged into regions, converted to line drawings. [@briceSceneAnalysisUsing1970, figure 7, figure 8, figure 12, figure 14]](figure/Shakey_vision.png)

One might question why they decided to identify only 4 kinds of objects. The reality is that it is simply very difficult to perform scene analysis on anything more complicated. Indeed, in a 1972 PhD thesis, David Waltz extended scene analysis to shadows, and had to catalog "thousands of junctions, in order to deal with cracks and shadows". [@waltzSheddingLightShadows1972, page 48; @nilssonQuestArtificialIntelligence2009, page 185]

![The junction rules used in analyzing a cube with a shadow. [@waltzSheddingLightShadows1972, page 54]](figure/Waltz_1972_scene_analysis.png)

Still, since Shakey's computer ran at just 0.3 MIPS, it was doing the best it could. As compute improves, the best paradigm changes from the logical approach, to the statistical, to the neural. This was another early instance of the bitter lesson.

![A particular example computer vision system `ISUPPOSEW` (1971) that processes the scene as a list of line segments. I picked this mainly for its aesthetic quality. [@raphaelResearchApplicationsArtificial1971, page 197]](figure/ISUPPOSEW.png)

Other than the difficulty of programming (enumerating thousands of rules can't be fun), it was also very brittle. Moravec reports an early 1977 robot that featurizes camera images taken between two steps in the robot trajectory and compares them to triangulate the scene. It only worked on very clean and uncluttered scenes with sharp images:

> The program would take a picture and choose up to a hundred features. It would then drive the robot forward about a meter, stop, take another picture, and search for the same features in the second image. Then it would invoke the camera solver to find the robot movement and the three-dimensional locations of the features that explained their apparent motion from one image to the other. Despite much fine-tuning, the program's error rate never dropped below about one wrong motion solution in four, meaning the robot could move about four meters before becoming confused about its position--discouraging. The camera solver repeatedly tweaked an estimate of the robot's motion to make the features line up as well as possible, and threw away those that seemed too far off... 10--20% of the feature matches were wrong, often because an area chosen in a first image had, in a second image, been eclipsed or changed in appearance by point-of-view or lighting effects or camera noise... It was necessary to track about one hundred features to succeed even three steps in four, consuming several minutes of computer time. Months of fiddling with the program's mathematics and assumptions made little difference.
>
> [@moravecRobotMereMachine1999, page 30]

In fact, there was a little bit of faking in the demo movie:

> its most impressive feat -- moving a wedge to a block, ascending it, and pushing off a smaller block -- was recorded on film piecemeal, requiring multiple takes -- and several hours -- for each error-prone stage... Shakey's vision programs, as most others of the time, reduced images to a short list of geometric edges before doing anything else. The approach was quite inappropriate for outdoor scenes containing few simple edges, but many complicated shapes and color patterns.
>
> [@moravecRobotMereMachine1999, page 26 -- 28]

In 1973, Duda and Hart from the Shakey team published the famous "Duda and Hart" book on pattern classification [@dudaPatternClassificationScene1973]. The book contained two halves. The first half was statistical: Bayes, nearest neighbors, perceptron, clustering, etc. The second half was on scene analysis. It is instructive to compare the first edition with the second, published in 2001 [@dudaPatternClassification2001], almost completely statistical. There were new chapters on neural networks, Boltzmann machines, decision trees, and so on. In contrast, scene analysis was completely removed. It says something about the obsolescence of scene analysis even in 2001, as Duda and Hart deleted half of their most famous book just to avoid talking about it. In fact, the only mention is a condemnation:

> Some of the earliest work on three-dimensional object recognition relied on complex grammars which described the relationships of corners and edges, in block structures such arches and towers. It was found that such systems were very brittle; they failed whenever there were errors in feature extraction, due to occlusion and even minor misspecifications of the model. For the most part, then, grammatical methods have been abandoned for object recognition and scene analysis.
> 
> [@dudaPatternClassification2001, section 8.8]

Concurrent to Shakey, in 1966, some undergraduate students were assigned to constructing "a significant part of a visual system" in a single summer. Of course, it failed at this task, as vision turned out to be the part of AI least amenable to logical approaches.

As of 2007, such logical AI programs for vision had been completely obsoleted by statistical methods:

> one prominent vision researcher told me that the "residue of model-based vision is close to zero", and another told me that "most current robotic systems use vision hacks" instead of general-purpose, science-based scene-analysis methods.
>
> --- [@nilssonQuestArtificialIntelligence2009, page 20]

## That Oriental Threat

### Japan was Number One

> Tactically, the attack on Pearl Harbor was one of the most brilliantly executed strokes of the war.  But politically, it was most unwise, for America fought back. Today, 40 years after the end of World War II, the Japanese are on the move again in one of history’s most brilliant commercial offensives, as they go about dismantling American industry. Whether they are still only smart, or have finally learned to be wiser than we, will be tested in the next 10 years. Only then will we know who finally won the war 50 years before.
>
> [@whiteDangerJapan1985]

In 1980, the Japanese was ready to take on the world. Despite the devastation of war, Japan quickly emerged as a peer competitor to America, leading to the Japan-panic symbolized by [*Japan as Number One* (1979)](https://en.wikipedia.org/wiki/Japan_as_Number_One%3A_Lessons_for_America). Cyberpunk stories tell of a neon future with equal mindshare in English and Japanese, where massive zaibatsus battle with conglomerates over the emergent planetary commercium. By GDP, Japan became Number 2 in 1968, and by 1990 it was just 30% smaller than America's, and it had been growing about 1%/yr more. At this rate, it would have become Number 1 in 2020.

We have been robbed of this future. Japan's GDP reached a high point in 1995, and promptly got lost for [~~10~~ ~~20~~ 30 years](https://en.wikipedia.org/wiki/Lost_Decades). Many people gave many reasons, such as the liquidity trap, the demographic crisis, the Plaza Accords, and so on. For our purposes, we just need to know that Japan had been knocked out of the AI game after this.

### The Fifth Generation

Like how the American ARPA pushed behind the development of AI and computing, the Japanese [MITI](https://en.wikipedia.org/wiki/Ministry_of_International_Trade_and_Industry) had coordinated the development of many industries in Japan, and seeing the rise of expert systems, the time seemed ripe for a coordinated attack on AI. On the back of great optimism, the Fifth Generation Computer System (FGCS) project launched in 1982, led by Kazuhiro Fuchi. Its goals was ambitious to the point of revolutionary, and so was its schedule.

> Even those who adore--the word is not too strong--their unusual director are often dismayed by him. A month after the center formally opened, the hardware committee met with Fuchi and showed him the fast-track two-year plan they'd devised for producing the prototype hardware scheduled for the first three-year phase. Instead of being pleased, Fuchi flew into a rage. That alone is unusual enough among Japanese managers, but what Fuchi wants is even more upsetting: cut that schedule down to a year and a half, he demands. The hardware committee is in shock. They already think themselves reckless in their two-year schedule. Fuchi will have none of it. "We have to manage to do this!" he says angrily. After a little while he calms down. "Go and think it over," he says more reasonably. "If you absolutely have them. But see if it can't be done in a year and a half. Loosen up on the quality assurance and give me a real machine in a year and a half."
>
> ...
> 
> Everybody knows that Fuchi has irrevocably resigned from his former post at the Electrotechnical Laboratory, a startling step for any Japanese employee, all the more one with such seniority. A high roller, he's placing all his bets on the Fifth Generation project. The legends add that Fuchi would have been eligible for a comfortable government pension if he'd only waited two or three months to resign his position at ETL, but he spurned anything so trivial as personal financial security to delay his project even by months. This is sensational to the young researchers who have grown up in the lifetime employment system of Japan.
> 
> [@feigenbaumFifthGenerationArtificial1984, pages 110--112]

::: {#fig-fgcp-orgchart layout-ncol=2}

![[@aisoFifthGenerationComputer1988, figure 6]](figure/FGCS_orgchart_1.png)

![[@feigenbaumFifthGenerationArtificial1984, page 123]](figure/FGCS_orgchart_2.png)

Overall plan of the FGCS.

:::

The name "fifth generation" came from the idea of 4 generations of hardware: vacuum tube, transistor, integrated circuits, VLSI. The generation following these would be the fifth, consisting of massively parallel computers natively running logical programs, including logical AI systems. The project had the following parts: parallel logic programming, hardware, and AI.

The project saw the future, and it would be parallel logic programming, for which they decided on a variant of [Prolog](https://en.wikipedia.org/wiki/Prolog) that uses "flat guarded Horn clauses". [@feigenbaumJapaneseNationalFifth1993] But why did they pick an obscure language, not the famed Lisp?

> "Prolog can be seen as an extension of Lisp." He says Prolog provides the extra functionality of “pattern matching and non-determinism,” and is "capable of integrating interesting features of other languages such as Smalltalk, PS, and APL."
>
> [@warrenViewFifthGeneration1982]

Vague, but that is as good an explanation as I could find.

In terms of hardware, they planned for parallel computers that runs Prolog operations natively, aiming to eventually reach a target of $10^9 \;\mathrm{LIPS}$ -- 1 giga logical inferences per second.[^lips-definition] They also aimed for VLSI chips of $10^7$ transistors per chip. Since in 1982, a standard computer could perform Prolog operations at $10^5 \;\mathrm{LIPS}$, this implied they planned for a doubling time of 9 months. [@bramerFifthGenerationAnnotated1984, page 6]

[^lips-definition]: A "logical inference" is technically defined as one Prolog procedure call. More intuitively, it is roughly equivalent to one atomic logical operation, such as performing one *modus ponens*, one variable substitution, etc.

They aimed to develop multiple AI systems, each of which would be an expert system written in their version of Prolog. The inference engine would be a standard logic programming system, similar to the planner used in Shakey. The knowledge base was the key. It would be written in a "knowledge-base management software using relational algebra", which probably meant a [relational database](https://en.wikipedia.org/wiki/Relational_database). They planned that, at the end of the project, they would have the software and the hardware to handle expert systems with $10^4$ rules and $10^4$ objects, with "semi-automated" knowledge acquisition. [@warrenViewFifthGeneration1982; @feigenbaumJapaneseNationalFifth1993]

Specific plans they had for AI systems [@feigenbaumJapaneseNationalFifth1993; @moto-okaFifthGenerationComputers1984; @feigenbaumFifthGenerationArtificial1984, pages 121--131; @bramerFifthGenerationAnnotated1984, page 6; @feigenbaumRiseExpertCompany1988, pages 202--203]:

* Computer-readable dictionaries with around 2 million entries in total. These would include a word dictionary with 800K words, a "concept classification dictionary" of 400K concepts, including a general thesaurus, and a "concept description dictionary" of another 400K concepts. These would serve as a general-purpose knowledge base that any expert system could utilize.
* English-Japanese translator with vocabulary size ≥100,000, at ≥90% accuracy (with the last 10% fixed up by humans).
* Continuous ASR with vocabulary size ≥50,000 words and 95% per-word accuracy from at least "a few hundred" speakers, in both English and Japanese, at ≤3× real time.
* A question-answering system built on top of the ASR system. It would converse with the user with synthesized speech in Japanese or English. The first planned system would handle document queries in the computing literature. It would have vocabulary size ≥5,000 and ≥10,000 inference rules. After its success, more systems would be built for other professional fields.
* Natural language processing with vocabulary size ≤100,000, and ≤2000 grammatical rules, achieving ≥99% accuracy in syntactic analysis.
* An image processing system with hardware featurizers, that can store ≥10,000 pieces of graphic and image information in its knowledge base. It would be applied to computer-aided design and manufacture and analysis of aerial and satellite images, medical images, etc.

### Autopsy

After 10 years and \$400M, the FGCS wrapped up in 1992. The MITI announced that it would give away the developed software for free. Takers were few among academia, and none among business. Even Feigenbaum regarded it as a failure, when he had motivation to present it in the best light, as the unofficial face of the expert system approach.

> The problem for Japan is that the computer industry shifted so rapidly that the technological path the Fifth Generation took -- which seemed a wise choice in 1982 -- turned out to be at odds with the computer industry's direction by 1992. In a sense, Japan's ability to stay the course in pursuit of a long-term payoff -- usually considered one of the country's strongest assets -- turned into a liability. A similar challenge for Japan may now be arising in high-definition television. Japan's HDTV system, which has been in development for two decades, is now coming to market just as some engineers believe that a major shift to digital television technology will make the Japanese analog approach obsolete.
> 
> ... "If it had really caught on, the Japanese companies would not have let it go," said Edward Feigenbaum... While the project developed some interesting computer designs and software, he said, even in Japan "no one is using the technology."
> 
> [@pollackFifthGenerationBecame1992]

> On the research side, it made little progress in AI, NLP, natural interface, knowledge acquisition, construction of large knowledge bases. On the commercial side, its architectures were a commercial failure as they require the user to use an unfamiliar paradigm (parallel logical programming) on hardware that is not better than standard hardware. The promised applied expert systems were not developed because the research did not progress as planned.
> 
> [@feigenbaumJapaneseNationalFifth1993, table 2]

On the software side, they had 100 MLIPS machines, but no expert system to run with. Like the American expert systems companies, the FGCS researchers found that knowledge bases simply did not reach up to that scale.

> ICOT set out to build a machine that could provide upwards of a 100 MLIPs of 'symbol crunching' power. However, there was no application whose clear need for such horsepower drove the development and whose success (or failure) would serve as the tangible evaluation of the effort. From our current perspective it is clear that there could not have been such an application, since any such application would necessarily have had to contain very large stores of knowledge; but in 1982 when the project began, there were no techniques available for capturing and managing such a large knowledge base. Even today, after more than a decade of research into knowledge representation, we have only a small base of experience and very few tested techniques for this task.
>
> [@feigenbaumJapaneseNationalFifth1993]

On the hardware side, the fastest systems they built were the PIM/p with 512 processors at peak performance 156 MLIPS, and PIM/m with 256 processors at 153 MLIPS. Both had clock frequency 16 MHz. They sticked to the original estimate, made at the start of the project, that their logic-specific chips could run logical inferences at approximately 100 times faster than a general computer. [^pim-p-citation]

![The PIM/p computer with 512 processors. [Source](https://www.ueda.info.waseda.ac.jp/AITEC_ICOT_ARCHIVES/ICOT/Museum/MACHINE/pim-spec-J.html)](figure/PIM-P-512.jpg)

[^pim-p-citation]: 
    > たとえば、512 台のプロセッサを持つ PIM モデル p ではピーク性能で156 MLIPS (1秒間に1億5600万回の推論処理を行なう速度)、256 台のプロセッサを持 つモデル m では153 MLIPSという、汎用大型機の約100倍に当たる世界最高の 推論処理速度を達成している。実際の応用システムでの利用でも、アルゴリズ ムの工夫とあいまって、512 台にいたるまでほぼプロセッサの台数に比例する並 列処理効果を得ている。
    > 
    > [第五世代コンピュータ・プロジェクト 最終評価報告書: 並列記号処理ハードウェア技術](https://www.ueda.info.waseda.ac.jp/AITEC_ICOT_ARCHIVES/ICOT/Museum/FinalReport/node17.html#SECTION03022300000000000000)

    [Top Page for FGCS Museum: ５つのＰＩＭの概要](https://www.ueda.info.waseda.ac.jp/AITEC_ICOT_ARCHIVES/ICOT/Museum/MACHINE/pim-spec-J.html)


![The LIPS of computers produced by the FGCS at its ending in 1992, with forecasts up to 2000. [@furukawaFifthGenerationComputer1993, figure 7]](figure/FGCS_scaling_plot.png)

::: {.callout-note title="1 LIPS = 100 OPS?" collapse="true" }

From the start of the project, they assumed that 1 LIP takes about 100 operations on a conventional computer. They also planned that, at the end of the project, there should be a machine with 1000 processors achieving 1 GLIPS, implying at least 1 MLIPS per processor. [@feigenbaumJapaneseNationalFifth1993]

According to [@bramerFifthGenerationAnnotated1984, page 6], as of 1984, a conventional computer cound run Prolog at 10--100 KLIPS, and that 1 LIPS = 100--1000 OPS. Now, they did not state what counts as "conventional", but probably they meant something between Sun-1 (0.5 MIPS) and Cray-1 (150 MIPS). Unfortunately, this gives us a ridiculously wide range of 1 LIPS = 5--1500 OPS.

As far as I see, they probably just got that number by running some Prolog programs on the computers they have available in 1982. Fortunately, someone did exactly this experiment and found that 1 LIPS = 53 OPS as of 1990 [@kellerGigaLIPFastEnough1990], so the 1 LIPS = 100 OPS rule is good enough.

:::

Now, assuming that 1 LIPS = 100 OPS, then to reach 1 GLIPS on a conventional computer would require 100 GFLOPS. Now, according to [TOP500 list of 1993-11](https://www.top500.org/lists/top500/1993/11/), the second-fastest computer in the world was a 1024-core [Connection Machine](https://en.wikipedia.org/wiki/Connection_Machine)-5 (developed by the [Strategic Computing Initiative](#sec-sci)), which ran at 60 GFLOPS. So just by making a parallel computer with 1000 standard floating-point processors, they made something that reached the Japanese target of 1 GLIP/sec, without ever dealing with specialized hardware! Yet another example of specialized hardware overtaken by general computers, like Rosenblatt's [Tobermory](https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/index.html#the-perceptron-controversy-1960s), and the [Lisp machines](https://en.wikipedia.org/wiki/Lisp_machine).

> The PIM hardware seems destined for the same fate. The processing elements in the PIMs have cycle times no better than 60 ns \[= 16 MHz\]... this leaves the uniprocessor performance lagging behind the best of today's conventional microprocessors \[like [i486 at 50 MHz](https://en.wikipedia.org/wiki/I486)\]. Both HP and DEC have announced the imminent introduction of uniprocessors of between 100 and 200 MIPS. The interconnection networks in the PIMs do not seem to constitute an advance over those explored in other parallel systems. Finally, the PIMs ... do not have floating point hardware. While the interconnection networks of the PIMs have reasonable performance, this performance is comparable to that of commercial parallel machines in the US such as the CM-5.
> 
> But these days, few people want specialized computers for artificial intelligence, preferring powerful general-purpose machines like those made by Sun Microsystems... And a host of scrappy American companies have sprung up to sell massively parallel computers with tens of thousands of processors, far more than the Fifth Generation machines.
>
> [@pollackFifthGenerationBecame1992]

Optimistically, MITI launched two more projects at the end of FGCS, with even more vague names ["Real World Computing Partnership"](https://web.archive.org/web/19961220074411/http://rwcp.or.jp/) and "Sixth Generation Computing Project" on neural network computers with ≥10,000 optical-computing processors. [@crossJapaneseClashComputers1990][^japanese-vagueness]

[^japanese-vagueness]: Japanese bureaucracies seem to be particularly vague, even in scientific projects like the FGCS. In *Japan as Number One* (1979), Vogel mentioned the same thing about the vagueness in government policy plans. He argued it was because of ["Nemawashi"](https://en.wikipedia.org/wiki/Nemawashi), where each plan must offend nobody and everyone gets a "fair share".

Though the FGCS project was a failure, and Japan no longered mattered in AI since its end, it had an indirect effect. Feigenbaum and the long-time AI journalist [Pamela McCorduck](https://en.wikipedia.org/wiki/Pamela_McCorduck) coauthored a book [@feigenbaumFifthGenerationArtificial1984] that became the hottest book inside the Beltway. Ominously, they warned of imminent Japanese domination:

> We calculate that in 1982, the aggregate spent in the United States on artificial intelligence research from all sources -- governmental and private -- was about 50 million. This is just about equal to the amount the Japanese government expects to spend on an average per year over the next ten years for its Fifth Generation (and does not count Japanese internal industrial AI support that may double or triple the amount). If we continue as we have, we two nations will act as guinea pigs for an interesting experiment in planned, as opposed to unplanned, research... The Japanese have announced that in ten years they will produce knowledge information processors. Several options are open to Americans, but few of them offer truly palatable alternatives to undertaking our own version... The United States should form a national center for knowledge technology... The center we propose would be an expression and institutional embodiment of national will \[like NASA\].
>
> [@feigenbaumFifthGenerationArtificial1984, pages 261--269]

The hardware developed, but there was no use for it. Parallel logic programming was difficult for the outside programmers, used to serial imperative programming. 

And they were not the first to raise the alarm:

> In January 1981, Professor [Arvind](https://en.wikipedia.org/wiki/Arvind_(computer_scientist)) of MIT returned from Tokyo with an early report on the Fifth Generation Project, the same report Feigenbaum had put in his "to read sometime" pile at Stanford a few months earlier. Arvind showed it to [Michael Dertouzos](https://en.wikipedia.org/wiki/Michael_Dertouzos), a professor and director of the MIT Laboratory for Computer Science. Dertouzos recollected in notes: "I panic. My colleagues are (way too) relaxed about it and tell me that I am over-reacting." One of the things that troubled Dertouzos was the similarities between the Japanese plan and long-range plans at MIT.
>
> [@feigenbaumFifthGenerationArtificial1984, pages 199]

Partly as a result of this, multiple countries launched their own national AI projects,[^fgcs-list] the most notable of which was the [Strategic Computing Initiative](#sec-sci), funded by the DARPA, which would be much more successful.

[^fgcs-list]: FGCS (Japan, 1982--1992), Alvey Programme (UK, 1984--1990), ESPRIT programs (Europe, 1983--1998), and the Strategic Computing Initiative (America, 1984--1993).

## Early war machines

The dream of peace produces war machines. Ancient Athenians dreamed of [Talos](https://en.wikipedia.org/wiki/Talos), a bronze giant who defended Crete, while the Jews of Prague dreamed of [Golem](https://en.wikipedia.org/wiki/Golem#Etymology). The ages has been too kind to Zhuge Liang, and his wheelbarrow, invented for carrying war supplies, became the legend of [wooden robot oxen](https://en.wikipedia.org/wiki/Wooden_ox).

The industrial revolution produced early ideas of feedback mechanism, such as the [centrifugal governor](https://en.wikipedia.org/wiki/Centrifugal_governor) and the [gyro autopilot](https://en.wikipedia.org/wiki/Gyroscopic_autopilot). It took until WWII for someone to put together high explosives, slow burning fuel, and the feedback mechanism, to create the cruise missile. The first of its kind was the V-1 rocket, which already incorporates the basic features of all cruise missiles.

The V-1 looks like a small unmanned jet airplane. It has the following senses: roll (gyro), pitch (gyro), yaw (magnetic compass), altitude (barometer), distance traveled (vane anemometer). The roll, pitch, yaw, and altitude are maintained by negative feedback. So for example, if the missile is heading east to the set-point of yaw, a valve would open, and compressed gas would force the rudder to turn, which yaws the missile west. As soon as the vane anemometer has turned a designated number, the missile considers itself to have reached the target. It turns off the engine and sharply dives to the ground, and explodes upon impact.

```python
class V1Missile:
    def __init__(self, target_distance, setpoint_yaw, setpoint_pitch, setpoint_altitude):
        self.target_distance = target_distance
        self.setpoint_yaw = setpoint_yaw
        ...
        self.engine_active = True

    def read_sensors(self):
        self.current_roll = read_gyro_roll()
        ...
        self.distance_traveled = read_vane_anemometer_distance()

    def control_roll(self):
        roll_error = self.setpoint_roll - self.current_roll
        adjust_roll_control_surfaces(roll_error)
    ...

    def run_guidance(self):
         if self.distance_traveled >= self.target_distance:
            self.engine_active = False
            adjust_dive_control_surfaces()
            time.sleep(calculate_impact_time(self.current_altitude))
            explode()
         else:
             self.read_sensors()
             self.control_roll()
             self.control_pitch()
             self.control_yaw()
             self.control_altitude()
```

Other than V-1 and V-2, there were no autonomous war machines during WWII, though there were several radio-controlled weapons such as [explosive little tanks](https://en.wikipedia.org/wiki/Goliath_tracked_mine) and [little planes for target practice](https://en.wikipedia.org/wiki/Radioplane_OQ-2). Skinner, thinking outside (inside?) the box, worked on [Project Pigeon](https://en.wikipedia.org/wiki/Project_Pigeon). He trained pigeons in skinner boxes to peck at the ship appearing on a screen. If the pigeon is pecking on the top-left, then the missile would turn to the bottom-right. In effect, the pigeon becomes the negative feedback controller. Though it was cancelled, it would have been considerably cheaper than [the Japanese version](https://en.wikipedia.org/wiki/Kamikaze) should it have ever reached production.

<video controls width=100%>
  <source src="figure/Project%20Pigeon.webm" type="video/webm" />
</video>

After WWII, some autonomous defense systems were developed and deployed, such as [close-in weapon systems](https://en.wikipedia.org/wiki/Close-in_weapon_system) on ships. These detect incoming incoming missiles and enemy aircraft by radar, computes their trajectories, and shoots them down. Since they must operate on the time-scale of seconds, they are fully automatic with no human in the loop. Despite this, these are quite uncontroversial and do not typically earn the title of "killer robots", presumably because compared to autonomous *offense*, defense is inherently more controllable and predictable in effect.

## Nuclear war machines

> Deterrence is the art of producing in the mind of the enemy the fear to attack. And so, because of the automated and irrevocable decision making process which rules out human meddling, the doomsday machine is terrifying. It's simple to understand. And completely credible, and convincing... When you merely wish to bury bombs, there is no limit to the size. After that they are connected to a gigantic complex of computers. Now then, a specific and clearly defined set of circumstances, under which the bombs are to be exploded, is programmed into a tape memory bank.
>
> --- [Dr. Strangelove](https://en.wikipedia.org/wiki/Dr._Strangelove) (1964)

During the Cold War, the following technological factors of nuclear weapons determined the grand nuclear strategy.

1. First-strike nuclear offense is impossible to defend against. Some bombs will get through all defense.
2. Nuclear weapon is so much more powerful than non-nuclear weapons, that the only proportionate deterrence to a nuclear attack is another nuclear attack.
3. First-strike capability is indistinguishable from second-strike capability.

Because of (1) and (2), the only way to deter a nuclear first-strike was to threaten a nuclear second-strike. This is the [MAD](https://en.wikipedia.org/wiki/Mutual_assured_destruction) nuclear deterrence doctrine. Because deterrence requires enough bombs to survive a first-strike, both sides would rather build up more second-strike bombs than the other side's first-strike bombs. Because of (3), there aren't "second-strike bombs" vs "first-strike bombs", only bombs. Therefore, we have a positive feedback loop where both sides aim to have more bombs than the other -- the [nuclear arms race](https://en.wikipedia.org/wiki/Nuclear_arms_race). Because having too many bombs increases the chance of accidents, both sides are motivated to slow down the race. Thus the [ABM Treaty](https://en.wikipedia.org/wiki/Anti-Ballistic_Missile_Treaty), where both sides agree to *not* build many missile defense systems! This paradoxical treaty was designed to make both sides *more* vulnerable to second-strike, meaning that less bombs are needed to ensure second-strike capability, thus complementing the [SALT treaties](https://en.wikipedia.org/wiki/Strategic_Arms_Limitation_Talks) that limited the number of bombs.

Both sides' nuclear technology went through several iterations, with increasing second-strike capability, and ended up with the "nuclear triad" of bombers that stay in the air 24/7, submarines hidden under the sea, and ICBMs hardened inside silos. Each of the three has different tradeoffs, necessitating all three to be maintained.

Of course, even if the triad survives the first-strike, it is no good if they won't activate. The command center might be destroyed. The communication lines might be cut. The soldiers might refuse to launch based on their own conscience. All these dangers lead to the pressure to automate second-strike. The pinnacle of this logic was the [Supersonic Low Altitude Missile (SLAM)](https://en.wikipedia.org/wiki/Supersonic_Low_Altitude_Missile), a cruise missile powered by a nuclear engine. The nuclear engine is like a fission nuclear reactor in nuclear power plants, except that the fission power does not boil water, but heat up air, which expands and shoots out from the tail of the missile, allowing it to fly at Mach 3.

Despite having no GPS (it was the 1960s!), because the SLAM would fly $\sim 200 \;\mathrm{m}$ above ground, it could navigate itself by [terrain contour matching](https://en.wikipedia.org/wiki/TERCOM): It compares a height-scan of the local terrain against a stored copy of the terrain. Even after dropping all its nuclear warheads, it can remain airborne for weeks, destroying the ground with sonic booms as overkill. The project was shelved in 1964, apparently considered too destabilizing, and they settled for just drilling the nuclear launch routines into the missileers until they work like robots that would not hesitate to execute the launch command.

![The terrain contour matching algorithm. By a curious coincidence, the algorithm typically attempts to find a line segment within the stored terrain map that minimizes the [Mean Absolute Deviation](https://en.wikipedia.org/wiki/Average_absolute_deviation) with the local terrain of the missile... also with the "MAD" acronym. [@goldenTerrainContourMatching1980, figure 2]](figure/TERCOM.png)

In the movie *Dr. Strangelove* (1964), nuclear deterrence was taken to its logical end point. In the movie, the Soviet Union built a "doomsday machine", which is a Cobalt bomb that when exploded, makes enough fallout to render the entire earth uninhabitable for a century. This was then connected to sensors around the Soviet Union, so that any nuclear attack automatically triggers it. Finally, the machine triggers if it detects attempts to un-trigger it, thus closing the logic loop and making it a fully automatic deterrence machine.[^dead-hand]

[^dead-hand]: I was going to write something about the [Dead Hand](https://en.wikipedia.org/wiki/Dead_Hand) system, but after a brief search, the available information looks too much like conspiracy theory and rumors, so I will not.

## That tank thing again

The previous sections has made it clear that ARPA has been inevitable behind every AI development since the 1960s. Indeed, ARPA funded 75--95% of all AI projects during the 1960s [@guiceControversyStateLord1998]. Though this essay is on logical AI, a brief detour about neural network is in order.

During the Cold War, intelligence, in the sense of intelligence-gathering and reconnaissance was a vital area of artificial intelligence. Some of the early neural networks, such as MINOS II, was explicitly built with an objective of scanning aerial photographs for interesting military targets like tanks. [@nilssonQuestArtificialIntelligence2009, pages 98--109] The CIA even [experimented with Rosenblatt's Mark I Perceptron machine](https://www.cia.gov/readingroom/document/cia-rdp78b04770a002300030027-6) for the same purpose [@irwinArtificialWorldsPerceptronic2024]. As an example, [@kanalRecognitionSystemDesign1964] describes a two-layered perceptron network, of type $\R^{N \times N} \to \{0, 1\}^{32\times 32} \to \{0, 1\}^{24} \to \{0, 1\}$. It works as follows:

* The grayscale photo is down-scaled and binarized by convolution with a [discrete Laplace filter](https://en.wikipedia.org/wiki/Discrete_Laplace_operator): $\R^{N \times N} \to \{0, 1\}^{32\times 32}$.
* The weights for the 24 hidden perceptrons are constructed by [linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis): $\{0, 1\}^{32\times 32} \to \{0, 1\}^{24}$
* The output perceptron is learned by the [perceptron learning rule](https://en.wikipedia.org/wiki/Perceptron#Learning_algorithm_for_a_single-layer_perceptron): $\{0, 1\}^{24} \to \{0, 1\}$.

::: {#fig-kanal-1964-neural-tanks layout-ncol=2}

![Grayscale photos, some containing tanks, and some not.](figure/kanal_1964_fig_tank_nontank_mosaic.png){#fig-kanal-1964-neural-tanks-tank-nontank-mosaic}

![A picture of a tank after convolution with a discrete Laplace filter.](figure/kanal_1964_fig_binary_image_tank.png){#fig-kanal-1964-neural-tanks-binary-image-tank}

![The architecture of the network.](figure/kanal_1964_fig_architecture.png){#fig-kanal-1964-neural-tanks-architecture}

Images from [@kanalRecognitionSystemDesign1964].
:::

Neural networks (such as Minsky's [SNARC](https://en.wikipedia.org/wiki/Stochastic_Neural_Analog_Reinforcement_Calculator) and Rosenblatt's perceptrons) were funded by the Office of Naval Research (ONR) on the order of \$50K, while logical AI was funded by ARPA, whose contracts were on the order of \$500K. This made a real difference in the days of mainframes. [@guiceControversyStateLord1998] See my essay on the [*Perceptron Controversy*](https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/) for details.

## Strategic Computing Project {#sec-sci}

The [Strategic Defense Initiative](https://en.wikipedia.org/wiki/Strategic_Defense_Initiative) (SDI), better known as "Star Wars", was announced by President Reagan in 1983. It aimed to shoot down Soviet ICBMs during their spaceflight with tools including lasers, particle-beam weapons, and ground and space-based missile systems. If this was successful, it would cripple Soviet second-strike capability. Although President Reagan claimed the project aimed to make nuclear weapons "impotent and obsolete", it would be incredible, even [idealists](https://en.wikipedia.org/wiki/Idealism_in_international_relations), to suppose that other countries would believe it. Indeed, the Soviet Premier Andropov immediately condemned this, suspecting it was in fact a plan to make America the only nuclear power immune to a second-strike and become the global hegemon.

Though the SDI did not bring the Star Wars, or make plans for AI, it had an indirect effect on intelligence in the age of war machines. In the same year, the Strategic Computing Project (SCI) began, and its funding was facilitated by the SDI, as well as the Japan scare from the Fifth Generation, which was the hottest topic in the Beltway.

> It is tempting to regard all this as just another skirmish in the trade wars, where engagements have already taken place in steel, automobiles, and consumer electronics... our national self-interest, not to mention our economic security, does not allow us this luxury. Information processing is an \$88-billion-per-year industry in the United States, and its loss would be disastrous. The default of this American industry, which has led the world for decades, would be a mortal economic wound... The superior technology usually wins the war – whether that war is martial, entrepreneurial, or cultural.
>
> [@feigenbaumFifthGenerationArtificial1984, pages 17--18]

It's war. America could not lose the AI race. The congress approved the SCI, a 10-year project that would end up costing \$1 billion funded by DARPA. The final plan was published in 1983-10 as [@darpaStrategicComputingNew1983].

![The SCI plan according to the bureaucrats. Yawn. [@rolandStrategicComputingDARPA2002, pages 77--78], which were cleaned up from [@darpaStrategicComputingNew1983, figure 4.1, figure 4.2]](figure/SCI_plan.png)

Other than the external reason of keeping up with the Japanese, there was an internal reason of SCI: The technology demands it. A large wave of computing is coming, and America could surf the wave into the age of machine intelligence, with the guidance of DARPA.

> In the early 1980s, advances in microelectronics design and manufacturing held out the promise of greatly improved computer chips. New concepts of computer architecture could conceivably exploit these chips in machines of unprecedented speed and power. Such computers might finally have the muscle necessary to achieve artificial intelligence -- machine capabilities comparable to those of human thought. This image of Strategic Computing credits the technology itself with driving the program. Ripe technology imposed an imperative to act.
>
> [@rolandStrategicComputingDARPA2002]

The SCI plan, like computer architecture, is a stack:

* Military applications
* General AI technologies: Speech recognition, natural language understanding, computer vision, and general expert systems.
* Computer architecture: Massively parallel computers (≥1000 processors at 1 teraFLOPS).
* Hardware: Better VLSI chips.

The project has 3 application areas: the "Battlem Management System" for the Navy, "Pilot's Associate" for the Air Force, and "Autonomous Land Vehicle" (ALV) for the Army. To develop these areas, it would develop 4 technologies: speech (for Pilot's Associate and Battle Management), language (for Battle Management), vision (for ALV), and general expert systems (for all). It would also develop massively parallel (≥1000 processors) teraFLOPS computers as its own bet on the fifth generation of computing.

How did it go? Spoiler: massively parallel computers at the teraOPS level succeeded, language abandoned, vision failed, self-driving cars improved but failed to reach the promise, generic expert systems failed, a few specific expert systems worked.

## Massively parallel

> Based on today's results and rationales, a petaflop before its time is as bad an investment as the teraflop before its time. Evolution and market forces are just fine... if we will just let them work.
>
> --- [Gordon Bell](https://en.wikipedia.org/wiki/Gordon_Bell) invoking the [Gods of Straight Lines](https://slatestarcodex.com/2018/11/26/is-science-slowing-down-2/) in 1992 [@bellUltracomputersTeraflopIts1992]

The planners recognized that the most demanding part of AI would be real-time computer vision, so SCI supported three types of massively parallel computers in the hopes that at least one of these would work for vision AI: [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) ([Connection Machines](https://en.wikipedia.org/wiki/Connection_Machine)), [MIMD](https://en.wikipedia.org/wiki/Multiple_instruction,_multiple_data) (the [BBN Butterfly](https://en.wikipedia.org/wiki/BBN_Butterfly)), [systolic array](https://en.wikipedia.org/wiki/Systolic_array) (the [Warp](https://en.wikipedia.org/wiki/WARP_(systolic_array))). Each of these was applied to "low-level vision", meaning that they convert the incoming stream of images into a featurized stream for higher-level logical processing [@weemsParallelProcessingDARPA1991]. Indeed, they estimated that to reach the final objective of "Reconnaissance in a dynamically changing environment. Ability to recognize targets and threats.", they needed a computer that ran at 1 teraOPS with 100 GB of memory.[^teraops-vision] Compared with this, they estimated that speech understanding required just 20 GLIPS (~1 teraOPS if we go by 1 LIPS = 50 OPS), and natural language understanding required a tiny 1 GLIPS. [@stefikStrategicComputingDARPA1985] There is also a vaguer objective of expert systems running 30,000 rules and firing 72,000 rules per second, which implies they expected each rule-firing to trigger 15 logical inferences on average.

Looking back from 2024, this was no coincidence: computer vision drove massively parallel programming in the 1980s, and it drove it again with gaming GPUs in the 2000s, and then again with convolutional neural networks around 2010. In each case, the requirement of "low-level" vision -- parallel processing over millions of pixels, with each pixel affected only by a few neighbors -- favored the "thousands of slow and dumb cores with small local memories, and talking with only neighbors" approach of connection machines in the 1980s, and then again with the modern GPUs. The surprise was simply that how much of vision is "low-level" -- we now have CNNs performing many "high-level" tasks like segmentation, height maps, object recognition, etc, with nary a global operation, feedback, or symbolic reasoning. The lesson I would take is that it is really hard to tell, without trying to make a machine to do it, whether a task is "high level" or "low level", and it is increasingly likely that there are no high level tasks.

[^teraops-vision]: Interestingly, [AlexNet](https://en.wikipedia.org/wiki/AlexNet) ran on two Nvidia GTX 580 with theoretical peak performance 1.6 TFLOPS. In [*When will computer hardware match the human brain?*](https://yuxi-liu-wired.github.io/docs/posts/1998-hans-moravec/), Hans Moravec argued that 10 MIPS for 1 hour is necessary for finding 3D objects in a cluttered bin well enough to pick the objects up, that is, to perform the [bin picking](https://en.wikipedia.org/wiki/Bin_picking) task. This translates to 36 GFLOPS for real-time vision. The 2022 SOTA vision system `OpenAI CLIP ViT-L/14` requires 150 GFLOP per forward pass, according to the formula from [Casson](https://www.adamcasson.com/posts/transformer-flops), so it requires 4 TFLOPS for real-time vision at 24 fps. All in all, the 1 TFLOPS estimate was so remarkably accurate for 1982.

> Compared to existing expert systems running 2,000 rules at 50--100 rules per second, SCI promised" multiple cooperating expert systems with planning capability" running 30,000 rules firing at 12,000 rules per second and six times real time.
>
> [@rolandStrategicComputingDARPA2002, page 195]

The previous scaling regime for supercomuting was Seymour Cray's crazy computers that pushed the clock rate to the limit. The Cray-1 (1972) had a single CPU at 80 MHz, and with pipeline parallelism could reach 240 MFLOPS. Since Cray thought silicon had reached the clock rate limit, he picked gallium arsenide for the Cray-3. The plan was to produce a system with 64 CPUs each capable of 1 GFLOPS.

The main rationale for massively parallel, distributed memory was two. One, low-level vision tasks, such as edge detection, are intrinsically local tasks. One does not need a Cray-level crazy CPU to run it, but can simply use many cheap slow chips running in parallel accessing local memories. This is particularly clear for Connection Machines, where each chip is a 1-bit processor and processes over its own RAM, and mostly communicates with just the neighbors.

THe 100 GB storage requirement was fairly easy, and it was simply a matter of scaling up the I/O speed and decreasing price. Connection Machines were shipped with [DataVaults](https://en.wikipedia.org/wiki/DataVault), which had up to 80 GB and I/O speed 320 MB/s.

Near the end of SCI, computers were within 10x of the teraFLOPS goal. Congress approved the [High Performance Computing Act of 1991](https://en.wikipedia.org/wiki/High_Performance_Computing_Act_of_1991) to fund the development of Internet infrastructure and more massively parallel computers -- rebranded as "high performance computing".

> By mid-1992 a completely new generation of computers have been introduced. Understanding this generation should make it possible to build the next-generation supercomputer class machine, that would reach a teraflop of peak power for a few, large-scale applications by the end of 1995.
>
> [@bellUltracomputersTeraflopIts1992]

![Progress towards the teraFLOPS computer. [@bellUltracomputersTeraflopIts1992, figure 1]](figure/Bell_1992_teraOPS.png)

The desired "1000 core computer" did come in 1991 as CM-5 at about 60 GFLOP/sec, 4300 FLOPS/\$ [@bellUltracomputersTeraflopIts1992], and a little late on schedule. The first teraFLOPS computer, [ASCI Red](https://en.wikipedia.org/wiki/ASCI_Red), arrived in 1996, with 9298 CPUs.[^asci-red] It was designed for simulating nuclear bomb tests. Indeed, since then 1993, almost all supercomputers in the world were massively parallel, as single-chip performance could not scale as fast as just packing more chips.

[^asci-red]: I couldn't find its price, but its successor ASCI White cost \$110 million, so if we assume ASCI Red cost \$100 million, then Gordon Bell was nearly right to predict that a teraFLOPS computer would sell for about \$40 million at 25 KFLOPS/\$ in 1995. [@bellUltracomputersTeraflopIts1992]

In short, the hardware delivered. The software mostly did not.

Other than massively parallel computers, there were also a few odds and ends. At the start of the program, there were brief efforts to get [gallium arsenide (GaAs)](https://en.wikipedia.org/wiki/Gallium_arsenide) chips working. Compared to silicon, GaAs has a wider bandgap, thus it tolarates higher temperatures and generally more extreme conditions. It started a running joke that GaAs would be the big silicon-chip killer in the next 10 years. 40 years later, it has yet to happen. And [Josephson junctions](https://en.wikipedia.org/wiki/Josephson_effect) were dropped from the final plan in 1983. 

More relevant was [MOSIS](https://en.wikipedia.org/wiki/MOSIS), an on-demand chip fabrication service that was affordable enough even for university projects, especially university courses that taught the new [VLSI](https://en.wikipedia.org/wiki/Very-large-scale_integration) method -- just send in the design and get your final project back! Indeed, Feng-hsiung Hsu fabbed earlier iterations of chess chips by MOSIS.

And the famed Lisp machines of Symbolics ended up with the same fate as the Japanese counterparts: why buy a specialized computer with a specialized language when you can wait a few years and get a general computer with a familiar language?

> By early 1986 researchers were choosing general-purpose work stations, such as the Sun Sparc, over LISP machines. The specialized architectures and low-volume production of the latter machines kept their prices near \$50,000 to \$100,000. Even with SC discounts, these machines were still three to four times as expensive as the new work stations entering the marketplace.. The computing power now available to individual researchers was unimagined by most when SC was first conceived. Capabilities then available only through time-sharing on mainframes could now be delivered by an affordable work station.
>
> [@rolandStrategicComputingDARPA2002, pages 144--145]

## Pilot's associate

There is little known about the Pilot's Associate project. The original plan called for 10,000-rule, real-time expert systems, animated displays with $10^8$ polygons per second, 200-word speaker-independent speech input system that works in high-noise environments, and a 1,000-word speech output system. Though the systems were demonstrated three times, there was no applications or follow-up. [@nilssonQuestArtificialIntelligence2009, pages 362--363] 

![Concept art for Pilot's Associate. [@rolandStrategicComputingDARPA2002, figure 7.1]](figure/Pilot_s_associate.png)

![The dataflow of Pilot's Associate in 1991, at its third and final demo. [@banksPilotsAssociateCooperative1991, figure 1]](figure/pilot_s_assoctiate_1991.png)

## Battle management

In the long-gone past, an army was like a clockwork toy. It can march, hold ground, or turn slowly. Frontlines were on the order of 1 km, since it would be hard to see the commander from farther away. The progress of warfare made the battlespace more complex.

Unsurprisingly, it was a secret thing. It was not described in detail anywhere. An expert system pioneer recalls:

> McCune: I built a system that I can't say very much about. It was a signal analysis system. I built it for $1 million. Five years later the boss of the boss of the boss of my client said, "Thank you, Brian. You saved me $500 million." That's what I'm talking about ROI. He said, "What other systems can you build for me?" I built him two more systems. So, it did pay off for the military.
> 
> [@allenExpertSystemsPioneer2018]

However, we know that the Navy did get what they wanted, and they did work.

> “Battle Management Ashore” was officially the Fleet Command Center Battle Management Program (FCCBMP). Like PA, FCCBMP was to consist of five expert systems. Each would perform a particular task for the headquarters of the Pacific Fleet at Pearl Harbor (Commander-In-Chief, Pacific Fleet, or CINCPACFLT ). First, the Force Requirements Expert System (FRESH) would monitor the readiness of the fleet and assist in allocating its forces according to the capabilities and status of the individual ships. The Capabilities Assessment Expert System (CASES), would compare the relative strength of United States and hostile forces. The Campaign Simulation Expert System (CAMPSIM) would simulate the outcome of different courses of action. The Operations Plan Generation Expert System (OPGEN) would develop operational plans according to specified strategies. Finally, the Strategy Generation and Evaluation Expert System (STRATUS) would assist in developing plans for theater-level strategy. 
> 
> [@rolandStrategicComputingDARPA2002, pages 218--219]

> In June 1986, FRESH Prototype One was installed in the test bed, and two months later it was demonstrated successfully to DARPA and the navy using the IRUS natural language generator developed by BBN, SAIC, and the Naval Ocean Systems Center (NOSC). At that time the IRUS vocabulary recognized 5,000 words, including proper names and naval terms; it successfully comprehended and responded to queries in terms usually used by the operations staff. By 1987 the enhanced system was performing in ninety minutes tasks that would usually take the CINCPACFLT staff fifteen hours. By then the navy employed the system routinely to monitor the readiness of its ships in the Pacific. 
> 
> [@rolandStrategicComputingDARPA2002, pages 265--266]

Lest some, out of moral integrity, claim that mixing military and AI only hurts everyone, there was one definite success story in practice: [DART](https://en.wikipedia.org/wiki/Dynamic_Analysis_and_Replanning_Tool).

> [Desert Shield/Storm](https://en.wikipedia.org/wiki/Gulf_War) in 1990--91 \[... had\] no prior buildup of troops or supplies. The US Department of Defense sealifted 2.4 million tons of cargo during the first six months of Desert Shield -- more than four times the cargo carried across the English Channel to Normandy during the D-Day invasion... About halfway through the six months of Desert Shield, Kral installed the system at the [USTRANSCOM](https://en.wikipedia.org/wiki/United_States_Transportation_Command) transportation command and the [US European command](https://en.wikipedia.org/wiki/United_States_European_Command), where it was used for the duration.
>
> [@hedbergDARTRevolutionizingLogistics2002]

We don't have any pictures of this, or even source code, as it is probably a state secret. Still, it is described as

> a GUI-based scheduler that took a mainframe flat file of the details of all items to be moved--dates to move, places to move to and from, and so forth--and loaded the data into an Oracle database. The scheduling was done on a front-end Sun-4 workstation.
> 
> [@hedbergDARTRevolutionizingLogistics2002]

And did it work?

> It enabled users to examine schedules at a higher level of abstraction, because it could readily aggregate modules. Planners could run strategic transportation models using DART in a matter of minutes rather than in hours or days. This enabled them to consider more alternatives and develop a more realistic action plan in far less time... The DART scheduling application paid back all of DARPA's 30 years of investment in AI in a matter of a few months, according to Victor Reis, Director of DARPA at the time.
>
> [@hedbergDARTRevolutionizingLogistics2002]

I guess one can interpret it as either "wars are really expensive" or "DARPA had invested too little in AI".

## Autonomous Land Vehicle

If the other parts of SCI was shrouded in mystery, the Autonomous Land Vehicle (ALV) part made up for this. The ALV project was designed for the army, with a budget of \$10 million contracted to multiple organizations, each taking care of part of the system. The project had yearly goals, starting with 10 km/h in 1985 and ending with 50 km/h in 1990.

::: {.callout-note title="The exact yearly goals" collapse="true" }

* 1985 - Road Following Demonstration: Vehicle traverses a 2 km preset route on a paved road at speeds up to 10 km/h. Forward motion only and no obstacle avoidance required.
* 1986 - Obstacle Avoidance Demonstration: Vehicle traverses 5 km road course at speeds up to 20 km/h; must recognize and maneuver to avoid fixed objects that are small with respect to road width.
* 1987 - Cross-country Route Planning Demonstration: Vehicle plans and executes a 5 km traverse of open desert terrain at speeds up to 5 km/h. Demonstrates soil and ground cover typing.
* 1988 - Road Network Route Planning and Obstacle Avoidance Demonstration: Vehicle plans and executes a 20 km point-to-point traverse through a road network at speeds up to 20 km/h using landmarks as navigation aids. Demonstration includes map updating and off-road maneuvering to avoid obstacles.
* 1989 - Cross-country Traverse with Landmark Recognition Demonstration: Vehicle plans and executes a 20 km traverse through desert terraín with obstacles at speeds up to 10 km/h. Demonstration includes replanning when confronted with impassable obstacles.
* 1990 - Mixed road and Open Terrain Demonstration: Vehicle plans and executes a 20 km traverse in wooded terrain with isolated obstacles and a 50 km traverse on paved and unpaved roads at speeds up to 50 km/h. Route planning includes multiple goals.

:::

What did the army expect to gain from this? Reconnaissance, "offensive and defensive missions" with a platoon of vehicles, and robotic variants of the [Armored Family of Vehicles](https://en.wikipedia.org/wiki/Armored_Systems_Modernization) that could execute missions "singularly, in packs, or in concert with manned vehicles", sometime after 1991. [@leightyDevelopingTechnologiesArmy1986]

Out of these, we focus on the Navigation Lab (Navlab) at Carnegie Mellon University, since that one had the greatest impact.

Navlab started work on ALV in 1984. The first vehicle they produced was Navlab 1 (1986). Its sensors included cameras, a sonar, a lidar, and an inertial guidance system (this was before GPS). Its only actuator was a stepped motor for steering. In the middle was a [Sun 3](https://en.wikipedia.org/wiki/Sun-3), a standard workstation of the time, and optionally a [Warp](https://en.wikipedia.org/wiki/WARP_(systolic_array)#Applications) computer, a [systolic array](https://en.wikipedia.org/wiki/Systolic_array) developed as part of SCI.

A Warp computer is made of a chain of processors. Each processor is programmable, and data "pulses through" the processors like blood flowing through vessels. This allowed it to perform data parallel computations, like 2D convolutions and other computer vision tasks. You can think of it as a GPU for the 1980s. In a 10-core Warp computer, each core ran at 10 MIPS, and the whole thing could run a neural network at 17 million "connections per second", meaning that it takes about $\frac{1.7 \times 10^7}{N}$ seconds to run one forward and backward pass over a neural network with $N$ weights. Impressively, this was 30\% faster than even the 65K-core Connection Machine-1. [@blellochNetworkLearningConnection1987; @pomerleauNeuralNetworkSimulation1988]

FIDO (Find Instead of Destroy Objects) was a [stereovision](https://en.wikipedia.org/wiki/Computer_stereo_vision) algorithm. It takes as input two cameras' images, and for each, uses a hand-designed "interest operator" to find landmark points, then use that to compute the 3D locations of the landmarks. These are sent to a path-planner. It ran at 4.8 sec/pass. No wonder the car could only drive 0.5 m/s. Work on FIDO stopped in 1987, since it was outclassed by the lidar. [@thorpeVisionNavigationCarnegie1990, chapter 14]

SCARF (Supervised Classification Applied to Road-Following) was developed since 1986. Schematically, the first version runs as follows. At each step, it compares each image pixel against a road color and an off-road color. It partitions an image's pixels into connected blobs of "probably road" and "probably off-road" by a simple Bayes classifier. It then converts the blobs into polygons, and compare each polygonal edge with an idealized road model (a projective transform of a circular arc), minimizing change compared to the previous frame's road model. It uses the best-fit road model to compute the new average on- and off-road pixel colors. It returns the fitted ideal road model, which is used to decide the turning angle.

In pseudocode:

```python
class SCARF:
    def __init__(self, ...):
        self.road_dist = (road_color_mean, road_color_variance)
        self.off_road_dist = (off_road_color_mean, off_road_color_variance)
        self.road_model = initial_road_model
        self.road_location = None

    def process_image(self, image):
        pixels = label_pixels(image, self.road_color, self.off_road_color)
        blobs = connected_components(pixels)
        polygons = fit_polygons(blobs)
        candidate_edges = select_road_edges(polygons)

        # update states
        self.road_model, self.road_location = fit_road_model(candidate_edges, self.road_model)
        road_pixels, off_road_pixels = divide_image(image, self.road_location)
        self.road_dist = (mean(road_pixels), variance(road_pixels))
        self.off_road_dist = (mean(off_road_pixels), variance(off_road_pixels))

        return self.road_model, self.road_location
```

SCARF ran at about 3 sec/pass. Later versions allowed multiple on- and off-road colors, and used [Hough transform](https://en.wikipedia.org/wiki/Hough_transform) to directly fit the edges of the idealized road model to image.

ALVINN, the first neural network driver, began in February 1989. It was trained off-line on a Warp, using purely generated images and driving instructions, presumably because they could not afford to store real images on disk. A training run took 8 hours on the Warp, but inference just took 0.75 s/image on a Sun 3, so they would train it on a Warp in the lab, and run it on a Sun 3 in the car. Soon, it set the Navlab speed record $1.3 \;\mathrm{m/s}$ on 1989-03-16.

In 1989-06, they were greatly surprised to find that ALVINN could be trained rapidly online, so they started doing runs where for the first 10 minutes a human would drive while the ALVINN is trained online in the Warp computer in the back of the car (often carrying around a graduate student watching it train), then ALVINN would take over control. This was possibly the first successful imitation learning application. They also found that, because a human driver would never go off the lanes, ALVINN would not know what to do if it starts going off the lane, so they programmed a data augmentation method by shearing the image left and right by 5 angles each, and the real human steering angle is shifted accordingly. In this way, each example is augmented to 11 examples. 

In 1989, Navlab 1 burned when conditioning system leaked liquid onto the computers. Navlab 2 was built in 1990 based on a [Humvee](https://en.wikipedia.org/wiki/Humvee), which could drive both on-road (110 km/h) and off-road (10 km/h). Information about the subsequent Navlab cars is scarce, but from what I gathered, they were generally statistical methods on handcrafted features, without neural networks, learned features, and with only a minimum amount of online learning.

In 1995, Navlab 5 drove across continental America "No Hands Across America" autonomously for 98% of the time at average speed 100 km/h. However, little happened subsequent this -- until 2004, when DARPA issued a [Grand Challenge](https://en.wikipedia.org/wiki/DARPA_Grand_Challenge) for self-driving again. The Stanford Team that won in 2005 later became [Google Waymo](https://en.wikipedia.org/wiki/Waymo).

## Demo or die

Well, remember that the planners wanted a demo of ALV every year. So how did the demos go?

A team at Martin Marietta Denver Aerospace was responsible for putting together the demos, and they called their vehicle "Alvin" (no relation to ALVINN). Like Navlab 1, the system essentially has two parts: the vision part, which extracts the shape of the road from the camera video, and the planner. Unlike Navlab 1, the planner was a sophisticated expert system that uses the road model output by the vision and its knowledge base (location, map, known objects, goal location, etc) to compute a driving plan.

> The knowledge base consists of a priori map data, and a set of routines for accessing the data. Currently, the map data contains information describing the road network being used as the ALV test track. The map data contains coordinates which specify the location of the roadway, as well as various significant features along the road, such as intersections, sharp curves, and several local road features.
>
> [@turkVITSaVisionSystem1988]

![The architecture of Alvin. It has a low-level vision model and a high-level expert system planner. [@turkVITSaVisionSystem1988, figure 1, figure 3]](figure/Alvin_architecture.png)

![How Alvin represents the road. [@turkVITSaVisionSystem1988, figure 4, figure 5]](figure/Alvin_scene_representation.png)

The Alvin vision system is simple. [@turkVideoRoadfollowingAutonomous1987] First, it uses a hand-picked[^alvin-hand-picked] vector of RGB values and dot-product it with every pixel in the image. If the result is greater than a threshold, then it's a road-pixel. Next, it fit the geometric model of a road so that as many road-pixels fall within the road model as possible. 

[^alvin-hand-picked]:
    Hand-picked?? Yes!

    > The orientation of the separating plane is relatively consistent under given weather and camera conditions. It can often be chosen by hand at the beginning of a run and not modified thereafter. However, we have found that some segmentation failures have occurred... We have therefore developed a method to dynamically compute the optimal plane orientation based on data from the image currently being processed.
    >
    > [@turkVideoRoadfollowingAutonomous1987]

```python
def alvin_vision(image):
    vector_rgb = np.array([..., ..., ...])
    threshold = ...
    road_pixel_coordinates = where(image @ vector_rgb > threshold)
    return fit_road_shape(road_pixels)
```

As we might expect by looking at this janky setup, Alvin's vision system was plagued by the same problem of logical AI: fragility. Every time a part upgrades, something breaks. Much of the high-level planning was written to deal with the fragility of the vision system. If the high-level planner detects that the scene model output by the vision system is probably nonsense, it simply discard it. The endless trouble encountered by Alvin deserves quotation in full:

> The vision system proved highly sensitive to environmental conditions--the quality of light, the location of the sun, shadows, and so on. The system worked differently from month to month, day to day, and even test to test. Sometimes it could accurately locate the edge of the road, sometimes not. The system reliably distinguished the pavement of the road from the dirt on the shoulders, but it was fooled by dirt that was tracked onto the roadway by heavy vehicles maneuvering around the ALV. In the fall, the sun, now lower in the sky, reflected brilliantly off the myriads of polished pebbles in the tarmac itself, producing glittering reflections that confused the vehicle. Shadows from trees presented problems, as did asphalt patches from the frequent road repairs made necessary by the harsh Colorado weather and the constant pounding of the eight-ton vehicle.
> 
> Perhaps more alarming to the Martin engineers was the early realization that there would not be one all-purpose, road-following algorithm. Different situations required different programs. The first road-following algorithm that Maryland installed on the vehicle, the “vanishing point” algorithm, had functioned satisfactorily in the lab but not on the road. Under certain conditions the vehicle thought the road had folded back under itself. This algorithm had to be replaced by the “flat-earth” algorithm, so-called because it worked by using a two-dimensional representation of the road and assuming that the road was perfectly flat. The algorithm was quick to run, but it was relatively inaccurate, and, not surprisingly, it worked only on flat ground. The third program, the “hill-and-dale” algorithm, used a three-dimensional representation of the road. It functioned better on uneven ground, but it did not work on curves. Maryland came up with a fourth algorithm, the “zero-bank” algorithm, which solved this problem; but it ran too slowly on the vehicle’s computers and had to be put off until phase II of the program...
> 
> Other problems were caused just by the sheer complexity of the system. By the November 1985 demonstration, 25,000–30,000 lines of code were running in real time on ten different processors... Each new feature and capability brought with it a host of unanticipated problems. A new panning system, installed in early 1986 to permit the camera to turn as the road curved, unexpectedly caused the vehicle to veer back and forth until it ran off the road altogether. The software glitch was soon fixed, but the panning system had to be scrapped anyway; the heavy, 40-pound camera stripped the device’s gears whenever the vehicle made a sudden stop.
> 
> [@rolandStrategicComputingDARPA2002, pages 234--235]

Given such serious problem, the team opted to just "study for the test". Like the many demos before and since then, it promises to do much more than it could, using techniques much less general than it should.

> Given such unanticipated difficulties and delays, Martin increasingly directed its efforts toward achieving just the specific capabilities required by the milestones, at the expense of developing more general capabilities... Martin’s selection of technology was conservative. It had to be, as the ALV program could afford neither the lost time nor the bad publicity that a major failure would bring... ADS’s obstacle-avoidance algorithm was so narrowly focused that the company was unable to test it in a parking lot; it worked only on roads.
> 
> [@rolandStrategicComputingDARPA2002, page 235]

Though Alvin passed one demo after another, the "demo or die" approach resulted in a series of demos made by "ad-hoc, off-the-shelf solutions" that did not generalize, or integrate the other technologies developed in the SCI.

> The experience with ALV mirrored what was going on elsewhere in the SC program. The applications failed to connect with the technology base. Instead, applications extemporized ad-hoc, off-the-shelf solutions to meet demonstration deadlines. Meanwhile, the many research projects in the technology base rose and fell on their own merits. Mutually incompatible, they seldom achieved integration, let alone transition... Takeo Kanade... criticized the program as “too much demo-driven... Instead of integrating the technologies developed in the SC tech base... effort is spent ‘shopping’ for existing techniques which can be put together just for the sake of a demonstration.” Based on the recommendations of the panel, DARPA quietly abandoned the milestones and ended the ALV’s development program.
> 
> [@rolandStrategicComputingDARPA2002, pages 243--245]

## General AI technologies

SCI called for three fields of basic AI in support of the above applications: speech recognition, computer vision, and general integration of logical AI techniques. Only speech recognition was a success.

The aspiration of the speech recognition task was to allow pilots to speak to the computer, so it eventually had to recognize continuous speech with noise. DARPA released a benchmark of 21000 sentences from 160 speakers reading out sentences "appropriate to a naval resource management task built around existing interactive database and graphics programs" [@priceDARPA1000wordResource1988]. This was one of the first public benchmarks, instilling a benchmark-centric culture around ASR early on. The HMM-based models, similar to those developed by Jelinek *et al* at IBM, reached the goal.

> Throughout the program, speech recognition held to its original goal of 10,000-word continuous speech recognition, using speaker-independent natural grammar, moderate noise, and low stress... Such systems were up and running by the end of SC, being integrated into military applications and undergoing development for commercial applications.
> 
> [@rolandStrategicComputingDARPA2002, page 211]
> 
> In January 1986 Carnegie Mellon had demonstrated a speech system that could recognize 250 words spoken continuously regardless of the speaker. By 1987 this system could cope with a 1,000-word vocabulary with 95 percent accuracy, operating at 10 times real time on a parallel system. Texas Instruments produced a robust 200-word connected-speech-recognition system that was installed on F-16 fighters for operational testing by pilots.
>
> [@rolandStrategicComputingDARPA2002, page 224]

The aspiration of the vision task was to allow armored vehicles to drive through hostile environments autonomously. The planners assumed that the correct computer vision architecture is hierarchical, in the style of David Marr's theory. Like Chomsky had done for language, Marr did for vision.

> Developments in vision were far more disappointing. Part of the reason is that expectations at the beginning of SC were so high. Prior research on computer vision may be grouped in three eras. In the 1950s and 1960s techniques from signal processing and statistical decision theory produced important developments in areas such as Synthetic Aperture Radar, image-enhancement, and terrain-matching, cruise-missile guidance. These were essentially ad hoc inventions, innocent of a conceptual paradigm. 
> 
> In the late 1970s, when DARPA was funding an image-understanding program aimed at photo interpretation, a “signals-to-symbols” paradigm gained currency. It was, however, a theory of low-level, general-purpose vision, limited essentially to two-dimensional analysis. Optimism that this limitation might be breached grew in the late 1970s, culminating in the 1982 publication of David Marr’s pathbreaking study, *Vision*, just as the SC plan was taking shape.
> 
> [@rolandStrategicComputingDARPA2002, page 212]

In detail, a computer vision system [according to Marr](#fig-marr-cylinder) would take input images as 2D arrays, then compute some local low-level features over them, such as edge detection by Sobel filtering, optical flows, etc, to create a "primal sketch". This featurized 2D array would then be globally processed to create a "2.5D sketch". Aided by the 2.5D sketch, the higher levels would then construct a 3D model of the scene.

In accordance to this, SCI had two benchmarks for computer vision. The first Image Understanding Benchmark tested for how fast the machine could do the primal sketches, with tasks like "Label connected components in a binary image" and "Hough transform of a binary image". The second one tested for how fast the machine could do the 2.5D sketches on simulated images of a 3D scene consisting of 2D rectangles of various sizes, brightnesses, orientations, and depths. [@weemsParallelProcessingDARPA1991; @weemsDARPAImageUnderstanding1991]

Did it work?

> \[By 1987, \] No longer did the program plan to achieve by 1993 a capability for “reconnaissance in a dynamically changing environment.” Neither did it claim to run programs in 1992 for 3-D vision at one trillion instructions per second or achieve “knowledge-based vision” on a parallel machine of one million processors running at one megahertz symbolic processing rate. Vision itself had proved to be a thorny problem, and the other technologies in the pyramid, such as architectures and expert systems, were not in place to break the impasse... Of the remaining realms of AI, vision proved the most disappointing. It fell farthest short of the goals originally planned, and it proved least responsive to the imposition of vastly increased computing power. Ma- chines could process the data coming in; they just couldn’t deploy algo- rithms that could interpret the images with human speed and accuracy.
>
> [@rolandStrategicComputingDARPA2002, page 213]

The ALV program was cancelled in 1988-04.

As for the general AI integration, the idea was similar to Shakey. Just like Shakey integrated the main logical AI techniques up to 1970, SCI called for integrating the main logical AI techniques up to 1980s. The SCI planners thought that the success of expert systems had shown that the time was ripe for a concerted push for a general expert system that combines the best ideas so far: blackboards (developed during the ARPA-funded speech understanding project), forward and reverse chaining, default reasoning, etc. This integration proved a failure, like the dream of a generally intelligent expert system. [@rolandStrategicComputingDARPA2002, page 243]

In 1990, funding for AI was killed, because the new director of SCI believed there is no unified AI principle, only particular AI systems demonstrating particular intelligences.

> In a cold and devastating review of “The Limits of Artificial Intelligence” prepared for the 1987 edition of The Encyclopedia of Artificial Intelligence, [\[Jacob\] Schwartz](https://en.wikipedia.org/wiki/Jacob_T._Schwartz) had argued that AI had yet to demonstrate “any unifying principles of self organization,” meaning that its “applications must still be seen as adaptations of diverse ideas rather than as systematic accomplishments of a still mythical AI technology.” ... he believed that expert systems were achieving what success they had by clever programming, not by the application of any general principles. His analysis bode ill for the prospects of achieving a generic expert system of the kind envisioned by the SC program and the contracts with Teknowledge and IntelliCorp. Indeed, Schwartz believed that the same critique applied to AI in general; he concluded that “it may be necessary to develop a relatively large number of artificial systems that mimic particular types of reasoning and mental functions in cases specialized enough to have particularly efficient treatment.”
>
> [@rolandStrategicComputingDARPA2002, pages 204--208]
