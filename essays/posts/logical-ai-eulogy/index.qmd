---
title: "Eulogy to Logical AI"
author: "Yuxi Liu"
date: "2024-01-23"
date-modified: "2024-01-23"
categories: [AI, scaling, history]
format:
  html:
    toc: true
description: "Good Old-Fashioned AI never die. They just fade away."

# image: "figure/banner.png"
status: "wip"
confidence: "likely"
importance: 3
---

{{< include ../../../utils/blog_utils/_macros.tex >}}

## Translation

> One naturally wonders if the problem of translation could conceivably be treated as a problem in cryptography. When I look at an article in Russian, I say: "This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode."
> 
> -- Warren Weaver, Letter to Norbert Wiener, 1947-03-04

During WWII, the science of communication and control took on a life-and-death importance. The mathematically perfect Enigma forced Allied mathematicians to turn their art of code into the science of information, so as to extract every last bit of information leaked out from the unknown enemy who was less than mathematically perfect, who made mistakes, who stuttered with verbal tics like `WETTER` or `KEINE BESONDEREN EREIGNISSE`. On two sides of the Atlantic, Alan Turing of computer science, and Claude Shannon of information theory, fought in this information warfare.

Before the war, some feared that the bombers would finally be the ultimate weapon, as a fleet of [them will always get through](https://en.wikipedia.org/wiki/The_bomber_will_always_get_through). Bombing was becoming a cyborg activity. The bombers were flying so high and so fast, the bombardiers needed [intricate bombsights](https://en.wikipedia.org/wiki/Norden_bombsight) filled with mechanical calculators, just to calculate the correct time to drop the bombs.

But the bombers would not go through after all, as radar screens and flak cannons raised invisible walls in the sky, and the anti-aircraft fire became another cyborg activity. Norbert Wiener developed his control theory in the context of anti-aircraft fire and radar screening. He thought of both as a form of deadly communication. A radar speaks to the aircraft, "Who and where are you?" Despite itself, the aircraft must answer. The radar's job is to speak clearly with the right ping and listen carefully with the right filter. In this context, he developed the [Wiener filter](https://en.wikipedia.org/wiki/Wiener_filter).

Anti-aircraft (AA) seems even less like a deadly communication, yet Wiener made it work. To shoot down an aircraft, one must predict where it will be a few seconds into the future, since that is how long bullets take to fly that high. The AA looks to the sky and asks, "Where are you going?". Despite itself, the aircraft speaks with where it had been in the past few seconds, as if writing a cursive word in the sky. The AA reads and understands this writing, and act accordingly. The past is a code for the future, like the Enigma is a code for the plaintext. [@yeangFilteringNoiseAntiaircraft2023]

If the soldiers are always preparing to fight the previous war, the same seems true for some mathematicians. Wiener and his collaborator, Warren Weaver, decided to tackle the problem of machine translation with the same tools they developed for war. If information theory helps with breaking the Enigma code, would it not also help with breaking the language codes?

The wartime metaphor would become ominously appropriate with the Cold War.

### Georgetown--IBM experiment

During the 1950s, electronic computers were mainly understood and used as tools for real-valued calculations, such as simulating nuclear explosions, the aerodynamics of ballistic missiles, macroeconomic planning, and other important real-valued functions that are necessary to safeguard freedom. However, there was already early attempts at using computers for symbolic calculations.

In a sense, this was quite old. Whereas Charles Babbage designed his computer as an arithmetic mill to grind out numerical tables, Ada Lovelace speculated that computers can grind out symbolic music too, as long as music and its transformation rules are encoded into integers.

On 1954-01-07, the world's first non-human translator appeared in the body of an IBM 701. At least, that is what the newspapers made it seam to be.

Back in 1952-06, at a MIT conference on machine translation, Leon Dostert was convinced that instead of arguments about whether MT works *in theory*, they needed to try it out on an actual problem to see if it would work *in practice*. This led to the [Georgetown--IBM experiment](https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment) 1.5 years later. It was the first public demonstration of machine translation -- from Russian to English, and was widely reported with titles like "Electronic brain translates Russian" or "Robot brain translates Russian into King's English". The project originated when a conference on machine translation took place at MIT i

The IBM 701 machine operated at $2000 \;\mathrm{FLOP/s}$ with read/write speed $3.6 \;\mathrm{kB/s}$ (in the form of 80-column punched cards). 

The program took up $2400 \;\mathrm{instruction} \times 18 \;\mathrm{bit/instruction} = 5.4 \;\mathrm{kB}$, whereas the dictionary took up $6000 \;\mathrm{word} \times 36 \;\mathrm{bit/word} = 27 \;\mathrm{kB}$. 

The dictionary is a table with 6 columns: Russian, English equivalent I, English equivalent II, Code 1, Code 2, Code 3. As we see, each Russian word has 1 or 2 possible English translations. The three `Code`s are essentially grammar categories. As an example, the suffix `-a` is coded as `(-a, of, , 131, 222, 25)`, while the word stem `ugl-` is coded as `(ugl-, coal, angle, 121, ***, 25)`.

The dictionary contains just 250 lexical items (stems and endings). Its grammar has just 6 rules. All input sentences must be made of words that are of form either `stem` or `stem-ending`. Some example translations included:

* Mi pyeryedayem mislyi posryedstvom ryechyi.
* We transmit thoughts by means of speech.

The algorithm is essentially a word-substitution program, using the 6 rules to disambiguate, and to decide whether to switch a word with a previous word. The word-order switch is necessary since Russian puts prepositions as word suffixes. For example, `ugl-a` would be word-substituted to `angle-by`, but must be translated as `by angle`.

The experiment was a hit, and there were some predictions of imminent breakthrough [@hutchinsFirstPublicDemonstration2005]:

* Such a device should be ready within three to five years... As soon as cards for Russian are completed, sets will be made for German and French. Then other Slavic, Germanic and Romance languages can be set up at will.
* 100 rules would be needed to govern 20,000 words for free translation.

From our perspective, these seem painfully optimistic. However, it was a common belief that electronic computers, like the IBM 701, were designed for numerical computation, something that is more difficult than natural language processing. As such, a machine translator needed not faster computers, but more data. Yet among the general optimism, there was a disquieting note:

> the formulation of logic required to convert word meanings properly even in a small segment of two languages necessitates two and a half times as many instructions to the computer as are required to simulate the flight of a guided missile.
>
> ["701 Translator", IBM Press release](https://aclanthology.org/www.mt-archive.info/IBM-1954.pdf) (1954-01-08)

::: {.callout-note title="Give me code or give me nothing!" collapse="true" }

One thing I dislike about some technical histories and overviews is that I keep getting a cotton-like, vaporwave feeling in the brain after reading them. It is easy to read an abstract story. 

For example, many AI papers by OpenAI after 2020 has become like that. In any case, I looked up the program, which appeared in [@ornsteinMechanicalTranslationNew1955] as a single giant flowchart. I didn't read the spaghetti code in detail, but it seems to me that it first parses the input sequence into words and sub-words, then it starts from left to right, for each word/sub-word, find the rule that applies to it. Executing the rule would pick an English translation for that word/sub-word, and either switch that fragment of translation with the previous fragment, or not. There are 6 rules, of which I just copy one, since the others look similarly boring:

> Choice-Rearrangement. If first code is `131`, is third code of preceding complete word or either portion (root or ending) of preceding subdivided word equal to `23`? If so, adopt English equivalent II of word carrying `131` and retain order of appearance of words in output; if not, adopt English equivalent I and reverse order of appearance of words in output.

The following is a rough Python sketch. It just implements half of rule 3, but gives you an idea of how the program would go. I estimate that it should take about 100 lines to implement a fully correct version. Even this rough sketch tells you that it is a very 1950s kind of program, with imperatives and if-then statements everywhere, combined with table lookups.

```python

dictionary = {
    "ugl-": ("coal", "angle", 121, 0, 25),
    "-a": ("of", "", 131, 222, 25), 
    ...
}

stems = [word[:-1] for word in dictionary.keys() if word[-1] = '-']
suffixes = [word[0:] for word in dictionary.keys() if word[0] = '-']

def class Word:
    def __init__(self, word, stems, suffixes):
      self.stem = word
      self.suffix = ''
      for stem in stems:
        for suffix in suffixes:
            if word = stem ++ suffix:
                self.stem = stem
                self.suffix = suffix
                return
    
def parse(sentence, stems, suffixes):
    words = sentence.split(' ')
    return [Word(word, stems, suffixes) for word in words]

def translate(words, dictionary):
    translation = []
    for i in range(words):
        word = words[i]
        if word.suffix = '':
            ...
            if dictionary[word.stem]['code 1'] == 131:
                if i > 0:
                    previous_word = words[i-1]
                    if dictionary[previous_word.stem]['code 3'] == 23 or (previous_word.suffix != '' and dictionary[previous_word.suffix]['code 3'] == 23):
                        flag = 2
                    else:
                        flag = 1
                else:
                    flag = 1
                if flag == 1:
                    translation.append(dictionary[word.stem]['code 1'])
                    translation[-2:-1] = [translation[-1], translation[-2]]
                else:
                    translation.append(dictionary[word.stem]['code 2'])
            ...
        else:
            # Translate stem, then suffix. It's a bit tedious.
    return translation.join(' ')
```

:::

The exact details on how the program was implemented on the IBM was non-trivial, since both the machine and the programming environment around it were designed for numerical computations, not discrete symbolic manipulations. [@sheridanResearchLanguageTranslation1955] described the details. The programming language LISP must wait until 1960 to appear. Dedicated to symbolic manipulations. It would dominate most of AI research until the 1980s.

Another interesting fact is the amount of restrictions placed on the demo: 250 words, each word having just 1 or 2 possible translations, and each Russian word is either a full word or a `stem-suffix`, etc. An even deeper restriction was entirely hidden from view: pronouns. In Russian, pronouns are often dropped when the verb form makes it clear. To avoid this problem, for all demonstrated sentences, the English pronouns occur only in translations of verbs in the third person plural.

The demo worked. The CIA started funding MT research at Georgetown University (eventually up to \$1,500,000), and other MT groups sprang up in America, Europe, and the Soviet Union. Subsequent research at Georgetown was based on a multi-level analysis (morphological, syntagmatic, syntax), later formalized as the [Vanquois triangle](https://en.wikipedia.org/wiki/Bernard_Vauquois#Vauquois_triangle).

![The Vanquois triangle.](figure/Vauquois%20triangle.png)

### Natural language compiler

### Winter comes

Around 1964, there was an uneasy feeling around AI. The post-war flood of governmental funds was slowing as the optimistic "Science, the Endless Frontier" lost its shine. There were much bigger projects to do, like sending 2 men to the moon or 3 million to Vietnam. The general atmosphere was subdued. Eventually funding would be cut even more with the Mansfield Amendments of 1973, which limited ARPA to only funding projects directly relevant to military applications.

> By 1966, it was estimated that roughly 20 million dollars (in contemporary dollars) had been spent on MT in the US, with Georgetown, at 1,317,239 dollars (93.5 percent from the CIA, 6.5 percent from the NSF) being the largest.. "In comparison, let us notice that in June 1952, when the First conference on Machine Translation convened at MIT, there was probably one person in the world engaged more than half-time in work on MT, namely myself \[Bar-Hillel\]". The budget had been roughly ten thousand dollars.
>
> [@gordinForgettingRediscoverySoviet2020]

In 1964, the US government created the ALPAC committee of 7 linguists "to advise the Department of Defense, the Central Intelligence Agency, and the National Science Foundation on research and development in the general field of mechanical translation of foreign languages". The roster of names makes it clear that MT was a matter of state security. It was all well and good if MT could eventually reach human level performance in a few centuries, or if MT research could *right now* inform the science of linguistics and assist the universe's ceaseless striving to rationally know itself, but let us never confuse the universal with the here and now.

> "Are you looking for the Secret Name, Scharlach?" ... in his voice Lönnrot detected a fatigued triumph, a hatred the size of the universe, a sadness no smaller than that hatred. "No. I am looking for something more ephemeral and slippery, I am looking for Erik Lönnrot..."
>
> --- Death and the Compass (1942)

The Georgetown--IBM experiment proved to be *too* good of a demo. The sentences were picked to present it in the best light, and the rules were written so that the machine would translate the sentences correctly. Subsequent MT research could not match the demo, and skeptics appeared.

> Dorset is "a great conversationalist ... but as a researcher I was unsure about him, whether he was just a figurehead or whether he was a bit of a fraud -- the Georgetown MT demonstrations seemed always to be contrived; they made impressive publicity for the sponsors, but they soured the atmosphere by raising expectations that nobody could possibly fulfill." ... MT colleague Winifred Lehmann was overheard describing him as "a wart on the field of linguistics".
>
> --- Old gossips quoted in [@gordinDostoevskyMachineGeorgetown2016]

The ALPAC committee worked for 2 more years before presenting the final report in 1966. The effect was twofold: One, a swift deep cut in governmental funding for MT research, not just in America, but also in the Soviet Union. Two, a general impression that MT had been debunked.

> Its effect was to bring to an end the substantial funding of MT research in the United States for some twenty years. More significantly, perhaps, was the clear message to the general public and the rest of the scientific community that MT was hopeless. For years afterwards, an interest in MT was something to keep quiet about; it was almost shameful. To this day, the 'failure' of MT is still repeated by many as an indisputable fact... from time to time in the next decades researchers would discuss among themselves whether "another ALPAC" might not be inflicted upon MT.
>
> [@hutchinsALPACFamousReport2003]

> The effect of the ALPAC report in 1966 was as great in the Soviet Union as in the United States. Many projects were not funded any more; machine translation went into decline. The authorities had seen the ALPAC documents and concluded that if the Americans did not think it worthwhile to support MT, if they did not think there was any hope of MT, then nor should we... \[But\] we had never pretended that we were doing actual machine translation, we were doing formal linguistics.
>
> [Igor A. Mel'čuk](https://en.wikipedia.org/wiki/Igor_Mel'%C4%8Duk) (2000), quoted in [@gordinForgettingRediscoverySoviet2020]

Yehoshua Bar-Hillel, speaking in the 1960s, against machine translation -- what he called FAHQT (Fully Automatic High-Quality Translation, or as I imagine him saying it, "Machine translation? Ah, FAHQT."). In short, he argued that statistical language modelling does not work better than manually programming in the rules. Specifically, the Winograd schema challenge (which was known in the 1960s, before Winograd discussed it) requires general world understanding, which is "utterly chimerical and hardly deserves any further discussion'.

> No justification has been given for the implicit belief of the "empiricists" that a grammar satisfactory for MT purposes will be compiled any quicker or more reliably by starting from scratch and "deriving" the rules of grammar from an analysis of a large corpus than by starting from some authoritative grammar and changing it, if necessary, in accordance with analysis of actual texts.  
> 
> ...
> 
> Whenever I offered [the Winograd challenge] to one of my colleagues working on MT, their first reaction was: "But why not envisage a system which will put this knowledge at the disposal of the translation machine?" ... such a suggestion amounts to, if taken seriously, is the requirement that a translation machine should not only be supplied with a dictionary but also with a universal encyclopedia. This is surely utterly chimerical and hardly deserves any further discussion.
>
> [@bar-hillelPresentStatusAutomatic1960]  

> It seems now quite certain to some of us, a small but apparently growing minority, that with all the progress made in hardware (i.e., apparatus), programming techniques and linguistic insight, the quality of fully autonomous mechanical translation, even when restricted to scientific or technological material, will never approach that of qualified human translators and that therefore Machine Translation will only under very exceptional circumstances be able to compete with human translation.  
>
> [@bar-hillelFutureMachineTranslation1964]  

Even Terry Winograd had a tentative guess that the Winograd challenge is too challenging, though it is clearly just a weak guess, not a firm prediction like the previous quotes.

> The limitations on the formalization of contextual meaning make it impossible at present -- and conceivably forever -- to design computer programs that come close to full mimicry of human language understanding.  
>
> [@winogradComputerSoftwareWorking1984]  


[@hutchinsALPACFamousReport2003]

Evaluation by demo is bad, because 

1. Demos are cherry-picked, and don't tell you how the system would work in practice.
2. Demos encourage an empirical kind of AI, an "artful deception" that is not science.

The argument against full machine translation was simple but devastating:

1. Natural language is highly ambiguous.
2. Disambiguation is not purely syntactical, and requires semantics -- world knowledge, or commonsense.
3. The amount of world knowledge, or commonsense, is extremely large, since after decades of manual entry, the machines still do not seem any closer to having commonsense.

There were several solutions:

1. Intelligence amplification: Instead of the mirage of replacing human translators, try to build little helper programs to help human translators.
2. Brute force logical programming: Scale up commonsense by hiring more linguists to program in increasingly large chunks of the world.
3. AI is a mirage, and the failures of MT is a symptom of that.

From our vantage point the actual solution turned out to be:

1. The bitter lesson: Wait a few decades, then train a giant neural network on a trillion words from the Internet.

There is a common mistake about the bitter lesson, that even Richard Sutton makes. It is not just that the bitter lesson is bitter, but also that it is *difficult*. People did not believe in it, not because they were afraid of bitterness, but because it was obviously stupid, a kind of straw man's argument.

Case study in not taking the bitter lesson:

> The case against machine translation as a solution to practical problems is overwhelming and has been made many times. I do not propose to repeat it in any detail here. It will, however, be worth a few words to make a *prima facie* case for the implausibility of practical machine translation if only so that the contrast with realistic approaches to the problem will be more striking... There is a great deal that computer scientists and linguists could contribute to the practical problem of producing translations, but, in their own interests as well as those of their customers, they should **never** be asked to provide an engineering solution to a problem that they only dimly understand.
> 
> I want to advocate a view of the problem in which machines are gradually, almost imperceptibly, allowed to take over certain functions in the overall translation process. First they will take over functions not essentially related to translation. Then, little by little, they will approach translation itself. The keynote will be *modesty*. At each stage, we will do only what we know we can do reliably.
>
> [@kayProperPlaceMen1997]

It is with regret that I report the subsequent career of John Pierce. After delivering those devastating criticisms with verve and consequence, he never returned to criticizing AI again, thus depriving us of much schadenfreude.

> There's sometimes a strong scholastic emphasis, or current, which causes some psychologists to produce very closely reasoned ideas that aren't checked at every point with experiment. Some of the theoretical linguists are like that. They are extremely plausible. But I know a fellow, [Victor Yngve](https://en.wikipedia.org/wiki/Victor_Yngve), who tried to write a transformational grammar of the English language, a reasonably complete one. It took him years and years and he never got it written. He kept finding difficulties that don't appear when you have a few nice examples of what a transformation of grammar is supposed to be all about.
>
> John Pierce, quoted in [@lyleInterviewJohnRobinson1979]

### ABBYY's last stand

In 2011, Kenneth Church proposed a 20-year cycle between "Empiricism" (turns out it works) and "Rationalism" (it must work if you think carefully about it), and argued that we were on the brink of a return to Rationalism:

* 1950s: Empiricism (Shannon, Skinner, Firth, Harris)
* 1970s: Rationalism (Chomsky, Minsky)
* 1990s: Empiricism (IBM Speech Group, AT&T Bell Labs)
* 2010s: A Return to Rationalism?

![The shift from Rationalism to Empiricism, as measured by the proportion of statistical papers submitted to the Association for Computational Linguistics. Based on two independent surveys by Bob Moore and Fred Jelinek.](figure/Kenneth_2011_fig_1.png)

> When we revived empiricism in the 1990s, we chose to reject the position of our teachers for pragmatic reasons. Data had become available like never before. What could we do with it? We argued that it is better to do something simple than nothing at all. Let's go pick some low hanging fruit. While trigrams cannot capture everything, they often work better than the alternatives... That argument made a lot of sense in the 1990s, especially given unrealistic expectations that had been raised during the previous \[expert systems\] boom. But today's students might be faced with a very different set of challenges in the not-too-distant future. What should they do when most of the low hanging fruit has been pretty much picked over? ... we should expect Machine Translation research to make more and more use of richer and richer linguistic representations. So too, there will soon be a day when stress will become important for speech recognition.
>
> [@churchPendulumSwungToo2011]

As of 2025, the pendulum has swung even deeper into Empiricism.

[@skorinkinABBYYsBitterLesson2024]

[ABBYY's Bitter Lesson](https://sysblok.ru/blog/gorkij-urok-abbyy-kak-lingvisty-proigrali-poslednjuju-bitvu-za-nlp/)[^abbyy-google-translate]

[^abbyy-google-translate]: I read the essay by pure Google Translate. It worked almost perfectly.

## Speech

### Early days

Automatic speech recognition (ASR) is the conversion of speech audio to text. Its history resembles the history of MT. 

It is hard to know what is closest at hand. Speaking is so natural that it takes effort to even notice that there is structure within the smallest sound, and it required the invention of the phonograph to notice the fine details of even a single vowel. In the early 20th century, as AT&T connected all of America with telephone lines, it funded research into efficient coding of speech, with the hopeful goal of saving on bandwidth. The rough idea is similar to the idea of mp3: If the engineers knew what mattered and what didn't matter in human speech recognition, then they could squeeze more telephone calls within the same line.

Concretely, one could imagine a device on a telephone that converts all the richness of a speech-stream into just 20 bit-streams, then braid those 20 bit-streams into a narrow frequency band, send it all the way across America, whereupon it gets decompressed back to the speech-stream, impoverished but still perfectly recognizable.

It began with recognition of individual spoken words, which was possible by featurizing the sound, then match that sequence of feature vectors against the template feature vectors.

An illustrative example of this early period of ASR was reported in [@denesDesignOperationMechanical1959]. It could spell individual word phonetically (i.e. "cartoon" spelled as "katun") -- if the word is made out of only 4 vowels and 9 consonants, in an alternating fashion (i.e. no two vowels together, or two consonants together).

The machine first uses a filter bank to featurize the input sound. Pairs of outputs from the filter bank are then multiplied to measure how likely the sound is a phoneme. For example, the outputs from 200 Hz and 320 Hz are multiplied together, and that measures how likely the sound is "m", because the two principal frequencies of "m" are close to 200 Hz and 320 Hz. Finally, a computer selects the most likely phoneme, based on the multiplied results and a simple statistical model of how likely a phoneme is to follow the previous phoneme.

The following pseudocode describes how the system works:

```python
def featurize(audio_segment):
    filter_bank_features = filter_bank(audio_segment)
    return {
        "m": filter_bank_features["200 Hz"] * filter_bank_features["320 Hz"],
        "i": filter_bank_features["250 Hz"] * filter_bank_features["3200 Hz"],
        ...
    }
def find_phoneme(audio_segment, previous_phoneme, phoneme_probability_model):
    features = featurize(audio_segment)
    phoneme_probabilities = phoneme_probability_model(previous_phoneme)
    best_score = 0
    best_phoneme = ''
    for phoneme in features.keys():
        score = features[phoneme] * phoneme_probabilities[phoneme]
        if score > best_score:
            best_phoneme = phoneme
            best_score = score
    return best_phoneme
```

![[@denesDesignOperationMechanical1959, figure 8]](figure/Denes_1959_fig_8.png)

### Logical ASR

In typical generative grammar of language, you start with a `SENTENCE`, and repeatedly rewrite it until you end up with a sentence like `Did you hit Tom?`

Here, we push this one level deeper. After a sentence is generated, you substitute each word with its standard pronunciation, resulting in a sequence like `dɪd/juː/hɪt/tɒm`. Next, apply more rewriting rules to account for the fact that we don't pronounce a whole sentence like individual words, but always "glide two words together". For example, `did you` would actually be pronounced like `dija`, so we account for this with a rewriting rule `d/juː -> jə`. Similarly, the double `t` in `hit Tom` would be merged to a single `t`, so we add a rewriting rule `t/t -> t`. And there is no gliding at `ə/h`, so we add `əh -> əh`.

After this transformation, we obtain `dɪjəhɪtɒm`

This is the basic idea of text-to-speech via generative grammar. For speech-to-text, one would first convert the speech audio into a sequence of phonemes,[^phone-vs-phoneme] then reverse the generative grammar in the same way as one uses generative grammar to parse the syntax tree of a sentence.

[^phone-vs-phoneme]: Sometimes you see people distinguish "phone" from "phoneme". In those cases, they make a finer distinction between what the speaker *intends to say* vs what actually comes out of their mouth. In this example, the phoneme would be `...` while the phone would be `...`. This precision is too annoying to me, so instead I will just say that the "phone" is just another "phoneme", and the speaker intends to say the phoneme sequence `...`, which the speaking cortex in the brain encodes into the phoneme sequence `...` to save work for the throat muscles.

Unlike other aspects of natural language processing, speech-to-text had never abandoned statistics. Even the most ardent logical AI researcher admit that speech is filled with dirty random noise, and so must be processed by statistical filtering before it is clean enough to run symbolic programs over.

There were many logical ASR systems tried, but most of them consisted of 3 layers:

1. Filter and segment speech into phone-like units, usually in steps of 10 milliseconds (because vowel [formants](https://en.wikipedia.org/wiki/Formant) are on the order of 100 Hz).
2. Use pattern recognition to identify the segments
3. Find the utterance that best fits the identified segment string.

Roughly speaking, "the fit" is usually measured by how many rules are broken, with the rules written by expert linguists. Some examples would make the structure clear.

### ARPA Speech Understanding Project

Like MT, there was also plenty of funding for ASR from the government. And we have not seen the last of John Pierce! 5 years after the ALPAC report debunked MT, he took aim at ASR with mostly the same arguments, but with even sharper language.

> speech recognition is attractive to money. The attraction is perhaps similar to the attraction of schemes for turning water into gasoline, extracting gold from the sea, curing cancer, or going to the moon... It is clear that glamor and any deceit in the field of speech recognition blind the takers of funds as much as they blind the givers of funds. Thus, we may pity workers whom we cannot respect. People who work in the field are full of innocent (in their own view) enthusiasm. What particular considerations have led to this enthusiasm?
> 
> ... Most recognizers behave, not like scientists, but like mad inventors or untrustworthy engineers. The typical recognizer gets it into his head that he can solve "the problem." The basis for this is either individual inspiration (the "mad inventor" source of knowledge) or acceptance of untested rules, schemes, or information (the untrustworthy engineer approach).
> 
> [@pierceWhitherSpeechRecognition1969]

Pierce gave the same answer as he gave in the ALPAC report. True ASR is impossible until a machine has a general understanding of language, which was very far away. What *appears* to be ASR is actually "artful deceit" made by mere engineering, not science. A true science of ASR is possible, by having a good scientific theory, then testing it, perhaps by building a device according to the theory, and see if it works as predicted.

> The typical recognizer will have none of this. He builds or programs an elaborate system that either does very little or flops in an obscure way. A lot of money and time are spent. No simple, clear, sure knowledge is gained. The work has been an experience, not an experiment.
>
> [@pierceWhitherSpeechRecognition1969]

I don't know what caused Pierce to fire off this fiery missive 3 years after the ALPAC report, but it did cause funding to decrease. [@jelinekMyBestFriends2005] Partly in reaction to this, ARPA started the Speech Understanding Project, a $15 million project to produce ASR, from 1971 to 1976. It funded 4 teams in 4 organizations, with the following goal: Demo a system that can do ASR on a fragment of English limited to 1000 words of vocabulary, and a single tiny domain. Accept new speakers after finetuning. At <10% semantic error, at 100 MIPSS (100 million instructions per second of speech).[^arpa-sur-full-specification]

[^arpa-sur-full-specification]: 
    The full specification is

    > Accept continuous speech from many cooperative speakers of the general American dialect, in a quiet room over a good quality microphone, allowing slight tuning of the system per speaker, but requiring only natural adaptation by the user, permitting a slightly selected vocabulary of 1,000 words, with a highly artificial syntax, and a task like the data management or computer status tasks (but not the computer consultant task), with a simple psychological model of the user, providing graceful interaction, tolerating less than 10% semantic error, in a few times real time, and be demonstrable in 1976 with a moderate chance of success.
    >
    > [@newellSpeechUnderstandingSystems1973, figure 1.1]

    but as noted in [@medressSpeechUnderstandingSystems1977, footnote 8], the report forgot to add in "100 MIPSS".

By the end of 1976, the results were out:

![[@ermanHearsayIISpeechUnderstandingSystem1980, figure 13]](figure/DARPA_SUR_final_results.png)

Out of the 4 teams, only Harpy achieved the target -- barely. It could recognize 1011 words, at 5% semantic error. It has size 1.2 MB. It costs 30 MIP to process one second of speech, and since it ran on a 0.4 MIPS PDP-KA10, it takes about 5 minutes to process a single 4-second sentence, costing about \$5 to process one sentence. Fine-tuning for a single speaker takes about 30 minutes, or \$30.[^harpy-price]

[^harpy-price]: According to [Cost of CPU Performance Through Time 1944-2003](https://www.jcmit.net/cpu-performance.htm), it cost \$500,000 to rent a PDP-KA10 for 1 year, or about \$1 per minute.

Who would be willing to pay \$5 to transcribe one sentence? Still, since the target was so ambitious, the project was still considered a great success:

> In 1971, when the program started, perhaps the majority of informed technical opinion put general speech recognition by computers as not possible in the foreseeable future and perhaps not possible at all... Informed technical opinion can now be that general cost-effective speech input to computers is an attainable goal. That is now our opinion.
>
> [@medressSpeechUnderstandingSystems1977]

> "How hard is the sentence understanding problem in the limited contexts investigated during the ARPA project?" In 1970, when compared with isolated word recognition, the problems seemed immense. After the limited success of Harpy, one becomes more optimistic about the abilities of future systems.
>
> [@klattReviewARPASpeech1977]

Harpy had the simplest architecture, with just two parts. 

* The lower divides the input audio into 10 ms segments. Each segment is featurized to a single $\R^{14}$ vector (they called it "linear prediction coefficients by spectral analysis", but I don't know what that means). If two segments have similar features, they are merged.
* The upper part is a 15000-state transition graph. Each graph node is an annotated sound symbol. There are 98 sound symbols like `G BURST 1` or `IY`. The annotations allow the system to convert any path through the graph into a text sentence, like "Give me a textbook by Gauss.".

They first designed a complete context-free grammar that incorporates phonetics, syntax, and semantics for making document retrieval requests in a highly simplified and stilted syntax. The 98 sound symbols were constructed by "careful analysis of 747 sentences". After the design was complete, the grammar was automatically compiled into the 15000-state transition graph.

To perform speech recognition, the lower part gives the upper part the featurized sequence of the audio $v_1, v_2, \dots, v_n$, and the upper part beam-searches for a path $x_1, x_2, \dots, x_n$ that approximately minimizes $\sum_i \| v_i - v(x_n)\|$, where $v(x_i)$ is the template feature vector for the sound $x_i$.

To finetune the system for a new speaker, the new speaker simply speaks 20 sentences designed for finetuning, and the system would recalculate the 98 template feature vectors.
::: {#fig-harpy layout-ncol=2}

![Overall design of Harpy. Note how semantic, syntactic, lexical, and word juncture rules are all compiled into a single network. [@reddyMachineModelsSpeech1980, figure 9.4]](figure/Reddy_1980_fig_9_4.png)

![A small fragment of Harpy's network. [@reddyMachineModelsSpeech1980, figure 9.5]](figure/Reddy_1980_fig_9_5.png)

![Example of beam search used in Harpy. [@reddyMachineModelsSpeech1980, figure 9.8]](figure/Reddy_1980_fig_9_8.png)

:::

Significantly, despite using a beam search, Harpy did *not* use a probabilistic model of language. It essentially treats language as Chomsky-nonrandom, and all the randomness comes from imperfect matching between the real feature vectors and the template feature vectors. Despite this, it out-performed other systems that did. It seems that the problem was that there was insufficient data to estimate the probabilities in a probabilistic language model.

> Several of the speech understanding systems used estimates of the probability of a phonetic or lexical decision given the acoustic data in scoring the goodness of a theory, and each seems to have gotten into trouble by so doing. The problem is to analyze enough data to be sure of the probability of infrequent confusions. This is nearly impossible if one wants to take into consideration factors such as phonetic environment.
> 
> [@klattReviewARPASpeech1977]

Just as significant, there was a lot of artificial intelligence, but very little machine learning. All the formal grammars were hand-written. It is revealing that in a comprehensive review of ASR [@reddySpeechRecognitionMachine1976], "knowledge acquisition" took up less than 2% of the whole paper. And what little it said comes down to that learning-based methods require a large dataset of carefully and densely annotated audio, which was expensive and did not exist.

The last great attempt at logical ASR was the Hearsay-II, a further development from Hearsay, an unsuccessful entrant to the ARPA Speech Understanding Project. It was a towering giant with 7 levels, from "data base interface" and "phrase" all the way down to the "segment" and "parameter" and 15 knowledge sources (collection of rules) for going up and down the levels of abstraction. To handle this pandemonium of knowledge sources, it uses a "shared blackboard" architecture, so that each source can chime in about whatever is currently being interpreted. This allowed it to still work even if some sources are removed, and to improve whenever any source gets added or upgraded.

On one hand, it is great for interpretability, as one can trace through the entire structure and see exactly how any spoken sentence is parsed into a textual sentence. On the other hand, the interpretation took 39 steps and looks like:

```txt
Step 23. KS: PREDICT & VERIFY*.
Stimulus: BY+FEIGENBAUM+AND+FELDMAN+]* (phrase).
Action: Predict ten preceding words. Reject five: ABSTRACTS, ARE, BOOKS, PAPERS, REFERENCED. Find two already on the blackboard: 
    ANY* (65,24: 49),
    THESE (25, 28:49).
Verify three more:
    ARTICLE (25, 9:52),
    WRITTEN (25, 24:52),
    ARTICLES (10, 9:52).
```

![The levels and knowledge sources of Hearsay-II as of 1976-09. Knowledge sources are indicated by vertical arcs with the circled ends indicating the input level and the pointed ends indicating output level. [@ermanHearsayIISpeechUnderstandingSystem1980, figure 2]](figure/Hearsay-II_levels.png)

![Overall architecture of Hearsay-II. [@ermanHearsayIISpeechUnderstandingSystem1980, figure 4]](figure/Hearsay-II_architecture.png)

![The example utterance. (a) the waveform of "Are any by Feigenbaum and Feldman?"; (b) the correct words (for reference), (c) segments; (d) syllable classes; (e) words (created by MOW), (f) words (created by VERIFY), (g) word sequences, (h) phrases. [@ermanHearsayIISpeechUnderstandingSystem1980, figure 5]](figure/Hearsay-II_example_sentence.png)

### Linguists fired

> Every time I fire a linguist, the performance of our speech recognition system goes up.
>
> --- Frederick Jelinek (1988) (not apocryphal!)

[Frederick Jelinek](https://en.wikipedia.org/wiki/Frederick_Jelinek) would go down in history as the one who fired linguists, but he did not start his life this way. In 1961, he sat in Chomsky's lectures, and "got the crazy notion that I should switch from Information Theory to Linguistics". His PhD advisor forbade him, so he remained in information theory. After graduation, he tried collaborating with the eminent linguist [Charles Hockett](https://en.wikipedia.org/wiki/Charles_F._Hockett), bu Hockett then turned to composing operas. "Discouraged a second time, I devoted the next 10 years to Information Theory." [@jelinekDawnStatisticalASR2009]

In 1972, he left academia and joined IBM, where he worked on ASR. The project started with two linguists, but they got frustrated and left of their own accord, leaving behind engineers and physicists with little understanding of linguistics.[^jelinek-famous-last-words]

> When our Continuous Speech Recognition group started its work at IBM Research, the management wanted to make sure that our endeavors were guided by strict scientific principle. They therefore placed into the group two linguists who were going to guide our progress. Both linguists were quite self-confident, sure that fast progress will be possible. For instance, when we (trained as engineers or physicists) were at a loss how to construct a language model, one of the linguists declared "I'll just write a little grammar."... After about a year of frustration the linguists left our group, returned to their basic research, and we were free to pursue our self-organized, data driven, statistical dream... mostly of engineers and physicists. Only 3 or 4 people out of 10 had any previous experience with speech. None had graduate training in that field. But several of us had a background in Information Theory and that influenced our thinking.
>
> [@jelinekMyBestFriends2005]

It was unclear what exactly frustrated the linguists, but probably it was because they did not want, or did not have, the stamina to produce generative grammar for non-toy English:

> In the 1970s NLP and ASR research was dominated by an artificial intelligence approach. Programs were rule-based, expert systems were beginning to take over... The purest linguists based their work on self-constructed examples, not on the prevalence of phenomena in observed data. As already mentioned, strict distinction between training and test was frequently ignored. Grammars were being written that applied to less than dozen verbs.
>
> [@jelinekMyBestFriends2005]

This dedication to toys rather than would appear again later in the saga of SHRDLU.

[^jelinek-famous-last-words]:
    More details about this funny episode:

    > When handling natural speech, the main question was how to estimate the language model $Pr(W)$. There was no simple way of achieving this. We thought that the right approach ought to be somehow related to English grammar. The linguist Stan Petrick, while he still was with us, said "Don't worry, I will just make a little grammar." Of course he never did, and the phrase acquired a mythical status in the manner of "famous last words."
    > 
    > [@jelinekDawnStatisticalASR2009]

The group naturally reproduced the information-theoretic framework for MT proposed by Shannon and Weaver, and adapted it directly to ASR.[^reproduced-shannon-weaver] In the framework, there are 4 components:

[^reproduced-shannon-weaver]: 
    Apparently the idea was obvious as soon as you think about ASR in the mindset of information theory.

    > As to our problem formulation, we were later somewhat surprised when it was revealed to be almost common sense. In fact, it was probably Bob Mercer who found the following quotations in an article by Weaver (1995): 
    > 
    > > When I look at an article in Russian I say: This is really written in English but it has been coded in some strange symbols. I will now proceed to decode it... the matter is probably absolutely basic -- namely the statistical character of the problem.
    > 
    > [@jelinekDawnStatisticalASR2009]

1. A language model: $Pr(\text{text})$. This models how a human thinks up what to say in its head.
2. A model of the speaker: $Pr(\text{sound}|\text{text})$. This models how the text to be spoken gets converted into actual sound out of its mouth.
3. Audio preprocessor: $\text{features} \mapsto \text{sound}$
4. Decoder: $\text{features} \mapsto \text{text}$

Step 1 of this framework turned out to be the critical idea. In the pure Chomskyan viewpoint, step 1 is illegitimate. In the information-theoretic viewpoint, step 1 is essential.

It was of course possible, and even obvious, that one can combine the best of both worlds, to combine both linguistic insights and statistical methods, such as a probabilistic CFG. However, they simply trained a dumb trigram language model on a large corpus. From what Jelinek said later,[^never-reluctant-linguistic] my guess is that they did try such "best of both worlds" approach, but the dumb trigram model just worked better.

[^never-reluctant-linguistic]:
    > We were never reluctant to include linguistic knowledge or intuition into our systems: if we didn't succeed, it was because we didn't find an efficient way to do include it.
    >
    > [@jelinekMyBestFriends2005]

The IBM project, compared to the others, was definitely big-data. This served them well for statistical ASR, and would soon serve them well again for statistical MT. Jelinek started with a toy model of language called "New Rayleigh", which is essentially a Markov chain model that generates sentences up to 8 words long, from a vocabulary of 250 words. They soon produced an ASR model that achieved perfect accuracy on this, so they had to go bigger.

![The New Rayleigh toy language. [@jelinekDawnStatisticalASR2009, figure 1]](figure/New_Rayleigh_toy_language.png)

The next dataset they obtained was the "laser patent corpus", consisting of 2 million words of patent applications in laser technology, with a vocabulary size of 10,000, twice as large as the [Brown Corpus (1961)](https://en.wikipedia.org/wiki/Brown_Corpus). They used this to train a 3-gram language model. The road to this dataset was quite circuitous, which anyone who has tried making a non-toy dataset can relate:

> The struggle to compile a suitable training corpus featured throughout the CSR \[Continuous Speech Recognition\] group's first decade. The researchers first considered using a collection of digitized IBM manuals, but found the vocabulary to be too extensive as well as so technical that it proved "difficult to pass ... off as English." They attempted to produce their own corpus, enlisting the wife of a lab staff member to type approximately a million words of text from children's novels, but there, too, the vocabulary proved too large for their purposes. Next, the group acquired a collection of laser patent text from the US Patent Office, which was both sufficiently extensive in size and sufficiently narrow in vocabulary that they were finally able to extract a million words of running text confined to a thousand-word vocabulary. Though the laser patent corpus was considered "naturally occurring," it was in fact meticulously constructed, even before researchers discarded all sentences containing vocabulary outside of the thousand most frequently occurring words. The complete patent text had to be first "subjected to intensive hand and computerized editing": eliminating duplicates, merging spelling variations, and substituting scientific symbols and formulas. The claims sections of the patents proved especially problematic due to "highly stylized" legal language, and were ultimately excised entirely.
> 
> By the 1980s hardware improvements allowed the IBM researchers to expand their recognition vocabulary to five thousand words. They also compiled over 100 million words of data from a variety of sources, including public domain books and magazines from the American Printing House for the Blind, the records of the Amoco oil corporation, and 2.5 million words of office correspondence supplied by physicist Richard Garwin, who, with the aid of four secretaries, maintained computer-formatted duplicates of all of his correspondence. The most substantial collection in this period, however, came as a direct result of IBM's unique industry dominance: the landmark federal antitrust lawsuit filed against the company in 1969. The case, which spanned thirteen years before it was finally dismissed in 1982, included testimony from 974 witnesses with resulting transcripts that totaled over one hundred thousand pages. The operation to digitize the deposition transcripts during the trial was so prodigious that it required a staff of dedicated keypunch operators large enough to fill a facility the size of a football field in White Plains, New York, where IBM's Data Processing Division was headquartered. While the suit itself proved extremely costly for IBM, its transcript digitization efforts provided the CSR group with their largest and most robust text collection, resulting in a corpus of one hundred million words. Finally, the CSR group discovered the Hansard corpus, a collection of digitized transcripts of the Canadian Parliament official proceedings in both English and French, in the mid-1980s and incorporated an additional hundred million words from its English text into their language model.
>
> [@liTheresNoData2023]

In 1976, their ASR system had already reached state of the art performance. On the same task as the ARPA project, it reached higher accuracy, and only took 30 MIP to process 1 second of speech. [@reddySpeechRecognitionMachine1976] Though logical ASR continued with systems like Hearsay-II (1980), as the 1980s went on, none could deny the practical success of statistical ASR anymore, even if, compared to logical ASR, statistical ASR was far from linguistic theories of human speech recognition.

In the 1990s, all state of the art ASR were statistical, usually HMM-based. Such a model has multiple levels.

At the top level, there is a Markov chain that serves as a language model, which emits individual words. This is usually an n-gram model, where $n$ is usually 3. In such a model, each node is a 3-gram. For example, the model can have state transitions like "- - what" → "- what is" → "what is the" → "is the last" → ..., and it would emit "what", "is", "the", "last", ...

This sequence of words is then converted to a sequence of phonemes, like "wɒtɪzðəlɑːst". Each phoneme, like "ɪ", corresponds to a trained Markov chain. That is, we have a Markov chain with around 3 to 5 states that emits feature vectors corresponding to a possible pronunciation of "ɪ". 

We can even hand-write such a Markov chain, as an example. We would start by collecting many recordings of "ɪ", then cut these into three parts: the start, the middle, and the end. We convert each part into a feature vector: $v_0, v_1, v_2$. We give each feature vector a state in the Markov chain. A trajectory through the Markov chain would start at $v_0$, stay there for a short while, then randomly jump to $v_1$, stay there for a long while, and then randomly jump to $v_2$, stay there for a short while, and finally jump to the "END" state. We would assign the transition probability $Pr(v_2 | v_2)$ to be roughly $1-1/N$, where $N$ is the average number of 10-ms segments that it takes to pronounce the middle of "ɪ".

The above system is the basic idea. It does not handle the effect of gliding between phonemes, like how "did you" would be pronounced as "dija". The standard way to handle this was the triphone model. The triphone model accounts for how a phoneme is changed by its two neighbors. For example, in the previous example, the "ɪ" should really be pronounced in the context of "tɪz". So, we would have a Markov chain just for "tɪz", which would generate a sequence of feature vectors that corresponds to how "ɪ" would be pronounced if it appears in the context of "tɪz". Since English has about 50 phonemes, a triphone model need ~2500 Markov chains, which explains why quad-phone models, or more, had not been popular. Even if one had the appetite for it, there was not enough data to train them.

You can add even more epicycles upon epicycles to account for more complex features like omitting phonemes, phonotactic constraints, prosody, etc.

![Rough sketch of HMM ASR. [Source](https://www.cs.cmu.edu/~roni/10601-slides/hmm-for-asr-whw.pdf)](figure/HMM_ASR.png)

### Collapse

What was so radical about Jelinek's approach?

One, to believe in the probabilistic system at all was a breakthrough during the Chomskyan period. As Liberman recounts in an anecdote in the 1970s:

> \[Kenneth Church was\] applying context-free parsing to phonotactic speech recognition, assuming a non-stochastic grammar. I suggested that Ken should find or create a collection of phonetic transcriptions, and use it to associate probabilities with his rewrite rules. Ken's response was to quote Richard Nixon's remark about Daniel Ellsberg: "We could kill him--but that would be wrong." Further discussion elicited a quote from one of his AI lab professors: "If you need to count higher than one, you've made a mistake."
>
> [@libermanObituaryFredJelinek2010]

Two, to throw away preconceptions of what the language model could be, and let results speak. He proceeded directly to using a 3-gram not because it resembles anything like how people really produce language, but because it works.

> In the future the design of a LM for a naturally generated text will probably involve considerations of syntax. semantics. and discourse pragmatics. So far no one has accomplished this.
> 
> [@jelinekSelfOrganizedContinuousSpeech1982] 

Three, to believe in the power of big data and big compute.

> Our models are derived from as much actual speech data as we can obtain and handle computationally. We have devised methods of automatic model computation, thus minimizing or completely eliminating human intervention. Our strategies are not based on rules developed from trying to intuit how people recognize sentences (as is prevalent elsewhere), although the basic structure of our models is, of course, man-made. This approach is both more accurate and more flexible; as the speaker or the components of the system change, our self-organizing programs remain valid, and computer time is all that is required to adjust to a new configuration.
> 
> [@jelinekSelfOrganizedContinuousSpeech1982]

More presciently, he was even considering the possibility of throwing away the preconceptions of what the features could be, and let data speak:

> Because we have found it more fruitful to view the acoustic processor as a data compressor than as an artificial phonetician, we do not attempt to identify phonetic segments in the continuous speech.
> 
> [@jelinekSelfOrganizedContinuousSpeech1982]

Previous MT systems typically performs multi-step processing on a raw audio signal, and somewhere in the middle, the audio signal would be transformed into a phonetic transcription. Jelinek, instead of following tradition and featurize audio into a list of phonemes ("an artificial phonetician"), just proposed to featurize audio in whatever way that seemed to work better for the system. As far as I see, this was not used in HMM ASR during the 1990s, but we know that automatic feature learning would finally be vindicated by neural networks around 2010.

In 2005, near the end of his life, Jelinek admitted that his quote was, despite his best hopes, genuine. 

> *Whenever I fire a linguist our system performance improves.* I have hoped for many years that this quote was only apocryphal, but at least two reliable witnesses have recently convinced me that I really stated this publicly in a conference talk (Jelinek, 1998). Accepting then that I really said it, I must first of all affirm that I never fired anyone, and a linguist least of all. So my motivation is defensive: to show that neither I nor my colleagues at IBM ever had any hostility to linguists or linguistics. In fact, we all hoped that linguists would provide us with needed help. We were never reluctant to include linguistic knowledge or intuition into our systems: if we didn't succeed, it was because we didn't find an efficient way to do include it.
>
> [@jelinekMyBestFriends2005]

Reflecting on a lifetime of multiple near-linguistic experiences, Jelinek seemed regretful. It is not that he wanted to fire linguists, it was simply that, somehow, attempting to incorporate linguistic expertise hurt performance.

Unlike in the case of MT, the collapse of logical AI in ASR was quite decisive. HMM-based statistical ASR completely replaced logical ASR in the 1990s, after which logical ASR was never revived. This was not the end of the history, however. Once ASR became a statistical problem, one immediately has the problem of feature engineering. Here, the history is similar to that of vision. One begins by imitating the human phoneticians with a nearest-neighbor matching to phonemes, and proceeds to handcrafted features computed over the audio spectrogram, to NN features learned over transforms of the spectrogram, and finally to NN features learned directly over the spectrogram. [@huangHistoricalPerspectiveSpeech2014] However, the story must be left for a future essay.

### Text to speech

While this section is titled "Speech", we have only discussed speech-to-text. Naturally, there is also the direction of text-to-speech. Here the history is more compressed, if for the simple reason that humans are *really good* at understanding speech. Even the most primitive kinds of text-to-speech was usable, and it was merely a matter of making it cheaper and less robotic-sounding.

At the forefront of speech synthesis research was Bell Labs, which was trying to compress and decompress speech to fit more telephone calls into telephone cables. In 1961 at Bell Labs, an IBM 7094 sang "Daisy Bell". Arthur Clarke, while visiting his friend John Pierce (this is the last time we'll see him), saw a demo of this, and he was so impressed that he put this scene into *2001: A Space Odyssey* (1968). [@woodRecollectionsJohnRobinson1991]

![The family tree of TTS systems up to 1987. [@klattReviewTexttospeechConversion1987, figure 4]](figure/TTS_family_tree.png)

In 1983, Digital Equipment Corporation started selling one of the first commercially successful TTS system, the [DECTalk](https://en.wikipedia.org/wiki/DECtalk), at the low price of $4000. It was based on the Klattalk of Dennis Klatt, who had dedicated his life to research on speech, and had the best TTS system available at the time. In a 1987 review of TTS [@klattReviewTexttospeechConversion1987], Klatt stayed close to the Chomskyan orthodoxy and described TTS as a problem of writing the generative grammar deeper down, from text all the way to the spectrogram of the physical sound.

::: {#fig-klatt-1987 layout-ncol=2}

![General architecture of a TTS system according to the Chomskyan orthodoxy. [@klattReviewTexttospeechConversion1987, figure 2]](figure/Klatt_1987_fig_2.png){#fig-todo}

![Architecture of Klattalk. [@klattReviewTexttospeechConversion1987, figure 3]](figure/Klatt_1987_fig_3.png){#fig-todo}

:::

In his literature review, Klatt did take notice of two upstart systems from the statistical side, one being [NETtalk of Sejnowski](https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/index.html#terence-sejnowski), a neural network model, and another being a statistical TTS system very similar to the IBM alignment model developed by Lucassen and Mercer, two comrades of Jelinek. He brushed both off as being inferior to a logical TTS system:

> When evaluated on the words of this training set, \[NETtalk\] was correct for about 90% of the phonemes and stress patterns. In some sense, this is a surprisingly good result in that so much knowledge could be embedded in a moderate number of about 25000 weights, but the performance is not nearly as accurate as that of a good set of letter-to-sound rules (performing without use of an exceptions dictionary, but with rules for recognizing common affixes)... the Lucassen and Mercer approach still results in an inferior words-correct error rate compared with traditional rule systems. Even a very powerful statistical package cannot yet discover much of the underlying structure in a process as complex as natural language... given the attention that NETtalk and other neuron-like devices have received recently, it is disturbing that NETtalk does not learn training set data perfectly, appears to make generalizations suboptimally, and has an overall performance that is not acceptable for a practical system. Furthermore, it is unlikely that larger training lexicons would converge to a more acceptable performance.

Indeed, though the word "rules" appears over 200 times in the review, the above dismissal was the only thing he had to say about learning-based methods. Hostility towards machine learning is a recurring feature among the Chomskyans. However, unlike the Chomskyans, Klatt was not going to be satisfied with writing toy respected the extreme complexity of a proper system of rules. And try he did:

> The hard part of text-to-speech synthesis is to calculate a string of LPC data, or formant-synthesis parameters, not from recorded speech, but from the letters and symbols of typed text... It's possible to write a simple program for this task, which produces robotlike speech-hard to understand and unpleasant to listen to. The alternative, which only Dennis Klatt and a few others have pursued, is to invest years of effort in devising an increasingly lengthy and subtle set of rules to eliminate the robotic accent.
> 
> He turns to a table with two volumes about the size of large world atlases, each stuffed with speech spectrograms... Spectrograms usually feature subtle and easily changing patterns. Klatt's task has been to reduce these subtleties to rules so that a computer can routinely translate ordinary text into appropriate spectrograms. "I've drawn a lot of lines on these spectrograms, made measurements by ruler, tabulated the results, typed in numbers, and done computer analyses," says Klatt.
> 
> As Klatt puts it, "Why doesn't DECtalk sound more like my original voice, after years of my trying to make it do so? According to the spectral comparisons, I'm getting pretty close. But there's something left that's elusive, that I haven't been able to capture. It has been possible to introduce these details and to resynthesize a very good quality of voice. But to say, 'here are the rules, now I can do it for any sentence' -- that's the step that's failed miserably every time."
> 
> But he has hope: "It's simply a question of finding the right model."
> 
> [@heppenheimerComputerTalkAmazing1984]

![Formant transitions for `[g]` as a function of preceding and following vowels. [@klattReviewTexttospeechConversion1987, figure 29]](figure/Klatt_1987_fig_29.png)

Klatt had been progressively losing his voice from thyroid cancer, and died in 1988. His voice was the template for "Perfect Paul" in DECTalk, which was the voice of Stephen Hawking, which he kept using even after better TTS systems were available. [@medeirosHowIntelGave2015]

## Vision


## Games

This chapter would be short since logical AI in games is well-known.

Artificial intelligence has abandoned the quest
for certainty and truth. The new patchwork rationalism is built upon
mounds of _micro-truths_ gleaned through common sense introspection, ad
hoc programming and so-called _knowledge acquisition_ techniques for
interviewing experts. The grounding on this shifting sand is pragmatic
in the crude sense__If it seems to be working, it_s right.

Samuel Checkers, DeepBlue.

## General

General Problem Solver, Symbolic hypothesis, McCarthy, Minsky

## Expert systems

> The miracle product is knowledge, and the Japanese are planning to package and sell it the way other nations package and sell energy, food, or manufactured goods... The essence of the computer revolution is that the burden of producing the future knowledge of the world will be transferred from human heads to machine artifacts.  
>
> [@feigenbaumFifthGenerationArtificial1984, Chapter 4]


## Language

### Chomsky

HMM models, and indeed, *all* probabilistic models of English, must fail, because English has central embedding.



### ELIZA

Weizenbaum became a fully human-centric critique of AI, culminating with *Computer Power and Human Reason: From Judgment to Calculation* (1976). In brief, the book had theese theses:

1. Computers might be intelligent, but they will never be wise (or to feel emotions, to love, etc). To "calculate" requires just intelligence, but to "judge" requires wisdom.
2. The development of AI threatens to replace judgment with calculation, which will destroy human dignity. This replacement of judgment with calculation is an absurd political ideology, and must be resisted politically.
3. If, however, we do not resist, but keep following this political ideology, we would end up with the obviously absurd conclusion that "the brain is merely a meat machine". *Reductio ad absurdum*.
4. This dangerous development of AI did not come from a wise scientific project of understanding how human intelligence works, but from a megalomaniac, obsessive-compulsive desire to make machine parodies of human behavior. The method of AI development was heuristic, empirical, a kind of "I wonder what the machine would do if I write this program...", without a scientific theory.

When the book came out, it received many reviews, and Weizenbaum wrote replies to these reviews. I found this one the funniest:

> I have in mind also the teaching urged on us by some leaders of the AI community that there is nothing unique about the human species, that in fact, the embrace of the illusion of human uniqueness amounts to a kind of species prejudice and is unworthy of enlightened intellectuals. If we find nothing abhorrent in the use of artificially sustained, disembodied animal brains as computer components, and if there is nothing that uniquely distinguishes the human species from animal species, then -- need I spell out where that idea leads?
>
> [@mccorduckMachinesWhoThink2004, page 370]

Apparently Weizenbaum, in his human-centered wisdom, rejected Darwinism as well. In any case, he stopped doing AI research since 1970, so it is quite useless to talk more about him.

### SHRDLU

For his PhD in mathematics, Terry Winograd programmed the SHRDLU during the years 1968--1970. In the program, the user carries on a conversation with the computer, moving objects, naming collections and querying the state of a simplified "blocks world", essentially a virtual box filled with different blocks. It was in around 500 KB of LISP code.

![The result of typing "Will you please stack up both of the red blocks and either a green cube or a pyramid?" [@winogradUnderstandingNaturalLanguage1972, figure 5]](figure/SHRDLU.png)

> it occupies around 200k of core (caution: indiscriminately running a job that big in the middle of the day is a good way to make enemies!!!!! Alway check the level of system usage before loading...)
>
> -- `mannew` from [the SHRDLU archive](code/SHRDLU.zip).

> The system answers questions, executes commands, and accepts information in normal English dialog. It uses semantic information and context to understand discourse and to disambiguate sentences. It combines a complete syntactic analysis of each sentence with a "heuristic understander" which uses different kinds of information about a sentence, other parts of the discourse, and general information about the world in deciding what the sentence means. It is based on the belief that a computer cannot deal reasonably with language unless it can "understand" the subject it is discussing. The program is given a detailed model of the knowledge needed by a simple robot having only a hand and an eye. We can give it instructions to manipulate toy objects, interrogate it about the scene, and give it information it will use in deduction. In addition to knowing the properties of toy objects, the program has a simple model of its own mentality. It can remember and discuss its plans and actions as well as carry them out. It enters into a dialog with a person, responding to English sentences with actions and English replies, and asking for clarification when its heuristic programs cannot understand a sentence through use of context and physical knowledge.
>
> Procedures as a Representation for Data in a Computer Program for Understanding Natural Language

As a fun side-note. The original SHRDLU had some manual fixes in the compiled assembly code (!). Terry Winograd's first research student rewrote much of SHRDLU so that it is portable. Some people sent letters (physical letters!) to request the code, and they would duly mail it out (by magnetic tape?). As one can imagine, only a few dozen source codes were mailed out.[^shrdlu-resurrection]

[^shrdlu-resurrection]: [SHRDLU resurrection](https://web.archive.org/web/20171117063022/http://www.semaphorecorp.com/misc/shrdlu.html). Created in 2002, and last updated on 2013-08-22.

It is pretty amusing now that 

## The Fifth Generation

In 1982, the Japanese was ready to take on the world.

With some irony, the Japanese might look back and call them the Gosei \[五世\].

## Expert systems

### Origins

Beginning in 1965, Edward Feigenbaum and Joshua Lederberg developed DENDRAL, first as a model to understand how organic chemists do their daily work: Given a molecule with this kind of mass spectra, this melting point, or other experimental data, what might be its chemical formula and molecular structure?



### The hype

### The fall

> One system for medical diagnosis, called [CADUCEUS](https://en.wikipedia.org/wiki/CADUCEUS_(expert_system)) (originally INTERNIST), has 500 disease profiles, 350 disease variations, several thousand symptoms, and 6,500 rules describing relations among symptoms. After fifteen years of development, the system is still not on the market. According to one report, it gave a correct diagnosis in only 75 percent of its carefully selected test cases. Nevertheless, Myers, the medical expert who developed it, "believes that the addition of another 50 [diseases] will make the system workable and, more importantly, practical."
>
> [@winograd10ThinkingMachines1991]

[@wolframAppraisalINTERNISTI1995]

Winograd argued that expert systems are structured like human bureaucracies, with formalized explicit rules, and so they have same strengths and weaknesses. This explains why they work only in stable and precise technical areas, where exceptions are not the rule. [@winograd10ThinkingMachines1991]

### Cyc

> AI has for many years understood enough about representation and inference to tackle this project, but no one has sat down and done it... only by pulling together the latest human interface tools, Lisp machines, ideas of enforced semantics, and funding for a decade-long effort could we attempt a project of this scale. [@lenatCycUsingCommon1985]

In 1984, [Douglas Lenat](https://en.wikipedia.org/wiki/Douglas_Lenat) began the Cyc project, an ambitious attempt to scale symbolic AI up to the real world. Like most expert systems, the Cyc project consists of a giant knowledge base encoded in a LISP-like symbolic logic language, upon which inference engines can be run to produce logical reasoning. Unlike most expert systems, the ambition of Cyc was universal: Its knowledge base would not be restricted to expert knowledge in a particular domain, but *all* commonsense knowledge in *all* domains that humans have commonsense about.

Even from the vantage point of 1985, it was clear to all that there was a lot of commonsense to code in, although few could have predicted that Lenat would persevere at it for over 30 years.

They themselves underestimated the difficulty. In 1990, they confidently titled a paper "Cyc: A midterm report" [@lenatCycMidtermReport1990], suggesting that they expected to be done around 1995.

The progress report in 1995 stated that, while the system is far from done, they have at least manually entered $10^5$ "general concepts" and $10^6$ "commonsense axioms" into Cyc, at the price of 100 person-years. [@lenatCycLargescaleInvestment1995]

> Moreover, statistics, colocation, and frequency do not resolve such questions. But the task goes from impossible to trivial if one already knows a few things about boxes and pens, police and demonstrators, and water and teakettles. The same sort of chicken-and-egg relationship characterizes CYC and ML because learning occurs at the fringe of what one already knows. Therefore, in the early 1980s, when the rest of the world was so enthusiastic about Natural Language Understanding, Machine Learning, and AI in general, we were pessimistic. We concluded the only way out of this codependency would be to prime the pump by manually crafting a million axioms covering an appreciable fraction of the required knowledge. That knowledge would serve as a critical mass, enabling further knowledge collection through NLU and ML, beginning in the mid-1990s. Mary Shepherd and I embarked on that task in 1984, knowing we had little chance of success, but seeing no alternative but to try... we are now moving toward the transition point where NLU and ML are supported. The rest of the world is disillusioned and pessimistic about symbolic AI, but ironically, as CYC reaches closure, our hopes for NLU and ML in the next 10 years are very high.
>
> [@lenatCycLargescaleInvestment1995]

> ... give CYC enough knowledge by the late 1990s to enable it to learn more by means of natural language conversations and reading. Soon thereafter, say by 2001, we planned to have it learning on its own, by automated-discovery methods guided by models or minitheories of the real world. To a large extent, that's just what we did. At the end of 1994, the CYC program was mature enough to spin off from MCC as a new company -- Cycorp -- to commercialize the technology and begin its widespread deployment.
>
> [@lenat20012001Common2001]

In 2016, Lenat finally declared the Cyc project "done" and set about commercializing it.

> Having spent the past 31 years memorizing an astonishing collection of general knowledge, the artificial-intelligence engine created by Doug Lenat is finally ready to go to work... most of what is left to be added is relevant to a specific area of expertise, such as finance or oncology.
> 
> Among other projects, the company is developing a personal assistant equipped with Cyc's general knowledge. This could perhaps lead to something similar to Siri... the CEO of Lucid says the new company is in talks with various others interested in using the Cyc knowledge base. Lucid has been working with the Cleveland Clinic, for example, to help automate the process of finding patients for clinical studies.
> 
> [@knightAI30Years2016]

That was essentially the last we heard from Cyc.

When looking at this figure from 1985, one is simultaneously filled with respect and sadness, for they were facing impossible odds, and yet they charged right into it.

![[@lenatCycUsingCommon1985, Figure 1]](figure/cyc_project_ontology.png)

Their "midterm report" only accentuates this sense of tragedy, of seeing them fighting impossible odds, and losing. They saw with clarity that there is no shortcut to intelligence, no "Maxwell's equations of thought".

> The majority of work in knowledge representation has dealt with the technicalities of relating predicate calculus to other formalisms and with the details of various schemes for default reasoning. There has almost been an aversion to addressing the problems that arise in actually representing large bodies of knowledge with content. However, deep, important issues must be addressed if we are to ever have a large intelligent knowledge-based program: What ontological categories would make up an adequate set for carving up the universe? How are they related? What are the important facts and heuristics most humans today know about solid objects? And so on. In short, we must bite the bullet.
>
> We don't believe there is any shortcut to being intelligent, any yet-to-be-discovered Maxwell's equations of thought, any AI Risc architecture that will yield vast amounts of problem-solving power. Although issues such as architecture are important, no powerful formalism can obviate the need for a lot of knowledge.
>
> By knowledge, we don't just mean dry, almanac-like or highly domain-specific facts. Rather, most of what we need to know to get by in the real world is prescientific (knowledge that is too commonsensical to be included in reference books; for example, animals live for a single solid interval of time, nothing can be in two places at once, animals don't like pain), dynamic (scripts and rules of thumb for solving problems) and metaknowledge (how to fill in gaps in the knowledge base, how to keep it organized, how to monitor and switch among problem-solving methods, and so on). Perhaps the hardest truth to face, one that AI has been trying to wriggle out of for 34 years, is that there is probably no elegant, effortless way to obtain this immense knowledge base. Rather, the bulk of the effort must (at least initially) be manual entry of assertion after assertion. [@lenatCycMidtermReport1990]

## Intelligence in the age of war machines

### Early war machines

The dream of peace produces war machines. Ancient Athenians dreamed of [Talos](https://en.wikipedia.org/wiki/Talos), a bronze giant who defended Crete, while the Jews of Prague dreamed of [Golem](https://en.wikipedia.org/wiki/Golem#Etymology). The ages has been too kind to Zhuge Liang, and his wheelbarrow, invented for carrying war supplies, became the legend of [wooden robot oxen](https://en.wikipedia.org/wiki/Wooden_ox).

The industrial revolution produced early ideas of feedback mechanism, such as the [centrifugal governor](https://en.wikipedia.org/wiki/Centrifugal_governor) and the [gyro autopilot](https://en.wikipedia.org/wiki/Gyroscopic_autopilot). It took until WWII for someone to put together high explosives, slow burning fuel, and the feedback mechanism, to create the cruise missile. The first of its kind was the V-1 rocket, which already incorporates the basic features of all cruise missiles.

The V-1 looks like a small unmanned jet airplane. It has the following senses: roll (gyro), pitch (gyro), yaw (magnetic compass), altitude (barometer), distance traveled (vane anemometer). The roll, pitch, yaw, and altitude are maintained by negative feedback. So for example, if the missile is heading east to the set-point of yaw, a valve would open, and compressed gas would force the rudder to turn, which yaws the missile west. As soon as the vane anemometer has turned a designated number, the missile considers itself to have reached the target. It turns off the engine and sharply dives to the ground, and explodes upon impact.

Other than V-1 and V-2, there were no autonomous war machines during WWII, though there were several radio-controlled weapons such as [explosive little tanks](https://en.wikipedia.org/wiki/Goliath_tracked_mine) and [little planes for target practice](https://en.wikipedia.org/wiki/Radioplane_OQ-2). Skinner, thinking outside (inside?) the box, worked on [Project Pigeon](https://en.wikipedia.org/wiki/Project_Pigeon). He trained pigeons in skinner boxes to peck at the ship appearing on a screen. If the pigeon is pecking on the top-left, then the missile would turn to the bottom-right. In effect, the pigeon becomes the negative feedback controller. Though it was cancelled, it would have been considerably cheaper than [the Japanese version](https://en.wikipedia.org/wiki/Kamikaze) should it have ever reached production.

<video controls width=100%>
  <source src="figure/Project%20Pigeon.webm" type="video/webm" />
</video>

After WWII, some autonomous defense systems were developed and deployed, such as [close-in weapon systems](https://en.wikipedia.org/wiki/Close-in_weapon_system) on ships. These detect incoming incoming missiles and enemy aircraft by radar, computes their trajectories, and shoots them down. Since they must operate on the time-scale of seconds, they are fully automatic with no human in the loop. Despite this, these are quite uncontroversial and do not typically earn the title of "killer robots", presumably because compared to autonomous *offense*, defense is inherently more controllable and predictable in effect.

### Nuclear war machines

> Deterrence is the art of producing in the mind of the enemy the fear to attack. And so, because of the automated and irrevocable decision making process which rules out human meddling, the doomsday machine is terrifying. It's simple to understand. And completely credible, and convincing... When you merely wish to bury bombs, there is no limit to the size. After that they are connected to a gigantic complex of computers. Now then, a specific and clearly defined set of circumstances, under which the bombs are to be exploded, is programmed into a tape memory bank.
>
> --- [Dr. Strangelove](https://en.wikipedia.org/wiki/Dr._Strangelove) (1964)

During the Cold War, the following technological factors of nuclear weapons determined the grand nuclear strategy.

1. First-strike nuclear offense is impossible to defend against. Some bombs will get through all defense.
2. Nuclear weapon is so much more powerful than non-nuclear weapons, that the only proportionate deterrence to a nuclear attack is another nuclear attack.
3. First-strike capability is indistinguishable from second-strike capability.

Because of (1) and (2), the only way to deter a nuclear first-strike was to threaten a nuclear second-strike. This is the [MAD](https://en.wikipedia.org/wiki/Mutual_assured_destruction) nuclear deterrence doctrine. Because deterrence requires enough bombs to survive a first-strike, both sides would rather build up more second-strike bombs than the other side's first-strike bombs. Because of (3), there aren't "second-strike bombs" vs "first-strike bombs", only bombs. Therefore, we have a positive feedback loop where both sides aim to have more bombs than the other -- the [nuclear arms race](https://en.wikipedia.org/wiki/Nuclear_arms_race). Because having too many bombs increases the chance of accidents, both sides are motivated to slow down the race. Thus the [ABM Treaty](https://en.wikipedia.org/wiki/Anti-Ballistic_Missile_Treaty), where both sides agree to *not* build many missile defense systems! This paradoxical treaty was designed to make both sides *more* vulnerable to second-strike, meaning that less bombs are needed to ensure second-strike capability.

Both sides' nuclear technology went through several iterations, with increasing second-strike capability, and ended up with the "nuclear triad" of bombers that stay in the air 24/7, submarines hidden under the sea, and ICBMs hardened inside silos. Each of the three has different tradeoffs, necessitating all three to be maintained.

Of course, even if the triad survives the first-strike, it is no good if they won't activate. The command center might be destroyed. The communication lines might be cut. The soldiers might refuse to launch based on their own conscience. All these dangers lead to the pressure to automate second-strike. The pinnacle of this logic was the [Supersonic Low Altitude Missile (SLAM)](https://en.wikipedia.org/wiki/Supersonic_Low_Altitude_Missile), a cruise missile powered by a nuclear engine. The nuclear engine is like a fission nuclear reactor in nuclear power plants, except that the fission power does not boil water, but heat up air, which expands and shoots out from the tail of the missile, allowing it to fly at Mach 3.

Despite having no GPS (it was the 1960s!), because the SLAM would fly $\sim 200 \;\mathrm{m}$ above ground, it could navigate itself by [terrain contour matching](https://en.wikipedia.org/wiki/TERCOM): It compares a height-scan of the local terrain against a stored copy of the terrain. Even after dropping all its nuclear warheads, it can remain airborne for weeks, destroying the ground with sonic booms as overkill. The project was shelved in 1964, apparently considered too destabilizing, and they settled for just drilling the nuclear launch routines into the missileers until they work like robots that would not hesitate to execute the launch command.

![The terrain contour matching algorithm. By a curious coincidence, the algorithm typically attempts to find a line segment within the stored terrain map that minimizes the [Mean Absolute Deviation](https://en.wikipedia.org/wiki/Average_absolute_deviation) with the local terrain of the missile... also with the "MAD" acronym. [@goldenTerrainContourMatching1980, figure 2]](figure/TERCOM.png)

In the movie *Dr. Strangelove* (1964), nuclear deterrence was taken to its logical end point. In the movie, the Soviet Union built a "doomsday machine", which is a Cobalt bomb that when exploded, makes enough fallout to render the entire earth uninhabitable for a century. This was then connected to sensors around the Soviet Union, so that any nuclear attack automatically triggers it. Finally, the machine triggers if it detects attempts to un-trigger it, thus closing the logic loop and making it a fully automatic deterrence machine.[^dead-hand]

[^dead-hand]: I was going to write something about the [Dead Hand](https://en.wikipedia.org/wiki/Dead_Hand) system, but after a brief search, the available information looks too much like conspiracy theory and rumors, so I will not.

### Intelligence

Though this essay is on logical AI, a brief detour about neural network is in order.

During the Cold War, intelligence, in the sense of intelligence-gathering and reconnaissance was a vital area of artificial intelligence. Some of the early neural networks, such as MINOS II, was explicitly built with an objective of scanning aerial photographs for interesting military targets like tanks. [@nilssonQuestArtificialIntelligence2009, pages 98--109] The CIA even [experimented with Rosenblatt's Mark I Perceptron machine](https://www.cia.gov/readingroom/document/cia-rdp78b04770a002300030027-6) for the same purpose [@irwinArtificialWorldsPerceptronic2024]. As an example, [@kanalRecognitionSystemDesign1964] describes a two-layered perceptron network, of type $\R^{N \times N} \to \{0, 1\}^{32\times 32} \to \{0, 1\}^{24} \to \{0, 1\}$. It works as follows:

* The grayscale photo is down-scaled and binarized by convolution with a [discrete Laplace filter](https://en.wikipedia.org/wiki/Discrete_Laplace_operator): $\R^{N \times N} \to \{0, 1\}^{32\times 32}$.
* The weights for the 24 hidden perceptrons are constructed by [linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis): $\{0, 1\}^{32\times 32} \to \{0, 1\}^{24}$
* The output perceptron is learned by the [perceptron learning rule](https://en.wikipedia.org/wiki/Perceptron#Learning_algorithm_for_a_single-layer_perceptron): $\{0, 1\}^{24} \to \{0, 1\}$.

::: {#fig-kanal-1964-neural-tanks layout-ncol=2}

![Grayscale photos, some containing tanks, and some not.](figure/kanal_1964_fig_tank_nontank_mosaic.png){#fig-kanal-1964-neural-tanks-tank-nontank-mosaic}

![A picture of a tank after convolution with a discrete Laplace filter.](figure/kanal_1964_fig_binary_image_tank.png){#fig-kanal-1964-neural-tanks-binary-image-tank}

![The architecture of the network.](figure/kanal_1964_fig_architecture.png){#fig-kanal-1964-neural-tanks-architecture}

Images from [@kanalRecognitionSystemDesign1964].
:::

### Strategic Computing Project

The Strategic Computing Project (SCI) was named to resemble the [Strategic Defense Initiative](https://en.wikipedia.org/wiki/Strategic_Defense_Initiative), better known as "Star Wars". The SDI was a project to 


## Philosophy

Astrophysicists, measuring the motion of galaxies, notice that the stars are rotating too fast. There is not enough matter to tether them, and they ought to fly out into the intergalactic emptiness. To solve this problem, they proposed "dark matter", unknown matter that provides the missing gravitational force, and physicists ever since had been searching them.

Similarly, sometimes when you read through a field of study, you notice that the arguments seem to rotate around some kind of unspoken assumption that you might see if you just take all the books and throw them to the ground, so that they are all opened at random places. Then you turn your head and squint at the words refracted through the eyelids, the pupils, and the cornea, which you have repurposed as a primitive kind of optical computer. And you see the dark matter, the intellectual centers of gravity.

> Under an invisible spell, they will each start out anew, only to end up revolving in the same orbit once again... their thinking is not nearly as much a discovery as it is a recognition, remembrance, a returning and homecoming into a distant, primordial, total economy of the soul, from which each concept once grew: -- to this extent, philosophizing is a type of atavism of the highest order.
>
> [@nietzscheGoodEvilPrelude2002, Section 1.20]

### The Symbolic Hypothesis

General Problem Solver

*Human Problem Solving*

![[@newellHumanProblemSolving1972, page 533]](figure/human_problem_solving_behavior_graph.png)

![[@newellHumanProblemSolving1972, page 534]](figure/human_problem_solving_transcript.png)


In [their Turing award lecture of 1975](https://yuxi-liu-wired.github.io/docs/posts/1975-herbert-simon-allen-newell/), Allen Newell and Herbert Simon gave a definitive statement of the Physical Symbol System Hypothesis for AI, which they had labored under, first unconsciously but then consciously, since the early 1950s.

They began by some examples of "qualitative structure" in science, which do not deal with numbers, but with symbols. For example, a basic statement of cell theory -- "organisms are made of little cells that are mostly alike" -- doesn't contain a single number, yet it has great significance. Such non-numerical discrete statements are made of "symbols", and they stated their **Physical Symbol System Hypothesis** (PSSH):

> A physical symbol system has the necessary and sufficient means for general intelligent action.

where a physical symbol system is essentially a machine that can be built in our physical world, and that manipulates with symbols. As an example, a digital computer running a LISP interpreter is a physical symbol system. We can attach cameras and wheels to the computer, so that the camera sends into the interpreter a symbolic representation of what it sees, and the wheels receives symbolic commands for motion. This is basically a robot according to the PSSH.

They also gave a **Heuristic Search Hypothesis**:

> A physical symbol system exercises its intelligence in problem solving by search-that is, by generating and progressively modifying symbol structures until it produces a solution structure.

What is not said is equally revealing. In the lecture, there were over 100 mentions of the word "search", but the only statement about learning is... a mention of Plato's theory of [anamnesis](https://en.wikipedia.org/wiki/Anamnesis_(philosophy))! For them, a symbolic system starts out already with a solution generator and a solution tester, and problem-solving is nothing but heuristically searching over the generated solutions until one passes the tester. Indeed, in their telling, AI research is just search, not learning.

> ... During the first decade or so of artificial intelligence research, the study of problem solving was almost synonymous with the study of search processes. From our characterization of problems and problem solving, it is easy to see why this was so. In fact, it might be asked whether it could be otherwise. ...  There is no mystery where the information that guided the search came from. We need not follow Plato in endowing the symbol system with a previous existence in which it already knew the solution. A moderately sophisticated generator-test system did the trick without invoking reincarnation.

### Cognitivism

There was a persistent legend about a certain psychologist, that he educated his own children with [skinner boxes](https://en.wikipedia.org/wiki/Operant_conditioning_chamber). While just a legend, the real B. F. Skinner did have a philosophy as radical as the legend suggests.

In the 1950s, Skinner dominated behaviorism, which dominated American psychology. In short, behaviorism models animal behavior as stimulus-response reflexes, which can be understood as parameterized functions $a_\theta(o)$, where $o$ stands for the observational stimulus, $\theta$ the internal parameters of the animal, and $a_\theta(o)$ the response action. The parameters $\theta$ is a function of the previous history of stimuli and reward/punishments: $(o_0, a_0, r_0, o_1, a_1, r_1, \dots)$. The set up is the same as modern reinforcement learning (RL).

[^skinner-box]:
    [Feynman told of the following maze-running rat legend](https://gwern.net/maze):

    > He had a long corridor with doors all along one side where the rats came in, and doors along the other side where the food was. He wanted to see if he could train the rats to go in at the third door down from wherever he started them off. No. The rats went immediately to the door where the food had been the time before. The question was, how did the rats know, because the corridor was so beautifully built and so uniform, that this was the same door as before? Obviously there was something about the door that was different from the other doors. So he painted the doors very carefully, arranging the textures on the faces of the doors exactly the same. Still the rats could tell. Then he thought maybe the rats were smelling the food, so he used chemicals to change the smell after each run. Still the rats could tell. Then he realized the rats might be able to tell by seeing the lights and the arrangement in the laboratory like any commonsense person. So he covered the corridor, and, still the rats could tell. He finally found that they could tell by the way the floor sounded when they ran over it. And he could only fix that by putting his corridor in sand. So he covered one after another of all possible clues and finally was able to fool the rats so that they had to learn to go in the third door. If he relaxed any of his conditions, the rats could tell.

The great thing about skinner boxes is that they are standardized to be lightproof, soundproof, and whatever-proof, thus controlling for all confounding variables. Before skinner boxes, mouse experiments were full of confounding variables, and had a kind of replication crisis in the 1930s. With the skinner box, the degrees of rat freedom are minimized, turning rats into standardized systems. This finally allowed measurable progress, allowing the breakout success of behaviorism in the 1950s.[^skinner-box]

Skinner's ambitions went far beyond rats. In 1957, he published [*Verbal Behavior*](https://en.wikipedia.org/wiki/Verbal_Behavior), in which he explained human language as stimulus-response networks, built up piece by piece during child development. To give an example, when one searches for a book with a title "Verbal Behavior", one would say "Verbal Behavior, Verbal Behavior, Verbal Behavior..." (a "self-echoic") while the eye scans the shelf. When the visual stimulus matches the verbal stimulus, the "grab book" action is triggered (a "tact"). The touch of the hand with the book then stops self-echoic behavior. In Skinner's terms, this verbal behavior is a "[descriptive autoclitic](https://en.wikipedia.org/wiki/Autoclitic)".

He was at his most radical in [*Beyond Freedom and Dignity* (1971)](https://en.wikipedia.org/wiki/Beyond_Freedom_and_Dignity), which essentially argued that human society will be reorganized according to behaviorist principles. Instead of the indirect and unreliable behavior control using verbal moral judgment, a society would use more direct operant conditioning methods that are experimentally proven by behaviorist psychologists.

Though behaviorism has remained alive and well to this day, linguistics took a sudden turn around 1960 thanks to Noam Chomsky. According to legend, Chomsky wrote a review of *Verbal Behavior* in 1959, in which he soundly routed behaviorist linguistics [@chomskyReviewSkinnersVerbal1959]. The truth is more complicated, since Chomsky also wrote several other famous works like [*Syntactic Structures* (1957)](https://en.wikipedia.org/wiki/Syntactic_Structures), [*Aspects of the Theory of Syntax* (1967)](https://en.wikipedia.org/wiki/Aspects_of_the_Theory_of_Syntax), and the foundational papers on formal grammar like the [Chomsky hierarchy](https://en.wikipedia.org/wiki/Chomsky_hierarchy).

Chomsky argued that there are two ways of doing research in psychology and linguistics: "empiricism" and "rationalism". Skinner's book was the best example of empiricism, and since Skinner's book is wrong, the book becomes a *reductio ad absurdum* of empiricism. Instead, one must turn back to rationalist psychology and linguistics.

> I had intended this review not specifically as a criticism of Skinner's speculations regarding language, but rather as a more general critique of behaviorist (I would now prefer to say "empiricist") speculation as to the nature of higher mental processes... I do not, in other words, see any way in which his proposals can be substantially improved within the general framework of behaviorist or neobehaviorist, or, more generally, empiricist ideas that has dominated much of modern linguistics, psychology, and philosophy. The conclusion that I hoped to establish in the review, by discussing these speculations in their most explicit and detailed form, was that the general point of view was largely mythology, and that its widespread acceptance is not the result of empirical support, persuasive reasoning, or the absence of a plausible alternative.
> 
> -- Preface to the 1967 reprint. [@chomskyReviewBFSkinners1967]

What is wrong with empiricism? Fundamentally, Chomsky's argument is based on two properties that every human language has:

* Infinity: There exists infinitely many grammatical sentences, and infinitely many ungrammatical sentences.
* Generativity: Humans can agree with each other whether a never-before-seen sentence is grammatical or ungrammatical.

Plato observed that, though we have only seen imperfect geometric shapes, we have a concept of perfect circles, triangles and so on. He inferred from this that we are born with the ideal concepts within us, to be matched against imperfect shapes out there. Similarly, Chomsky argued that we are born with the ideal Universal Grammar within us, to be matched against the language observations in the world.

For example, in the Universal Grammar, there is a setting called "subject-verb-object order", with 6 possible values: SVO, SOV, ..., OVS. An infant need only observe a few dozen sentences to fix this setting. Chomsky extended this project to all of natural language grammar.

Chomsky made a multi-pronged rejection of empiricism:

* Poverty of stimulus: Humans learn language with just a few years of language instruction. This is impossible if they "start from scratch". Thus, they have a lot of inborn grammar.
* Colorless green ideas sleep furiously: Because there are many meaningless but grammatical sentences, grammar is independent of meaning. Thus, grammar, unlike meaning, can be a small closed system that fits inside a brain at birth.
* Probability is meaningless: A sentence is either grammatical or not. THere is no in-between. Therefore it is meaningless to talk about the "probability of a sentence" like the empiricists.
* Impersonal abstraction: Real humans can make mistakes, but language itself makes no mistake. A tired human might say "Ideas sleep furiously." is ungrammatical 10\% of the times, but the English language *itself* would always say it is grammatical. Thus, the probabilities measured from real human behavior is meaningless for theoretical linguistics.
* Capacity, not adequacy (TODO find the exact words): Even if an empirical model of language, like the HMM, works 99% of the times, a single counterexample disproves it. To work "most of the times" is capacity. To work all the time is adequacy. Linguistic science is about adequacy, and capacity is sufficient only for engineering.

The 60 years afterwards have been one long struggle against empiricism along these lines. For example, he had repeatedly simplified the Universal Grammar down to the minimalist core, so that it could possibly fit inside a single gene that appeared once in the evolution of humans, "switching on" language for humans and no other species. And by "60 years", I meant it. He was nothing if not consistent:

> Evidently, one's ability to produce and recognize grammatical utterances is not based on notions of statistical approximation and the like... a structural analysis cannot be understood as a schematic summary developed by sharpening the blurred edges in the full statistical picture... that grammar is autonomous and independent of meaning, and that probabilistic models give no particular insight into some of the basic problems of syntactic structure.
>
> [@chomskySyntacticStructures1957, pages 16--17]

> But it must be recognized that the notion of "probability of a sentence" is an entirely useless one, under any known interpretation of this term.
>
> [@chomskyEmpiricalAssumptionsModern1969]

> It's true there's been a lot of work on trying to apply statistical models to various linguistic problems. I think there have been some successes, but a lot of failures. There is a notion of success ... which I think is novel in the history of science. It interprets success as approximating unanalyzed data. 
> 
> Chomsky at the MIT150: Brains, Minds and Machines Symposium (2011), quoted in [@norvigChomskyTwoCultures2017]

> Well, first we should ask the question whether large language models have achieved anything, anything, in this domain? Answer: No! They've achieved zero... GPT-4 is coming along, which is supposed to going to have a trillion parameters. It will be exactly the same. It'll use even more energy and achieve exactly nothing, for the same reasons. So there's nothing to discuss.
>
> [Machine Learning Street Talk: The Ghost in the Machine and the Limits of Human Understanding (2022)](https://whimsical.com/question-1-large-language-models-such-as-gpt-3-DJmQ8R5kD8tqvsXxgCN9vK)

Knowing his framework, we understand why he rejected both statistical models like n-grams or neural models like GPT-3 as "zero" in terms of advancing theoretical linguistics. They are empirically constructed: thousands of researchers were just trying things out, and eventually they started to work. Worse, both kinds of models assign *nothing but* probabilities to sentences!

The influence of Chomsky had been immense, both in linguistics and psychology. Chomskyan linguistics is the theoretical foundation to early MT systems like Vanquois' (TODO). In psychology, his approach became "cognitivism" TODO

### The Sub-symbolic Hypothesis

The dominance of the Symbolic Hypothesis was never complete. There were often objections to it.

In 1972, Hubert Dreyfus raised a fuss with a book *What Computers Can't Do: The Limits of Artificial Intelligence*. Unfortunately, his work was based on the phenomenology of Merleau-Ponty and Heidegger, who wrote like *real and genuine* philosophers,[^feigenbaum-cotton-candy] so I just resorted to checking [his Wikipedia article](https://en.wikipedia.org/wiki/Hubert_Dreyfus%27s_views_on_artificial_intelligence). From what I gathered, his argument was that the Symbolic Hypothesis is flawed in the sense that it is too "closed".

[^feigenbaum-cotton-candy]: 
    Edward Feigenbaum's reaction is representative of most AI scientists:
    
    > What artificial intelligence needs is a good Dreyfus. The conceptual problems in AI are really rough, and a guy like that could be an enormous help... But Dreyfus bludgeons us over the head with stuff he's misunderstood and is obsolete anyway -- and every time you confront him with one more intelligent program, he says, "I never said a computer couldn't do that." And what does he offer us instead? Phenomenology! That ball of fluff! That cotton candy! [@mccorduckMachinesWhoThink2004, pages 229--230]

Fortunately, before I gave up, someone pointed me to a better, previous report [@dreyfusAlchemyArtificialIntelligence1965]:

> having read "Alchemy and Artificial Intelligence" for this investigation... It seems to me that Dreyfus' 1965 critiques of 1960s AI approaches were largely correct, for roughly the right reasons, in a way that seems quite impressive in hindsight... Dreyfus' arguments are inspired by his background in phenomenological philosophy, but he expresses his arguments in clear and straightforward language... the AI community hardly responded to Dreyfus' critique at all -- and when they did, they often misrepresented his claims and arguments in ways that are easy to detect if one merely checks Dreyfus' original report.
>
> [@muehlhauserWhatShouldWe2016]

With that, I managed to understand Dreyfus' argument clearly. His main point is that logical AI, exemplified by Simon and Newell's GPS,  cannot reach human-level reasoning, because logical AI cannot perform 3 fundamental human forms of information processing: fringe consciousness, essence/accident discrimination, and ambiguity tolerance. These require brain-like computers to perform.

* Fringe consciousness: Chess players do not analyze positions like the GPS. Whereas the GPS performs a unified heuristic search, chess players seem to unconsciously scan a large number of positions, making some variations seem salient, which they then consciously "count out". The unconscious scanning is *not* search, since otherwise why not perform unconscious search all the way to the finish line? The conscious "counting out" is captured by the GPS, but the unconscious "zeroing in" is not.
* Essence/accident discrimination: Even on simple, well-defined problems like cryptogram puzzles, the GPS and humans reason very differently. Humans can simplify problems with symmetry like "for basically the same reason...", and backtrack based on an understanding of the overall problem structure. Even Simon and Newell have labelled their human subjects' reasoning steps as either "essential" or "inessential". GPS solves a problem like humans do only if the programmers have so helpfully pre-structured the problem with the essential features. These all show that human reasoning distinguishes some as essential while others as accidental, and focuses their conscious problem solving over the essential features, something GPS cannot do.
* Ambiguity tolerance: So far, machine translation and language understanding research had only worked on highly artificial and restricted domains, since none of them could handle the ambiguity in natural language. Humans can handle this massive ambiguity because humans process language not according to precise rules, but by unconscious processing that statistically combines context, goals, and background knowledge, and produces *precise enough* outputs.

From our vantage point, chess ended up falling to the heuristic search of DeepBlue, but Go did require combining "fringe consciousness" with heuristic search in the CNN+MCTS architecture of AlphaGo. The three forms of information processing he singled out as requiring brain-like computing turned out to be precisely where neural networks achieved breakout success. Of course, the logical AI camp would not go down without a fight, and with "Parallel Distributed Processing" -- the revival of neural networks in 1980s -- there would be a bitter dispute, centered around, of all things, how to form the past tense of English verbs.

[Douglas Hofstadter](https://en.wikipedia.org/wiki/Douglas_Hofstadter) raised a similar objection, but from within the logical AI tradition. His essential point was that logical AI, as conceived by Simon and Newell, is a closed system. For example, a chess computer might play a perfect chess even when the room is on fire. The concept of fire, or indeed anything that is beyond the mathematical strucure of chess, does not feature in the computer's symbolic universe, and so it does not exist for the computer.[^holt-chess-fire] Whereas Dreyfus argued that intelligence is open by the "cotton candy" of phenomenology, sieging the head from the outside in, Hofstadter used the Gödel incompleteness theorems to break the head open, from the inside out.

[^holt-chess-fire]:
    This example was given by Anatol Holt at ARPA Principal Investigators' Conference in 1974, and quoted in [@winograd10ThinkingMachines1991]:

    > A brilliant chess move while the room is filling with smoke because the house is burning down does not show intelligence. If the capacity for brilliant chess moves without regard to life circumstances deserves a name, I would naturally call it "artificial intelligence."

The rough idea is that any symbolic system that is powerful enough would be incomplete, and furthermore, would "reflect" its own incompleteness. The incompleteness theorems state that in any sufficiently powerful formal system, there are statements that are true about the system that cannot be proven within the system. For example, let $\Sigma$ be the logical system of Peano arithmetics, then by the incompleteness theorems, there are sentences like [Rosser's sentence](https://en.wikipedia.org/wiki/Rosser%27s_trick), or "PA is consistent", which are *provably unprovable* assuming PA is consistent.

Stated in another way, arithmetical truth cannot be defined in arithmetic -- [Tarski's undefinability](https://en.wikipedia.org/wiki/Tarski%27s_undefinability_theorem).

> O God, I could be bounded in a nut shell and count
> myself a king of infinite space, were it not that I
> have bad dreams.
>
> --- Hamlet, Act 2, Scene 2

In the context of AI, this means that a symbolic system, no matter how complex, will always have limitations in representing and reasoning about the world, even if the world is but itself. We need not assume there is an "outside". The symbolic system, even when floating in a mathematical vacuum, creates its own outside.

His solution was to add in some "strange loops", which would both create general intelligence and create consciousness in one fell swoop. These strange loops are hierarchical structures where crossing levels leads back to the starting point. In the context of AI, Hofstadter proposed that implementing strange loops within a symbolic system would allow the system to refer to itself and its own structure, potentially leading to self-awareness and general intelligence. This self-referential capability would enable the system to overcome the limitations imposed by the incompleteness theorems and exhibit more flexible and human-like intelligence.

Hofstadter was a far better writer than Dreyfus, and his 1979 book *Gödel, Escher, Bach* was a popular hit among both the common people and the computer scientists. Despite this, the trajectory of AI did not go as he expected.

> There may be programs which can beat anyone at chess, but they will not be exclusively chess players. They will be programs of general intelligence, and they will be just as temperamental as people. "Do you want to play chess?" "No, I'm bored with chess. Let's talk about poetry." [@hofstadterGodelEscherBach1999, page 678]

In the preface to the second edition (1999), Hofstadter admitted that this prediction went way off, but still committed to the philosophical belief behind these. Unfortunately, the disappointments did not stop coming. In 

With the rise of Transformer-based language models, . He expressed his confusion and traumatic response in a 2023 interview:

> \[In 1960,\] I knew how computers worked, and I knew how extraordinarily rigid they were. You made the slightest typing error and it completely ruined your program... It felt as if artificial intelligence was the art of trying to make very rigid systems behave as if they were fluid... I felt it would be hundreds of years before anything even remotely like a human mind would be asymptotically approaching the level of the human mind, but from beneath... But when certain systems started appearing maybe 20 years ago, they gave me pause. And then this started happening at an accelerating pace where unreachable goals and things that computers shouldn't be able to do started toppling. The defeat of Gary Kasparov by Deep Blue, and then going on to Go systems, systems that could defeat some of the best Go players in the world. And then systems got better and better at translation between languages and then at producing intelligible responses to difficult questions in natural language, and even writing poetry.
> 
> My whole intellectual edifice, my system of beliefs--it's a very traumatic experience when some of your most core beliefs about the world start collapsing... I think about it practically all the time, every single day... and it overwhelms me and depresses me in a way that I haven't been depressed for a very long time.
>
> [Reflections on AI](https://www.buzzsprout.com/222312/episodes/13125914), interview with Douglas Hofstadter by Amy Jo Kim (2023-06-29)

### Statistical learning theory

In the 1960s, Vladimir Vapnik was working on a Ph.D. in statistics, but due to multiple political problems, he was forced to withdraw the thesis and republish it as a book, in which he laid out his vision of statistical learning theory.

> From the KGB's point of view I was a wrong person to obtain the doctoral level: I was not a member of the Communist Party, I was Jewish, my PhD adviser, Alexander Lerner, had applied for immigration to Israel and became a "refusenik," some of my friends were dissidents, and so on... The main message that I tried to deliver in the book was that classical statistics could not overcome the curse of dimensionality but the new approach could. I devoted three chapters of the book to different classical approaches and demonstrated that none of them could overcome the curse of dimensionality. Only after that did I describe the new theory.
> 
> [@vapnikEstimationDependencesBased2006]

In short, Vapnik's theory is to on the one hand, minimize the training loss ("empirical risk minimization") by any statistical model, but on the other hand, avoid overfitting by controlling capacity ("VC dimension") of the model. This was a radical departure from the previous kind of "classical statistics", which, while not Chomskyan, was very close in spirit.

> (1) There is a group of philosophers who believe that the results of scientific discovery are the real laws that exist in nature. These philosophers are called the realists.
> 
> (2) There is another group of philosophers who believe the laws that are discovered by scientists are just an instrument to make a good prediction. The discovered laws can be very different from the ones that exist in Nature. These philosophers are called the instrumentalists.
> 
> The two types of approximations defined by classical discriminant analysis (using the generative model of data) and by statistical learning theory (using the function that explains the data best) reflect the positions of realists and instrumentalists in our simple model of the philosophy of generalization, the pattern recognition model. Later we will see that the position of philosophical instrumentalism played a crucial role in the success that pattern recognition technology has achieved.
>
> [vapnikEstimationDependencesBased2006, page 415]

Vapnik's books were chock-full of philosophical musings, now rarely read. Leo Breiman's philosophical musings, however, has remained famous. As a reference to [C. P. Snow's "Two Cultures" lecture of 1959](https://en.wikipedia.org/wiki/The_Two_Cultures), Breiman wrote [@breimanStatisticalModelingTwo2001] to contrast the "data modeling culture" with the "algorithmic modeling culture".

The data modeling culture first constructs a [generative model](https://en.wikipedia.org/wiki/Generative_model) of the data with a few parameters $\theta$, then construct [ways to estimate](https://en.wikipedia.org/wiki/Estimator) $\theta$ from data, with provable guarantees. This culture includes both the frequestists and the Bayesians.

On the plus side, a generative model with a few parameters is easy to interpret. On the minus side, if the statistician were to interpret parameters subsequently,[^easily-check-statistical-interpretation] then the parameters had better mean something real, which requires the model to be close to reality. That is, the price for interpretability is eternal vigilance (against model misspecification).

[^easily-check-statistical-interpretation]: One can easily check this by opening a journal in economics or medicine and look at the "results" section of some papers. There one would find parameters decorated with error bars, followed by policy recommendations, such as "Since $\theta_{6} > 0$ with statistical significance, we conclude that elevating melatonin has a positive impact...". If $\theta$ in this imaginary example is used in a data model like $y = \sum_i \theta_i x_i + \epsilon$, where $\epsilon$ is noise, then for the sake of this imaginary author, the real biological response of the human had better be sufficiently linear to $x_1, x_2, \dots$, at least for those $x$ within the therapeutic range.

> This enterprise has at its heart the belief that a statistician, by imagination and by looking at the data, can invent a reasonably good parametric class of models for a complex mechanism devised by nature... If the model is a poor emulation of nature, the conclusions may be wrong. These truisms have often been ignored in the enthusiasm for fitting data models. A few decades ago, the commitment to data models was such that even simple precautions such as residual analysis or goodness-of-fit tests were not used.

The algorithmic modeling culture in contrast is very empirical -- if it works on the training set, and if statistical learning theory predicts that its train-test gap is low ("capacity control" again), then it works. Data modeling culture was stuck with generative models, unable to use tools that work despite not interpretable as generative models -- random forests, boosting, bagging, neural networks, all the latest algorithms popping up in the 1990s.

Among the many replies to this bombshell of an editorial, [David Cox](https://en.wikipedia.org/wiki/David_Cox_(statistician)) went directly for the philosophical:

> Professor Breiman takes a rather defeatist attitude toward attempts to formulate underlying processes; is this not to reject the base of much scientific progress?

Whereas [Bradley Efron](https://en.wikipedia.org/wiki/Bradley_Efron) was like "hopefully you are just playing devil's advocate":

> A third front seems to have been opened in the long-running [frequentist-Bayesian wars](https://en.wikipedia.org/wiki/Foundations_of_statistics#Bayesian_inference_versus_frequentist_inference) by the advocates of algorithmic prediction, who don't really believe in any inferential school... The whole point of science is to open up black boxes, understand their insides, and build better boxes for the purposes of mankind. Leo himself is a notably successful scientist, so we can hope that the present paper was written more as an advocacy device than as the confessions of a born-again black boxist.



### Connectionism, the past tense debate, and whatever

"Connectionism" is a word you don't see much nowadays, but it was a big word back in the 1980s. It is hard to pin down, but if I summarize it, it is the result of philosophers in the 1980s noticing how researchers were trying neural networks on problems that had defied logical AI approaches, and somehow achieved state of the art, way past expectations. They say, "Weird! How is it possible for neural networks, written by people not having a deep knowledge of the problem domain, using such simplistic features, to work better than the best logical AI? I must philosophize this at once!"

Two clear camps immediately formed. On one side were the connectionists with [Paul Churchland](https://en.wikipedia.org/wiki/Paul_Churchland), [Patricia Churchland](https://en.wikipedia.org/wiki/Patricia_Churchland), [Paul Smolensky](https://en.wikipedia.org/wiki/Paul_Smolensky), [Jeffrey Elman](https://en.wikipedia.org/wiki/Jeffrey_Elman). On the other side were the cognitivists (or perhaps the rationalists) [Jerry Fodor](https://en.wikipedia.org/wiki/Jerry_Fodor), [Zenon Pylyshyn](https://en.wikipedia.org/wiki/Zenon_Pylyshyn), with the spirit of Noam Chomsky always present in the background.

Chapter 18 of *Parallel Distributed Processing* vol. 2 bore the unassuming title "On learning the past tenses of verbs in English" [@rumelhartLearningTensesEnglish1986]. Nobody would have predicted that it ignited a long and bitter dispute "the past tense debate". 


[@seidenbergQuasiregularityItsDiscontents2014]